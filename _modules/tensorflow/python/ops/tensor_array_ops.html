

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.tensor_array_ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.tensor_array_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.tensor_array_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;TensorArray: a dynamically sized array of Tensors.&quot;&quot;&quot;</span>
<span class="c1"># Mixture of pep8 and non-pep8 names, so disable pylint bad-name</span>
<span class="c1"># pylint: disable=g-bad-name</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">contextlib</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">weakref</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">errors_impl</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">type_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_data_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">list_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_should_use</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>


<span class="c1"># _GraphTensorArray accesses many of the hidden generated ops, but is in</span>
<span class="c1"># fact built to wrap these methods.</span>
<span class="c1"># pylint: disable=protected-access</span>
<span class="k">class</span> <span class="nc">_GraphTensorArray</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Graph-mode implementation of TensorArray.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">dtype</span><span class="p">,</span>
               <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">clear_after_read</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">tensor_array_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">handle</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">element_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a graph mode TensorArray.</span>

<span class="sd">    Args:</span>
<span class="sd">      dtype: (required) data type of the TensorArray.</span>
<span class="sd">      size: (optional) int32 scalar `Tensor`: the size of the TensorArray.</span>
<span class="sd">        Required if handle is not provided.</span>
<span class="sd">      dynamic_size: (optional) Python bool: If true, writes to the TensorArray</span>
<span class="sd">        can grow the TensorArray past its initial size.  Default: False.</span>
<span class="sd">      clear_after_read: Boolean (optional, default: True).  If True, clear</span>
<span class="sd">        TensorArray values after reading them.  This disables read-many</span>
<span class="sd">        semantics, but allows early release of memory.</span>
<span class="sd">      tensor_array_name: (optional) Python string: the name of the TensorArray.</span>
<span class="sd">        This is used when creating the TensorArray handle.  If this value is</span>
<span class="sd">        set, handle should be None.</span>
<span class="sd">      handle: (optional) A `Tensor` handle to an existing TensorArray.  If this</span>
<span class="sd">        is set, tensor_array_name should be None. Only supported in graph mode.</span>
<span class="sd">      flow: (optional) A float `Tensor` scalar coming from an existing</span>
<span class="sd">        `TensorArray.flow`. Only supported in graph mode.</span>
<span class="sd">      infer_shape: (optional, default: True) If True, shape inference</span>
<span class="sd">        is enabled.  In this case, all elements must have the same shape.</span>
<span class="sd">      element_shape: (optional, default: None) A `TensorShape` object specifying</span>
<span class="sd">        the shape constraints of each of the elements of the TensorArray.</span>
<span class="sd">        Need not be fully defined.</span>
<span class="sd">      colocate_with_first_write_call: If `True`, the TensorArray will be</span>
<span class="sd">        colocated on the same device as the Tensor used on its first write</span>
<span class="sd">        (write operations include `write`, `unstack`, and `split`).  If `False`,</span>
<span class="sd">        the TensorArray will be placed on the device determined by the</span>
<span class="sd">        device context available during its initialization.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if both handle and tensor_array_name are provided.</span>
<span class="sd">      TypeError: if handle is provided but is not a Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tensor_array_name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot construct with both handle and tensor_array_name&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Handle must be a Tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Size must be provided if handle is not provided&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot provide both a handle and size &quot;</span>
                       <span class="s2">&quot;at the same time&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">element_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot provide both a handle and element_shape &quot;</span>
                       <span class="s2">&quot;at the same time&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dynamic_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot provide both a handle and dynamic_size &quot;</span>
                       <span class="s2">&quot;at the same time&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">clear_after_read</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot provide both a handle and clear_after_read &quot;</span>
                       <span class="s2">&quot;at the same time&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">clear_after_read</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">clear_after_read</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="o">=</span> <span class="n">dynamic_size</span> <span class="ow">or</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>

    <span class="c1"># Used to keep track of what tensors the TensorArray should be</span>
    <span class="c1"># colocated with.  We choose to colocate the TensorArray with the</span>
    <span class="c1"># first tensor written to it.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with_first_write_call</span> <span class="o">=</span> <span class="n">colocate_with_first_write_call</span>
    <span class="k">if</span> <span class="n">colocate_with_first_write_call</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Record the current static shape for the array elements. The element</span>
    <span class="c1"># shape is defined either by `element_shape` or the shape of the tensor</span>
    <span class="c1"># of the first write. If `infer_shape` is true, all writes checks for</span>
    <span class="c1"># shape equality.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">element_shape</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span> <span class="o">=</span> <span class="n">infer_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="o">=</span> <span class="n">size</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArray&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">handle</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">flow</span><span class="p">])</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="n">handle</span>
        <span class="k">if</span> <span class="n">flow</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;flow must not be None if handle is not None.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span> <span class="o">=</span> <span class="n">flow</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Construct the TensorArray with an empty device.  The first</span>
        <span class="c1"># write into the TensorArray from a Tensor with a set device</span>
        <span class="c1"># will retroactively set the device value of this op.</span>
        <span class="k">def</span> <span class="nf">create</span><span class="p">():</span>
          <span class="sd">&quot;&quot;&quot;Create the TensorArray op.&quot;&quot;&quot;</span>
          <span class="k">return</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_v3</span><span class="p">(</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
              <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span>
              <span class="n">element_shape</span><span class="o">=</span><span class="n">element_shape</span><span class="p">,</span>
              <span class="n">identical_element_shapes</span><span class="o">=</span><span class="n">infer_shape</span><span class="p">,</span>
              <span class="n">dynamic_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">,</span>
              <span class="n">clear_after_read</span><span class="o">=</span><span class="n">clear_after_read</span><span class="p">,</span>
              <span class="n">tensor_array_name</span><span class="o">=</span><span class="n">tensor_array_name</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">colocate_with_first_write_call</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span> <span class="o">=</span> <span class="n">create</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span> <span class="o">=</span> <span class="n">create</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">flow</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">element_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_check_element_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Changes the element shape of the array given a shape to merge with.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: A `TensorShape` object to merge with.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the provided shape is incompatible with the current</span>
<span class="sd">          element shape of the `TensorArray`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Inconsistent shapes: saw </span><span class="si">%s</span><span class="s2"> but expected </span><span class="si">%s</span><span class="s2"> &quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_maybe_colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Colocate operations with an internal colocation group or `value`.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: `Tensor`, the tensor to try to colocate with.</span>

<span class="sd">    Yields:</span>
<span class="sd">      Does not yield anything, but the new context is a colocation context.</span>

<span class="sd">    If no internal colocation group is set, colocate with `value` and set</span>
<span class="sd">    the internal colocation group to be value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with_first_write_call</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">yield</span>

  <span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="n">flow</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="c1"># tensor_array_grad requires a flow input when forward</span>
    <span class="c1"># TensorArrays are dynamically sized.  This forces the creation</span>
    <span class="c1"># of the grad TensorArray only once the final forward array&#39;s size</span>
    <span class="c1"># is fixed.</span>
    <span class="k">if</span> <span class="n">flow</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">flow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayGrad&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">]):</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">):</span>
        <span class="n">g_handle</span><span class="p">,</span> <span class="n">unused_flow</span> <span class="o">=</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_grad_v3</span><span class="p">(</span>
            <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">,</span> <span class="n">flow_in</span><span class="o">=</span><span class="n">flow</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">g_handle</span><span class="p">]):</span>
          <span class="n">flow</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">flow</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gradient_flow&quot;</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">TensorArray</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
            <span class="n">handle</span><span class="o">=</span><span class="n">g_handle</span><span class="p">,</span>
            <span class="n">flow</span><span class="o">=</span><span class="n">flow</span><span class="p">,</span>
            <span class="n">infer_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">,</span>
            <span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">g</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span>
        <span class="c1"># pylint: enable=protected-access</span>
        <span class="k">return</span> <span class="n">g</span>

  <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_read_v3</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
        <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span>
        <span class="n">flow_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">:</span>
      <span class="n">value</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayWrite&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">]):</span>
      <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
      <span class="n">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_check_element_shape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_colocate_with</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="n">flow_out</span> <span class="o">=</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_write_v3</span><span class="p">(</span>
            <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
            <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">flow_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow_out</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayStack&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">]):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="ow">and</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
          <span class="n">value</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">)]</span> <span class="o">+</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">:</span>
      <span class="n">element_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">element_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_gather_v3</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">flow_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">element_shape</span><span class="o">=</span><span class="n">element_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">:</span>
      <span class="n">value</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="n">value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_concat_v3</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
        <span class="n">flow_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">element_shape_except0</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">:</span>
      <span class="n">value</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">return</span> <span class="n">value</span>

  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">unstack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayUnstack&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">value</span><span class="p">]):</span>
      <span class="n">num_elements</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
          <span class="n">indices</span><span class="o">=</span><span class="n">math_ops</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_elements</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayScatter&quot;</span><span class="p">,</span>
                        <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">indices</span><span class="p">]):</span>
      <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
      <span class="n">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_element_shape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
      <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_colocate_with</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="n">flow_out</span> <span class="o">=</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_scatter_v3</span><span class="p">(</span>
            <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">flow_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow_out</span><span class="p">)</span>

  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArraySplit&quot;</span><span class="p">,</span>
                        <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">]):</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
      <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_colocate_with</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="n">lengths_64</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="n">clengths</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">lengths_64</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">clengths</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">clengths</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">clengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="n">clengths</span><span class="o">.</span><span class="n">min</span><span class="p">():</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_check_element_shape</span><span class="p">(</span>
                  <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="n">clengths</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                      <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
        <span class="n">flow_out</span> <span class="o">=</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_split_v3</span><span class="p">(</span>
            <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">lengths</span><span class="o">=</span><span class="n">lengths_64</span><span class="p">,</span>
            <span class="n">flow_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow_out</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_size_v3</span><span class="p">(</span>
          <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">flow_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">flow</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">tensor_array_close_v3</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_GraphTensorArrayV2</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Graph-mode implementation of TensorArray backed by TensorLists.</span>

<span class="sd">  The backing tensor of this TensorArray is a TensorList variant tensor which is</span>
<span class="sd">  stored in the `flow`. The `handle` is always none here. The reason we use the</span>
<span class="sd">  `flow` field and not the `handle` field is to ensure backwards compatibility</span>
<span class="sd">  with legacy control flow.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">dtype</span><span class="p">,</span>
               <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">clear_after_read</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">tensor_array_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">handle</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">element_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a graph mode TensorArray.</span>

<span class="sd">    Args:</span>
<span class="sd">      dtype: (required) data type of the TensorArray.</span>
<span class="sd">      size: (optional) int32 scalar `Tensor`: the size of the TensorArray.</span>
<span class="sd">        Required if flow is not provided.</span>
<span class="sd">      dynamic_size: (optional) Python bool: If true, writes to the TensorArray</span>
<span class="sd">        can grow the TensorArray past its initial size.  Default: False.</span>
<span class="sd">      clear_after_read: (optional) unused. Not supported in TensorLists.</span>
<span class="sd">      tensor_array_name: (optional) unused.</span>
<span class="sd">      handle: (optional) Must always be None.</span>
<span class="sd">      flow: (optional) A variant `Tensor` scalar for a TensorList.</span>
<span class="sd">      infer_shape: (optional, default: True) If True, shape inference is</span>
<span class="sd">        enabled.  In this case, all elements must have the same shape.</span>
<span class="sd">      element_shape: (optional, default: None) A `TensorShape` object specifying</span>
<span class="sd">        the shape constraints of each of the elements of the TensorArray. Need</span>
<span class="sd">        not be fully defined.</span>
<span class="sd">      colocate_with_first_write_call: (optional). unused.</span>
<span class="sd">      name: (optional) A name for the operation.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if both handle and tensor_array_name are provided.</span>
<span class="sd">      TypeError: if handle is provided but is not a Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">handle</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">del</span> <span class="n">handle</span>
    <span class="k">del</span> <span class="n">clear_after_read</span>
    <span class="k">del</span> <span class="n">tensor_array_name</span>
    <span class="k">del</span> <span class="n">colocate_with_first_write_call</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="o">=</span> <span class="n">dynamic_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="o">=</span> <span class="n">size</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">flow</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">flow</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">flow</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;flow must be a variant tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">flow</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Size must be provided if flow is not provided&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">flow</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot provide both a flow and size &quot;</span>
                       <span class="s2">&quot;at the same time&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">flow</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">element_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot provide both a flow and element_shape &quot;</span>
                       <span class="s2">&quot;at the same time&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>

    <span class="c1"># Record the current static shape for the array elements. The element</span>
    <span class="c1"># shape is defined either by `element_shape` or the shape of the tensor</span>
    <span class="c1"># of the first write. If `infer_shape` is true, all writes checks for</span>
    <span class="c1"># shape equality.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">element_shape</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span> <span class="o">=</span> <span class="n">infer_shape</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayV2&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">size</span><span class="p">,</span> <span class="n">flow</span><span class="p">])</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">flow</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_reserve</span><span class="p">(</span>
            <span class="n">element_shape</span><span class="o">=</span><span class="n">element_shape</span><span class="p">,</span>
            <span class="n">num_elements</span><span class="o">=</span><span class="n">size</span><span class="p">,</span>
            <span class="n">element_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span> <span class="o">=</span> <span class="n">flow</span>

    <span class="c1"># For backwards compatibility.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with_first_write_call</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">flow</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">element_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># We intentionally do not raise an error so that legacy while_loop does not</span>
    <span class="c1"># complain.</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">_check_element_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Changes the element shape of the array given a shape to merge with.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: A `TensorShape` object to merge with.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the provided shape is incompatible with the current</span>
<span class="sd">          element shape of the `TensorArray`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Inconsistent shapes: saw </span><span class="si">%s</span><span class="s2"> but expected </span><span class="si">%s</span><span class="s2"> &quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="n">flow</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Not supported.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayV2Read&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span> <span class="n">index</span><span class="p">]):</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_get_item</span><span class="p">(</span>
          <span class="n">input_handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
          <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span>
          <span class="n">element_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
          <span class="n">element_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayV2Write&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">]):</span>
      <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
      <span class="n">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_check_element_shape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="n">flow_out</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_set_item</span><span class="p">(</span>
          <span class="n">input_handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
          <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span>
          <span class="n">item</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
          <span class="n">resize_if_index_out_of_bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow_out</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayV2Stack&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">]):</span>
      <span class="c1"># TODO(b/139941163): remove constant_value after changing num_elements to regular input</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ta_size</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ta_size</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_stack</span><span class="p">(</span>
          <span class="n">input_handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
          <span class="n">element_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
          <span class="n">num_elements</span><span class="o">=</span><span class="n">ta_size</span><span class="p">,</span>
          <span class="n">element_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_gather</span><span class="p">(</span>
        <span class="n">input_handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">element_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">element_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">:</span>
      <span class="n">element_shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">element_shape</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">value</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_concat</span><span class="p">(</span>
        <span class="n">input_handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span>
        <span class="n">element_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">element_shape</span><span class="o">=</span><span class="n">element_shape</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span>

  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">unstack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayUnstack&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span> <span class="n">value</span><span class="p">]):</span>
      <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
      <span class="n">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_check_element_shape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
      <span class="n">flow_out</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_from_tensor</span><span class="p">(</span>
          <span class="n">tensor</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">element_shape</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
      <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow_out</span><span class="p">)</span>

  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArrayScatter&quot;</span><span class="p">,</span>
                        <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">indices</span><span class="p">]):</span>
      <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
      <span class="n">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_check_element_shape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
      <span class="n">flow_out</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_scatter</span><span class="p">(</span>
          <span class="n">tensor</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">element_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">,</span>
          <span class="n">input_handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow_out</span><span class="p">)</span>

  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;TensorArraySplit&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">]):</span>
      <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
      <span class="n">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="n">lengths_64</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="n">clengths</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">lengths_64</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">clengths</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">clengths</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">clengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="n">clengths</span><span class="o">.</span><span class="n">min</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_element_shape</span><span class="p">(</span>
                <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="n">clengths</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                    <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
      <span class="n">flow_out</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_split</span><span class="p">(</span>
          <span class="n">tensor</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
          <span class="n">lengths</span><span class="o">=</span><span class="n">lengths_64</span><span class="p">,</span>
          <span class="n">element_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">element_shape</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flow_out</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_length</span><span class="p">(</span><span class="n">input_handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flow</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<span class="c1"># pylint: enable=protected-access</span>


<span class="k">class</span> <span class="nc">_EagerTensorArray</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Eager-compatible implementation of TensorArray.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">dtype</span><span class="p">,</span>
               <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">clear_after_read</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">tensor_array_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">handle</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">element_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a TensorArray compatible with eager execution.</span>

<span class="sd">    Args:</span>
<span class="sd">      dtype: (required) data type of the TensorArray.</span>
<span class="sd">      size: (optional) int32 scalar `Tensor`: the size of the TensorArray.</span>
<span class="sd">        Required if handle is not provided.</span>
<span class="sd">      dynamic_size: (optional) Python bool: If true, writes to the TensorArray</span>
<span class="sd">        can grow the TensorArray past its initial size.  Default: False.</span>
<span class="sd">      clear_after_read: Boolean (optional, default: True).  If True, clear</span>
<span class="sd">        TensorArray values after reading them.  This disables read-many</span>
<span class="sd">        semantics, but allows early release of memory.</span>
<span class="sd">      tensor_array_name: unused.</span>
<span class="sd">      handle: unsupported.</span>
<span class="sd">      flow: unsupported.</span>
<span class="sd">      infer_shape: used for error checking, same semantics as TensorArray.</span>
<span class="sd">      element_shape: used for error checking, same semantics as TensorArray.</span>
<span class="sd">      colocate_with_first_write_call: unsupported.</span>
<span class="sd">      name: unsupported.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: handle or flow are supplied, or if size is not supplied.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">del</span> <span class="p">(</span><span class="n">flow</span><span class="p">,</span> <span class="n">tensor_array_name</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>  <span class="c1"># Unused.</span>

    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;TensorArray handles are not supported when eager &quot;</span>
                       <span class="s2">&quot;execution is enabled.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Size must be declared for TensorArrays when eager &quot;</span>
                       <span class="s2">&quot;execution is enabled.&quot;</span><span class="p">)</span>

    <span class="c1"># These attributes are not meaningful when eager is enabled, but some</span>
    <span class="c1"># library functions (e.g., those in control_flow_ops.py) access them to</span>
    <span class="c1"># create new tensor arrays; as such, we define them for the sake of</span>
    <span class="c1"># compatibility.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># we assign a dummy value to _flow in case other code assumes it to be</span>
    <span class="c1"># a Tensor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span> <span class="o">=</span> <span class="n">infer_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">element_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_colocate_with_first_write_call</span> <span class="o">=</span> <span class="n">colocate_with_first_write_call</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="o">=</span> <span class="n">dynamic_size</span> <span class="ow">or</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_clear_after_read</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">True</span> <span class="k">if</span> <span class="n">clear_after_read</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">clear_after_read</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_previously_read_indices</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="n">size</span> <span class="o">=</span> <span class="n">size</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">flow</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For compatibility; flows are not meaningful when eager is enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flow</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For compatibility; handles are not meaningful when eager is enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">element_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span>

  <span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;TensorArray.grad is not supported when executing eagerly; eager&#39;s &quot;</span>
        <span class="s2">&quot;gradient implementation does not use/need this function to compute &quot;</span>
        <span class="s2">&quot;gradients of operations that use TensorArrays.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">name</span>  <span class="c1"># not meaningful when executing eagerly.</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
          <span class="s2">&quot;Reading from negative indices (index </span><span class="si">%d</span><span class="s2">) is not allowed.&quot;</span> <span class="o">%</span> <span class="n">index</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Tried to read from index </span><span class="si">%d</span><span class="s2"> but array size is: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">)))</span>

    <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_previously_read_indices</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
            <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;Could not read index </span><span class="si">%d</span><span class="s2"> twice because it was cleared after &quot;</span>
            <span class="s2">&quot;a previous read (perhaps try setting clear_after_read = false?)&quot;</span> <span class="o">%</span>
            <span class="n">index</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_zero</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clear_after_read</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_previously_read_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>

  <span class="k">def</span> <span class="nf">_write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Writes `value` into index named by `index`.</span>

<span class="sd">    Args:</span>
<span class="sd">      index: 0-D.  int32 scalar with the index to write to.</span>
<span class="sd">      value: N-D.  Tensor of type `dtype`.  The `Tensor` to write to `index`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      errors_impl.InvalidArgumentError: `value` dtype does not match dtype.</span>
<span class="sd">      errors_impl.OutOfRangeError: `index` is out of bounds.</span>
<span class="sd">      ValueError: shape of `value` is not consistent with inferred shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
          <span class="s2">&quot;Writing to negative indices (index </span><span class="si">%d</span><span class="s2">) is not allowed.&quot;</span> <span class="o">%</span> <span class="n">index</span><span class="p">)</span>

    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="n">size</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">(</span>
            <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;Tried to write to index </span><span class="si">%d</span><span class="s2"> but array is not resizeable and size &quot;</span>
            <span class="s2">&quot;is: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">index</span> <span class="o">-</span> <span class="n">size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">!=</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
          <span class="s2">&quot;TensorArray dtype is </span><span class="si">%s</span><span class="s2"> but Op is trying to write dtype </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Incompatible shape for value (</span><span class="si">%s</span><span class="s2">), expected (</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">name</span>  <span class="c1"># not meaningful when executing eagerly.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_write</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_maybe_zero</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ix</span><span class="p">):</span>
    <span class="n">val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val</span>

  <span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_zero</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">name</span>  <span class="c1"># not meaningful when executing eagerly.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_zero</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_zero</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span> <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">))],</span>
          <span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">OpError</span><span class="p">:</span>
      <span class="c1"># Reproduce a subset of the error-handling for graph-mode TensorArrays.</span>
      <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">]</span>
      <span class="n">ndims</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">ndims</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">]</span>
      <span class="k">if</span> <span class="mi">0</span> <span class="ow">in</span> <span class="n">ndims</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">ndims</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
            <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Concat saw a scalar shape at index </span><span class="si">%d</span><span class="s2"> but requires &quot;</span>
            <span class="s2">&quot;at least vectors.&quot;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span>

  <span class="k">def</span> <span class="nf">unstack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot unstack </span><span class="si">%d</span><span class="s2"> tensors into a TensorArray of static size </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span> <span class="o">=</span> <span class="n">tensors</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">name</span>  <span class="c1"># not meaningful when executing eagerly.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">value</span><span class="p">)):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_write</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="c1"># TODO(b/129870929): Fix after all callers provide proper init dtype.</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
    <span class="n">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
    <span class="n">sum_lengths</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected lengths to be a vector, received shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected value to be at least a vector, &quot;</span>
          <span class="s2">&quot;but received shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">sum_lengths</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">!=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">0</span><span class="p">]:</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected sum of lengths to be equal to &quot;</span>
          <span class="s2">&quot;values.shape[0], but sum of lengths is </span><span class="si">%d</span><span class="s2"> and &quot;</span>
          <span class="s2">&quot;value&#39;s shape is: </span><span class="si">%s</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">sum_lengths</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                                     <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="ow">and</span> <span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;TensorArray&#39;s size is not equal to the size of &quot;</span>
          <span class="s2">&quot;lengths (</span><span class="si">%d</span><span class="s2"> vs. </span><span class="si">%d</span><span class="s2">), and the TensorArray is not marked as &quot;</span>
          <span class="s2">&quot;dynamically resizeable&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">),</span>
                                      <span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See TensorArray.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">name</span>  <span class="c1"># not meaningful when executing eagerly.</span>
    <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">name</span>  <span class="c1"># not meaningful when executing eagerly.</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_array</span><span class="p">[:]</span>


<span class="c1"># TensorArray is designed to hide an underlying implementation object</span>
<span class="c1"># and as such accesses many of that object&#39;s hidden fields.</span>
<span class="c1"># pylint: disable=protected-access</span>
<span class="c1"># pylint:disable=line-too-long</span>
<div class="viewcode-block" id="TensorArray"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;TensorArray&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorArray</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</span>

<span class="sd">  This class is meant to be used with dynamic iteration primitives such as</span>
<span class="sd">  `while_loop` and `map_fn`.  It supports gradient back-propagation via special</span>
<span class="sd">  &quot;flow&quot; control flow dependencies.</span>

<span class="sd">  Example 1: Plain reading and writing.</span>

<span class="sd">  &gt;&gt;&gt; ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)</span>
<span class="sd">  &gt;&gt;&gt; ta = ta.write(0, 10)</span>
<span class="sd">  &gt;&gt;&gt; ta = ta.write(1, 20)</span>
<span class="sd">  &gt;&gt;&gt; ta = ta.write(2, 30)</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; ta.read(0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=10.0&gt;</span>
<span class="sd">  &gt;&gt;&gt; ta.read(1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=20.0&gt;</span>
<span class="sd">  &gt;&gt;&gt; ta.read(2)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=30.0&gt;</span>
<span class="sd">  &gt;&gt;&gt; ta.stack()</span>
<span class="sd">  &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],</span>
<span class="sd">  dtype=float32)&gt;</span>

<span class="sd">  Example 2: Fibonacci sequence algorithm that writes in a loop then returns.</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def fibonacci(n):</span>
<span class="sd">  ...   ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)</span>
<span class="sd">  ...   ta = ta.unstack([0., 1.])</span>
<span class="sd">  ...</span>
<span class="sd">  ...   for i in range(2, n):</span>
<span class="sd">  ...     ta = ta.write(i, ta.read(i - 1) + ta.read(i - 2))</span>
<span class="sd">  ...</span>
<span class="sd">  ...   return ta.stack()</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; fibonacci(7)</span>
<span class="sd">  &lt;tf.Tensor: shape=(7,), dtype=float32,</span>
<span class="sd">  numpy=array([0., 1., 1., 2., 3., 5., 8.], dtype=float32)&gt;</span>

<span class="sd">  Example 3: A simple loop interacting with a `tf.Variable`.</span>

<span class="sd">  &gt;&gt;&gt; v = tf.Variable(1)</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)</span>
<span class="sd">  ...</span>
<span class="sd">  ...   for i in tf.range(x):</span>
<span class="sd">  ...     v.assign_add(i)</span>
<span class="sd">  ...     ta = ta.write(i, v)</span>
<span class="sd">  ...</span>
<span class="sd">  ...   return ta.stack()</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; f(5)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 1,  2,  4,  7, 11],</span>
<span class="sd">  dtype=int32)&gt;</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">dtype</span><span class="p">,</span>
               <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">clear_after_read</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">tensor_array_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">handle</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">element_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct a new TensorArray or wrap an existing TensorArray handle.</span>

<span class="sd">    A note about the parameter `name`:</span>

<span class="sd">    The name of the `TensorArray` (even if passed in) is uniquified: each time</span>
<span class="sd">    a new `TensorArray` is created at runtime it is assigned its own name for</span>
<span class="sd">    the duration of the run.  This avoids name collisions if a `TensorArray`</span>
<span class="sd">    is created within a `while_loop`.</span>

<span class="sd">    Args:</span>
<span class="sd">      dtype: (required) data type of the TensorArray.</span>
<span class="sd">      size: (optional) int32 scalar `Tensor`: the size of the TensorArray.</span>
<span class="sd">        Required if handle is not provided.</span>
<span class="sd">      dynamic_size: (optional) Python bool: If true, writes to the TensorArray</span>
<span class="sd">        can grow the TensorArray past its initial size.  Default: False.</span>
<span class="sd">      clear_after_read: Boolean (optional, default: True).  If True, clear</span>
<span class="sd">        TensorArray values after reading them.  This disables read-many</span>
<span class="sd">        semantics, but allows early release of memory.</span>
<span class="sd">      tensor_array_name: (optional) Python string: the name of the TensorArray.</span>
<span class="sd">        This is used when creating the TensorArray handle.  If this value is</span>
<span class="sd">        set, handle should be None.</span>
<span class="sd">      handle: (optional) A `Tensor` handle to an existing TensorArray.  If this</span>
<span class="sd">        is set, tensor_array_name should be None. Only supported in graph mode.</span>
<span class="sd">      flow: (optional) A float `Tensor` scalar coming from an existing</span>
<span class="sd">        `TensorArray.flow`. Only supported in graph mode.</span>
<span class="sd">      infer_shape: (optional, default: True) If True, shape inference</span>
<span class="sd">        is enabled.  In this case, all elements must have the same shape.</span>
<span class="sd">      element_shape: (optional, default: None) A `TensorShape` object specifying</span>
<span class="sd">        the shape constraints of each of the elements of the TensorArray.</span>
<span class="sd">        Need not be fully defined.</span>
<span class="sd">      colocate_with_first_write_call: If `True`, the TensorArray will be</span>
<span class="sd">        colocated on the same device as the Tensor used on its first write</span>
<span class="sd">        (write operations include `write`, `unstack`, and `split`).  If `False`,</span>
<span class="sd">        the TensorArray will be placed on the device determined by the</span>
<span class="sd">        device context available during its initialization.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if both handle and tensor_array_name are provided.</span>
<span class="sd">      TypeError: if handle is provided but is not a Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span>
        <span class="p">(</span><span class="n">flow</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">flow</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">)):</span>
      <span class="c1"># It is possible to create a Variant-style TensorArray even in eager mode,</span>
      <span class="c1"># and this is fine but can have performance implications in eager.</span>
      <span class="c1"># An example of when this happens is if a tf.function returns a</span>
      <span class="c1"># TensorArray in its output; its flow variant object is returned to Eager.</span>
      <span class="c1"># This can be wrapped back up in a Variant-style TensorArray.</span>
      <span class="n">implementation</span> <span class="o">=</span> <span class="n">_EagerTensorArray</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">flow</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">flow</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span> <span class="ow">or</span>
          <span class="n">control_flow_util</span><span class="o">.</span><span class="n">EnableControlFlowV2</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())):</span>
      <span class="n">implementation</span> <span class="o">=</span> <span class="n">_GraphTensorArrayV2</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">implementation</span> <span class="o">=</span> <span class="n">_GraphTensorArray</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span> <span class="o">=</span> <span class="n">implementation</span><span class="p">(</span>
        <span class="n">dtype</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span>
        <span class="n">dynamic_size</span><span class="o">=</span><span class="n">dynamic_size</span><span class="p">,</span>
        <span class="n">clear_after_read</span><span class="o">=</span><span class="n">clear_after_read</span><span class="p">,</span>
        <span class="n">tensor_array_name</span><span class="o">=</span><span class="n">tensor_array_name</span><span class="p">,</span>
        <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">,</span>
        <span class="n">flow</span><span class="o">=</span><span class="n">flow</span><span class="p">,</span>
        <span class="n">infer_shape</span><span class="o">=</span><span class="n">infer_shape</span><span class="p">,</span>
        <span class="n">element_shape</span><span class="o">=</span><span class="n">element_shape</span><span class="p">,</span>
        <span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="n">colocate_with_first_write_call</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">parent</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">flow</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The flow `Tensor` forcing ops leading to this TensorArray state.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">_flow</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The data type of this TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The reference to the TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">handle</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">element_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `tf.TensorShape` of elements in this TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">element_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dynamic_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Python bool; if `True` the TensorArray can grow dynamically.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">_dynamic_size</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># TODO(slebedev): consider making public or changing TensorArrayStructure</span>
    <span class="c1"># to access _implementation directly. Note that dynamic_size is also</span>
    <span class="c1"># only used by TensorArrayStructure.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">_infer_shape</span>

<div class="viewcode-block" id="TensorArray.identity"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.identity">[docs]</a>  <span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a TensorArray with the same content and properties.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A new TensorArray object with flow that ensures the control dependencies</span>
<span class="sd">      from the contexts will become control dependencies for writes, reads, etc.</span>
<span class="sd">      Use this object all for subsequent operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">identity</span><span class="p">()</span></div>

<div class="viewcode-block" id="TensorArray.grad"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.grad">[docs]</a>  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">flow</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">flow</span><span class="o">=</span><span class="n">flow</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.read"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.read">[docs]</a>  <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Read the value at location `index` in the TensorArray.</span>

<span class="sd">    Args:</span>
<span class="sd">      index: 0-D.  int32 tensor with the index to read from.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The tensor at index `index`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.write"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.write">[docs]</a>  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span><span class="p">(</span><span class="n">warn_in_eager</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Write `value` into index `index` of the TensorArray.</span>

<span class="sd">    Args:</span>
<span class="sd">      index: 0-D.  int32 scalar with the index to write to.</span>
<span class="sd">      value: N-D.  Tensor of type `dtype`.  The Tensor to write to this index.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A new TensorArray object with flow that ensures the write occurs.</span>
<span class="sd">      Use this object all for subsequent operations.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if there are more writers than specified.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.stack"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.stack">[docs]</a>  <span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the values in the TensorArray as a stacked `Tensor`.</span>

<span class="sd">    All of the values must have been written and their shapes must all match.</span>
<span class="sd">    If input shapes have rank-`R`, then output shape will have rank-`(R+1)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      All the tensors in the TensorArray stacked into one tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.gather"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.gather">[docs]</a>  <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return selected values in the TensorArray as a packed `Tensor`.</span>

<span class="sd">    All of selected values must have been written and their shapes</span>
<span class="sd">    must all match.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: A `1-D` `Tensor` taking values in `[0, max_value)`.  If</span>
<span class="sd">        the `TensorArray` is not dynamic, `max_value=size()`.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The tensors in the `TensorArray` selected by `indices`, packed into one</span>
<span class="sd">      tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.concat"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.concat">[docs]</a>  <span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the values in the TensorArray as a concatenated `Tensor`.</span>

<span class="sd">    All of the values must have been written, their ranks must match, and</span>
<span class="sd">    and their shapes must all match for all dimensions except the first.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      All the tensors in the TensorArray concatenated into one tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.unstack"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.unstack">[docs]</a>  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">unstack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Unstack the values of a `Tensor` in the TensorArray.</span>

<span class="sd">    If input value shapes have rank-`R`, then the output TensorArray will</span>
<span class="sd">    contain elements whose shapes are rank-`(R-1)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: (N+1)-D.  Tensor of type `dtype`.  The Tensor to unstack.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A new TensorArray object with flow that ensures the unstack occurs.</span>
<span class="sd">      Use this object all for subsequent operations.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the shape inference fails.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.scatter"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.scatter">[docs]</a>  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scatter the values of a `Tensor` in specific indices of a `TensorArray`.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: A `1-D` `Tensor` taking values in `[0, max_value)`.  If</span>
<span class="sd">        the `TensorArray` is not dynamic, `max_value=size()`.</span>
<span class="sd">      value: (N+1)-D.  Tensor of type `dtype`.  The Tensor to unpack.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A new TensorArray object with flow that ensures the scatter occurs.</span>
<span class="sd">      Use this object all for subsequent operations.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the shape inference fails.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.split"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.split">[docs]</a>  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Split the values of a `Tensor` into the TensorArray.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: (N+1)-D.  Tensor of type `dtype`.  The Tensor to split.</span>
<span class="sd">      lengths: 1-D.  int32 vector with the lengths to use when splitting</span>
<span class="sd">        `value` along its first dimension.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A new TensorArray object with flow that ensures the split occurs.</span>
<span class="sd">      Use this object all for subsequent operations.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the shape inference fails.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.size"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.size">[docs]</a>  <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the size of the TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArray.close"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArray.close">[docs]</a>  <span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
  <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Close the current TensorArray.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<span class="k">def</span> <span class="nf">build_ta_with_new_flow</span><span class="p">(</span><span class="n">old_ta</span><span class="p">,</span> <span class="n">flow</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Builds a TensorArray with a new `flow` tensor.&quot;&quot;&quot;</span>
  <span class="c1"># Sometimes we get old_ta as the implementation, sometimes it&#39;s the</span>
  <span class="c1"># TensorArray wrapper object.</span>
  <span class="n">impl</span> <span class="o">=</span> <span class="p">(</span><span class="n">old_ta</span><span class="o">.</span><span class="n">_implementation</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_ta</span><span class="p">,</span> <span class="n">TensorArray</span><span class="p">)</span>
          <span class="k">else</span> <span class="n">old_ta</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">impl</span><span class="p">,</span> <span class="n">_GraphTensorArrayV2</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">control_flow_util</span><span class="o">.</span><span class="n">EnableControlFlowV2</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())):</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Attempting to build a graph-mode TF2-style &quot;</span>
                                <span class="s2">&quot;TensorArray from either an eager-mode &quot;</span>
                                <span class="s2">&quot;TensorArray or a TF1-style TensorArray.  &quot;</span>
                                <span class="s2">&quot;This is not currently supported.  You may be &quot;</span>
                                <span class="s2">&quot;attempting to capture a TensorArray &quot;</span>
                                <span class="s2">&quot;inside a tf.function or tf.data map function. &quot;</span>
                                <span class="s2">&quot;Instead, construct a new TensorArray inside &quot;</span>
                                <span class="s2">&quot;the function.&quot;</span><span class="p">)</span>
  <span class="n">new_ta</span> <span class="o">=</span> <span class="n">TensorArray</span><span class="p">(</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">impl</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">handle</span><span class="o">=</span><span class="n">impl</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span>
      <span class="n">flow</span><span class="o">=</span><span class="n">flow</span><span class="p">,</span>
      <span class="n">infer_shape</span><span class="o">=</span><span class="n">impl</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">,</span>
      <span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="n">impl</span><span class="o">.</span><span class="n">_colocate_with_first_write_call</span><span class="p">)</span>
  <span class="n">new_impl</span> <span class="o">=</span> <span class="n">new_ta</span><span class="o">.</span><span class="n">_implementation</span>
  <span class="n">new_impl</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="o">=</span> <span class="n">impl</span><span class="o">.</span><span class="n">_dynamic_size</span>
  <span class="n">new_impl</span><span class="o">.</span><span class="n">_size</span> <span class="o">=</span> <span class="n">impl</span><span class="o">.</span><span class="n">_size</span>
  <span class="n">new_impl</span><span class="o">.</span><span class="n">_colocate_with</span> <span class="o">=</span> <span class="n">impl</span><span class="o">.</span><span class="n">_colocate_with</span>
  <span class="n">new_impl</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="n">impl</span><span class="o">.</span><span class="n">_element_shape</span>  <span class="c1"># Share _element_shape.</span>
  <span class="k">return</span> <span class="n">new_ta</span>

<span class="c1"># pylint: enable=protected-access</span>


<span class="k">def</span> <span class="nf">_check_dtypes</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
        <span class="s2">&quot;Error: Input value </span><span class="si">{}</span><span class="s2"> has dtype </span><span class="si">{}</span><span class="s2">, but expected dtype </span><span class="si">{}</span><span class="s2">.  &quot;</span>
        <span class="s2">&quot;This leads to undefined behavior and will be an error &quot;</span>
        <span class="s2">&quot;in future versions of TensorFlow.  Traceback:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
            <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())))</span>


<div class="viewcode-block" id="TensorArraySpec"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArraySpec">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;TensorArraySpec&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorArraySpec</span><span class="p">(</span><span class="n">type_spec</span><span class="o">.</span><span class="n">TypeSpec</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Type specification for a `tf.TensorArray`.&quot;&quot;&quot;</span>

  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;_element_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;_dynamic_size&quot;</span><span class="p">,</span> <span class="s2">&quot;_infer_shape&quot;</span><span class="p">]</span>

  <span class="n">value_type</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="n">TensorArray</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">element_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
               <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a type specification for a `tf.TensorArray`.</span>

<span class="sd">    Args:</span>
<span class="sd">      element_shape: The shape of each element in the `TensorArray`.</span>
<span class="sd">      dtype: Data type of the `TensorArray`.</span>
<span class="sd">      dynamic_size: Whether the `TensorArray` can grow past its initial size.</span>
<span class="sd">      infer_shape: Whether shape inference is enabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">element_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="o">=</span> <span class="n">dynamic_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span> <span class="o">=</span> <span class="n">infer_shape</span>

<div class="viewcode-block" id="TensorArraySpec.is_compatible_with"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArraySpec.is_compatible_with">[docs]</a>  <span class="k">def</span> <span class="nf">is_compatible_with</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">type_spec</span><span class="o">.</span><span class="n">TypeSpec</span><span class="p">):</span>
      <span class="n">other</span> <span class="o">=</span> <span class="n">type_spec</span><span class="o">.</span><span class="n">type_spec_from_value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="c1"># Note: we intentionally exclude infer_shape in this check.</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">TensorArraySpec</span><span class="p">)</span> <span class="ow">and</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span> <span class="ow">and</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">)</span> <span class="ow">and</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorArraySpec.most_specific_compatible_type"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArraySpec.most_specific_compatible_type">[docs]</a>  <span class="k">def</span> <span class="nf">most_specific_compatible_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">other</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Types are not compatible&quot;</span><span class="p">)</span>
    <span class="n">infer_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">_infer_shape</span>
    <span class="k">return</span> <span class="n">TensorArraySpec</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="o">.</span><span class="n">most_specific_compatible_shape</span><span class="p">(</span>
            <span class="n">other</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">,</span> <span class="n">infer_shape</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_serialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_component_specs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tensor_spec</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">([],</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">)]</span>

  <span class="k">def</span> <span class="nf">_to_components</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">TensorArray</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;value must be a TensorArray, but saw: </span><span class="si">{}</span><span class="s2">&quot;</span>
                      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">flow</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">value</span><span class="o">.</span><span class="n">flow</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">value</span><span class="o">.</span><span class="n">flow</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Convert to a TF2-style TensorArray.</span>
      <span class="c1"># TODO(ebrevdo): Add an &quot;_as_variant&quot; method to TensorArray class, or</span>
      <span class="c1"># &quot;implementation / as_variant&quot; arg to TensorArray constructor.</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;convert_tensor_array&quot;</span><span class="p">):</span>
        <span class="n">flow</span> <span class="o">=</span> <span class="n">list_ops</span><span class="o">.</span><span class="n">tensor_list_from_tensor</span><span class="p">(</span>
            <span class="n">tensor</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">stack</span><span class="p">(),</span> <span class="n">element_shape</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">element_shape</span><span class="p">)</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">flow</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_from_components</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">):</span>
    <span class="c1"># This will return a TF2 Graph-style TensorArray because tensor_list[0] is</span>
    <span class="c1"># a variant object.  size == -1 implies unknown size.</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">TensorArray</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">flow</span><span class="o">=</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">dynamic_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">,</span>
        <span class="n">infer_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">_implementation</span><span class="o">.</span><span class="n">_element_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">]</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">ret</span>

<div class="viewcode-block" id="TensorArraySpec.from_value"><a class="viewcode-back" href="../../../../index.html#tensorflow.TensorArraySpec.from_value">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_value</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">TensorArray</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected value to be a TensorArray, but saw: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span>
                      <span class="nb">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">TensorArraySpec</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">element_shape</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">element_shape</span><span class="p">,</span>
        <span class="n">dynamic_size</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">dynamic_size</span><span class="p">,</span>
        <span class="n">infer_shape</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span></div>

  <span class="k">def</span> <span class="nf">_to_legacy_output_types</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="k">def</span> <span class="nf">_to_legacy_output_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Sneak the dynamic_size and infer_shape values into the legacy shape.</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span>
                                     <span class="p">])</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_element_shape</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_to_legacy_output_classes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">TensorArray</span></div>


<span class="c1"># Register the TypeSpec for TensorArray.  If TensorArray is updated to be a</span>
<span class="c1"># CompositeTensor, then this registration can be deleted.</span>
<span class="n">type_spec</span><span class="o">.</span><span class="n">register_type_spec_from_value_converter</span><span class="p">(</span>
    <span class="n">TensorArray</span><span class="p">,</span> <span class="n">TensorArraySpec</span><span class="o">.</span><span class="n">from_value</span><span class="p">,</span> <span class="n">allow_subclass</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>