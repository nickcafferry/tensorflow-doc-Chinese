

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.control_flow_ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.control_flow_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.control_flow_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Control Flow Operations.</span>

<span class="sd">See the [autograph](https://www.tensorflow.org/guide/autograph) guide.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># pylint: disable=g-bad-name</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">functools</span>

<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">attr_value_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="k">import</span> <span class="n">control_flow_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">composite_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">errors</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">type_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_util</span> <span class="k">as</span> <span class="n">util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_logging_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">tensor_array_ops</span>
<span class="c1"># go/tf-wildcard-import</span>
<span class="c1"># pylint: disable=wildcard-import,undefined-variable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.gen_control_flow_ops</span> <span class="k">import</span> <span class="o">*</span>
<span class="c1"># pylint: enable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_should_use</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.lazy_loader</span> <span class="k">import</span> <span class="n">LazyLoader</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="c1"># This is to avoid a circular dependency:</span>
<span class="c1"># cond_v2 -&gt; gradients_util -&gt; control_flow_ops</span>
<span class="n">cond_v2</span> <span class="o">=</span> <span class="n">LazyLoader</span><span class="p">(</span><span class="s2">&quot;cond_v2&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span>
                     <span class="s2">&quot;tensorflow.python.ops.cond_v2&quot;</span><span class="p">)</span>

<span class="c1"># This is to avoid circular dependencies:</span>
<span class="c1"># while_v2 -&gt; control_flow_ops</span>
<span class="c1"># while_v2 -&gt; gradients_util -&gt; control_flow_ops</span>
<span class="n">while_v2</span> <span class="o">=</span> <span class="n">LazyLoader</span><span class="p">(</span><span class="s2">&quot;while_v2&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span>
                      <span class="s2">&quot;tensorflow.python.ops.while_v2&quot;</span><span class="p">)</span>

<span class="c1"># We override the &#39;tuple&#39; for a control flow op, so we keep python&#39;s</span>
<span class="c1"># existing &#39;tuple&#39; for later use in this module.</span>
<span class="n">_basetuple</span> <span class="o">=</span> <span class="nb">tuple</span>


<span class="k">def</span> <span class="nf">_summarize_eager</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a summarized string representation of eager `tensor`.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: EagerTensor to summarize</span>
<span class="sd">    summarize: Include these many first elements of `array`</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Emulate the behavior of Tensor::SummarizeValue()</span>
  <span class="k">if</span> <span class="n">summarize</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">summarize</span> <span class="o">=</span> <span class="mi">3</span>
  <span class="k">elif</span> <span class="n">summarize</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">summarize</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

  <span class="c1"># reshape((-1,)) is the fastest way to get a flat array view</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">_rank</span><span class="p">():</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
    <span class="n">lst</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat</span><span class="p">[:</span><span class="n">summarize</span><span class="p">]]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">flat</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
      <span class="n">lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># tensor.numpy() returns a scalar for zero dimensional arrays</span>
    <span class="k">if</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">summarize</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
      <span class="n">lst</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">())]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">lst</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">return</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>


<span class="c1"># pylint: disable=protected-access</span>


<span class="c1"># Assert and Print are special symbols in python, so we must</span>
<span class="c1"># use an upper-case version of them.</span>
<div class="viewcode-block" id="Assert"><a class="viewcode-back" href="../../../../index.html#tensorflow.Assert">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;debugging.Assert&quot;</span><span class="p">,</span> <span class="s2">&quot;Assert&quot;</span><span class="p">)</span>
<span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
<span class="k">def</span> <span class="nf">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Asserts that the given condition is true.</span>

<span class="sd">  If `condition` evaluates to false, print the list of tensors in `data`.</span>
<span class="sd">  `summarize` determines how many entries of the tensors to print.</span>

<span class="sd">  NOTE: In graph mode, to ensure that Assert executes, one usually attaches</span>
<span class="sd">  a dependency:</span>

<span class="sd">  ```python</span>
<span class="sd">  # Ensure maximum element of x is smaller or equal to 1</span>
<span class="sd">  assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])</span>
<span class="sd">  with tf.control_dependencies([assert_op]):</span>
<span class="sd">    ... code using x ...</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    condition: The condition to evaluate.</span>
<span class="sd">    data: The tensors to print out when condition is false.</span>
<span class="sd">    summarize: Print this many entries of each tensor.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    assert_op: An `Operation` that, when executed, raises a</span>
<span class="sd">    `tf.errors.InvalidArgumentError` if `condition` is not true.</span>
<span class="sd">    @compatibility(eager)</span>
<span class="sd">    returns None</span>
<span class="sd">    @end_compatibility</span>

<span class="sd">  Raises:</span>
<span class="sd">    @compatibility(eager)</span>
<span class="sd">    `tf.errors.InvalidArgumentError` if `condition` is not true</span>
<span class="sd">    @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">condition</span><span class="p">:</span>
      <span class="n">xs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
      <span class="n">data_str</span> <span class="o">=</span> <span class="p">[</span><span class="n">_summarize_eager</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">summarize</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
      <span class="k">raise</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="n">node_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Expected &#39;</span><span class="si">%s</span><span class="s2">&#39; to be true. Summarized data: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_str</span><span class="p">)))</span>
    <span class="k">return</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Assert&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">{</span><span class="n">dtypes</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">):</span>
      <span class="c1"># As a simple heuristic, we assume that string and int32 are</span>
      <span class="c1"># on host to avoid the need to use cond. If it is not case,</span>
      <span class="c1"># we will pay the price copying the tensor to host memory.</span>
      <span class="k">return</span> <span class="n">gen_logging_ops</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">summarize</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Assert&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">condition</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Condition&quot;</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">true_assert</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">gen_logging_ops</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
            <span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">summarize</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Assert&quot;</span><span class="p">)</span>

      <span class="n">guarded_assert</span> <span class="o">=</span> <span class="n">cond</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">no_op</span><span class="p">,</span> <span class="n">true_assert</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;AssertGuard&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="k">return</span>
      <span class="k">return</span> <span class="n">guarded_assert</span><span class="o">.</span><span class="n">op</span></div>


<span class="k">def</span> <span class="nf">_Identity</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return a tensor with the same shape and contents as the input tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A Tensor.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same type and value as the input Tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_is_ref_dtype</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">ref_identity</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_Identity</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_NextIteration</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_is_ref_dtype</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">ref_next_iteration</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">next_iteration</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_NextIteration</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_Enter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
           <span class="n">frame_name</span><span class="p">,</span>
           <span class="n">is_constant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">parallel_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
           <span class="n">use_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">use_input_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates or finds a child frame, and makes `data` available to it.</span>

<span class="sd">  The unique `frame_name` is used by the `Executor` to identify frames. If</span>
<span class="sd">  `is_constant` is true, `data` is a constant in the child frame; otherwise</span>
<span class="sd">  it may be changed in the child frame. At most `parallel_iterations`</span>
<span class="sd">  iterations are run in parallel in the child frame.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: The tensor to be made available to the child frame.</span>
<span class="sd">    frame_name: The name of the child frame.</span>
<span class="sd">    is_constant: If true, the output is constant within the child frame.</span>
<span class="sd">    parallel_iterations: The number of iterations allowed to run in parallel.</span>
<span class="sd">    use_ref: If true, use ref_enter if data is of ref type.</span>
<span class="sd">    use_input_shape: If true, set the result&#39;s shape based on data&#39;s shape.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The same tensor as `data`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_is_ref_dtype</span> <span class="ow">and</span> <span class="n">use_ref</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">ref_enter</span><span class="p">(</span>
          <span class="n">data</span><span class="p">,</span> <span class="n">frame_name</span><span class="p">,</span> <span class="n">is_constant</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">enter</span><span class="p">(</span>
          <span class="n">data</span><span class="p">,</span> <span class="n">frame_name</span><span class="p">,</span> <span class="n">is_constant</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_input_shape</span><span class="p">:</span>
      <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">result</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">enter_component</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">_Enter</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">frame_name</span><span class="p">,</span> <span class="n">is_constant</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="p">,</span> <span class="n">use_ref</span><span class="p">,</span>
                    <span class="n">use_input_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">enter_component</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">exit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Exits the current frame to its parent frame.</span>

<span class="sd">  Exit makes its input `data` available to the parent frame.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: The tensor to be made available to the parent frame.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The same tensor as `data`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_is_ref_dtype</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">ref_exit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">_exit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">exit</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">switch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Forwards `data` to an output determined by `pred`.</span>

<span class="sd">  If `pred` is false, the `data` input is forwarded to the first output.</span>
<span class="sd">  Otherwise, the data goes to the second output.</span>

<span class="sd">  This op handles `Tensor`s and `IndexedSlices`.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: The tensor to be forwarded to the appropriate output.</span>
<span class="sd">    pred: A scalar that specifies which output port will receive data.</span>
<span class="sd">    dtype: Optional element type for the returned tensor. If missing, the type</span>
<span class="sd">      is inferred from the type of `value`.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `(output_false, output_true)`: If `pred` is true, data will be forwarded</span>
<span class="sd">    to `output_true`, otherwise it goes to `output_false`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Switch&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">pred</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pred&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
      <span class="n">tensors</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">mapped</span> <span class="o">=</span> <span class="p">[</span><span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
      <span class="n">mapped_f</span><span class="p">,</span> <span class="n">mapped_t</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">mapped</span><span class="p">)</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mapped_f</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
              <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mapped_t</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_SwitchRefOrTensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Switch&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Forwards `data` to an output determined by `pred`.</span>

<span class="sd">  If `pred` is false, the `data` input is forwarded to the first output.</span>
<span class="sd">  Otherwise, the data goes to the second output.</span>

<span class="sd">  This op handles `Tensor`s and `IndexedSlices`.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: The tensor to be forwarded to the appropriate output.</span>
<span class="sd">    pred: A scalar that specifies which output port will receive data.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `(output_false, output_true)`: If `pred` is true, data will be forwarded to</span>
<span class="sd">    `output_true`, otherwise it goes to `output_false`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if data is not a Tensor or IndexedSlices</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
  <span class="c1"># NOTE(vrv): ops.colocate_with(data, ignore_existing=True) below</span>
  <span class="c1"># addresses the following scenario.</span>
  <span class="c1">#</span>
  <span class="c1"># Assume you execute Optimizer.apply_gradients() in a branch of a cond().</span>
  <span class="c1">#</span>
  <span class="c1"># 1. The update op is created inside a `with ops.colocate(var):` block</span>
  <span class="c1">#</span>
  <span class="c1"># 2. Some tensor `data` is captured and a switch is created in a</span>
  <span class="c1">#    `with ops.colocate_with(data):` block.</span>
  <span class="c1">#</span>
  <span class="c1"># with ops.colocate_with(var):</span>
  <span class="c1">#  with ops.colocate_with(data):</span>
  <span class="c1">#    op = ...</span>
  <span class="c1">#</span>
  <span class="c1"># var and data may be pinned to different devices, so we want to ops</span>
  <span class="c1"># created within ops.colocate_with(data) to ignore the existing stack.</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_is_ref_dtype</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">return</span> <span class="n">ref_switch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">switch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the value of an available element of `inputs`.</span>

<span class="sd">  This op tests each of the tensors in `inputs` in turn to determine if any of</span>
<span class="sd">  them is available. If it finds an available tensor, it returns it and its</span>
<span class="sd">  index in `inputs`.</span>

<span class="sd">  It is an error if more than one tensor in `inputs` is available. If no tensor</span>
<span class="sd">  in `inputs` is available, the returned tensor and index are not set.</span>

<span class="sd">  This op handles both `Tensor`s and `IndexedSlices`. If inputs has a mix of</span>
<span class="sd">  `Tensor`s and `IndexedSlices`, all inputs are converted to IndexedSlices</span>
<span class="sd">  before merging.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: The input tensors, at most one of which is available.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple containing the chosen input tensor and its index in `inputs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If any of the inputs is None, or inputs are IndexedSlices and</span>
<span class="sd">      some but not all have a dense_shape property.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">inp</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;At least one of the merge inputs is None: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">inputs</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Merge&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_is_ref_dtype</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">return</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">ref_merge</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># If there is a mix of tensors and indexed slices, then convert the</span>
      <span class="c1"># tensors to indexed slices.</span>
      <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">_as_indexed_slices_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>

      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

      <span class="n">flat_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
      <span class="n">merged_results</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">gen_control_flow_ops</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">flat_inputs</span><span class="p">)</span>
      <span class="p">]</span>
      <span class="n">flat_merged</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span> <span class="k">for</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">merged_results</span><span class="p">]</span>
      <span class="n">chosen_index</span> <span class="o">=</span> <span class="n">merged_results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">merged_inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">flat_merged</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">merged_inputs</span><span class="p">,</span> <span class="n">chosen_index</span><span class="p">)</span>


<span class="c1"># pylint: enable=protected-access</span>


<span class="k">def</span> <span class="nf">_convert_tensorarray_to_flow</span><span class="p">(</span><span class="n">tensor_or_tensor_array</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_or_tensor_array</span><span class="p">,</span> <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tensor_or_tensor_array</span><span class="o">.</span><span class="n">flow</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor_or_tensor_array</span>


<span class="k">def</span> <span class="nf">_convert_flows_to_tensorarrays</span><span class="p">(</span><span class="n">tensors_or_tensorarrays</span><span class="p">,</span> <span class="n">tensors_or_flows</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors_or_tensorarrays</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors_or_flows</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Lengths of original Tensor list and new list do not match: </span><span class="si">%d</span><span class="s2"> vs. </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
        <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tensors_or_tensorarrays</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors_or_flows</span><span class="p">)))</span>
  <span class="k">return</span> <span class="p">[</span>
      <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">build_ta_with_new_flow</span><span class="p">(</span><span class="n">ta</span><span class="p">,</span> <span class="n">t_or_flow</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
          <span class="n">ta</span><span class="p">,</span> <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">)</span> <span class="k">else</span> <span class="n">t_or_flow</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">ta</span><span class="p">,</span> <span class="n">t_or_flow</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tensors_or_tensorarrays</span><span class="p">,</span> <span class="n">tensors_or_flows</span><span class="p">)</span>
  <span class="p">]</span>


<span class="k">def</span> <span class="nf">_ShapeLessThanOrEqual</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">shape2</span><span class="o">.</span><span class="n">dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">if</span> <span class="n">shape1</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="n">shape2</span><span class="o">.</span><span class="n">ndims</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">False</span>
  <span class="k">for</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape1</span><span class="o">.</span><span class="n">dims</span><span class="p">,</span> <span class="n">shape2</span><span class="o">.</span><span class="n">dims</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dim2</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dim1</span><span class="o">.</span><span class="n">value</span> <span class="o">!=</span> <span class="n">dim2</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">False</span>
  <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_get_shape_invariant</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns shape invariant(s) for the given variable.</span>

<span class="sd">  Args:</span>
<span class="sd">    var: The tensor whose shape is described.</span>
<span class="sd">    shape: The shape invariant for the tensor.  If not specified, then a default</span>
<span class="sd">      shape invariant for `var` is returned.</span>

<span class="sd">  Returns:</span>
<span class="sd">    `TensorShape` or `list` of `TensorShape`: The shape invariant for `var` (if</span>
<span class="sd">    it is a `Tensor`), or the shape invariants for the components that comprise</span>
<span class="sd">    `var` (if it is a `CompositeTensor`).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="c1"># Get a TypeSpec for `var`.</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">spec</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">_type_spec</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">spec</span> <span class="o">=</span> <span class="n">_shape_invariant_to_type_spec</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>

    <span class="n">tensor_specs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tspec</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">tspec</span> <span class="ow">in</span> <span class="n">tensor_specs</span><span class="p">]</span>

  <span class="k">elif</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">var</span><span class="o">.</span><span class="n">shape</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tensor_spec</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">shape</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;TensorSpec </span><span class="si">%r</span><span class="s2"> is not compatible with </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">shape</span><span class="o">.</span><span class="n">shape</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">type_spec</span><span class="o">.</span><span class="n">TypeSpec</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;TypeSpec </span><span class="si">%r</span><span class="s2"> is not compatible with </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">shape</span>


<span class="k">def</span> <span class="nf">_shape_invariant_to_type_spec</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts a shape invariant to a TypeSpec.</span>

<span class="sd">  Args:</span>
<span class="sd">    var: The tensor whose shape is described by the shape invariant.</span>
<span class="sd">    shape: A `TypeSpec` or `TensorShape`.  If `shape` is already a `TypeSpec`,</span>
<span class="sd">      then it is simply returned as-is.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `TypeSpec` for `var`, consistent with the given shape.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">type_spec</span><span class="o">.</span><span class="n">type_spec_from_value</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">type_spec</span><span class="o">.</span><span class="n">TypeSpec</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;TypeSpec </span><span class="si">%r</span><span class="s2"> is not compatible with </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">shape</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s2">&quot;Expected shape to be a TypeSpec, TensorShape or None, got </span><span class="si">%r</span><span class="s2"> for&quot;</span>
        <span class="s2">&quot; value </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="p">))</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tensor_spec</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">var</span><span class="o">.</span><span class="n">_shape_invariant_to_type_spec</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;To describe or constrain a </span><span class="si">%s</span><span class="s2">, use a </span><span class="si">%s</span><span class="s2"> instead of a TensorShape.&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">var</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">_type_spec</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected var to be a Tensor or CompositeTensor, got </span><span class="si">%s</span><span class="s2">&quot;</span>
                    <span class="o">%</span> <span class="n">var</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_SetShapeInvariants</span><span class="p">(</span><span class="n">input_vars</span><span class="p">,</span> <span class="n">enter_vars</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set the shapes of the tensors in `enter_vars` to `shapes`.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_vars: A list of tensors that are inputs to `enter_vars`.</span>
<span class="sd">    enter_vars: A list of tensors whose shapes will be set.</span>
<span class="sd">    shapes: A (possibly nested) list of shapes.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If any tensor in `enter_vars` has a less specific shape</span>
<span class="sd">      than its corresponding shape in `shapes`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">shapes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span>
  <span class="n">flat_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">flat_shapes</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`shapes` must be a (possibly nested) list of shapes.&quot;</span><span class="p">)</span>
  <span class="c1"># Check that the shapes of the inputs are less than the shape invariants,</span>
  <span class="c1"># and set the shapes of `enter_vars` to the shape invariants.</span>
  <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_vars</span><span class="p">,</span> <span class="n">enter_vars</span><span class="p">,</span> <span class="n">flat_shapes</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">_ShapeLessThanOrEqual</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="n">shape</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The shape invariant specified for </span><span class="si">%s</span><span class="s2"> is not compatible with &quot;</span>
            <span class="s2">&quot;the initial shape of the loop variable. It enters the loop &quot;</span>
            <span class="s2">&quot;with shape </span><span class="si">%s</span><span class="s2">, but the specified shape invariant is </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
            <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="n">shape</span><span class="p">))</span>
      <span class="n">var</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">var</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_EnforceShapeInvariant</span><span class="p">(</span><span class="n">merge_var</span><span class="p">,</span> <span class="n">next_var</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Check if the shapes of the loops variables are invariants.</span>

<span class="sd">  Args:</span>
<span class="sd">    merge_var: The list of tensors representing the initial values of the loop</span>
<span class="sd">      variables.</span>
<span class="sd">    next_var: The list of tensors representing the values of the loop variables</span>
<span class="sd">      after one loop iteration.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If any tensor in `merge_var` has a more specific shape than</span>
<span class="sd">      its corresponding tensor in `next_var`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">merge_var</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">m_shape</span> <span class="o">=</span> <span class="n">merge_var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="n">n_shape</span> <span class="o">=</span> <span class="n">next_var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_ShapeLessThanOrEqual</span><span class="p">(</span><span class="n">n_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">):</span>
      <span class="n">enter</span> <span class="o">=</span> <span class="n">merge_var</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span>
      <span class="k">assert</span> <span class="n">util</span><span class="o">.</span><span class="n">IsLoopEnter</span><span class="p">(</span><span class="n">enter</span><span class="p">)</span>
      <span class="n">input_t</span> <span class="o">=</span> <span class="n">enter</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Input tensor &#39;</span><span class="si">%s</span><span class="s2">&#39; enters the loop with shape </span><span class="si">%s</span><span class="s2">, but has shape </span><span class="si">%s</span><span class="s2"> &quot;</span>
          <span class="s2">&quot;after one iteration. To allow the shape to vary across iterations, &quot;</span>
          <span class="s2">&quot;use the `shape_invariants` argument of tf.while_loop to specify a &quot;</span>
          <span class="s2">&quot;less-specific shape.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">input_t</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">n_shape</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">merge_var</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_AddNextAndBackEdge</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">enforce_shape_invariant</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add NextIteration and back edge from v to m.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">_NextIteration</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">enforce_shape_invariant</span><span class="p">:</span>
      <span class="c1"># Make sure the shapes of loop outputs are correct. We do this before</span>
      <span class="c1"># calling _update_input, which will raise a less-helpful error message if</span>
      <span class="c1"># the types don&#39;t match.</span>
      <span class="c1"># TODO(skyewm): call this for other cases below (needs testing)</span>
      <span class="n">_EnforceShapeInvariant</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">def</span> <span class="nf">update_component</span><span class="p">(</span><span class="n">m_component</span><span class="p">,</span> <span class="n">v_component</span><span class="p">):</span>
      <span class="n">m_component</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_component</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">v</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">_as_indexed_slices</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">_NextIteration</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">update_component</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">v</span>


<span class="nd">@six</span><span class="o">.</span><span class="n">add_metaclass</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABCMeta</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ControlFlowContext</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;The base class for control flow context.</span>

<span class="sd">  The usage pattern is a sequence of (Enter, Exit) followed by a final</span>
<span class="sd">  ExitResult.</span>

<span class="sd">  We maintain the following state for control flow contexts during graph</span>
<span class="sd">  construction:</span>
<span class="sd">   1. graph has _control_flow_context: the current context used to</span>
<span class="sd">      construct new nodes. Changed by ctxt.Enter() and ctxt.Exit()</span>
<span class="sd">   2. op has _control_flow_context: the context to which the op belongs.</span>
<span class="sd">      Set at the time the op is created. Immutable.</span>
<span class="sd">   3. A ControlFlowContext has _outer_context: the context in which this</span>
<span class="sd">      context is created. Set at the time a context is created. Immutable.</span>
<span class="sd">   4. A ControlFlowContext has _context_stack.</span>
<span class="sd">      Pushed and popped by ctxt.Enter() and ctxt.Exit()</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nested_contexts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">_nested_contexts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context_stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">values_def</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_values_from_proto</span><span class="p">(</span><span class="n">values_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># The names of tensors that have been already seen in this context.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
      <span class="c1"># The keys are the names of tensors referenced by but external to this</span>
      <span class="c1"># context. Each value is the Tensor that should be used by this context to</span>
      <span class="c1"># access the key value (e.g. a switch output guarding a cond input value).</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="k">def</span> <span class="nf">_init_values_from_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes values and external_values from `ValuesDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      values_def: `ValuesDef` protocol buffer.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values_def</span><span class="p">,</span> <span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">ValuesDef</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">values_def</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">values_def</span><span class="o">.</span><span class="n">external_values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">k</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
    <span class="n">op_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span>
        <span class="n">op</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="p">])</span>
    <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">op_names</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">op</span><span class="p">)</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">outer_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the context containing this context.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">grad_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Abstract method&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Abstract method&quot;</span><span class="p">)</span>

  <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
  <span class="k">def</span> <span class="nf">to_control_flow_context_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_def</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Serializes this into `context_def`.</span>

<span class="sd">    Args:</span>
<span class="sd">      context_def: a `ControlFlowContextDef` protocol buffer.</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Abstract method&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_to_values_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts the values to a `ValuesDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `ValuesDef` protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">values_def</span> <span class="o">=</span> <span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">ValuesDef</span><span class="p">()</span>
    <span class="n">values_def</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
        <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)])</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">k</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">values_def</span><span class="o">.</span><span class="n">external_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values_def</span>

  <span class="k">def</span> <span class="nf">AddName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">def</span> <span class="nf">Enter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enter this control flow context.&quot;&quot;&quot;</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">())</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Exit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Exit this control flow context.&quot;&quot;&quot;</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="n">last_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="n">last_context</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">EnterGradientColocation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Start building a gradient colocated with an op.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">EnterGradientColocation</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">ExitGradientColocation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Start building a gradient colocated with an op.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">ExitGradientColocation</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">ExitResult</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make a list of tensors available in the outer context.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddName</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
      <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">GetWhileContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the while context containing this context.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">_RemoveExternalControlEdges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Remove any external control dependency on this op.&quot;&quot;&quot;</span>
    <span class="n">while_ctxt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span>
    <span class="c1"># A control input of `op` is internal if it is in the same while</span>
    <span class="c1"># loop context as the enclosing while loop context of self.</span>
    <span class="k">if</span> <span class="n">while_ctxt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">internal_control_inputs</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">internal_control_inputs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">:</span>
        <span class="n">ctxt</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">GetOutputContext</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ctxt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ctxt</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span> <span class="o">==</span> <span class="n">while_ctxt</span><span class="p">:</span>
          <span class="n">internal_control_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">external_control_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">internal_control_inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">):</span>
      <span class="n">external_control_inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
          <span class="nb">set</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">internal_control_inputs</span><span class="p">))</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_remove_all_control_inputs</span><span class="p">()</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_add_control_inputs</span><span class="p">(</span><span class="n">internal_control_inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">internal_control_inputs</span><span class="p">,</span> <span class="n">external_control_inputs</span>

  <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">AddInnerOp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Notifies a scope about an operator added to an inner scope.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddInnerOp</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">GetControlPivot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the pivot node for this context, or None.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">IsWhileContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">IsCondContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">IsXLAContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>


<span class="k">class</span> <span class="nc">CondContext</span><span class="p">(</span><span class="n">ControlFlowContext</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;The context for the conditional construct.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">pred</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">pivot</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">branch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cond_text&quot;</span><span class="p">,</span>
               <span class="n">context_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a `CondContext`.</span>

<span class="sd">    Args:</span>
<span class="sd">      pred: The `boolean` tensor for the conditional predicate.</span>
<span class="sd">      pivot: The predicate tensor in this branch.</span>
<span class="sd">      branch: 0 or 1 representing this branch.</span>
<span class="sd">      name: Name of the `CondContext` python object.</span>
<span class="sd">      context_def: Optional `ContextDef` protocol buffer to initialize the</span>
<span class="sd">        `CondContext` object from.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add. Only used when</span>
<span class="sd">        initialing from protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">context_def</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_proto</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Initializes the default fields.</span>
      <span class="n">ControlFlowContext</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span> <span class="o">=</span> <span class="n">pred</span>  <span class="c1"># The boolean tensor for the cond predicate</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span> <span class="o">=</span> <span class="n">pivot</span>  <span class="c1"># The predicate tensor in this branch</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_branch</span> <span class="o">=</span> <span class="n">branch</span>  <span class="c1"># 0 or 1 representing this branch</span>

      <span class="c1"># Values considered to have been already seen in this context. pred is not</span>
      <span class="c1"># included in this context.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">pred</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">pivot</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">pivot</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_init_from_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new `CondContext` from protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      context_def: `CondContextDef` protocol buffer.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">CondContextDef</span><span class="p">)</span>
    <span class="c1"># Create from context_def.</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">context_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">pred_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">pivot_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_branch</span> <span class="o">=</span> <span class="n">context_def</span><span class="o">.</span><span class="n">branch</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CondContext</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">values_def</span><span class="o">=</span><span class="n">context_def</span><span class="o">.</span><span class="n">values_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">pivot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">branch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_branch</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">grad_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span><span class="o">.</span><span class="n">grad_state</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span><span class="o">.</span><span class="n">back_prop</span>
    <span class="k">return</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">GetControlPivot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span>

  <span class="k">def</span> <span class="nf">to_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a `CondContext` to a `CondContextDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `CondContextDef` protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">export_scope</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">export_scope</span><span class="p">)):</span>
      <span class="n">context_def</span> <span class="o">=</span> <span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">CondContextDef</span><span class="p">()</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">context_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">pred_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                   <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">pivot_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                    <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_branch</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">values_def</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">CondContext</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_to_values_def</span><span class="p">(</span><span class="n">export_scope</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">nested</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nested_contexts</span><span class="p">:</span>
        <span class="n">nested_def</span> <span class="o">=</span> <span class="n">context_def</span><span class="o">.</span><span class="n">nested_contexts</span><span class="o">.</span><span class="n">add</span><span class="p">()</span>
        <span class="n">nested</span><span class="o">.</span><span class="n">to_control_flow_context_def</span><span class="p">(</span><span class="n">nested_def</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">context_def</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_proto</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a `CondContext` object created from `context_def`.&quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">CondContext</span><span class="p">(</span><span class="n">context_def</span><span class="o">=</span><span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>

    <span class="n">ret</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">nested_def</span> <span class="ow">in</span> <span class="n">context_def</span><span class="o">.</span><span class="n">nested_contexts</span><span class="p">:</span>
      <span class="n">from_control_flow_context_def</span><span class="p">(</span><span class="n">nested_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">to_control_flow_context_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_def</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">context_def</span><span class="o">.</span><span class="n">cond_ctxt</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_proto</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">AddValue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add `val` to the current context and its outer context recursively.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">val</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">:</span>
      <span class="c1"># Use the real value if it comes from outer context. This is needed in</span>
      <span class="c1"># particular for nested conds.</span>
      <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">val</span> <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">result</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">val</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddValue</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">result</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_SwitchRefOrTensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">_branch</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddInnerOp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>

      <span class="n">result</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">prevent_fetching</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">result</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>

      <span class="c1"># Mark Switch output as seen by this context and any outer contexts,</span>
      <span class="c1"># just like what we do for normal op outputs in _AddOpInternal() below.</span>
      <span class="n">ctxt</span> <span class="o">=</span> <span class="bp">self</span>
      <span class="k">while</span> <span class="n">ctxt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">ctxt</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">ctxt</span> <span class="o">=</span> <span class="n">ctxt</span><span class="o">.</span><span class="n">_outer_context</span>
        <span class="c1"># pylint: enable=protected-access</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">AddOp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_AddOpInternal</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_AddOpInternal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add `op` to the current context.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="c1"># If we&#39;re in a while loop, remove any control inputs from outside the</span>
      <span class="c1"># loop.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_RemoveExternalControlEdges</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

      <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
          <span class="n">util</span><span class="o">.</span><span class="n">OpInContext</span><span class="p">(</span><span class="n">input_op</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_op</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">):</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">op</span><span class="o">.</span><span class="n">_add_control_input</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
        <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Make each input to &#39;op&#39; available in this CondContext. If an input is</span>
      <span class="c1"># already part of this context there&#39;s nothing to do, but if it&#39;s</span>
      <span class="c1"># external, AddValue() will handle adding the appropriate Switch node and</span>
      <span class="c1"># other bookkeeping.</span>
      <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;Merge&quot;</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;NextIteration&quot;</span><span class="p">:</span>
          <span class="c1"># Edge case: if we&#39;re importing a while loop inside this CondContext,</span>
          <span class="c1"># AddValue() will not correctly handle the NextIteration inputs to</span>
          <span class="c1"># Merge node. The problem is that the NextIteration should also be</span>
          <span class="c1"># part of this context, but if we&#39;re importing it won&#39;t have been</span>
          <span class="c1"># processed and added to the context yet, so AddValue() will try to</span>
          <span class="c1"># add a Switch which results in an invalid graph. Instead, we use the</span>
          <span class="c1"># NextIteration input as-is here, and it will eventually be added to</span>
          <span class="c1"># the context via AddOp().</span>
          <span class="n">real_x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">real_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddValue</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">real_x</span> <span class="o">!=</span> <span class="n">x</span><span class="p">:</span>
          <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">real_x</span><span class="p">)</span>
          <span class="c1"># pylint: enable=protected-access</span>
      <span class="c1"># Remove any external control dependency on this op.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_RemoveExternalControlEdges</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_is_function</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">)</span> <span class="ow">or</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;SymbolicGradient&quot;</span><span class="p">:</span>
        <span class="n">op</span><span class="o">.</span><span class="n">_add_control_input</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>

    <span class="c1"># Mark op&#39;s outputs as seen by this context and any outer contexts.</span>
    <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">]</span>
    <span class="n">ctxt</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">while</span> <span class="n">ctxt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">ctxt</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">output_names</span><span class="p">)</span>
      <span class="n">ctxt</span> <span class="o">=</span> <span class="n">ctxt</span><span class="o">.</span><span class="n">_outer_context</span>
      <span class="c1"># pylint: enable=protected-access</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">util</span><span class="o">.</span><span class="n">IsLoopExit</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
      <span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">prevent_fetching</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddInnerOp</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_ProcessOutputTensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Process an output tensor of a conditional branch.&quot;&quot;&quot;</span>
    <span class="n">real_val</span> <span class="o">=</span> <span class="n">val</span>
    <span class="k">if</span> <span class="n">val</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">:</span>
      <span class="c1"># Handle the special case of lambda: x</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
        <span class="n">real_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddValue</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">real_val</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">real_val</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">real_val</span>
      <span class="n">real_val</span> <span class="o">=</span> <span class="n">_SwitchRefOrTensor</span><span class="p">(</span><span class="n">real_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">_branch</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">real_val</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">external_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">external_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">real_val</span> <span class="o">=</span> <span class="n">external_val</span>
    <span class="k">return</span> <span class="n">real_val</span>

  <span class="k">def</span> <span class="nf">_BuildCondTensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
      <span class="c1"># Use pivot as the proxy for this op.</span>
      <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">v</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">v</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">_convert_tensorarray_to_flow</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ProcessOutputTensor</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">BuildCondBranch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add the subgraph defined by fn() to the graph.&quot;&quot;&quot;</span>
    <span class="n">pre_summaries</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">_SUMMARY_COLLECTION</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">original_result</span> <span class="o">=</span> <span class="n">fn</span><span class="p">()</span>
    <span class="n">post_summaries</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">_SUMMARY_COLLECTION</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">post_summaries</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">pre_summaries</span><span class="p">):</span>
      <span class="n">new_summaries</span> <span class="o">=</span> <span class="n">post_summaries</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">pre_summaries</span><span class="p">):]</span>
      <span class="n">summary_ref</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">_SUMMARY_COLLECTION</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">summary_ref</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">pre_summaries</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">new_summaries</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">original_result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">no_op</span><span class="p">(),</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">original_result</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
          <span class="n">original_result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
              <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">original_result</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">original_result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_BuildCondTensor</span><span class="p">,</span> <span class="n">original_result</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">)):</span>
      <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">original_result</span><span class="p">,</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">IsCondContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_UnpackIfSingleton</span><span class="p">(</span><span class="n">res</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="c1"># pylint: disable=g-doc-args</span>
<div class="viewcode-block" id="cond"><a class="viewcode-back" href="../../../../index.html#tensorflow.cond">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;cond&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fn1&quot;</span><span class="p">,</span> <span class="s2">&quot;fn2&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cond</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span>
         <span class="n">true_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
         <span class="n">false_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
         <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
         <span class="n">fn1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
         <span class="n">fn2</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return `true_fn()` if the predicate `pred` is true else `false_fn()`.</span>

<span class="sd">  `true_fn` and `false_fn` both return lists of output tensors. `true_fn` and</span>
<span class="sd">  `false_fn` must have the same non-zero number and type of outputs.</span>

<span class="sd">  **WARNING**: Any Tensors or Operations created outside of `true_fn` and</span>
<span class="sd">  `false_fn` will be executed regardless of which branch is selected at runtime.</span>

<span class="sd">  Although this behavior is consistent with the dataflow model of TensorFlow,</span>
<span class="sd">  it has frequently surprised users who expected a lazier semantics.</span>
<span class="sd">  Consider the following simple program:</span>

<span class="sd">  ```python</span>
<span class="sd">  z = tf.multiply(a, b)</span>
<span class="sd">  result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))</span>
<span class="sd">  ```</span>

<span class="sd">  If `x &lt; y`, the `tf.add` operation will be executed and `tf.square`</span>
<span class="sd">  operation will not be executed. Since `z` is needed for at least one</span>
<span class="sd">  branch of the `cond`, the `tf.multiply` operation is always executed,</span>
<span class="sd">  unconditionally.</span>

<span class="sd">  Note that `cond` calls `true_fn` and `false_fn` *exactly once* (inside the</span>
<span class="sd">  call to `cond`, and not at all during `Session.run()`). `cond`</span>
<span class="sd">  stitches together the graph fragments created during the `true_fn` and</span>
<span class="sd">  `false_fn` calls with some additional graph nodes to ensure that the right</span>
<span class="sd">  branch gets executed depending on the value of `pred`.</span>

<span class="sd">  `tf.cond` supports nested structures as implemented in</span>
<span class="sd">  `tensorflow.python.util.nest`. Both `true_fn` and `false_fn` must return the</span>
<span class="sd">  same (possibly nested) value structure of lists, tuples, and/or named tuples.</span>
<span class="sd">  Singleton lists and tuples form the only exceptions to this: when returned by</span>
<span class="sd">  `true_fn` and/or `false_fn`, they are implicitly unpacked to single values.</span>
<span class="sd">  This behavior is disabled by passing `strict=True`.</span>

<span class="sd">  Args:</span>
<span class="sd">    pred: A scalar determining whether to return the result of `true_fn` or</span>
<span class="sd">      `false_fn`.</span>
<span class="sd">    true_fn: The callable to be performed if pred is true.</span>
<span class="sd">    false_fn: The callable to be performed if pred is false.</span>
<span class="sd">    strict: A boolean that enables/disables &#39;strict&#39; mode; see above.</span>
<span class="sd">    name: Optional name prefix for the returned tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Tensors returned by the call to either `true_fn` or `false_fn`. If the</span>
<span class="sd">    callables return a singleton list, the element is extracted from the list.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `true_fn` or `false_fn` is not callable.</span>
<span class="sd">    ValueError: if `true_fn` and `false_fn` do not return the same number of</span>
<span class="sd">      tensors, or return tensors of different types.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant(2)</span>
<span class="sd">  y = tf.constant(5)</span>
<span class="sd">  def f1(): return tf.multiply(x, 17)</span>
<span class="sd">  def f2(): return tf.add(y, 23)</span>
<span class="sd">  r = tf.cond(tf.less(x, y), f1, f2)</span>
<span class="sd">  # r is set to f1().</span>
<span class="sd">  # Operations in f2 (e.g., tf.add) are not executed.</span>
<span class="sd">  ```</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Always enable control flow v2 if building a function, regardless of toggle.</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">util</span><span class="o">.</span><span class="n">EnableControlFlowV2</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()):</span>
    <span class="k">return</span> <span class="n">cond_v2</span><span class="o">.</span><span class="n">cond_v2</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="c1"># We needed to make true_fn/false_fn keyword arguments for</span>
  <span class="c1"># backwards-compatibility. This check exists so that we can convert back to</span>
  <span class="c1"># having them be positional arguments.</span>
  <span class="c1"># TODO(josh11b): Make `true_fn` and `false_fn` positional arguments after</span>
  <span class="c1"># `fn1` and `fn2` are deleted.</span>
  <span class="k">if</span> <span class="n">fn1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">true_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cond(): true_fn and fn1 may not be set simultaneously.&quot;</span><span class="p">)</span>
    <span class="n">true_fn</span> <span class="o">=</span> <span class="n">fn1</span>
  <span class="k">elif</span> <span class="n">true_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cond(): true_fn argument required&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">fn2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">false_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cond(): false_fn and fn2 may not be set simultaneously.&quot;</span><span class="p">)</span>
    <span class="n">false_fn</span> <span class="o">=</span> <span class="n">fn2</span>
  <span class="k">elif</span> <span class="n">false_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cond(): false_fn argument required&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">true_fn</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;true_fn must be callable.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">false_fn</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;false_fn must be callable.&quot;</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;cond&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">pred</span><span class="p">]):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">pred</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">true_fn</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">false_fn</span><span class="p">()</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">strict</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_UnpackIfSingleton</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># Add the Switch to the graph.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pred must not be a Python bool&quot;</span><span class="p">)</span>
    <span class="n">p_2</span><span class="p">,</span> <span class="n">p_1</span> <span class="o">=</span> <span class="n">switch</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">pivot_1</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">p_1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;switch_t&quot;</span><span class="p">)</span>
    <span class="n">pivot_2</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">p_2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;switch_f&quot;</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pred_id&quot;</span><span class="p">)</span>
    <span class="c1"># Disable the fetching of tensors that are only on one branch of cond.</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="p">[</span><span class="n">p_1</span><span class="p">,</span> <span class="n">p_2</span><span class="p">,</span> <span class="n">pivot_1</span><span class="p">,</span> <span class="n">pivot_2</span><span class="p">,</span> <span class="n">pred</span><span class="p">]:</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">prevent_fetching</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># Build the graph for the true branch in a new context.</span>
    <span class="n">context_t</span> <span class="o">=</span> <span class="n">CondContext</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">pivot_1</span><span class="p">,</span> <span class="n">branch</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">context_t</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
      <span class="n">orig_res_t</span><span class="p">,</span> <span class="n">res_t</span> <span class="o">=</span> <span class="n">context_t</span><span class="o">.</span><span class="n">BuildCondBranch</span><span class="p">(</span><span class="n">true_fn</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">orig_res_t</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;true_fn must have a return value.&quot;</span><span class="p">)</span>
      <span class="n">context_t</span><span class="o">.</span><span class="n">ExitResult</span><span class="p">(</span><span class="n">res_t</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">context_t</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>

    <span class="c1"># Build the graph for the false branch in a new context.</span>
    <span class="n">context_f</span> <span class="o">=</span> <span class="n">CondContext</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">pivot_2</span><span class="p">,</span> <span class="n">branch</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">context_f</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
      <span class="n">orig_res_f</span><span class="p">,</span> <span class="n">res_f</span> <span class="o">=</span> <span class="n">context_f</span><span class="o">.</span><span class="n">BuildCondBranch</span><span class="p">(</span><span class="n">false_fn</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">orig_res_f</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;false_fn must have a return value.&quot;</span><span class="p">)</span>
      <span class="n">context_f</span><span class="o">.</span><span class="n">ExitResult</span><span class="p">(</span><span class="n">res_f</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">context_f</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">strict</span><span class="p">:</span>
      <span class="n">orig_res_t</span> <span class="o">=</span> <span class="n">_UnpackIfSingleton</span><span class="p">(</span><span class="n">orig_res_t</span><span class="p">)</span>
      <span class="n">orig_res_f</span> <span class="o">=</span> <span class="n">_UnpackIfSingleton</span><span class="p">(</span><span class="n">orig_res_f</span><span class="p">)</span>

    <span class="c1"># Check that the return values of the two branches have the same structure.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">orig_res_t</span><span class="p">,</span> <span class="n">orig_res_f</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
      <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_cast_indexed_slice_indices</span><span class="p">,</span> <span class="n">orig_res_t</span><span class="p">,</span> <span class="n">orig_res_f</span><span class="p">)</span>
      <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_cast_indexed_slice_indices</span><span class="p">,</span> <span class="n">res_t</span><span class="p">,</span> <span class="n">res_f</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">orig_res_t</span><span class="p">,</span> <span class="n">orig_res_f</span><span class="p">,</span>
                                   <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;Incompatible return types of true_fn and false_fn: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
      <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Incompatible return values of true_fn and false_fn: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>

    <span class="c1"># Add the final merge to the graph.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">res_t</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;true_fn and false_fn must return at least one result.&quot;</span><span class="p">)</span>

    <span class="n">res_t_flat</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">res_t</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">res_f_flat</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">res_f</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">res_t_flat</span><span class="p">,</span> <span class="n">res_f_flat</span><span class="p">):</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Outputs of true_fn and false_fn must have the same type: &quot;</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="n">merges</span> <span class="o">=</span> <span class="p">[</span><span class="n">merge</span><span class="p">(</span><span class="n">pair</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">res_f_flat</span><span class="p">,</span> <span class="n">res_t_flat</span><span class="p">)]</span>
    <span class="n">merges</span> <span class="o">=</span> <span class="n">_convert_flows_to_tensorarrays</span><span class="p">(</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">orig_res_t</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">merges</span><span class="p">)</span>

    <span class="c1"># Only add non-nested conds to the collection. Any nested control flow will</span>
    <span class="c1"># be encapsulated in the root context.</span>
    <span class="k">assert</span> <span class="n">context_t</span><span class="o">.</span><span class="n">outer_context</span> <span class="o">==</span> <span class="n">context_f</span><span class="o">.</span><span class="n">outer_context</span>
    <span class="k">if</span> <span class="n">context_t</span><span class="o">.</span><span class="n">outer_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">COND_CONTEXT</span><span class="p">,</span> <span class="n">context_t</span><span class="p">)</span>
      <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">COND_CONTEXT</span><span class="p">,</span> <span class="n">context_f</span><span class="p">)</span>

    <span class="n">merges</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span>
        <span class="n">structure</span><span class="o">=</span><span class="n">orig_res_t</span><span class="p">,</span> <span class="n">flat_sequence</span><span class="o">=</span><span class="n">merges</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Singleton lists and tuples are automatically unpacked if strict == False.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">strict</span><span class="p">:</span>
      <span class="n">merges</span> <span class="o">=</span> <span class="n">_UnpackIfSingleton</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">merges</span></div>


<span class="k">def</span> <span class="nf">_cast_indexed_slice_indices</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Cast IndexedSlice.indices from int32 to int64 where necessary.</span>

<span class="sd">  If `a` and `b` are both IndexedSlices, and their indices have different</span>
<span class="sd">  dtypes, then cast both their dtypes to `int64` (modifies `a` and `b`</span>
<span class="sd">  in-place).  Otherwise, does nothing.</span>

<span class="sd">  Args:</span>
<span class="sd">    a: A value, which may be an IndexedSlices.</span>
<span class="sd">    b: A value, which may be an IndexedSlices.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">)</span>
      <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">a</span><span class="o">.</span><span class="n">_indices</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">b</span><span class="o">.</span><span class="n">_indices</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>


<span class="c1"># pylint: enable=g-doc-args</span>
<span class="c1"># pylint: enable=redefined-outer-name</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;cond&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">cond_for_tf_v2</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">true_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">false_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return `true_fn()` if the predicate `pred` is true else `false_fn()`.</span>

<span class="sd">  `true_fn` and `false_fn` both return lists of output tensors. `true_fn` and</span>
<span class="sd">  `false_fn` must have the same non-zero number and type of outputs.</span>

<span class="sd">  **WARNING**: Any Tensors or Operations created outside of `true_fn` and</span>
<span class="sd">  `false_fn` will be executed regardless of which branch is selected at runtime.</span>

<span class="sd">  Although this behavior is consistent with the dataflow model of TensorFlow,</span>
<span class="sd">  it has frequently surprised users who expected a lazier semantics.</span>
<span class="sd">  Consider the following simple program:</span>

<span class="sd">  ```python</span>
<span class="sd">  z = tf.multiply(a, b)</span>
<span class="sd">  result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))</span>
<span class="sd">  ```</span>

<span class="sd">  If `x &lt; y`, the `tf.add` operation will be executed and `tf.square`</span>
<span class="sd">  operation will not be executed. Since `z` is needed for at least one</span>
<span class="sd">  branch of the `cond`, the `tf.multiply` operation is always executed,</span>
<span class="sd">  unconditionally.</span>

<span class="sd">  Note that `cond` calls `true_fn` and `false_fn` *exactly once* (inside the</span>
<span class="sd">  call to `cond`, and not at all during `Session.run()`). `cond`</span>
<span class="sd">  stitches together the graph fragments created during the `true_fn` and</span>
<span class="sd">  `false_fn` calls with some additional graph nodes to ensure that the right</span>
<span class="sd">  branch gets executed depending on the value of `pred`.</span>

<span class="sd">  `tf.cond` supports nested structures as implemented in</span>
<span class="sd">  `tensorflow.python.util.nest`. Both `true_fn` and `false_fn` must return the</span>
<span class="sd">  same (possibly nested) value structure of lists, tuples, and/or named tuples.</span>
<span class="sd">  Singleton lists and tuples form the only exceptions to this: when returned by</span>
<span class="sd">  `true_fn` and/or `false_fn`, they are implicitly unpacked to single values.</span>

<span class="sd">  Note: It is illegal to &quot;directly&quot; use tensors created inside a cond branch</span>
<span class="sd">  outside it, e.g. by storing a reference to a branch tensor in the python</span>
<span class="sd">  state. If you need to use a tensor created in a branch function you should</span>
<span class="sd">  return it as an output of the branch function and use the output from</span>
<span class="sd">  `tf.cond` instead.</span>

<span class="sd">  Args:</span>
<span class="sd">    pred: A scalar determining whether to return the result of `true_fn` or</span>
<span class="sd">      `false_fn`.</span>
<span class="sd">    true_fn: The callable to be performed if pred is true.</span>
<span class="sd">    false_fn: The callable to be performed if pred is false.</span>
<span class="sd">    name: Optional name prefix for the returned tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Tensors returned by the call to either `true_fn` or `false_fn`. If the</span>
<span class="sd">    callables return a singleton list, the element is extracted from the list.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `true_fn` or `false_fn` is not callable.</span>
<span class="sd">    ValueError: if `true_fn` and `false_fn` do not return the same number of</span>
<span class="sd">      tensors, or return tensors of different types.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant(2)</span>
<span class="sd">  y = tf.constant(5)</span>
<span class="sd">  def f1(): return tf.multiply(x, 17)</span>
<span class="sd">  def f2(): return tf.add(y, 23)</span>
<span class="sd">  r = tf.cond(tf.less(x, y), f1, f2)</span>
<span class="sd">  # r is set to f1().</span>
<span class="sd">  # Operations in f2 (e.g., tf.add) are not executed.</span>
<span class="sd">  ```</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cond</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">true_fn</span><span class="o">=</span><span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="o">=</span><span class="n">false_fn</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_resource_safe_shape</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the shape of t or the variable it points to.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">resource</span><span class="p">:</span>
    <span class="k">while</span> <span class="n">t</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="c1"># TODO(yuanbyu): Consider having a unified notion of context for</span>
<span class="c1"># not only conditionals and loops but also control dependency and</span>
<span class="c1"># subgraphs.</span>
<span class="k">class</span> <span class="nc">WhileContext</span><span class="p">(</span><span class="n">ControlFlowContext</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;The context for the loop construct.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">maximum_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">parallel_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
               <span class="n">back_prop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">swap_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;while_context&quot;</span><span class="p">,</span>
               <span class="n">grad_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">context_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;&quot;Creates a `WhileContext`.</span>

<span class="sd">    Args:</span>
<span class="sd">      maximum_iterations: Optional upper bound on number of loop iterations.</span>
<span class="sd">      parallel_iterations: The number of iterations allowed to run in parallel.</span>
<span class="sd">      back_prop: Whether backprop is enabled for this while loop.</span>
<span class="sd">      swap_memory: Whether GPU-CPU memory swap is enabled for this loop.</span>
<span class="sd">      name: Optional name prefix for the returned tensors.</span>
<span class="sd">      grad_state: The gradient loop state.</span>
<span class="sd">      context_def: Optional `WhileContextDef` protocol buffer to initialize the</span>
<span class="sd">        `Whilecontext` python object from.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add. Only used when</span>
<span class="sd">        initialing from protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context_def</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_proto</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ControlFlowContext</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_args</span><span class="p">(</span><span class="n">maximum_iterations</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="p">,</span> <span class="n">back_prop</span><span class="p">,</span>
                           <span class="n">swap_memory</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="c1"># The gradient loop state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_grad_state</span> <span class="o">=</span> <span class="n">grad_state</span>

  <span class="k">def</span> <span class="nf">_init_from_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maximum_iterations</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="p">,</span> <span class="n">back_prop</span><span class="p">,</span>
                      <span class="n">swap_memory</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new `WhileContext` from arguments.</span>

<span class="sd">    Args:</span>
<span class="sd">      maximum_iterations: Optional upper bound on number of loop iterations.</span>
<span class="sd">      parallel_iterations: The number of iterations allowed to run in parallel.</span>
<span class="sd">      back_prop: Whether backprop is enabled for this while loop.</span>
<span class="sd">      swap_memory: Whether GPU-CPU memory swap is enabled for this loop.</span>
<span class="sd">      name: Optional name prefix for the returned tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `parallel_iterations` has invalid value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_iterations</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">parallel_iterations</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`parallel_iterations` must be a positive integer: &quot;</span>
                       <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">parallel_iterations</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maximum_iterations</span> <span class="o">=</span> <span class="n">maximum_iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span> <span class="o">=</span> <span class="n">parallel_iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_back_prop</span> <span class="o">=</span> <span class="n">back_prop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_swap_memory</span> <span class="o">=</span> <span class="n">swap_memory</span>
    <span class="c1"># We use this node to control constants created by the pred lambda.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_pred</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># We use this node to control constants created by the body lambda.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_body</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># The boolean tensor for loop termination condition. Used in code</span>
    <span class="c1"># generation for gradient computation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># The list of exit tensors for loop variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loop_exits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># The list of enter tensors for loop variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loop_enters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_init_from_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new `WhileContext` from protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      context_def: `WhileContextDef` protocol buffer.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">WhileContextDef</span><span class="p">)</span>
    <span class="c1"># Create from context_def.</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">context_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">context_def</span><span class="o">.</span><span class="n">maximum_iterations_name</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maximum_iterations</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">maximum_iterations_name</span><span class="p">,</span>
                                 <span class="n">import_scope</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maximum_iterations</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span> <span class="o">=</span> <span class="n">context_def</span><span class="o">.</span><span class="n">parallel_iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_back_prop</span> <span class="o">=</span> <span class="n">context_def</span><span class="o">.</span><span class="n">back_prop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_swap_memory</span> <span class="o">=</span> <span class="n">context_def</span><span class="o">.</span><span class="n">swap_memory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_pred</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">pivot_for_pred_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
    <span class="c1"># We use this node to control constants created by the body lambda.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_body</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">pivot_for_body_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
    <span class="c1"># The boolean tensor for loop termination condition. Used in code</span>
    <span class="c1"># generation for gradient computation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">context_def</span><span class="o">.</span><span class="n">pivot_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
    <span class="c1"># The list of exit tensors for loop variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loop_exits</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">exit_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">exit_name</span> <span class="ow">in</span> <span class="n">context_def</span><span class="o">.</span><span class="n">loop_exit_names</span>
    <span class="p">]</span>
    <span class="c1"># The list of enter tensors for loop variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loop_enters</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">enter_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">enter_name</span> <span class="ow">in</span> <span class="n">context_def</span><span class="o">.</span><span class="n">loop_enter_names</span>
    <span class="p">]</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">WhileContext</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">values_def</span><span class="o">=</span><span class="n">context_def</span><span class="o">.</span><span class="n">values_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>

    <span class="c1"># import_scope causes self.name to be different from the original serialized</span>
    <span class="c1"># context&#39;s name. Rewrite &quot;frame_name&quot; attrs with the new name.</span>
    <span class="k">if</span> <span class="n">import_scope</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">:</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">)</span><span class="o">.</span><span class="n">op</span>
        <span class="k">if</span> <span class="n">util</span><span class="o">.</span><span class="n">IsLoopEnter</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
          <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;frame_name&quot;</span><span class="p">,</span>
                       <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)))</span>
          <span class="c1"># pylint: enable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">maximum_iterations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The maximum number of iterations that will be executed.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maximum_iterations</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">parallel_iterations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The number of iterations allowed to run in parallel.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True iff backprop is enabled for this while loop.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_back_prop</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">swap_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True iff GPU-CPU memory swap is enabled for this while loop.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_swap_memory</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">pivot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The boolean tensor representing the loop termination condition.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">loop_enters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The list of enter tensors for loop variables.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loop_enters</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">loop_exits</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The list of exit tensors for loop variables.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loop_exits</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">grad_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The gradient loop state.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_state</span>

  <span class="k">def</span> <span class="nf">to_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a `WhileContext` to a `WhileContextDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `WhileContextDef` protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">export_scope</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">export_scope</span><span class="p">)):</span>
      <span class="n">context_def</span> <span class="o">=</span> <span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">WhileContextDef</span><span class="p">()</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">context_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">parallel_iterations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maximum_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">context_def</span><span class="o">.</span><span class="n">maximum_iterations_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_maximum_iterations</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">back_prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_back_prop</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">swap_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_swap_memory</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">pivot_for_pred_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_pred</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">pivot_for_body_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_body</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">pivot_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                    <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">loop_exit_names</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loop_exits</span>
      <span class="p">])</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">loop_enter_names</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loop_enters</span>
      <span class="p">])</span>
      <span class="n">context_def</span><span class="o">.</span><span class="n">values_def</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">WhileContext</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_to_values_def</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">nested</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nested_contexts</span><span class="p">:</span>
        <span class="n">nested_def</span> <span class="o">=</span> <span class="n">context_def</span><span class="o">.</span><span class="n">nested_contexts</span><span class="o">.</span><span class="n">add</span><span class="p">()</span>
        <span class="n">nested</span><span class="o">.</span><span class="n">to_control_flow_context_def</span><span class="p">(</span><span class="n">nested_def</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">context_def</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">to_control_flow_context_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_def</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">context_def</span><span class="o">.</span><span class="n">while_ctxt</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_proto</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">))</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_proto</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a `WhileContext` object created from `context_def`.</span>

<span class="sd">    Args:</span>
<span class="sd">      context_def: A `WhileContextDef` protocol buffer.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `WhileContext` Python object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">WhileContext</span><span class="p">(</span><span class="n">context_def</span><span class="o">=</span><span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">nested_def</span> <span class="ow">in</span> <span class="n">context_def</span><span class="o">.</span><span class="n">nested_contexts</span><span class="p">:</span>
      <span class="n">from_control_flow_context_def</span><span class="p">(</span><span class="n">nested_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">GetWhileContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">GetControlPivot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_body</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_body</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_pred</span>

  <span class="k">def</span> <span class="nf">AddValue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add `val` to the current context and its outer context recursively.&quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">val</span>
    <span class="n">new_value</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span>
    <span class="c1"># Don&#39;t treat ops in this context as new values. Usually all known values</span>
    <span class="c1"># are in self._values, except when we&#39;re importing a while loop inside this</span>
    <span class="c1"># WhileContext. Since there&#39;s a cycle in this case, `val` may be part of the</span>
    <span class="c1"># imported while loop but not yet processed by this context and added to</span>
    <span class="c1"># self._values in _AddOpInternal. We only want to process external input</span>
    <span class="c1"># tensors to the while loop here.</span>
    <span class="n">new_value</span> <span class="o">&amp;=</span> <span class="n">val</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="n">new_value</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

      <span class="c1"># If we are in a grad context and val is from its forward context,</span>
      <span class="c1"># use GetRealValue(), which adds the logic to save the history of</span>
      <span class="c1"># val in forward.</span>
      <span class="n">grad_ctxt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">grad_ctxt</span><span class="p">:</span>
        <span class="n">grad_ctxt</span> <span class="o">=</span> <span class="n">grad_ctxt</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">grad_ctxt</span><span class="o">.</span><span class="n">grad_state</span><span class="p">:</span>
          <span class="n">forward_ctxt</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">util</span><span class="o">.</span><span class="n">IsLoopExit</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">op</span><span class="p">):</span>
            <span class="n">forward_ctxt</span> <span class="o">=</span> <span class="n">forward_ctxt</span><span class="o">.</span><span class="n">outer_context</span>
            <span class="k">if</span> <span class="n">forward_ctxt</span><span class="p">:</span>
              <span class="n">forward_ctxt</span> <span class="o">=</span> <span class="n">forward_ctxt</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span>
          <span class="k">if</span> <span class="n">forward_ctxt</span> <span class="o">==</span> <span class="n">grad_ctxt</span><span class="o">.</span><span class="n">grad_state</span><span class="o">.</span><span class="n">forward_context</span><span class="p">:</span>
            <span class="n">real_val</span> <span class="o">=</span> <span class="n">grad_ctxt</span><span class="o">.</span><span class="n">grad_state</span><span class="o">.</span><span class="n">GetRealValue</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">real_val</span>
            <span class="k">return</span> <span class="n">real_val</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddValue</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
      <span class="c1"># Create an Enter to make `result` known to this loop context.</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">enter</span> <span class="o">=</span> <span class="n">_Enter</span><span class="p">(</span>
            <span class="n">result</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
            <span class="n">is_constant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">parallel_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span><span class="p">)</span>
        <span class="n">enter</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">prevent_feeding</span><span class="p">(</span><span class="n">enter</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddInnerOp</span><span class="p">(</span><span class="n">enter</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># Fix the control inputs and control flow context of these enter ops.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_FixControlInputsAndContext</span><span class="p">([</span><span class="n">enter</span><span class="p">])</span>

      <span class="c1"># Add `enter` in this context.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">enter</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="p">[</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">enter</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">enter</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">actual_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_external_values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">actual_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">actual_val</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">AddOp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add `op` to the current context.&quot;&quot;&quot;</span>
    <span class="c1"># For a reduction op, if op is in a grad context and its input is from</span>
    <span class="c1"># its forward context, moving op to the forward context means we would</span>
    <span class="c1"># store the tensor after the reduction as opposed to the tensor before</span>
    <span class="c1"># reduction, and therefore could significantly reduce memory consumption.</span>
    <span class="c1"># For now, we do this only for a few ops.</span>
    <span class="c1">#</span>
    <span class="c1"># If in XLA context, do not move constant ops to forward pass as pushing to</span>
    <span class="c1"># and popping from a stack removes the constant property of an op and breaks</span>
    <span class="c1"># XLA compilation, which requires certain inputs to be constant for certain</span>
    <span class="c1"># ops.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">util</span><span class="o">.</span><span class="n">IsInXLAContext</span><span class="p">(</span><span class="n">op</span><span class="p">)</span> <span class="ow">and</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;Shape&quot;</span><span class="p">,</span> <span class="s2">&quot;Size&quot;</span><span class="p">,</span> <span class="s2">&quot;Rank&quot;</span><span class="p">}:</span>
      <span class="n">grad_ctxt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">grad_ctxt</span><span class="p">:</span>
        <span class="n">grad_ctxt</span> <span class="o">=</span> <span class="n">grad_ctxt</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">grad_ctxt</span><span class="o">.</span><span class="n">grad_state</span><span class="p">:</span>
          <span class="n">op_input_forward_ctxt</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">op_input_forward_ctxt</span> <span class="o">==</span> <span class="n">grad_ctxt</span><span class="o">.</span><span class="n">grad_state</span><span class="o">.</span><span class="n">forward_context</span><span class="p">:</span>
            <span class="n">op_input_ctxt</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>
            <span class="n">op</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="n">op_input_ctxt</span><span class="p">)</span>
            <span class="n">op_input_ctxt</span><span class="o">.</span><span class="n">_AddOpInternal</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
            <span class="k">return</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_AddOpInternal</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_AddOpInternal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add `op` to the current context.</span>

<span class="sd">    We move any external control dependencies of the op to the loop pivot, to</span>
<span class="sd">    ensure they get executed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="c1"># Remove any external control dependency on this op</span>
      <span class="n">control_inputs</span><span class="p">,</span> <span class="n">external_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RemoveExternalControlEdges</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># Add a control edge from the control pivot to this op.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">control_inputs</span><span class="p">:</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">op</span><span class="o">.</span><span class="n">_add_control_input</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GetControlPivot</span><span class="p">()</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
        <span class="c1"># pylint: enable=protected-access</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">real_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddValue</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">real_x</span> <span class="o">!=</span> <span class="n">x</span><span class="p">:</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">real_x</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="c1"># Remove any external control dependency on this op.</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">external_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RemoveExternalControlEdges</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># Add a control dependency to prevent loop invariants from</span>
      <span class="c1"># enabling ops that should not be executed.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_MaybeAddControlDependency</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">external_inputs</span><span class="p">:</span>
      <span class="c1"># Use an identity to pull control inputs as data inputs. Note that we</span>
      <span class="c1"># ignore ops which don&#39;t have outputs. TODO(apassos): fix that</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
        <span class="n">external_inputs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">op</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">external_inputs</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">outputs</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_add_control_inputs</span><span class="p">(</span><span class="n">external_inputs</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">util</span><span class="o">.</span><span class="n">IsLoopExit</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
      <span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">prevent_fetching</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
        <span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">prevent_feeding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddInnerOp</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_MaybeAddControlDependency</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a control input to the op if it only depends on loop invariants.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_IsOpFree</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Determines if `op` needs a control dependency.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_is_function</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">)</span> <span class="ow">or</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;SymbolicGradient&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
      <span class="c1"># pylint: enable=protected-access</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">util</span><span class="o">.</span><span class="n">IsLoopConstantEnter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="p">):</span>
          <span class="k">return</span> <span class="kc">False</span>
      <span class="k">return</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="n">_IsOpFree</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_add_control_input</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GetControlPivot</span><span class="p">()</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">AddForwardLoopCounter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outer_grad_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a loop that counts the number of iterations.</span>

<span class="sd">    This is added to the forward loop at the time when we start to</span>
<span class="sd">    create the loop for backprop gradient computation. Called in</span>
<span class="sd">    the outer context of this forward context.</span>

<span class="sd">    The pseudocode is:</span>
<span class="sd">      `n = 0; while (_pivot) { n++; }`</span>

<span class="sd">    Note that a control dependency is added to `n` to ensure the correct</span>
<span class="sd">    execution order of stack push ops.</span>

<span class="sd">    Args:</span>
<span class="sd">      outer_grad_state: The outer grad state. None if not nested.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The number of iterations taken by the forward loop and the loop index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;f_count&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">outer_grad_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Force the stack pushes of i-th execution of an inner loop to be ordered</span>
      <span class="c1"># before the pushes of (i+1)-th execution of the same inner loop.</span>
      <span class="n">outer_add_op</span> <span class="o">=</span> <span class="n">outer_grad_state</span><span class="o">.</span><span class="n">forward_index</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span>
      <span class="n">n</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_add_control_input</span><span class="p">(</span><span class="n">outer_add_op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">AddName</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">enter_n</span> <span class="o">=</span> <span class="n">_Enter</span><span class="p">(</span>
        <span class="n">n</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
        <span class="n">is_constant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">parallel_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;f_count&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_enters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">enter_n</span><span class="p">)</span>

    <span class="n">merge_n</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">enter_n</span><span class="p">,</span> <span class="n">enter_n</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">switch_n</span> <span class="o">=</span> <span class="n">switch</span><span class="p">(</span><span class="n">merge_n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="p">)</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">switch_n</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">next_n</span> <span class="o">=</span> <span class="n">_NextIteration</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">merge_n</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_n</span><span class="p">)</span>

    <span class="n">total_iterations</span> <span class="o">=</span> <span class="n">exit</span><span class="p">(</span><span class="n">switch_n</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;f_count&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_exits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_iterations</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ExitResult</span><span class="p">([</span><span class="n">total_iterations</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">total_iterations</span><span class="p">,</span> <span class="n">next_n</span>

  <span class="k">def</span> <span class="nf">AddBackpropLoopCounter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">outer_grad_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add the backprop loop that controls the iterations.</span>

<span class="sd">    This is added to the backprop loop. It is used to control the loop</span>
<span class="sd">    termination of the backprop loop. Called in the outer context of</span>
<span class="sd">    this grad context.</span>

<span class="sd">    The pseudocode is:</span>
<span class="sd">      `n = count; while (n &gt;= 1) { n--; }`</span>

<span class="sd">    Note that a control dependency is added to `final_zero` to ensure the</span>
<span class="sd">    correct execution order of stack pop ops.</span>

<span class="sd">    Args:</span>
<span class="sd">      count: The number of iterations for backprop.</span>
<span class="sd">      outer_grad_state: The outer grad state. None if not nested.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The loop index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">in_separate_functions</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">in_separate_functions</span><span class="p">:</span>
      <span class="c1"># Brings the count into this graph</span>
      <span class="n">count</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># TODO(apassos) XLA expects this constant to be created outside the loop,</span>
      <span class="c1"># so doing that for now.</span>
      <span class="n">one</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_count&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">AddName</span><span class="p">(</span><span class="n">count</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">enter_count</span> <span class="o">=</span> <span class="n">_Enter</span><span class="p">(</span>
        <span class="n">count</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
        <span class="n">is_constant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">parallel_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_count&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_enters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">enter_count</span><span class="p">)</span>

    <span class="n">merge_count</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">enter_count</span><span class="p">,</span> <span class="n">enter_count</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_pred</span> <span class="o">=</span> <span class="n">merge_count</span>

    <span class="k">if</span> <span class="n">in_separate_functions</span><span class="p">:</span>
      <span class="n">one</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_count&quot;</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">merge_count</span><span class="p">,</span> <span class="n">one</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span> <span class="o">=</span> <span class="n">loop_cond</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_count&quot;</span><span class="p">)</span>
    <span class="n">switch_count</span> <span class="o">=</span> <span class="n">switch</span><span class="p">(</span><span class="n">merge_count</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="p">)</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">switch_count</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">one</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_body</span> <span class="o">=</span> <span class="n">index</span>
    <span class="n">next_count</span> <span class="o">=</span> <span class="n">_NextIteration</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">merge_count</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_count</span><span class="p">)</span>

    <span class="n">final_zero</span> <span class="o">=</span> <span class="n">exit</span><span class="p">(</span><span class="n">switch_count</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_count&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_exits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">final_zero</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">outer_grad_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Force the stack pops of i-th execution of an inner loop to be ordered</span>
      <span class="c1"># before the pops of (i+1)-th execution of the same inner loop.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">outer_grad_state</span><span class="o">.</span><span class="n">grad_sync</span><span class="o">.</span><span class="n">_add_control_input</span><span class="p">(</span><span class="n">final_zero</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ExitResult</span><span class="p">([</span><span class="n">final_zero</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">next_count</span>

  <span class="k">def</span> <span class="nf">AddBackpropAccumulator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add an accumulation loop for every loop invariant.</span>

<span class="sd">    This is added to the backprop loop. It is used to accumulate partial</span>
<span class="sd">    gradients within each loop iteration. Called when in the gradient while</span>
<span class="sd">    context.</span>

<span class="sd">    The pseudocode is:</span>
<span class="sd">      ```</span>
<span class="sd">      acc = 0.0;</span>
<span class="sd">      while (_pivot) {</span>
<span class="sd">        acc += grad;</span>
<span class="sd">      }</span>
<span class="sd">      ```</span>

<span class="sd">    Args:</span>
<span class="sd">      op: The Enter op for a loop invariant.</span>
<span class="sd">      grad: The partial gradient of an iteration for a loop invariant.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The gradient for a loop invariant.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="c1"># Create a zeros tensor with the right shape for acc. If we don&#39;t</span>
    <span class="c1"># know the full shape statically, we will have to get the shape</span>
    <span class="c1"># dynamically from the forward inference. Getting the shape right</span>
    <span class="c1"># for the zeros is only needed for the base case when the loop exits</span>
    <span class="c1"># without running any iterations.</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
      <span class="n">acc</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">,</span> <span class="n">WhileContext</span><span class="p">)</span> <span class="ow">and</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">grad_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1"># We are in a nested while loop.</span>
        <span class="n">forward_ctxt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_state</span><span class="o">.</span><span class="n">forward_context</span>
        <span class="n">forward_ctxt</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
        <span class="n">zeros_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">forward_ctxt</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
        <span class="n">outer_grad_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_state</span><span class="o">.</span><span class="n">outer_grad_state</span>
        <span class="n">history_zeros_shape</span> <span class="o">=</span> <span class="n">outer_grad_state</span><span class="o">.</span><span class="n">AddForwardAccumulator</span><span class="p">(</span>
            <span class="n">zeros_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
        <span class="n">real_shape</span> <span class="o">=</span> <span class="n">outer_grad_state</span><span class="o">.</span><span class="n">AddBackpropAccumulatedValue</span><span class="p">(</span>
            <span class="n">history_zeros_shape</span><span class="p">,</span> <span class="n">zeros_shape</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">real_shape</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
        <span class="n">zeros_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zeros_shape</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">AddName</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">enter_acc</span> <span class="o">=</span> <span class="n">_Enter</span><span class="p">(</span>
        <span class="n">acc</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
        <span class="n">is_constant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">parallel_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_enters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">enter_acc</span><span class="p">)</span>

    <span class="n">merge_acc</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">enter_acc</span><span class="p">,</span> <span class="n">enter_acc</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">switch_acc_false</span><span class="p">,</span> <span class="n">switch_acc_true</span> <span class="o">=</span> <span class="n">switch</span><span class="p">(</span><span class="n">merge_acc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="p">)</span>

    <span class="n">add_acc</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">switch_acc_true</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
    <span class="n">next_acc</span> <span class="o">=</span> <span class="n">_NextIteration</span><span class="p">(</span><span class="n">add_acc</span><span class="p">)</span>
    <span class="n">merge_acc</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_acc</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="n">result_acc</span> <span class="o">=</span> <span class="n">exit</span><span class="p">(</span><span class="n">switch_acc_false</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_exits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result_acc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ExitResult</span><span class="p">([</span><span class="n">result_acc</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result_acc</span>

  <span class="k">def</span> <span class="nf">AddBackpropIndexedSlicesAccumulator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This is used for accumulating gradients that are IndexedSlices.</span>

<span class="sd">    This is essentially the equivalent of AddBackpropAccumulator but optimized</span>
<span class="sd">    for things like updating embeddings from within a while loop.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: The Enter op for a loop invariant.</span>
<span class="sd">      grad: The partial gradients represented as an IndexedSlices.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The accumulated IndexedSlices gradient of the loop invariant.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span>
    <span class="n">dense_shape</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
      <span class="n">values_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">Dimension</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">+</span>
                                              <span class="n">values</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
      <span class="n">values_acc</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
          <span class="mi">0</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">values_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">values_shape</span> <span class="o">=</span> <span class="n">_resource_safe_shape</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">1</span><span class="p">:]</span>
      <span class="n">values_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="n">values_shape</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">values_acc</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">values_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">indices_acc</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">shape_acc</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">dense_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">dense_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
        <span class="n">shape_acc</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="n">dense_shape</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_acc</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span>
                <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">AddName</span><span class="p">(</span><span class="n">values_acc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">AddName</span><span class="p">(</span><span class="n">indices_acc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">init_acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">indices_acc</span><span class="p">,</span> <span class="n">values_acc</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">shape_acc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">AddName</span><span class="p">(</span><span class="n">shape_acc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">init_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape_acc</span><span class="p">)</span>

    <span class="c1"># Set use_input_shape=False since the accumulator tensors will grow in</span>
    <span class="c1"># size. If use_input_shape=True, the _update_input call below will result in</span>
    <span class="c1"># incompatible shapes.</span>
    <span class="n">enter_acc</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">_Enter</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
            <span class="n">is_constant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">parallel_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span><span class="p">,</span>
            <span class="n">use_input_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">init_acc</span>
    <span class="p">]</span>
    <span class="c1"># Manually set appropriate partial shapes.</span>
    <span class="n">enter_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">values_acc</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">enter_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">values_acc</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_enters</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">enter_acc</span><span class="p">)</span>

    <span class="n">merge_acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">merge</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">enter_acc</span><span class="p">]</span>
    <span class="n">switch_acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">switch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">merge_acc</span><span class="p">]</span>

    <span class="c1"># The actual accumulation.</span>
    <span class="n">acc_indexed_slices</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">xa</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xv</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">xa</span><span class="p">,</span> <span class="n">xv</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">switch_acc</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">])</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">shape_acc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># For the shape we just keep the maximum</span>
      <span class="n">acc_indexed_slices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dense_shape</span><span class="p">,</span> <span class="n">switch_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>

    <span class="n">next_acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">_NextIteration</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">acc_indexed_slices</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">xm</span><span class="p">,</span> <span class="n">xn</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">merge_acc</span><span class="p">,</span> <span class="n">next_acc</span><span class="p">):</span>
      <span class="n">xm</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_update_input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">xn</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="n">exit_acc</span> <span class="o">=</span> <span class="p">[</span><span class="n">exit</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_acc&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">switch_acc</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loop_exits</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">exit_acc</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ExitResult</span><span class="p">(</span><span class="n">exit_acc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">exit_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">values</span><span class="o">=</span><span class="n">exit_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">dense_shape</span><span class="o">=</span><span class="n">exit_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">shape_acc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_InitializeValues</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Makes the values known to this context.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_BuildLoop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">original_loop_vars</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">,</span>
                 <span class="n">shape_invariants</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Core: Add the loop termination condition and body to the graph.&quot;&quot;&quot;</span>
    <span class="n">flat_loop_vars</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">original_loop_vars</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Let the context know the loop variables so the loop variables</span>
    <span class="c1"># would be added in the outer contexts properly.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_InitializeValues</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">)</span>
    <span class="n">real_vars</span> <span class="o">=</span> <span class="n">loop_vars</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
      <span class="n">real_vars</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddValue</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loop_vars</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
      <span class="n">enter_vars</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">_Enter</span><span class="p">(</span>
              <span class="n">x</span><span class="p">,</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
              <span class="n">is_constant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">parallel_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_parallel_iterations</span><span class="p">,</span>
              <span class="n">use_input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">shape_invariants</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">real_vars</span>
      <span class="p">]</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">enter_vars</span><span class="p">:</span>
        <span class="n">x</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">prevent_feeding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span><span class="o">.</span><span class="n">AddInnerOp</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># Finds the closest enclosing non-None control pivot.</span>
    <span class="n">outer_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outer_context</span>
    <span class="n">control_pivot</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">while</span> <span class="n">outer_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">control_pivot</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">control_pivot</span> <span class="o">=</span> <span class="n">outer_context</span><span class="o">.</span><span class="n">GetControlPivot</span><span class="p">()</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">outer_context</span> <span class="o">=</span> <span class="n">outer_context</span><span class="o">.</span><span class="n">_outer_context</span>
      <span class="c1"># pylint: enable=protected-access</span>

    <span class="k">if</span> <span class="n">control_pivot</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">enter_vars</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">util</span><span class="o">.</span><span class="n">IsLoopConstantEnter</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="p">):</span>
          <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">var</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_add_control_input</span><span class="p">(</span><span class="n">control_pivot</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
          <span class="c1"># pylint: enable=protected-access</span>
    <span class="n">_SetShapeInvariants</span><span class="p">(</span><span class="n">real_vars</span><span class="p">,</span> <span class="n">enter_vars</span><span class="p">,</span> <span class="n">shape_invariants</span><span class="p">)</span>

    <span class="c1"># Fix the control inputs and control flow context of these enter ops.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_FixControlInputsAndContext</span><span class="p">(</span><span class="n">enter_vars</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_InitializeValues</span><span class="p">(</span><span class="n">enter_vars</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loop_enters</span> <span class="o">=</span> <span class="n">enter_vars</span>

    <span class="n">merge_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">merge</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">enter_vars</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_pred</span> <span class="o">=</span> <span class="n">merge_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Build the graph for pred.</span>
    <span class="n">merge_vars_with_tensor_arrays</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">_convert_flows_to_tensorarrays</span><span class="p">(</span><span class="n">flat_loop_vars</span><span class="p">,</span> <span class="n">merge_vars</span><span class="p">))</span>
    <span class="n">packed_vars</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span>
        <span class="n">structure</span><span class="o">=</span><span class="n">original_loop_vars</span><span class="p">,</span>
        <span class="n">flat_sequence</span><span class="o">=</span><span class="n">merge_vars_with_tensor_arrays</span><span class="p">,</span>
        <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">(</span><span class="o">*</span><span class="n">packed_vars</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span> <span class="o">=</span> <span class="n">loop_cond</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;LoopCond&quot;</span><span class="p">)</span>
    <span class="n">switch_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">_SwitchRefOrTensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pivot</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">merge_vars</span><span class="p">]</span>

    <span class="c1"># Build the graph for body.</span>
    <span class="n">vars_for_body</span> <span class="o">=</span> <span class="p">[</span><span class="n">_Identity</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">switch_vars</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pivot_for_body</span> <span class="o">=</span> <span class="n">vars_for_body</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Convert TensorArray flow variables inside the context back into</span>
    <span class="c1"># their associated TensorArrays for calling the body.</span>
    <span class="n">vars_for_body_with_tensor_arrays</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">_convert_flows_to_tensorarrays</span><span class="p">(</span><span class="n">flat_loop_vars</span><span class="p">,</span> <span class="n">vars_for_body</span><span class="p">))</span>
    <span class="n">packed_vars_for_body</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span>
        <span class="n">structure</span><span class="o">=</span><span class="n">original_loop_vars</span><span class="p">,</span>
        <span class="n">flat_sequence</span><span class="o">=</span><span class="n">vars_for_body_with_tensor_arrays</span><span class="p">,</span>
        <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">pre_summaries</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">_SUMMARY_COLLECTION</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">body_result</span> <span class="o">=</span> <span class="n">body</span><span class="p">(</span><span class="o">*</span><span class="n">packed_vars_for_body</span><span class="p">)</span>
    <span class="n">post_summaries</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">_SUMMARY_COLLECTION</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence_or_composite</span><span class="p">(</span><span class="n">body_result</span><span class="p">):</span>
      <span class="n">body_result</span> <span class="o">=</span> <span class="p">[</span><span class="n">body_result</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">post_summaries</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">pre_summaries</span><span class="p">):</span>
      <span class="n">new_summaries</span> <span class="o">=</span> <span class="n">post_summaries</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">pre_summaries</span><span class="p">):]</span>
      <span class="n">summary_ref</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">_SUMMARY_COLLECTION</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">summary_ref</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">pre_summaries</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">new_summaries</span><span class="p">):</span>

        <span class="k">def</span> <span class="nf">map_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
          <span class="c1"># TODO(apassos) figure out how to trigger with tensor arrays as well</span>
          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">x</span>
          <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">body_result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
            <span class="n">map_fn</span><span class="p">,</span> <span class="n">body_result</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Compare the structure types of input and output of body.</span>
    <span class="c1"># For backwards compatibility, the first layer is forced to a list</span>
    <span class="c1"># during this comparison, because inputs are typically lists and</span>
    <span class="c1"># outputs of the body are typically tuples.</span>
    <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">packed_vars_for_body</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">body_result</span><span class="p">),</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Store body_result to keep track of TensorArrays returned by body</span>
    <span class="n">original_body_result</span> <span class="o">=</span> <span class="n">body_result</span>
    <span class="c1"># Convert TensorArrays returned by body into their flow variables</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
        <span class="n">_convert_tensorarray_to_flow</span><span class="p">,</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">body_result</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor_or_composite</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="c1"># Add NextIteration and the back edges to complete the loop.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">merge_vars</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of inputs and outputs of body must match &quot;</span>
                       <span class="s2">&quot;loop_vars: </span><span class="si">%d</span><span class="s2">, </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">merge_vars</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)))</span>
    <span class="n">next_vars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">merge_vars</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
      <span class="n">next_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_AddNextAndBackEdge</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>

    <span class="c1"># Add the exit ops.</span>
    <span class="n">exit_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">exit</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">switch_vars</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loop_exits</span> <span class="o">=</span> <span class="n">exit_vars</span>

    <span class="c1"># Exit the loop.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ExitResult</span><span class="p">(</span><span class="n">exit_vars</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">original_body_result</span><span class="p">,</span> <span class="n">exit_vars</span>

  <span class="k">def</span> <span class="nf">BuildLoop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">,</span> <span class="n">shape_invariants</span><span class="p">,</span>
                <span class="n">return_same_structure</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add the loop termination condition and body to the graph.&quot;&quot;&quot;</span>

    <span class="c1"># Keep original_loop_vars to identify which are TensorArrays</span>
    <span class="n">original_loop_vars</span> <span class="o">=</span> <span class="n">loop_vars</span>
    <span class="c1"># Convert TensorArrays to their flow variables</span>
    <span class="n">loop_vars</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
        <span class="n">_convert_tensorarray_to_flow</span><span class="p">,</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loop_vars</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor_or_composite</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shape_invariants</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">shape_invariants</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">_get_shape_invariant</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">loop_vars</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
      <span class="c1"># _BuildLoop calls _update_input in several places. _mutation_lock()</span>
      <span class="c1"># ensures a Session.run call cannot occur between creating and mutating</span>
      <span class="c1"># new ops.</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_mutation_lock</span><span class="p">():</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">original_body_result</span><span class="p">,</span> <span class="n">exit_vars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_BuildLoop</span><span class="p">(</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">original_loop_vars</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">,</span> <span class="n">shape_invariants</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>

    <span class="n">flat_result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">original_body_result</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Convert TensorArray flow variables outside the context back into</span>
    <span class="c1"># their associated TensorArrays for returning to caller.</span>
    <span class="n">exit_vars_with_tensor_arrays</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">_convert_flows_to_tensorarrays</span><span class="p">(</span><span class="n">flat_result</span><span class="p">,</span> <span class="n">exit_vars</span><span class="p">))</span>
    <span class="n">packed_exit_vars</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span>
        <span class="n">structure</span><span class="o">=</span><span class="n">original_body_result</span><span class="p">,</span>
        <span class="n">flat_sequence</span><span class="o">=</span><span class="n">exit_vars_with_tensor_arrays</span><span class="p">,</span>
        <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_same_structure</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">packed_exit_vars</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">packed_exit_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">exit_vars</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">packed_exit_vars</span>

  <span class="k">def</span> <span class="nf">_FixControlInputsAndContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enters</span><span class="p">):</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">enters</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> not supported&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">:</span>
        <span class="n">inp_op</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span>
        <span class="n">control_inputs</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">_control_dependencies_for_inputs</span><span class="p">([</span><span class="n">inp_op</span><span class="p">])</span>
        <span class="n">outer_control_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
          <span class="c1"># We need to keep control inputs that are in any ancestor</span>
          <span class="c1"># ControlFlowContext, and within outer WhileContext.</span>
          <span class="n">keep_as_control_input</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="n">op_ctxt</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">GetOutputContext</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
          <span class="n">outer_ctxt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_context</span>
          <span class="n">outer_while_context</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">outer_ctxt</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
                                 <span class="n">outer_ctxt</span><span class="o">.</span><span class="n">GetWhileContext</span><span class="p">())</span>
          <span class="k">while</span> <span class="n">outer_ctxt</span> <span class="o">!=</span> <span class="n">op_ctxt</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">outer_ctxt</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">outer_ctxt</span> <span class="o">==</span> <span class="n">outer_while_context</span><span class="p">:</span>
              <span class="n">keep_as_control_input</span> <span class="o">=</span> <span class="kc">False</span>
              <span class="k">break</span>
            <span class="n">outer_ctxt</span> <span class="o">=</span> <span class="n">outer_ctxt</span><span class="o">.</span><span class="n">outer_context</span>
          <span class="k">if</span> <span class="n">keep_as_control_input</span><span class="p">:</span>
            <span class="n">outer_control_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_add_control_inputs</span><span class="p">(</span><span class="n">outer_control_inputs</span><span class="p">)</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">_record_op_seen_by_control_dependencies</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">IsWhileContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="c1"># @TODO(b/133606651) Replace &quot;shape_invariants&quot; with &quot;loop_vars_signature&quot;.</span>
<span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;while_loop&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_arg_values</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span>
    <span class="sd">&quot;&quot;&quot;back_prop=False is deprecated. Consider using tf.stop_gradient instead.</span>
<span class="sd">Instead of:</span>
<span class="sd">results = tf.while_loop(c, b, vars, back_prop=False)</span>
<span class="sd">Use:</span>
<span class="sd">results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">warn_once</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">back_prop</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">while_loop_v2</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span>
                  <span class="n">body</span><span class="p">,</span>
                  <span class="n">loop_vars</span><span class="p">,</span>
                  <span class="n">shape_invariants</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">parallel_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                  <span class="n">back_prop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">swap_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                  <span class="n">maximum_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Repeat `body` while the condition `cond` is true.</span>

<span class="sd">  `cond` is a callable returning a boolean scalar tensor. `body` is a callable</span>
<span class="sd">  returning a (possibly nested) tuple, namedtuple or list of tensors of the same</span>
<span class="sd">  arity (length and structure) and types as `loop_vars`. `loop_vars` is a</span>
<span class="sd">  (possibly nested) tuple, namedtuple or list of tensors that is passed to both</span>
<span class="sd">  `cond` and `body`. `cond` and `body` both take as many arguments as there are</span>
<span class="sd">  `loop_vars`.</span>

<span class="sd">  In addition to regular Tensors or IndexedSlices, the body may accept and</span>
<span class="sd">  return TensorArray objects.  The flows of the TensorArray objects will</span>
<span class="sd">  be appropriately forwarded between loops and during gradient calculations.</span>

<span class="sd">  Note that `while_loop` calls `cond` and `body` *exactly once* (inside the</span>
<span class="sd">  call to `while_loop`, and not at all during `Session.run()`). `while_loop`</span>
<span class="sd">  stitches together the graph fragments created during the `cond` and `body`</span>
<span class="sd">  calls with some additional graph nodes to create the graph flow that</span>
<span class="sd">  repeats `body` until `cond` returns false.</span>

<span class="sd">  For correctness, `tf.while_loop()` strictly enforces shape invariants for</span>
<span class="sd">  the loop variables. A shape invariant is a (possibly partial) shape that</span>
<span class="sd">  is unchanged across the iterations of the loop. An error will be raised</span>
<span class="sd">  if the shape of a loop variable after an iteration is determined to be more</span>
<span class="sd">  general than or incompatible with its shape invariant. For example, a shape</span>
<span class="sd">  of [11, None] is more general than a shape of [11, 17], and [11, 21] is not</span>
<span class="sd">  compatible with [11, 17]. By default (if the argument `shape_invariants` is</span>
<span class="sd">  not specified), it is assumed that the initial shape of each tensor in</span>
<span class="sd">  `loop_vars` is the same in every iteration. The `shape_invariants` argument</span>
<span class="sd">  allows the caller to specify a less specific shape invariant for each loop</span>
<span class="sd">  variable, which is needed if the shape varies between iterations. The</span>
<span class="sd">  `tf.Tensor.set_shape`</span>
<span class="sd">  function may also be used in the `body` function to indicate that</span>
<span class="sd">  the output loop variable has a particular shape. The shape invariant for</span>
<span class="sd">  SparseTensor and IndexedSlices are treated specially as follows:</span>

<span class="sd">  a) If a loop variable is a SparseTensor, the shape invariant must be</span>
<span class="sd">  TensorShape([r]) where r is the rank of the dense tensor represented</span>
<span class="sd">  by the sparse tensor. It means the shapes of the three tensors of the</span>
<span class="sd">  SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here</span>
<span class="sd">  is the shape of the SparseTensor.dense_shape property. It must be the shape of</span>
<span class="sd">  a vector.</span>

<span class="sd">  b) If a loop variable is an IndexedSlices, the shape invariant must be</span>
<span class="sd">  a shape invariant of the values tensor of the IndexedSlices. It means</span>
<span class="sd">  the shapes of the three tensors of the IndexedSlices are (shape, [shape[0]],</span>
<span class="sd">  [shape.ndims]).</span>

<span class="sd">  `while_loop` implements non-strict semantics, enabling multiple iterations</span>
<span class="sd">  to run in parallel. The maximum number of parallel iterations can be</span>
<span class="sd">  controlled by `parallel_iterations`, which gives users some control over</span>
<span class="sd">  memory consumption and execution order. For correct programs, `while_loop`</span>
<span class="sd">  should return the same result for any parallel_iterations &gt; 0.</span>

<span class="sd">  For training, TensorFlow stores the tensors that are produced in the</span>
<span class="sd">  forward inference and are needed in back propagation. These tensors are a</span>
<span class="sd">  main source of memory consumption and often cause OOM errors when training</span>
<span class="sd">  on GPUs. When the flag swap_memory is true, we swap out these tensors from</span>
<span class="sd">  GPU to CPU. This for example allows us to train RNN models with very long</span>
<span class="sd">  sequences and large batches.</span>

<span class="sd">  Args:</span>
<span class="sd">    cond: A callable that represents the termination condition of the loop.</span>
<span class="sd">    body: A callable that represents the loop body.</span>
<span class="sd">    loop_vars: A (possibly nested) tuple, namedtuple or list of numpy array,</span>
<span class="sd">      `Tensor`, and `TensorArray` objects.</span>
<span class="sd">    shape_invariants: The shape invariants for the loop variables.</span>
<span class="sd">    parallel_iterations: The number of iterations allowed to run in parallel. It</span>
<span class="sd">      must be a positive integer.</span>
<span class="sd">    back_prop: (optional) Deprecated. False disables support for back</span>
<span class="sd">      propagation. Prefer using `tf.stop_gradient` instead.</span>
<span class="sd">    swap_memory: Whether GPU-CPU memory swap is enabled for this loop.</span>
<span class="sd">    maximum_iterations: Optional maximum number of iterations of the while loop</span>
<span class="sd">      to run.  If provided, the `cond` output is AND-ed with an additional</span>
<span class="sd">      condition ensuring the number of iterations executed is no greater than</span>
<span class="sd">      `maximum_iterations`.</span>
<span class="sd">    name: Optional name prefix for the returned tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The output tensors for the loop variables after the loop. The return value</span>
<span class="sd">      has the same structure as `loop_vars`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `cond` or `body` is not callable.</span>
<span class="sd">    ValueError: if `loop_vars` is empty.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```python</span>
<span class="sd">  i = tf.constant(0)</span>
<span class="sd">  c = lambda i: tf.less(i, 10)</span>
<span class="sd">  b = lambda i: (tf.add(i, 1), )</span>
<span class="sd">  r = tf.while_loop(c, b, [i])</span>
<span class="sd">  ```</span>

<span class="sd">  Example with nesting and a namedtuple:</span>

<span class="sd">  ```python</span>
<span class="sd">  import collections</span>
<span class="sd">  Pair = collections.namedtuple(&#39;Pair&#39;, &#39;j, k&#39;)</span>
<span class="sd">  ijk_0 = (tf.constant(0), Pair(tf.constant(1), tf.constant(2)))</span>
<span class="sd">  c = lambda i, p: i &lt; 10</span>
<span class="sd">  b = lambda i, p: (i + 1, Pair((p.j + p.k), (p.j - p.k)))</span>
<span class="sd">  ijk_final = tf.while_loop(c, b, ijk_0)</span>
<span class="sd">  ```</span>

<span class="sd">  Example using shape_invariants:</span>

<span class="sd">  ```python</span>
<span class="sd">  i0 = tf.constant(0)</span>
<span class="sd">  m0 = tf.ones([2, 2])</span>
<span class="sd">  c = lambda i, m: i &lt; 10</span>
<span class="sd">  b = lambda i, m: [i+1, tf.concat([m, m], axis=0)]</span>
<span class="sd">  tf.while_loop(</span>
<span class="sd">      c, b, loop_vars=[i0, m0],</span>
<span class="sd">      shape_invariants=[i0.get_shape(), tf.TensorShape([None, 2])])</span>
<span class="sd">  ```</span>

<span class="sd">  Example which demonstrates non-strict semantics: In the following</span>
<span class="sd">  example, the final value of the counter `i` does not depend on `x`. So</span>
<span class="sd">  the `while_loop` can increment the counter parallel to updates of `x`.</span>
<span class="sd">  However, because the loop counter at one loop iteration depends</span>
<span class="sd">  on the value at the previous iteration, the loop counter itself cannot</span>
<span class="sd">  be incremented in parallel. Hence if we just want the final value of the</span>
<span class="sd">  counter (which we print on the line `print(sess.run(i))`), then</span>
<span class="sd">  `x` will never be incremented, but the counter will be updated on a</span>
<span class="sd">  single thread. Conversely, if we want the value of the output (which we</span>
<span class="sd">  print on the line `print(sess.run(out).shape)`), then the counter may be</span>
<span class="sd">  incremented on its own thread, while `x` can be incremented in</span>
<span class="sd">  parallel on a separate thread. In the extreme case, it is conceivable</span>
<span class="sd">  that the thread incrementing the counter runs until completion before</span>
<span class="sd">  `x` is incremented even a single time. The only thing that can never</span>
<span class="sd">  happen is that the thread updating `x` can never get ahead of the</span>
<span class="sd">  counter thread because the thread incrementing `x` depends on the value</span>
<span class="sd">  of the counter.</span>

<span class="sd">  ```python</span>
<span class="sd">  import tensorflow as tf</span>

<span class="sd">  n = 10000</span>
<span class="sd">  x = tf.constant(list(range(n)))</span>
<span class="sd">  c = lambda i, x: i &lt; n</span>
<span class="sd">  b = lambda i, x: (tf.compat.v1.Print(i + 1, [i]), tf.compat.v1.Print(x + 1,</span>
<span class="sd">  [i], &quot;x:&quot;))</span>
<span class="sd">  i, out = tf.while_loop(c, b, (0, x))</span>
<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">      print(sess.run(i))  # prints [0] ... [9999]</span>

<span class="sd">      # The following line may increment the counter and x in parallel.</span>
<span class="sd">      # The counter thread may get ahead of the other thread, but not the</span>
<span class="sd">      # other way around. So you may see things like</span>
<span class="sd">      # [9996] x:[9987]</span>
<span class="sd">      # meaning that the counter thread is on iteration 9996,</span>
<span class="sd">      # while the other thread is on iteration 9987</span>
<span class="sd">      print(sess.run(out).shape)</span>
<span class="sd">  ```</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">while_loop</span><span class="p">(</span>
      <span class="n">cond</span><span class="o">=</span><span class="n">cond</span><span class="p">,</span>
      <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span>
      <span class="n">loop_vars</span><span class="o">=</span><span class="n">loop_vars</span><span class="p">,</span>
      <span class="n">shape_invariants</span><span class="o">=</span><span class="n">shape_invariants</span><span class="p">,</span>
      <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">,</span>
      <span class="n">back_prop</span><span class="o">=</span><span class="n">back_prop</span><span class="p">,</span>
      <span class="n">swap_memory</span><span class="o">=</span><span class="n">swap_memory</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">maximum_iterations</span><span class="o">=</span><span class="n">maximum_iterations</span><span class="p">,</span>
      <span class="n">return_same_structure</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="c1"># pylint: disable=redefined-outer-name</span>
<div class="viewcode-block" id="while_loop"><a class="viewcode-back" href="../../../../index.html#tensorflow.while_loop">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;while_loop&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">while_loop</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span>
               <span class="n">body</span><span class="p">,</span>
               <span class="n">loop_vars</span><span class="p">,</span>
               <span class="n">shape_invariants</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">parallel_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
               <span class="n">back_prop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">swap_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">maximum_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">return_same_structure</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Repeat `body` while the condition `cond` is true.</span>

<span class="sd">  `cond` is a callable returning a boolean scalar tensor. `body` is a callable</span>
<span class="sd">  returning a (possibly nested) tuple, namedtuple or list of tensors of the same</span>
<span class="sd">  arity (length and structure) and types as `loop_vars`. `loop_vars` is a</span>
<span class="sd">  (possibly nested) tuple, namedtuple or list of tensors that is passed to both</span>
<span class="sd">  `cond` and `body`. `cond` and `body` both take as many arguments as there are</span>
<span class="sd">  `loop_vars`.</span>

<span class="sd">  In addition to regular Tensors or IndexedSlices, the body may accept and</span>
<span class="sd">  return TensorArray objects.  The flows of the TensorArray objects will</span>
<span class="sd">  be appropriately forwarded between loops and during gradient calculations.</span>

<span class="sd">  Note that `while_loop` calls `cond` and `body` *exactly once* (inside the</span>
<span class="sd">  call to `while_loop`, and not at all during `Session.run()`). `while_loop`</span>
<span class="sd">  stitches together the graph fragments created during the `cond` and `body`</span>
<span class="sd">  calls with some additional graph nodes to create the graph flow that</span>
<span class="sd">  repeats `body` until `cond` returns false.</span>

<span class="sd">  For correctness, `tf.while_loop()` strictly enforces shape invariants for</span>
<span class="sd">  the loop variables. A shape invariant is a (possibly partial) shape that</span>
<span class="sd">  is unchanged across the iterations of the loop. An error will be raised</span>
<span class="sd">  if the shape of a loop variable after an iteration is determined to be more</span>
<span class="sd">  general than or incompatible with its shape invariant. For example, a shape</span>
<span class="sd">  of [11, None] is more general than a shape of [11, 17], and [11, 21] is not</span>
<span class="sd">  compatible with [11, 17]. By default (if the argument `shape_invariants` is</span>
<span class="sd">  not specified), it is assumed that the initial shape of each tensor in</span>
<span class="sd">  `loop_vars` is the same in every iteration. The `shape_invariants` argument</span>
<span class="sd">  allows the caller to specify a less specific shape invariant for each loop</span>
<span class="sd">  variable, which is needed if the shape varies between iterations. The</span>
<span class="sd">  `tf.Tensor.set_shape`</span>
<span class="sd">  function may also be used in the `body` function to indicate that</span>
<span class="sd">  the output loop variable has a particular shape. The shape invariant for</span>
<span class="sd">  SparseTensor and IndexedSlices are treated specially as follows:</span>

<span class="sd">  a) If a loop variable is a SparseTensor, the shape invariant must be</span>
<span class="sd">  TensorShape([r]) where r is the rank of the dense tensor represented</span>
<span class="sd">  by the sparse tensor. It means the shapes of the three tensors of the</span>
<span class="sd">  SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here</span>
<span class="sd">  is the shape of the SparseTensor.dense_shape property. It must be the shape of</span>
<span class="sd">  a vector.</span>

<span class="sd">  b) If a loop variable is an IndexedSlices, the shape invariant must be</span>
<span class="sd">  a shape invariant of the values tensor of the IndexedSlices. It means</span>
<span class="sd">  the shapes of the three tensors of the IndexedSlices are (shape, [shape[0]],</span>
<span class="sd">  [shape.ndims]).</span>

<span class="sd">  `while_loop` implements non-strict semantics, enabling multiple iterations</span>
<span class="sd">  to run in parallel. The maximum number of parallel iterations can be</span>
<span class="sd">  controlled by `parallel_iterations`, which gives users some control over</span>
<span class="sd">  memory consumption and execution order. For correct programs, `while_loop`</span>
<span class="sd">  should return the same result for any parallel_iterations &gt; 0.</span>

<span class="sd">  For training, TensorFlow stores the tensors that are produced in the</span>
<span class="sd">  forward inference and are needed in back propagation. These tensors are a</span>
<span class="sd">  main source of memory consumption and often cause OOM errors when training</span>
<span class="sd">  on GPUs. When the flag swap_memory is true, we swap out these tensors from</span>
<span class="sd">  GPU to CPU. This for example allows us to train RNN models with very long</span>
<span class="sd">  sequences and large batches.</span>

<span class="sd">  Args:</span>
<span class="sd">    cond: A callable that represents the termination condition of the loop.</span>
<span class="sd">    body: A callable that represents the loop body.</span>
<span class="sd">    loop_vars: A (possibly nested) tuple, namedtuple or list of numpy array,</span>
<span class="sd">      `Tensor`, and `TensorArray` objects.</span>
<span class="sd">    shape_invariants: The shape invariants for the loop variables.</span>
<span class="sd">    parallel_iterations: The number of iterations allowed to run in parallel. It</span>
<span class="sd">      must be a positive integer.</span>
<span class="sd">    back_prop: Whether backprop is enabled for this while loop.</span>
<span class="sd">    swap_memory: Whether GPU-CPU memory swap is enabled for this loop.</span>
<span class="sd">    name: Optional name prefix for the returned tensors.</span>
<span class="sd">    maximum_iterations: Optional maximum number of iterations of the while loop</span>
<span class="sd">      to run.  If provided, the `cond` output is AND-ed with an additional</span>
<span class="sd">      condition ensuring the number of iterations executed is no greater than</span>
<span class="sd">      `maximum_iterations`.</span>
<span class="sd">    return_same_structure: If True, output has same structure as `loop_vars`. If</span>
<span class="sd">      eager execution is enabled, this is ignored (and always treated as True).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The output tensors for the loop variables after the loop.</span>
<span class="sd">     If `return_same_structure` is True, the return value has the same</span>
<span class="sd">     structure as `loop_vars`.</span>
<span class="sd">     If `return_same_structure` is False, the return value is a Tensor,</span>
<span class="sd">     TensorArray or IndexedSlice if the length of `loop_vars` is 1, or a list</span>
<span class="sd">     otherwise.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `cond` or `body` is not callable.</span>
<span class="sd">    ValueError: if `loop_vars` is empty.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```python</span>
<span class="sd">  i = tf.constant(0)</span>
<span class="sd">  c = lambda i: tf.less(i, 10)</span>
<span class="sd">  b = lambda i: tf.add(i, 1)</span>
<span class="sd">  r = tf.while_loop(c, b, [i])</span>
<span class="sd">  ```</span>

<span class="sd">  Example with nesting and a namedtuple:</span>

<span class="sd">  ```python</span>
<span class="sd">  import collections</span>
<span class="sd">  Pair = collections.namedtuple(&#39;Pair&#39;, &#39;j, k&#39;)</span>
<span class="sd">  ijk_0 = (tf.constant(0), Pair(tf.constant(1), tf.constant(2)))</span>
<span class="sd">  c = lambda i, p: i &lt; 10</span>
<span class="sd">  b = lambda i, p: (i + 1, Pair((p.j + p.k), (p.j - p.k)))</span>
<span class="sd">  ijk_final = tf.while_loop(c, b, ijk_0)</span>
<span class="sd">  ```</span>

<span class="sd">  Example using shape_invariants:</span>

<span class="sd">  ```python</span>
<span class="sd">  i0 = tf.constant(0)</span>
<span class="sd">  m0 = tf.ones([2, 2])</span>
<span class="sd">  c = lambda i, m: i &lt; 10</span>
<span class="sd">  b = lambda i, m: [i+1, tf.concat([m, m], axis=0)]</span>
<span class="sd">  tf.while_loop(</span>
<span class="sd">      c, b, loop_vars=[i0, m0],</span>
<span class="sd">      shape_invariants=[i0.get_shape(), tf.TensorShape([None, 2])])</span>
<span class="sd">  ```</span>

<span class="sd">  Example which demonstrates non-strict semantics: In the following</span>
<span class="sd">  example, the final value of the counter `i` does not depend on `x`. So</span>
<span class="sd">  the `while_loop` can increment the counter parallel to updates of `x`.</span>
<span class="sd">  However, because the loop counter at one loop iteration depends</span>
<span class="sd">  on the value at the previous iteration, the loop counter itself cannot</span>
<span class="sd">  be incremented in parallel. Hence if we just want the final value of the</span>
<span class="sd">  counter (which we print on the line `print(sess.run(i))`), then</span>
<span class="sd">  `x` will never be incremented, but the counter will be updated on a</span>
<span class="sd">  single thread. Conversely, if we want the value of the output (which we</span>
<span class="sd">  print on the line `print(sess.run(out).shape)`), then the counter may be</span>
<span class="sd">  incremented on its own thread, while `x` can be incremented in</span>
<span class="sd">  parallel on a separate thread. In the extreme case, it is conceivable</span>
<span class="sd">  that the thread incrementing the counter runs until completion before</span>
<span class="sd">  `x` is incremented even a single time. The only thing that can never</span>
<span class="sd">  happen is that the thread updating `x` can never get ahead of the</span>
<span class="sd">  counter thread because the thread incrementing `x` depends on the value</span>
<span class="sd">  of the counter.</span>

<span class="sd">  ```python</span>
<span class="sd">  import tensorflow as tf</span>

<span class="sd">  n = 10000</span>
<span class="sd">  x = tf.constant(list(range(n)))</span>
<span class="sd">  c = lambda i, x: i &lt; n</span>
<span class="sd">  b = lambda i, x: (tf.compat.v1.Print(i + 1, [i]), tf.compat.v1.Print(x + 1,</span>
<span class="sd">  [i], &quot;x:&quot;))</span>
<span class="sd">  i, out = tf.while_loop(c, b, (0, x))</span>
<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">      print(sess.run(i))  # prints [0] ... [9999]</span>

<span class="sd">      # The following line may increment the counter and x in parallel.</span>
<span class="sd">      # The counter thread may get ahead of the other thread, but not the</span>
<span class="sd">      # other way around. So you may see things like</span>
<span class="sd">      # [9996] x:[9987]</span>
<span class="sd">      # meaning that the counter thread is on iteration 9996,</span>
<span class="sd">      # while the other thread is on iteration 9987</span>
<span class="sd">      print(sess.run(out).shape)</span>
<span class="sd">  ```</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">cond</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cond must be callable.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">body</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;body must be callable.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">parallel_iterations</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parallel_iterations must be a positive integer.&quot;</span><span class="p">)</span>

  <span class="c1"># Always enable control flow v2 if building a function, regardless of toggle.</span>
  <span class="n">executing_eagerly</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">util</span><span class="o">.</span><span class="n">EnableControlFlowV2</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="n">executing_eagerly</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">while_v2</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
        <span class="n">cond</span><span class="p">,</span>
        <span class="n">body</span><span class="p">,</span>
        <span class="n">loop_vars</span><span class="p">,</span>
        <span class="n">shape_invariants</span><span class="o">=</span><span class="n">shape_invariants</span><span class="p">,</span>
        <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">,</span>
        <span class="n">maximum_iterations</span><span class="o">=</span><span class="n">maximum_iterations</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">return_same_structure</span><span class="o">=</span><span class="n">return_same_structure</span><span class="p">,</span>
        <span class="n">back_prop</span><span class="o">=</span><span class="n">back_prop</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;while&quot;</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">loop_vars</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No loop variables provided&quot;</span><span class="p">)</span>
    <span class="n">try_to_pack</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">return_same_structure</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">maximum_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">maximum_iterations</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">maximum_iterations</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;maximum_iterations&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">maximum_iterations</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;maximum_iterations must be a scalar, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                         <span class="n">maximum_iterations</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">executing_eagerly</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">maximum_iterations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">maximum_iterations</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">maximum_iterations</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;iteration_counter&quot;</span><span class="p">)</span>
      <span class="n">orig_cond</span> <span class="o">=</span> <span class="n">cond</span>
      <span class="n">orig_body</span> <span class="o">=</span> <span class="n">body</span>
      <span class="k">if</span> <span class="n">try_to_pack</span><span class="p">:</span>
        <span class="n">loop_vars</span> <span class="o">=</span> <span class="p">(</span><span class="n">counter</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">lv</span><span class="p">:</span> <span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">maximum_iterations</span><span class="p">,</span> <span class="n">orig_cond</span><span class="p">(</span><span class="n">lv</span><span class="p">)))</span>
        <span class="n">body</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">lv</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">orig_body</span><span class="p">(</span><span class="n">lv</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">loop_vars</span> <span class="o">=</span> <span class="p">(</span><span class="n">counter</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">)</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">lv</span><span class="p">:</span> <span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">maximum_iterations</span><span class="p">,</span> <span class="n">orig_cond</span><span class="p">(</span><span class="o">*</span><span class="n">lv</span><span class="p">)))</span>
        <span class="n">body</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">lv</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">orig_body</span><span class="p">(</span><span class="o">*</span><span class="n">lv</span><span class="p">))</span>
      <span class="n">try_to_pack</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">executing_eagerly</span><span class="p">:</span>
      <span class="n">packed</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># whether the body result was packed into a 1-item tuple</span>

      <span class="n">loop_var_structure</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">type_spec</span><span class="o">.</span><span class="n">type_spec_from_value</span><span class="p">,</span>
                                              <span class="nb">list</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">))</span>
      <span class="k">while</span> <span class="n">cond</span><span class="p">(</span><span class="o">*</span><span class="n">loop_vars</span><span class="p">):</span>
        <span class="n">loop_vars</span> <span class="o">=</span> <span class="n">body</span><span class="p">(</span><span class="o">*</span><span class="n">loop_vars</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">try_to_pack</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">)):</span>
          <span class="n">packed</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="n">loop_vars</span> <span class="o">=</span> <span class="p">(</span><span class="n">loop_vars</span><span class="p">,)</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">loop_var_structure</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">loop_vars</span><span class="p">))</span>

      <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

      <span class="n">loop_vars</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">convert</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">maximum_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loop_vars</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loop_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">packed</span> <span class="k">else</span> <span class="n">loop_vars</span>

    <span class="k">if</span> <span class="n">shape_invariants</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">maximum_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape_invariants</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([]),</span> <span class="n">shape_invariants</span><span class="p">)</span>

      <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span>
          <span class="n">loop_vars</span><span class="p">,</span> <span class="n">shape_invariants</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">shape_invariants</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
          <span class="n">_get_shape_invariant</span><span class="p">,</span>
          <span class="n">loop_vars</span><span class="p">,</span>
          <span class="n">shape_invariants</span><span class="p">,</span>
          <span class="n">expand_composites</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">loop_context</span> <span class="o">=</span> <span class="n">WhileContext</span><span class="p">(</span>
        <span class="n">maximum_iterations</span><span class="o">=</span><span class="n">maximum_iterations</span><span class="p">,</span>
        <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">,</span>
        <span class="n">back_prop</span><span class="o">=</span><span class="n">back_prop</span><span class="p">,</span>
        <span class="n">swap_memory</span><span class="o">=</span><span class="n">swap_memory</span><span class="p">)</span>
    <span class="c1"># Only add non-nested loops to the collection. Any nested control flow will</span>
    <span class="c1"># be encapsulated in the root context.</span>
    <span class="k">if</span> <span class="n">loop_context</span><span class="o">.</span><span class="n">outer_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">WHILE_CONTEXT</span><span class="p">,</span> <span class="n">loop_context</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">loop_context</span><span class="o">.</span><span class="n">BuildLoop</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">loop_vars</span><span class="p">,</span> <span class="n">shape_invariants</span><span class="p">,</span>
                                    <span class="n">return_same_structure</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">maximum_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span></div>


<span class="c1"># pylint: enable=redefined-outer-name</span>


<span class="k">def</span> <span class="nf">_AsTensorList</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return x as a list of Tensors or IndexedSlices.</span>

<span class="sd">  For entries of `x` that are Operations, this returns an Identity of `p`</span>
<span class="sd">  with a dependency on the operation.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor/IndexedSlices/Operation or a list or tuple of them.</span>
<span class="sd">    p: A Tensor to return for entries in `x` that are Operations.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of Tensors or IndexedSlices.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">)):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

  <span class="n">l</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
      <span class="n">v</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">v</span><span class="p">],</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
              <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">indices</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">l</span>


<span class="k">def</span> <span class="nf">_CheckResults</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="p">(</span>
      <span class="s2">&quot;Values returned by a() and b() must have the same length.&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Values returned by a() [</span><span class="si">%s</span><span class="s2">] and b() [</span><span class="si">%s</span><span class="s2">] must have &quot;</span>
        <span class="s2">&quot;the same type: </span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">with_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Produces the content of `output_tensor` only after `dependencies`.</span>

<span class="sd">  In some cases, a user may want the output of an operation to be</span>
<span class="sd">  consumed externally only after some other dependencies have run</span>
<span class="sd">  first. This function ensures returns `output_tensor`, but only after all</span>
<span class="sd">  operations in `dependencies` have run. Note that this means that there is</span>
<span class="sd">  no guarantee that `output_tensor` will be evaluated after any `dependencies`</span>
<span class="sd">  have run.</span>

<span class="sd">  See also `tf.tuple` and `tf.group`.</span>

<span class="sd">  Args:</span>
<span class="sd">    dependencies: Iterable of operations to run before this op finishes.</span>
<span class="sd">    output_tensor: A `Tensor` or `IndexedSlices` that will be returned.</span>
<span class="sd">    name: (Optional) A name for this operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Same as `output_tensor`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `output_tensor` is not a `Tensor` or `IndexedSlices`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">output_tensor</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;control_dependency&quot;</span><span class="p">,</span>
                      <span class="nb">list</span><span class="p">(</span><span class="n">dependencies</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">output_tensor</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">):</span>
        <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_Identity</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
              <span class="n">_Identity</span><span class="p">(</span><span class="n">output_tensor</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">),</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
              <span class="n">output_tensor</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_GroupControlDeps</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">deps</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">deps</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dev</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">dev</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># TODO(touts): Accept &quot;inputs&quot; as a list.</span>
<div class="viewcode-block" id="group"><a class="viewcode-back" href="../../../../index.html#tensorflow.group">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;group&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">group</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create an op that groups multiple operations.</span>

<span class="sd">  When this op finishes, all ops in `inputs` have finished. This op has no</span>
<span class="sd">  output.</span>

<span class="sd">  See also `tf.tuple` and</span>
<span class="sd">  `tf.control_dependencies`.</span>

<span class="sd">  Args:</span>
<span class="sd">    *inputs: Zero or more tensors to group.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    An Operation that executes all its inputs.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If an unknown keyword argument is provided.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown keyword arguments: &quot;</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;group_deps&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="c1"># Grouping no inputs means do nothing</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># Sorts *inputs according to their devices.</span>
    <span class="n">ops_on_device</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># device -&gt; operations specified on the device.</span>
    <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected tf.group() expected Tensor arguments not &quot;</span>
                        <span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; with type &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">inp</span><span class="p">)))</span>
      <span class="n">dev</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">device</span>
      <span class="k">if</span> <span class="n">dev</span> <span class="ow">in</span> <span class="n">ops_on_device</span><span class="p">:</span>
        <span class="n">ops_on_device</span><span class="p">[</span><span class="n">dev</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ops_on_device</span><span class="p">[</span><span class="n">dev</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">inp</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ops_on_device</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># 1-level tree. The root node is the returned NoOp node.</span>
      <span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">deps</span><span class="p">),</span> <span class="o">=</span> <span class="n">ops_on_device</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">_GroupControlDeps</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">deps</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># 2-level tree. The root node is the returned NoOp node.</span>
    <span class="c1"># deps contains 1 NoOp node for each device.</span>
    <span class="n">deps</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">device_key</span><span class="p">(</span><span class="n">dev</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;A sort key that allows None to be compared to strings.&quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">dev</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dev</span>

    <span class="k">for</span> <span class="n">dev</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ops_on_device</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">device_key</span><span class="p">):</span>
      <span class="n">deps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_GroupControlDeps</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">ops_on_device</span><span class="p">[</span><span class="n">dev</span><span class="p">]))</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">deps</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;tuple&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">tuple_v2</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">control_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Group tensors together.</span>

<span class="sd">  This creates a tuple of tensors with the same values as the `tensors`</span>
<span class="sd">  argument, except that the value of each tensor is only returned after the</span>
<span class="sd">  values of all tensors have been computed.</span>

<span class="sd">  `control_inputs` contains additional ops that have to finish before this op</span>
<span class="sd">  finishes, but whose outputs are not returned.</span>

<span class="sd">  This can be used as a &quot;join&quot; mechanism for parallel computations: all the</span>
<span class="sd">  argument tensors can be computed in parallel, but the values of any tensor</span>
<span class="sd">  returned by `tuple` are only available after all the parallel computations</span>
<span class="sd">  are done.</span>

<span class="sd">  See also `tf.group` and</span>
<span class="sd">  `tf.control_dependencies`.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensors: A list of `Tensor`s or `IndexedSlices`, some entries can be `None`.</span>
<span class="sd">    control_inputs: List of additional ops to finish before returning.</span>
<span class="sd">    name: (optional) A name to use as a `name_scope` for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Same as `tensors`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `tensors` does not contain any `Tensor` or `IndexedSlices`.</span>
<span class="sd">    TypeError: If `control_inputs` is not a list of `Operation` or `Tensor`</span>
<span class="sd">      objects.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">control_inputs</span><span class="o">=</span><span class="n">control_inputs</span><span class="p">)</span>  <span class="c1"># pylint: disable=redefined-builtin</span>


<div class="viewcode-block" id="tuple"><a class="viewcode-back" href="../../../../index.html#tensorflow.tuple">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;tuple&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">control_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Group tensors together.</span>

<span class="sd">  This creates a tuple of tensors with the same values as the `tensors`</span>
<span class="sd">  argument, except that the value of each tensor is only returned after the</span>
<span class="sd">  values of all tensors have been computed.</span>

<span class="sd">  `control_inputs` contains additional ops that have to finish before this op</span>
<span class="sd">  finishes, but whose outputs are not returned.</span>

<span class="sd">  This can be used as a &quot;join&quot; mechanism for parallel computations: all the</span>
<span class="sd">  argument tensors can be computed in parallel, but the values of any tensor</span>
<span class="sd">  returned by `tuple` are only available after all the parallel computations</span>
<span class="sd">  are done.</span>

<span class="sd">  See also `tf.group` and</span>
<span class="sd">  `tf.control_dependencies`.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensors: A list of `Tensor`s or `IndexedSlices`, some entries can be `None`.</span>
<span class="sd">    name: (optional) A name to use as a `name_scope` for the operation.</span>
<span class="sd">    control_inputs: List of additional ops to finish before returning.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Same as `tensors`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `tensors` does not contain any `Tensor` or `IndexedSlices`.</span>
<span class="sd">    TypeError: If `control_inputs` is not a list of `Operation` or `Tensor`</span>
<span class="sd">      objects.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tensors</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;tuple&quot;</span><span class="p">,</span> <span class="n">tensors</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">)</span> <span class="ow">or</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">or</span>
              <span class="n">t</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span>
    <span class="p">]</span>
    <span class="n">gating_ops</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span><span class="o">.</span><span class="n">op</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span>
        <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">control_inputs</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
          <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">op</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Control input must be Operation or Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">c</span><span class="p">)</span>
        <span class="n">gating_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="c1"># Note that in order to ensure ordering in the pbtxt, we must take care to</span>
    <span class="c1"># ensure the order here.</span>
    <span class="n">gating_ops</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">gating_ops</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">op</span><span class="p">:</span> <span class="n">op</span><span class="o">.</span><span class="n">_id</span><span class="p">)</span>  <span class="c1"># Uniquify ops.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">gating_ops</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must have at least one Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tensors</span><span class="p">)</span>
    <span class="n">gate</span> <span class="o">=</span> <span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="n">gating_ops</span><span class="p">)</span>
    <span class="n">tpl</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">tpl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">with_dependencies</span><span class="p">([</span><span class="n">gate</span><span class="p">],</span> <span class="n">t</span><span class="p">))</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">gate</span><span class="p">]):</span>
          <span class="n">tpl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">group</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tpl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tpl</span></div>


<span class="k">def</span> <span class="nf">_assert_at_most_n_true</span><span class="p">(</span><span class="n">predicates</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns an Assert op that checks that at most n predicates are True.</span>

<span class="sd">  Args:</span>
<span class="sd">    predicates: list of bool scalar tensors.</span>
<span class="sd">    n: maximum number of true predicates allowed.</span>
<span class="sd">    msg: Error message.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">preds_c</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">predicates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;preds_c&quot;</span><span class="p">)</span>
  <span class="n">num_true_conditions</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">preds_c</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;num_true_conds&quot;</span><span class="p">)</span>
  <span class="n">condition</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="n">num_true_conditions</span><span class="p">,</span>
                                  <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_true_conds&quot;</span><span class="p">))</span>
  <span class="n">preds_names</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">predicates</span><span class="p">)</span>
  <span class="n">error_msg</span> <span class="o">=</span> <span class="p">[</span>
      <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: more than </span><span class="si">%d</span><span class="s2"> conditions (</span><span class="si">%s</span><span class="s2">) evaluated as True:&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">preds_names</span><span class="p">),</span> <span class="n">preds_c</span>
  <span class="p">]</span>
  <span class="k">return</span> <span class="n">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">error_msg</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">predicates</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_case_create_default_action</span><span class="p">(</span><span class="n">predicates</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates default action for a list of actions and their predicates.</span>

<span class="sd">  It uses the input actions to select an arbitrary as default and makes sure</span>
<span class="sd">  that corresponding predicates have valid values.</span>

<span class="sd">  Args:</span>
<span class="sd">    predicates: a list of bool scalar tensors</span>
<span class="sd">    actions: a list of callable objects which return tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    a callable</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">predicates</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># could pick any</span>
  <span class="n">predicate</span><span class="p">,</span> <span class="n">action</span> <span class="o">=</span> <span class="n">predicates</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
  <span class="n">other_predicates</span><span class="p">,</span> <span class="n">other_actions</span> <span class="o">=</span> <span class="n">predicates</span><span class="p">[:</span><span class="n">k</span><span class="p">],</span> <span class="n">actions</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">default_action</span><span class="p">():</span>
    <span class="n">others_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Implementation error: &quot;</span>
                  <span class="s2">&quot;selected default action #</span><span class="si">%d</span><span class="s2"> was called, but some of other &quot;</span>
                  <span class="s2">&quot;predicates are True: &quot;</span> <span class="o">%</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">default_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Input error: &quot;</span>
                   <span class="s2">&quot;None of conditions evaluated as True:&quot;</span><span class="p">,</span>
                   <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">predicates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;preds_c&quot;</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span>
        <span class="n">_assert_at_most_n_true</span><span class="p">(</span><span class="n">other_predicates</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="n">others_msg</span><span class="p">),</span>
        <span class="n">Assert</span><span class="p">(</span><span class="n">predicate</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">default_msg</span><span class="p">)</span>
    <span class="p">]):</span>
      <span class="k">return</span> <span class="n">action</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">default_action</span><span class="p">,</span> <span class="n">other_predicates</span><span class="p">,</span> <span class="n">other_actions</span>


<span class="k">def</span> <span class="nf">_case_verify_and_canonicalize_args</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                                       <span class="n">allow_python_preds</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Verifies input arguments for the case function.</span>

<span class="sd">  Args:</span>
<span class="sd">    pred_fn_pairs: Dict or list of pairs of a boolean scalar tensor, and a</span>
<span class="sd">      callable which returns a list of tensors.</span>
<span class="sd">    exclusive: True iff at most one predicate is allowed to evaluate to `True`.</span>
<span class="sd">    name: A name for the case operation.</span>
<span class="sd">    allow_python_preds: if true, pred_fn_pairs may contain Python bools in</span>
<span class="sd">      addition to boolean Tensors</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is not a list/dictionary.</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.</span>
<span class="sd">    TypeError: If `fns[i]` is not callable for any i, or `default` is not</span>
<span class="sd">               callable.</span>

<span class="sd">  Returns:</span>
<span class="sd">    a tuple &lt;list of scalar bool tensors, list of callables&gt;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;fns must be a list, tuple, or dict&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">):</span>
    <span class="n">pred_fn_pairs</span> <span class="o">=</span> <span class="n">pred_fn_pairs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="c1"># No name to sort on in eager mode. Use dictionary traversal order,</span>
      <span class="c1"># which is nondeterministic in versions of Python &lt; 3.6</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">exclusive</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unordered dictionaries are not supported for the &quot;</span>
                         <span class="s2">&quot;`pred_fn_pairs` argument when `exclusive=False` and &quot;</span>
                         <span class="s2">&quot;eager mode is enabled.&quot;</span><span class="p">)</span>
      <span class="n">pred_fn_pairs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pred_fn_pairs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
          <span class="n">pred_fn_pairs</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">exclusive</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: An unordered dictionary of predicate/fn pairs was &quot;</span>
            <span class="s2">&quot;provided, but exclusive=False. The order of conditional &quot;</span>
            <span class="s2">&quot;tests is deterministic but not guaranteed.&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">pred_fn_pair</span> <span class="ow">in</span> <span class="n">pred_fn_pairs</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pred_fn_pair</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred_fn_pair</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Each entry in pred_fn_pairs must be a 2-tuple&quot;</span><span class="p">)</span>
    <span class="n">pred</span><span class="p">,</span> <span class="n">fn</span> <span class="o">=</span> <span class="n">pred_fn_pair</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">pred</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pred must be Tensor of type bool: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">pred</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">allow_python_preds</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pred must be a Tensor, got: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pred must be a Tensor or bool, got: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">pred</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;fn for pred </span><span class="si">%s</span><span class="s2"> must be callable.&quot;</span> <span class="o">%</span> <span class="n">pred</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="n">predicates</span><span class="p">,</span> <span class="n">actions</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">pred_fn_pairs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">predicates</span><span class="p">,</span> <span class="n">actions</span>


<span class="k">def</span> <span class="nf">_case_helper</span><span class="p">(</span><span class="n">cond_fn</span><span class="p">,</span>
                 <span class="n">pred_fn_pairs</span><span class="p">,</span>
                 <span class="n">default</span><span class="p">,</span>
                 <span class="n">exclusive</span><span class="p">,</span>
                 <span class="n">name</span><span class="p">,</span>
                 <span class="n">allow_python_preds</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">cond_kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implementation of case that allows for different cond functions.</span>

<span class="sd">  Args:</span>
<span class="sd">    cond_fn: method that has signature and semantics of `cond` above.</span>
<span class="sd">    pred_fn_pairs: Dict or list of pairs of a boolean scalar tensor, and a</span>
<span class="sd">      callable which returns a list of tensors.</span>
<span class="sd">    default: Optional callable that returns a list of tensors.</span>
<span class="sd">    exclusive: True iff at most one predicate is allowed to evaluate to `True`.</span>
<span class="sd">    name: A name for this operation (optional).</span>
<span class="sd">    allow_python_preds: if true, pred_fn_pairs may contain Python bools in</span>
<span class="sd">      addition to boolean Tensors</span>
<span class="sd">    **cond_kwargs: keyword arguments that will be passed to `cond_fn`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The tensors returned by the first pair whose predicate evaluated to True, or</span>
<span class="sd">    those returned by `default` if none does.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is not a list/dictionary.</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.</span>
<span class="sd">    TypeError: If `fns[i]` is not callable for any i, or `default` is not</span>
<span class="sd">               callable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">predicates</span><span class="p">,</span> <span class="n">actions</span> <span class="o">=</span> <span class="n">_case_verify_and_canonicalize_args</span><span class="p">(</span>
      <span class="n">pred_fn_pairs</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">allow_python_preds</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;case&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">predicates</span><span class="p">]):</span>
    <span class="k">if</span> <span class="n">default</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">default</span><span class="p">,</span> <span class="n">predicates</span><span class="p">,</span> <span class="n">actions</span> <span class="o">=</span> <span class="n">_case_create_default_action</span><span class="p">(</span>
          <span class="n">predicates</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">default</span>
    <span class="c1"># To eval conditions in direct order we create nested conditions in reverse:</span>
    <span class="c1">#   cond_fn(c[0], true_fn=.., false_fn=cond_fn(c[1], ...))</span>
    <span class="k">for</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">predicates</span><span class="p">,</span> <span class="n">actions</span><span class="p">))):</span>
      <span class="n">fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
          <span class="n">cond_fn</span><span class="p">,</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">true_fn</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">false_fn</span><span class="o">=</span><span class="n">fn</span><span class="p">,</span> <span class="o">**</span><span class="n">cond_kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">exclusive</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span>
          <span class="n">_assert_at_most_n_true</span><span class="p">(</span>
              <span class="n">predicates</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s2">&quot;Input error: exclusive=True&quot;</span><span class="p">)</span>
      <span class="p">]):</span>
        <span class="k">return</span> <span class="n">fn</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">fn</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_indexed_case_verify_and_canonicalize_args</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">,</span> <span class="n">default</span><span class="p">,</span>
                                               <span class="n">branch_index</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Verifies input arguments for the case function.</span>

<span class="sd">  Args:</span>
<span class="sd">    branch_fns: Dict or list of pairs of an `int` and a callable which</span>
<span class="sd">      returns a list of tensors.</span>
<span class="sd">    default: Optional callable that returns a list of tensors.</span>
<span class="sd">    branch_index: Optional int `Tensor`, which selects for the corresponding</span>
<span class="sd">      pred_fn_pair.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `branch_fns` is not a list/dictionary.</span>
<span class="sd">    TypeError: If `branch_fns` is a list but does not contain 2-tuples or</span>
<span class="sd">               callables.</span>
<span class="sd">    TypeError: If `fns[i]` is not callable for any i, or `default` is not</span>
<span class="sd">               callable.</span>

<span class="sd">  Returns:</span>
<span class="sd">    branch_fns: validated list of callables for each branch (default last).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">branch_index</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;branch_index must a Tensor, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="nb">type</span><span class="p">(</span><span class="n">branch_index</span><span class="p">)))</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">branch_index</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;branch_index must an integer Tensor, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">branch_index</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">branch_fns</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must provide at least one item in branch_fns&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;branch_fns must be a list, tuple, or dict&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="n">branch_fns</span> <span class="o">=</span> <span class="n">branch_fns</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>

  <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">callable</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">branch_fns</span><span class="p">):</span>
    <span class="n">branch_fns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">key_fn_pair</span> <span class="ow">in</span> <span class="n">branch_fns</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key_fn_pair</span><span class="p">,</span> <span class="n">_basetuple</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">key_fn_pair</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Each entry in branch_fns must be a 2-tuple&quot;</span><span class="p">)</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">branch_fn</span> <span class="o">=</span> <span class="n">key_fn_pair</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;key must be a Python `int`, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">key</span><span class="p">)))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">branch_fn</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;fn for key </span><span class="si">{}</span><span class="s2"> must be callable.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>

  <span class="n">keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">branch_fns</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">min</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">max</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">keys</span><span class="p">))</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">keys</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;branch indices (keys) must form contiguous range of [0 to </span><span class="si">{}</span><span class="s2">) but &quot;</span>
        <span class="s2">&quot;found {{</span><span class="si">{}</span><span class="s2">}}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">keys</span><span class="p">),</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">keys</span><span class="p">)))))</span>
  <span class="n">actions</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">)]</span>
  <span class="k">if</span> <span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">default</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">actions</span>


<span class="k">def</span> <span class="nf">_indexed_case_helper</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">,</span> <span class="n">default</span><span class="p">,</span> <span class="n">branch_index</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implementation of case that emits the n-way indexed Case op.</span>

<span class="sd">  Args:</span>
<span class="sd">    branch_fns: Dict or list of pairs of a boolean scalar tensor, and a</span>
<span class="sd">      callable which returns a list of tensors.</span>
<span class="sd">    default: Optional callable that returns a list of tensors.</span>
<span class="sd">    branch_index: Optional int `Tensor`, which selects for the corresponding</span>
<span class="sd">      pred_fn_pair.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The tensors returned by the pair whose key matched branch_index, or</span>
<span class="sd">    those returned by `default` if none does.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `branch_fns` is not a list/dictionary.</span>
<span class="sd">    TypeError: If `branch_fns` is a list but does not contain 2-tuples or</span>
<span class="sd">               callables.</span>
<span class="sd">    TypeError: If `fns[i]` is not callable for any i, or `default` is not</span>
<span class="sd">               callable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">branch_fns</span> <span class="o">=</span> <span class="n">_indexed_case_verify_and_canonicalize_args</span><span class="p">(</span>
      <span class="n">branch_fns</span><span class="p">,</span> <span class="n">default</span><span class="p">,</span> <span class="n">branch_index</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;case&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">branch_index</span><span class="p">]):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">branch_index</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">):</span>
      <span class="n">branch_index</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
          <span class="n">math_ops</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">branch_index</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
          <span class="o">|</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">branch_index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">)),</span>
          <span class="nb">len</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">branch_index</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">branch_fns</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">branch_index</span><span class="p">)]()</span>
    <span class="k">return</span> <span class="n">cond_v2</span><span class="o">.</span><span class="n">indexed_case</span><span class="p">(</span><span class="n">branch_index</span><span class="p">,</span> <span class="n">branch_fns</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;case&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">case_v2</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;case&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a case operation.</span>

<span class="sd">  See also `tf.switch_case`.</span>

<span class="sd">  The `pred_fn_pairs` parameter is a list of pairs of size N.</span>
<span class="sd">  Each pair contains a boolean scalar tensor and a python callable that</span>
<span class="sd">  creates the tensors to be returned if the boolean evaluates to True.</span>
<span class="sd">  `default` is a callable generating a list of tensors. All the callables</span>
<span class="sd">  in `pred_fn_pairs` as well as `default` (if provided) should return the same</span>
<span class="sd">  number and types of tensors.</span>

<span class="sd">  If `exclusive==True`, all predicates are evaluated, and an exception is</span>
<span class="sd">  thrown if more than one of the predicates evaluates to `True`.</span>
<span class="sd">  If `exclusive==False`, execution stops at the first predicate which</span>
<span class="sd">  evaluates to True, and the tensors generated by the corresponding function</span>
<span class="sd">  are returned immediately. If none of the predicates evaluate to True, this</span>
<span class="sd">  operation returns the tensors generated by `default`.</span>

<span class="sd">  `tf.case` supports nested structures as implemented in</span>
<span class="sd">  `tf.contrib.framework.nest`. All of the callables must return the same</span>
<span class="sd">  (possibly nested) value structure of lists, tuples, and/or named tuples.</span>
<span class="sd">  Singleton lists and tuples form the only exceptions to this: when returned by</span>
<span class="sd">  a callable, they are implicitly unpacked to single values. This</span>
<span class="sd">  behavior is disabled by passing `strict=True`.</span>

<span class="sd">  @compatibility(v2)</span>
<span class="sd">  `pred_fn_pairs` could be a dictionary in v1. However, tf.Tensor and</span>
<span class="sd">  tf.Variable are no longer hashable in v2, so cannot be used as a key for a</span>
<span class="sd">  dictionary.  Please use a list or a tuple instead.</span>
<span class="sd">  @end_compatibility</span>


<span class="sd">  **Example 1:**</span>

<span class="sd">  Pseudocode:</span>

<span class="sd">  ```</span>
<span class="sd">  if (x &lt; y) return 17;</span>
<span class="sd">  else return 23;</span>
<span class="sd">  ```</span>

<span class="sd">  Expressions:</span>

<span class="sd">  ```python</span>
<span class="sd">  f1 = lambda: tf.constant(17)</span>
<span class="sd">  f2 = lambda: tf.constant(23)</span>
<span class="sd">  r = tf.case([(tf.less(x, y), f1)], default=f2)</span>
<span class="sd">  ```</span>

<span class="sd">  **Example 2:**</span>

<span class="sd">  Pseudocode:</span>

<span class="sd">  ```</span>
<span class="sd">  if (x &lt; y &amp;&amp; x &gt; z) raise OpError(&quot;Only one predicate may evaluate to True&quot;);</span>
<span class="sd">  if (x &lt; y) return 17;</span>
<span class="sd">  else if (x &gt; z) return 23;</span>
<span class="sd">  else return -1;</span>
<span class="sd">  ```</span>

<span class="sd">  Expressions:</span>

<span class="sd">  ```python</span>
<span class="sd">  def f1(): return tf.constant(17)</span>
<span class="sd">  def f2(): return tf.constant(23)</span>
<span class="sd">  def f3(): return tf.constant(-1)</span>
<span class="sd">  r = tf.case([(tf.less(x, y), f1), (tf.greater(x, z), f2)],</span>
<span class="sd">           default=f3, exclusive=True)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    pred_fn_pairs: List of pairs of a boolean scalar tensor and a callable which</span>
<span class="sd">      returns a list of tensors.</span>
<span class="sd">    default: Optional callable that returns a list of tensors.</span>
<span class="sd">    exclusive: True iff at most one predicate is allowed to evaluate to `True`.</span>
<span class="sd">    strict: A boolean that enables/disables &#39;strict&#39; mode; see above.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The tensors returned by the first pair whose predicate evaluated to True, or</span>
<span class="sd">    those returned by `default` if none does.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is not a list/tuple.</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.</span>
<span class="sd">    TypeError: If `fns[i]` is not callable for any i, or `default` is not</span>
<span class="sd">               callable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_case_helper</span><span class="p">(</span>
      <span class="n">cond</span><span class="p">,</span>
      <span class="n">pred_fn_pairs</span><span class="p">,</span>
      <span class="n">default</span><span class="p">,</span>
      <span class="n">exclusive</span><span class="p">,</span>
      <span class="n">name</span><span class="p">,</span>
      <span class="n">allow_python_preds</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>


<div class="viewcode-block" id="case"><a class="viewcode-back" href="../../../../index.html#tensorflow.case">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;case&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">case</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="p">,</span>
         <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
         <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">name</span><span class="o">=</span><span class="s2">&quot;case&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a case operation.</span>

<span class="sd">  See also `tf.switch_case`.</span>

<span class="sd">  The `pred_fn_pairs` parameter is a dict or list of pairs of size N.</span>
<span class="sd">  Each pair contains a boolean scalar tensor and a python callable that</span>
<span class="sd">  creates the tensors to be returned if the boolean evaluates to True.</span>
<span class="sd">  `default` is a callable generating a list of tensors. All the callables</span>
<span class="sd">  in `pred_fn_pairs` as well as `default` (if provided) should return the same</span>
<span class="sd">  number and types of tensors.</span>

<span class="sd">  If `exclusive==True`, all predicates are evaluated, and an exception is</span>
<span class="sd">  thrown if more than one of the predicates evaluates to `True`.</span>
<span class="sd">  If `exclusive==False`, execution stops at the first predicate which</span>
<span class="sd">  evaluates to True, and the tensors generated by the corresponding function</span>
<span class="sd">  are returned immediately. If none of the predicates evaluate to True, this</span>
<span class="sd">  operation returns the tensors generated by `default`.</span>

<span class="sd">  `tf.case` supports nested structures as implemented in</span>
<span class="sd">  `tf.contrib.framework.nest`. All of the callables must return the same</span>
<span class="sd">  (possibly nested) value structure of lists, tuples, and/or named tuples.</span>
<span class="sd">  Singleton lists and tuples form the only exceptions to this: when returned by</span>
<span class="sd">  a callable, they are implicitly unpacked to single values. This</span>
<span class="sd">  behavior is disabled by passing `strict=True`.</span>

<span class="sd">  If an unordered dictionary is used for `pred_fn_pairs`, the order of the</span>
<span class="sd">  conditional tests is not guaranteed. However, the order is guaranteed to be</span>
<span class="sd">  deterministic, so that variables created in conditional branches are created</span>
<span class="sd">  in fixed order across runs.</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Unordered dictionaries are not supported in eager mode when `exclusive=False`.</span>
<span class="sd">  Use a list of tuples instead.</span>
<span class="sd">  @end_compatibility</span>


<span class="sd">  **Example 1:**</span>

<span class="sd">  Pseudocode:</span>

<span class="sd">  ```</span>
<span class="sd">  if (x &lt; y) return 17;</span>
<span class="sd">  else return 23;</span>
<span class="sd">  ```</span>

<span class="sd">  Expressions:</span>

<span class="sd">  ```python</span>
<span class="sd">  f1 = lambda: tf.constant(17)</span>
<span class="sd">  f2 = lambda: tf.constant(23)</span>
<span class="sd">  r = tf.case([(tf.less(x, y), f1)], default=f2)</span>
<span class="sd">  ```</span>

<span class="sd">  **Example 2:**</span>

<span class="sd">  Pseudocode:</span>

<span class="sd">  ```</span>
<span class="sd">  if (x &lt; y &amp;&amp; x &gt; z) raise OpError(&quot;Only one predicate may evaluate to True&quot;);</span>
<span class="sd">  if (x &lt; y) return 17;</span>
<span class="sd">  else if (x &gt; z) return 23;</span>
<span class="sd">  else return -1;</span>
<span class="sd">  ```</span>

<span class="sd">  Expressions:</span>

<span class="sd">  ```python</span>
<span class="sd">  def f1(): return tf.constant(17)</span>
<span class="sd">  def f2(): return tf.constant(23)</span>
<span class="sd">  def f3(): return tf.constant(-1)</span>
<span class="sd">  r = tf.case({tf.less(x, y): f1, tf.greater(x, z): f2},</span>
<span class="sd">           default=f3, exclusive=True)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    pred_fn_pairs: Dict or list of pairs of a boolean scalar tensor and a</span>
<span class="sd">      callable which returns a list of tensors.</span>
<span class="sd">    default: Optional callable that returns a list of tensors.</span>
<span class="sd">    exclusive: True iff at most one predicate is allowed to evaluate to `True`.</span>
<span class="sd">    strict: A boolean that enables/disables &#39;strict&#39; mode; see above.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The tensors returned by the first pair whose predicate evaluated to True, or</span>
<span class="sd">    those returned by `default` if none does.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is not a list/dictionary.</span>
<span class="sd">    TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.</span>
<span class="sd">    TypeError: If `fns[i]` is not callable for any i, or `default` is not</span>
<span class="sd">               callable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_case_helper</span><span class="p">(</span>
      <span class="n">cond</span><span class="p">,</span>
      <span class="n">pred_fn_pairs</span><span class="p">,</span>
      <span class="n">default</span><span class="p">,</span>
      <span class="n">exclusive</span><span class="p">,</span>
      <span class="n">name</span><span class="p">,</span>
      <span class="n">allow_python_preds</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span></div>


<div class="viewcode-block" id="switch_case"><a class="viewcode-back" href="../../../../index.html#tensorflow.switch_case">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;switch_case&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">switch_case</span><span class="p">(</span><span class="n">branch_index</span><span class="p">,</span>
                <span class="n">branch_fns</span><span class="p">,</span>
                <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;switch_case&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a switch/case operation, i.e. an integer-indexed conditional.</span>

<span class="sd">  See also `tf.case`.</span>

<span class="sd">  This op can be substantially more efficient than `tf.case` when exactly one</span>
<span class="sd">  branch will be selected. `tf.switch_case` is more like a C++ switch/case</span>
<span class="sd">  statement than `tf.case`, which is more like an if/elif/elif/else chain.</span>

<span class="sd">  The `branch_fns` parameter is either a dict from `int` to callables, or list</span>
<span class="sd">  of (`int`, callable) pairs, or simply a list of callables (in which case the</span>
<span class="sd">  index is implicitly the key). The `branch_index` `Tensor` is used to select an</span>
<span class="sd">  element in `branch_fns` with matching `int` key, falling back to `default`</span>
<span class="sd">  if none match, or `max(keys)` if no `default` is provided. The keys must form</span>
<span class="sd">  a contiguous set from `0` to `len(branch_fns) - 1`.</span>

<span class="sd">  `tf.switch_case` supports nested structures as implemented in `tf.nest`. All</span>
<span class="sd">  callables must return the same (possibly nested) value structure of lists,</span>
<span class="sd">  tuples, and/or named tuples.</span>

<span class="sd">  **Example:**</span>

<span class="sd">  Pseudocode:</span>

<span class="sd">  ```c++</span>
<span class="sd">  switch (branch_index) {  // c-style switch</span>
<span class="sd">    case 0: return 17;</span>
<span class="sd">    case 1: return 31;</span>
<span class="sd">    default: return -1;</span>
<span class="sd">  }</span>
<span class="sd">  ```</span>
<span class="sd">  or</span>
<span class="sd">  ```python</span>
<span class="sd">  branches = {0: lambda: 17, 1: lambda: 31}</span>
<span class="sd">  branches.get(branch_index, lambda: -1)()</span>
<span class="sd">  ```</span>

<span class="sd">  Expressions:</span>

<span class="sd">  ```python</span>
<span class="sd">  def f1(): return tf.constant(17)</span>
<span class="sd">  def f2(): return tf.constant(31)</span>
<span class="sd">  def f3(): return tf.constant(-1)</span>
<span class="sd">  r = tf.switch_case(branch_index, branch_fns={0: f1, 1: f2}, default=f3)</span>
<span class="sd">  # Equivalent: tf.switch_case(branch_index, branch_fns={0: f1, 1: f2, 2: f3})</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    branch_index: An int Tensor specifying which of `branch_fns` should be</span>
<span class="sd">      executed.</span>
<span class="sd">    branch_fns: A `dict` mapping `int`s to callables, or a `list` of</span>
<span class="sd">      (`int`, callable) pairs, or simply a list of callables (in which case the</span>
<span class="sd">      index serves as the key). Each callable must return a matching structure</span>
<span class="sd">      of tensors.</span>
<span class="sd">    default: Optional callable that returns a structure of tensors.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The tensors returned by the callable identified by `branch_index`, or those</span>
<span class="sd">    returned by `default` if no key matches and `default` was provided, or those</span>
<span class="sd">    returned by the max-keyed `branch_fn` if no `default` is provided.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `branch_fns` is not a list/dictionary.</span>
<span class="sd">    TypeError: If `branch_fns` is a list but does not contain 2-tuples or</span>
<span class="sd">               callables.</span>
<span class="sd">    TypeError: If `fns[i]` is not callable for any i, or `default` is not</span>
<span class="sd">               callable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_indexed_case_helper</span><span class="p">(</span><span class="n">branch_fns</span><span class="p">,</span> <span class="n">default</span><span class="p">,</span> <span class="n">branch_index</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">XLAControlFlowContext</span><span class="p">(</span><span class="n">ControlFlowContext</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for XLA and TPU control flow contexts.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">XLAControlFlowContext</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="s2">&quot;XLAControlFlowContext&quot;</span>

  <span class="k">def</span> <span class="nf">to_control_flow_context_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_def</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># pylint: disable=useless-super-delegation</span>
    <span class="c1"># NOTE(slebedev): the method is required by `ControlFlowContext`.</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">XLAControlFlowContext</span><span class="p">,</span>
          <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">to_control_flow_context_def</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">IsXLAContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">AddOp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">AddValue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">from_control_flow_context_def</span><span class="p">(</span><span class="n">context_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Deserializes `context_def` into the appropriate ControlFlowContext.</span>

<span class="sd">  Args:</span>
<span class="sd">    context_def: ControlFlowContextDef proto</span>
<span class="sd">    import_scope: Optional `string`. Name scope to add.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A ControlFlowContext subclass</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context_def</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s2">&quot;cond_ctxt&quot;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">CondContext</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span>
        <span class="n">context_def</span><span class="o">.</span><span class="n">cond_ctxt</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">context_def</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s2">&quot;while_ctxt&quot;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">WhileContext</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span>
        <span class="n">context_def</span><span class="o">.</span><span class="n">while_ctxt</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Unknown ControlFlowContextDef field: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                            <span class="n">context_def</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;ctxt&quot;</span><span class="p">))</span>


<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">COND_CONTEXT</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">CondContextDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">CondContext</span><span class="o">.</span><span class="n">to_proto</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">CondContext</span><span class="o">.</span><span class="n">from_proto</span><span class="p">)</span>

<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">WHILE_CONTEXT</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">control_flow_pb2</span><span class="o">.</span><span class="n">WhileContextDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">WhileContext</span><span class="o">.</span><span class="n">to_proto</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">WhileContext</span><span class="o">.</span><span class="n">from_proto</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>