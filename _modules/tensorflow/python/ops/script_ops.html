

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.script_ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.script_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.script_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Script Language Operators.&quot;&quot;&quot;</span>

<span class="c1"># pylint: disable=g-bad-name</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">threading</span>

<span class="c1"># Used by py_util.cc to get tracebacks.</span>
<span class="kn">import</span> <span class="nn">traceback</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">import</span> <span class="nn">weakref</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">_pywrap_py_func</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">backprop</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">func_graph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_script_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">lazy_loader</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_inspect</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="n">autograph</span> <span class="o">=</span> <span class="n">lazy_loader</span><span class="o">.</span><span class="n">LazyLoader</span><span class="p">(</span>
    <span class="s2">&quot;autograph&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span>
    <span class="s2">&quot;tensorflow.python.autograph.impl.api&quot;</span><span class="p">)</span>


<span class="c1"># Map from EagerPyFunc token to tuple (tape, eager args, eager outputs);</span>
<span class="c1"># used for differentiation.</span>
<span class="n">tape_cache</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">def</span> <span class="nf">_maybe_copy_to_context_device</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">device_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Copy an EagerTensor to the current device if it&#39;s not on `device_name`.&quot;&quot;&quot;</span>
  <span class="n">in_device</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">backing_device</span>
  <span class="k">if</span> <span class="n">device_name</span> <span class="o">==</span> <span class="n">in_device</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Note that EagerTensor._copy bypasses the placer and copies to the context</span>
    <span class="c1"># device, which means e.g. int32 Tensors which would normally be forced onto</span>
    <span class="c1"># the CPU can instead be placed on the GPU. This is necessary so that the</span>
    <span class="c1"># PyFunc kernel always returns Tensors on the device it&#39;s executing on.</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">_copy</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">class</span> <span class="nc">EagerFunc</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A wrapper for a function owned by an EagerPyFunc.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">is_grad_func</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an EagerFunc.</span>

<span class="sd">    Args:</span>
<span class="sd">      func: The function to wrap.</span>
<span class="sd">      Tout: A list of datatypes for the output; an empty list if the output is</span>
<span class="sd">        None.</span>
<span class="sd">      is_grad_func: Whether this EagerFunc is the gradient of another</span>
<span class="sd">        EagerPyFunc.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_func</span> <span class="o">=</span> <span class="n">func</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_out_dtypes</span> <span class="o">=</span> <span class="n">Tout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_grad_func</span> <span class="o">=</span> <span class="n">is_grad_func</span>

  <span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts `value` to a tensor of type `dtype`, with error checking.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: The tensor to convert.</span>
<span class="sd">      dtype: The desired dtype.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of type `dtype`, or a zeros tensor if value is None and</span>
<span class="sd">      this function is in fact a gradient function.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: if `value` is a variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">ResourceVariable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Attempting to return a variable from an eagerly executed py_func. &quot;</span>
          <span class="s2">&quot;Only numeric data structures like Tensors or NumPy arrays should &quot;</span>
          <span class="s2">&quot;be returned; to return the value of a variable, make sure to obtain &quot;</span>
          <span class="s2">&quot;the Tensor backing it by calling `.read_value()` on the variable in &quot;</span>
          <span class="s2">&quot;question: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_grad_func</span><span class="p">:</span>
      <span class="c1"># Gradient functions may legitimately return a list that contains</span>
      <span class="c1"># both Tensors and Python Nones. Unfortunately this breaks the</span>
      <span class="c1"># OpKernel, so for now we replace None objects with zeros, which is</span>
      <span class="c1"># mathematically correct but will prevent short-circuiting gradient</span>
      <span class="c1"># computations.</span>
      <span class="c1">#</span>
      <span class="c1"># TODO(akshayka): Make it possible to return a list of both Tensors and</span>
      <span class="c1"># Nones from an EagerPyFunc.</span>
      <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">token</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Passes `args` to `self._func`, which is executed eagerly.&quot;&quot;&quot;</span>

    <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">eager_mode</span><span class="p">(),</span> <span class="n">backprop</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
      <span class="c1"># Only watch tensors with a floating or complex dtype.</span>
      <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
      <span class="c1"># copy the returned tensors to the PyFunc op&#39;s device if necessary.</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">device</span>
      <span class="k">if</span> <span class="n">device_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># &quot;None&quot; here means &quot;CPU&quot;, from the nullptr convention with C++ device</span>
        <span class="c1"># pointers.</span>
        <span class="n">device_name</span> <span class="o">=</span> <span class="s2">&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span>
              <span class="n">_maybe_copy_to_context_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_convert</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
                                            <span class="n">device_name</span><span class="p">)</span>
              <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_out_dtypes</span><span class="p">)</span>
          <span class="p">]</span>
        <span class="k">elif</span> <span class="n">ret</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="n">_maybe_copy_to_context_device</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_out_dtypes</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">device_name</span><span class="p">)</span>
    <span class="n">tape_cache</span><span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">token</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tape</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>


<span class="k">class</span> <span class="nc">FuncRegistry</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A helper class to keep track of registered py functions.</span>

<span class="sd">  FuncRegistry keeps a map from unique tokens (string) to python</span>
<span class="sd">  functions, which takes numpy arrays and outputs numpy arrays.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="c1"># Only store weakrefs to the functions. The strong reference is stored in</span>
    <span class="c1"># the graph.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_funcs</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">WeakValueDictionary</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_ctx</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># N.B. This is needed to support calling py_func with GPU tensors,</span>
    <span class="c1"># which must be transferred to CPU if used in any of the NumPy APIs.</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">_handle</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">insert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Registers `func` and returns a unique token for this entry.&quot;&quot;&quot;</span>
    <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_unique_token</span><span class="p">()</span>
    <span class="c1"># Store a weakref to the function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_funcs</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span>
    <span class="k">return</span> <span class="n">token</span>

  <span class="k">def</span> <span class="nf">remove</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Removes the registered function corresponding to `token`.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_funcs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts an arg to numpy, avoiding dangerous string and unicode dtypes.</span>

<span class="sd">    Numpy pads with zeros when using string and unicode dtypes if different</span>
<span class="sd">    components of a tensor have different lengths.  This is bad: ignoring the</span>
<span class="sd">    padding is wrong for text data, and removing the padding is wrong for binary</span>
<span class="sd">    data.  To avoid this bug, we redo the conversion using an object dtype.</span>
<span class="sd">    Additionally, we convert unicode strings to (byte-)strings for</span>
<span class="sd">    compatibility.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: Value to convert to a numpy array.</span>
<span class="sd">      dtype: (Optional.) Desired NumPy type for the returned value.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A numpy array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;S&quot;</span> <span class="ow">and</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">value</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">result</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;U&quot;</span> <span class="ow">and</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">value</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf8&quot;</span><span class="p">))(</span><span class="n">value</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">result</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;U&quot;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bytes_</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calls the registered function for `token` with args.</span>

<span class="sd">    Args:</span>
<span class="sd">      token: A key into this `FuncRegistry` identifying which function to call.</span>
<span class="sd">      device: Name of the device on which outputs of `token`&#39;s corresponding</span>
<span class="sd">        operation should be placed. Used iff the function registered for `token`</span>
<span class="sd">        is an EagerPyFunc.</span>
<span class="sd">      args: The arguments to pass to the function registered for `token`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The output of the function registered for `token`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if no function is registered for `token`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_funcs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;callback </span><span class="si">%s</span><span class="s2"> is not found&quot;</span> <span class="o">%</span> <span class="n">token</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">EagerFunc</span><span class="p">):</span>
      <span class="c1"># NB: Different invocations of the same py_func will share the same</span>
      <span class="c1"># token, and the entries they stash in the tape_cache will collide.</span>
      <span class="c1"># In practice, when executing a graph, this should only happen if</span>
      <span class="c1"># the py_func is in a while_loop whose iterations are run in parallel</span>
      <span class="c1"># or if the graph is being driven by concurrent session.run() calls.</span>
      <span class="c1">#</span>
      <span class="c1"># TODO(akshayka): Key the tape cache in a thread-safe way.</span>
      <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">token</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
      <span class="c1"># Strings seem to lead to a memory leak here if they&#39;re not wrapped in a</span>
      <span class="c1"># list.</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">binary_type</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">ret</span><span class="p">]</span>
      <span class="c1"># Ensures that we return either a single numpy array or a list of numpy</span>
      <span class="c1"># arrays.</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_convert</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns how many functions are currently registered.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_funcs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_next_unique_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a unique token.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="n">uid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="s2">&quot;pyfunc_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">uid</span>


<span class="c1"># Global registry for py functions.</span>
<span class="n">_py_funcs</span> <span class="o">=</span> <span class="n">FuncRegistry</span><span class="p">()</span>

<span class="n">_pywrap_py_func</span><span class="o">.</span><span class="n">initialize_py_trampoline</span><span class="p">(</span><span class="n">_py_funcs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_internal_py_func</span><span class="p">(</span><span class="n">func</span><span class="p">,</span>
                      <span class="n">inp</span><span class="p">,</span>
                      <span class="n">Tout</span><span class="p">,</span>
                      <span class="n">stateful</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">eager</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">is_grad_func</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;See documentation for py_func and eager_py_func.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected func to be callable, got func of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="nb">type</span><span class="p">(</span><span class="n">func</span><span class="p">)))</span>

  <span class="n">original_func</span> <span class="o">=</span> <span class="n">func</span>
  <span class="n">func</span> <span class="o">=</span> <span class="n">autograph</span><span class="o">.</span><span class="n">do_not_convert</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

  <span class="n">is_list_or_tuple</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Tout</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">is_list_or_tuple</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">Tout</span> <span class="o">=</span> <span class="p">[</span><span class="n">Tout</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">eager</span><span class="p">:</span>
    <span class="n">func</span> <span class="o">=</span> <span class="n">EagerFunc</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">is_grad_func</span><span class="p">)</span>

  <span class="c1"># Tying the registered function&#39;s lifetime with the current default graph is</span>
  <span class="c1"># not reliable. For example, Estimator-based binaries may switch graphs in</span>
  <span class="c1"># between model training end evaluation, via saved_model. Those binaries work</span>
  <span class="c1"># because the original function is global, and break once the registered</span>
  <span class="c1"># function is an anonymous lambda, like the one produced by do_not_convert.</span>
  <span class="c1"># To avoid breaking those cases, we attach the wrapper to the original</span>
  <span class="c1"># function so that their lifetime is connected.</span>
  <span class="c1"># TODO(b/144286616): Remove this.</span>
  <span class="k">if</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">isfunction</span><span class="p">(</span><span class="n">original_func</span><span class="p">):</span>
    <span class="c1"># Note: this check is needed because original_func may be a descriptor</span>
    <span class="c1"># (https://docs.python.org/3/howto/descriptor.html)</span>
    <span class="c1"># and we can&#39;t attach attributes to those.</span>
    <span class="n">original_func</span><span class="o">.</span><span class="n">ag_dnc_wrapper__</span> <span class="o">=</span> <span class="n">func</span>

  <span class="n">token</span> <span class="o">=</span> <span class="n">_py_funcs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
  <span class="c1"># We tie the registered function&#39;s lifetime with the current default graph,</span>
  <span class="c1"># i.e., when the current graph is destroyed, we remove its py funcs.</span>
  <span class="n">graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>

  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">current_graph</span> <span class="o">=</span> <span class="n">graph</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">function</span><span class="o">.</span><span class="n">_FuncGraph</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">_outer_graph</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">):</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">outer_graph</span>
    <span class="k">if</span> <span class="n">graph</span> <span class="ow">is</span> <span class="n">current_graph</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="c1"># TODO(zhifengc): Consider adding a Graph method to collect</span>
  <span class="c1"># `cleanup` objects in one of its member.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;_py_funcs_used_in_graph&quot;</span><span class="p">):</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">_py_funcs_used_in_graph</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="c1"># Store a reference to the function in the graph to ensure it stays alive</span>
  <span class="c1"># as long as the graph lives. When the graph is destroyed, the function</span>
  <span class="c1"># is left to the garbage collector for destruction as well.</span>
  <span class="n">graph</span><span class="o">.</span><span class="n">_py_funcs_used_in_graph</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">if</span> <span class="n">eager</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_script_ops</span><span class="o">.</span><span class="n">eager_py_func</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">is_async</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">is_async</span><span class="p">(),</span>
        <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">stateful</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_script_ops</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span>
          <span class="nb">input</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_script_ops</span><span class="o">.</span><span class="n">py_func_stateless</span><span class="p">(</span>
          <span class="nb">input</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span> <span class="k">if</span> <span class="n">is_list_or_tuple</span> <span class="k">else</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="c1"># TODO(akshayka): Implement higher-order derivatives.</span>
<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;EagerPyFunc&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_EagerPyFuncGrad</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">*</span><span class="n">dy</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the gradient of an EagerPyFunc.&quot;&quot;&quot;</span>

  <span class="n">token</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">eagerly_executed_grad</span><span class="p">(</span><span class="o">*</span><span class="n">dy</span><span class="p">):</span>
    <span class="n">tape</span><span class="p">,</span> <span class="n">eager_inputs</span><span class="p">,</span> <span class="n">eager_outputs</span> <span class="o">=</span> <span class="n">tape_cache</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">eager_outputs</span><span class="p">,</span> <span class="n">eager_inputs</span><span class="p">,</span> <span class="n">output_gradients</span><span class="o">=</span><span class="n">dy</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_internal_py_func</span><span class="p">(</span>
        <span class="n">func</span><span class="o">=</span><span class="n">eagerly_executed_grad</span><span class="p">,</span>
        <span class="n">inp</span><span class="o">=</span><span class="n">dy</span><span class="p">,</span>
        <span class="n">Tout</span><span class="o">=</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">],</span>
        <span class="n">eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">is_grad_func</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;py_function&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">eager_py_func</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a python function into a TensorFlow op that executes it eagerly.</span>

<span class="sd">  This function allows expressing computations in a TensorFlow graph as</span>
<span class="sd">  Python functions. In particular, it wraps a Python function `func`</span>
<span class="sd">  in a once-differentiable TensorFlow operation that executes it with eager</span>
<span class="sd">  execution enabled. As a consequence, `tf.py_function` makes it</span>
<span class="sd">  possible to express control flow using Python constructs (`if`, `while`,</span>
<span class="sd">  `for`, etc.), instead of TensorFlow control flow constructs (`tf.cond`,</span>
<span class="sd">  `tf.while_loop`). For example, you might use `tf.py_function` to</span>
<span class="sd">  implement the log huber function:</span>

<span class="sd">  ```python</span>
<span class="sd">  def log_huber(x, m):</span>
<span class="sd">    if tf.abs(x) &lt;= m:</span>
<span class="sd">      return x**2</span>
<span class="sd">    else:</span>
<span class="sd">      return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))</span>

<span class="sd">  x = tf.compat.v1.placeholder(tf.float32)</span>
<span class="sd">  m = tf.compat.v1.placeholder(tf.float32)</span>

<span class="sd">  y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)</span>
<span class="sd">  dy_dx = tf.gradients(y, x)[0]</span>

<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">    # The session executes `log_huber` eagerly. Given the feed values below,</span>
<span class="sd">    # it will take the first branch, so `y` evaluates to 1.0 and</span>
<span class="sd">    # `dy_dx` evaluates to 2.0.</span>
<span class="sd">    y, dy_dx = sess.run([y, dy_dx], feed_dict={x: 1.0, m: 2.0})</span>
<span class="sd">  ```</span>

<span class="sd">  You can also use `tf.py_function` to debug your models at runtime</span>
<span class="sd">  using Python tools, i.e., you can isolate portions of your code that</span>
<span class="sd">  you want to debug, wrap them in Python functions and insert `pdb` tracepoints</span>
<span class="sd">  or print statements as desired, and wrap those functions in</span>
<span class="sd">  `tf.py_function`.</span>

<span class="sd">  For more information on eager execution, see the</span>
<span class="sd">  [Eager guide](https://tensorflow.org/guide/eager).</span>

<span class="sd">  `tf.py_function` is similar in spirit to `tf.compat.v1.py_func`, but unlike</span>
<span class="sd">  the latter, the former lets you use TensorFlow operations in the wrapped</span>
<span class="sd">  Python function. In particular, while `tf.compat.v1.py_func` only runs on CPUs</span>
<span class="sd">  and</span>
<span class="sd">  wraps functions that take NumPy arrays as inputs and return NumPy arrays as</span>
<span class="sd">  outputs, `tf.py_function` can be placed on GPUs and wraps functions</span>
<span class="sd">  that take Tensors as inputs, execute TensorFlow operations in their bodies,</span>
<span class="sd">  and return Tensors as outputs.</span>

<span class="sd">  Like `tf.compat.v1.py_func`, `tf.py_function` has the following limitations</span>
<span class="sd">  with respect to serialization and distribution:</span>

<span class="sd">  * The body of the function (i.e. `func`) will not be serialized in a</span>
<span class="sd">    `GraphDef`. Therefore, you should not use this function if you need to</span>
<span class="sd">    serialize your model and restore it in a different environment.</span>

<span class="sd">  * The operation must run in the same address space as the Python program</span>
<span class="sd">    that calls `tf.py_function()`. If you are using distributed</span>
<span class="sd">    TensorFlow, you must run a `tf.distribute.Server` in the same process as the</span>
<span class="sd">    program that calls `tf.py_function()` and you must pin the created</span>
<span class="sd">    operation to a device in that server (e.g. using `with tf.device():`).</span>


<span class="sd">  Args:</span>
<span class="sd">    func: A Python function which accepts a list of `Tensor` objects having</span>
<span class="sd">      element types that match the corresponding `tf.Tensor` objects in `inp`</span>
<span class="sd">      and returns a list of `Tensor` objects (or a single `Tensor`, or `None`)</span>
<span class="sd">      having element types that match the corresponding values in `Tout`.</span>
<span class="sd">    inp: A list of `Tensor` objects.</span>
<span class="sd">    Tout: A list or tuple of tensorflow data types or a single tensorflow data</span>
<span class="sd">      type if there is only one, indicating what `func` returns; an empty list</span>
<span class="sd">      if no value is returned (i.e., if the return value is `None`).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` or a single `Tensor` which `func` computes; an empty list</span>
<span class="sd">    if `func` returns None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">host_address_space</span><span class="p">()):</span>
      <span class="k">return</span> <span class="n">_internal_py_func</span><span class="p">(</span>
          <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">_internal_py_func</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">py_func_common</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">stateful</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a python function and uses it as a TensorFlow op.</span>

<span class="sd">  Given a python function `func`, which takes numpy arrays as its</span>
<span class="sd">  arguments and returns numpy arrays as its outputs, wrap this function as an</span>
<span class="sd">  operation in a TensorFlow graph. The following snippet constructs a simple</span>
<span class="sd">  TensorFlow graph that invokes the `np.sinh()` NumPy function as a operation</span>
<span class="sd">  in the graph:</span>

<span class="sd">  ```python</span>
<span class="sd">  def my_func(x):</span>
<span class="sd">    # x will be a numpy array with the contents of the placeholder below</span>
<span class="sd">    return np.sinh(x)</span>
<span class="sd">  input = tf.compat.v1.placeholder(tf.float32)</span>
<span class="sd">  y = tf.compat.v1.py_func(my_func, [input], tf.float32)</span>
<span class="sd">  ```</span>

<span class="sd">  **N.B.** The `tf.compat.v1.py_func()` operation has the following known</span>
<span class="sd">  limitations:</span>

<span class="sd">  * The body of the function (i.e. `func`) will not be serialized in a</span>
<span class="sd">    `GraphDef`. Therefore, you should not use this function if you need to</span>
<span class="sd">    serialize your model and restore it in a different environment.</span>

<span class="sd">  * The operation must run in the same address space as the Python program</span>
<span class="sd">    that calls `tf.compat.v1.py_func()`. If you are using distributed</span>
<span class="sd">    TensorFlow, you</span>
<span class="sd">    must run a `tf.distribute.Server` in the same process as the program that</span>
<span class="sd">    calls</span>
<span class="sd">    `tf.compat.v1.py_func()` and you must pin the created operation to a device</span>
<span class="sd">    in that</span>
<span class="sd">    server (e.g. using `with tf.device():`).</span>

<span class="sd">  Args:</span>
<span class="sd">    func: A Python function, which accepts `ndarray` objects as arguments and</span>
<span class="sd">      returns a list of `ndarray` objects (or a single `ndarray`). This function</span>
<span class="sd">      must accept as many arguments as there are tensors in `inp`, and these</span>
<span class="sd">      argument types will match the corresponding `tf.Tensor` objects in `inp`.</span>
<span class="sd">      The returns `ndarray`s must match the number and types defined `Tout`.</span>
<span class="sd">      Important Note: Input and output numpy `ndarray`s of `func` are not</span>
<span class="sd">        guaranteed to be copies. In some cases their underlying memory will be</span>
<span class="sd">        shared with the corresponding TensorFlow tensors. In-place modification</span>
<span class="sd">        or storing `func` input or return values in python datastructures</span>
<span class="sd">        without explicit (np.)copy can have non-deterministic consequences.</span>
<span class="sd">    inp: A list of `Tensor` objects.</span>
<span class="sd">    Tout: A list or tuple of tensorflow data types or a single tensorflow data</span>
<span class="sd">      type if there is only one, indicating what `func` returns.</span>
<span class="sd">    stateful: (Boolean.) If True, the function should be considered stateful. If</span>
<span class="sd">      a function is stateless, when given the same input it will return the same</span>
<span class="sd">      output and have no observable side effects. Optimizations such as common</span>
<span class="sd">      subexpression elimination are only performed on stateless operations.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` or a single `Tensor` which `func` computes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">])</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">result</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># Mimic the automatic unwrapping in graph-mode py_func</span>
      <span class="n">result</span><span class="p">,</span> <span class="o">=</span> <span class="n">result</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">host_address_space</span><span class="p">()):</span>
      <span class="k">return</span> <span class="n">_internal_py_func</span><span class="p">(</span>
          <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span>
          <span class="n">inp</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span>
          <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span>
          <span class="n">stateful</span><span class="o">=</span><span class="n">stateful</span><span class="p">,</span>
          <span class="n">eager</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">_internal_py_func</span><span class="p">(</span>
      <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">stateful</span><span class="o">=</span><span class="n">stateful</span><span class="p">,</span> <span class="n">eager</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;tf.py_func is deprecated in TF V2. Instead, there are two</span>
<span class="s2">    options available in V2.</span>
<span class="s2">    - tf.py_function takes a python function which manipulates tf eager</span>
<span class="s2">    tensors instead of numpy arrays. It&#39;s easy to convert a tf eager tensor to</span>
<span class="s2">    an ndarray (just call tensor.numpy()) but having access to eager tensors</span>
<span class="s2">    means `tf.py_function`s can use accelerators such as GPUs as well as</span>
<span class="s2">    being differentiable using a gradient tape.</span>
<span class="s2">    - tf.numpy_function maintains the semantics of the deprecated tf.py_func</span>
<span class="s2">    (it is not differentiable, and manipulates numpy arrays). It drops the</span>
<span class="s2">    stateful argument making all functions stateful.</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;py_func&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">py_func</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">stateful</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">py_func_common</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">stateful</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">py_func</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">py_func_common</span><span class="o">.</span><span class="vm">__doc__</span>


<div class="viewcode-block" id="numpy_function"><a class="viewcode-back" href="../../../../index.html#tensorflow.numpy_function">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;numpy_function&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">numpy_function</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a python function and uses it as a TensorFlow op.</span>

<span class="sd">  Given a python function `func` wrap this function as an operation in a</span>
<span class="sd">  TensorFlow function. `func` must take numpy arrays as its arguments and</span>
<span class="sd">  return numpy arrays as its outputs.</span>

<span class="sd">  The following example creates a TensorFlow graph with `np.sinh()` as an</span>
<span class="sd">  operation in the graph:</span>

<span class="sd">  &gt;&gt;&gt; def my_numpy_func(x):</span>
<span class="sd">  ...   # x will be a numpy array with the contents of the input to the</span>
<span class="sd">  ...   # tf.function</span>
<span class="sd">  ...   return np.sinh(x)</span>
<span class="sd">  &gt;&gt;&gt; @tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])</span>
<span class="sd">  ... def tf_function(input):</span>
<span class="sd">  ...   y = tf.numpy_function(my_numpy_func, [input], tf.float32)</span>
<span class="sd">  ...   return y * y</span>
<span class="sd">  &gt;&gt;&gt; tf_function(tf.constant(1.))</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.3810978&gt;</span>

<span class="sd">  Comparison to `tf.py_function`:</span>
<span class="sd">  `tf.py_function` and `tf.numpy_function` are very similar, except that</span>
<span class="sd">  `tf.numpy_function` takes numpy arrays, and not `tf.Tensor`s. If you want the</span>
<span class="sd">  function to contain `tf.Tensors`, and have any TensorFlow operations executed</span>
<span class="sd">  in the function be differentiable, please use `tf.py_function`.</span>

<span class="sd">  Note: The `tf.numpy_function` operation has the following known</span>
<span class="sd">  limitations:</span>

<span class="sd">  * The body of the function (i.e. `func`) will not be serialized in a</span>
<span class="sd">    `tf.SavedModel`. Therefore, you should not use this function if you need to</span>
<span class="sd">    serialize your model and restore it in a different environment.</span>

<span class="sd">  * The operation must run in the same address space as the Python program</span>
<span class="sd">    that calls `tf.numpy_function()`. If you are using distributed</span>
<span class="sd">    TensorFlow, you must run a `tf.distribute.Server` in the same process as the</span>
<span class="sd">    program that calls `tf.numpy_function`  you must pin the created</span>
<span class="sd">    operation to a device in that server (e.g. using `with tf.device():`).</span>

<span class="sd">  * Since the function takes numpy arrays, you cannot take gradients</span>
<span class="sd">    through a numpy_function. If you require something that is differentiable,</span>
<span class="sd">    please consider using tf.py_function.</span>

<span class="sd">  * The resulting function is assumed stateful and will never be optimized.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: A Python function, which accepts `numpy.ndarray` objects as arguments</span>
<span class="sd">      and returns a list of `numpy.ndarray` objects (or a single</span>
<span class="sd">      `numpy.ndarray`). This function must accept as many arguments as there are</span>
<span class="sd">      tensors in `inp`, and these argument types will match the corresponding</span>
<span class="sd">      `tf.Tensor` objects in `inp`. The returns `numpy.ndarray`s must match the</span>
<span class="sd">      number and types defined `Tout`.</span>
<span class="sd">      Important Note: Input and output `numpy.ndarray`s of `func` are not</span>
<span class="sd">        guaranteed to be copies. In some cases their underlying memory will be</span>
<span class="sd">        shared with the corresponding TensorFlow tensors. In-place modification</span>
<span class="sd">        or storing `func` input or return values in python datastructures</span>
<span class="sd">        without explicit (np.)copy can have non-deterministic consequences.</span>
<span class="sd">    inp: A list of `tf.Tensor` objects.</span>
<span class="sd">    Tout: A list or tuple of tensorflow data types or a single tensorflow data</span>
<span class="sd">      type if there is only one, indicating what `func` returns.</span>
<span class="sd">    name: (Optional) A name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Single or list of `tf.Tensor` which `func` computes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">py_func_common</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">stateful</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;PyFunc&quot;</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;PyFuncStateless&quot;</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>