

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.variable_scope &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.variable_scope</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.variable_scope</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;A class to store named variables and a scope operator to manage sharing.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span> <span class="k">as</span> <span class="nn">collections_lib</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">enum</span>  <span class="c1"># pylint: disable=g-bad-import-order</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six</span> <span class="k">import</span> <span class="n">iteritems</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span><span class="p">,</span> <span class="nb">zip</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">tf2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="k">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">monitoring</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">init_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">variables</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">function_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_contextlib</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_inspect</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;AUTO_REUSE&quot;</span><span class="p">,</span> <span class="s2">&quot;VariableScope&quot;</span><span class="p">,</span> <span class="s2">&quot;get_variable_scope&quot;</span><span class="p">,</span> <span class="s2">&quot;get_variable&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_local_variable&quot;</span><span class="p">,</span> <span class="s2">&quot;variable_scope&quot;</span><span class="p">,</span> <span class="s2">&quot;variable_op_scope&quot;</span><span class="p">,</span>
    <span class="s2">&quot;no_regularizer&quot;</span><span class="p">,</span> <span class="s2">&quot;VariableSynchronization&quot;</span><span class="p">,</span> <span class="s2">&quot;VariableAggregation&quot;</span>
<span class="p">]</span>

<span class="n">_api_usage_gauge</span> <span class="o">=</span> <span class="n">monitoring</span><span class="o">.</span><span class="n">BoolGauge</span><span class="p">(</span>
    <span class="s2">&quot;/tensorflow/api/resource_variables&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Whether variable_scope.enable_resource_variables() is called.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_PartitionInfo</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Holds partition info used by initializer functions.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full_shape</span><span class="p">,</span> <span class="n">var_offset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      full_shape: Tuple or list of `int` indicating the full combined shape of</span>
<span class="sd">        the partitioned variables.</span>
<span class="sd">      var_offset: Tuple or list of `int` specifying offset of this partition</span>
<span class="sd">        with respect to the full variable for each dimension.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `full_shape` or `var_offset` is not a sequence.</span>
<span class="sd">      ValueError: If `full_shape` or `var_offset` differ in length. If</span>
<span class="sd">        `var_offset` exceeds `full_shape` in any dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">full_shape</span><span class="p">,</span> <span class="n">collections_lib</span><span class="o">.</span><span class="n">Sequence</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">full_shape</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;`full_shape` must be a sequence (like tuple or list) instead of &quot;</span> <span class="o">+</span>
          <span class="nb">type</span><span class="p">(</span><span class="n">full_shape</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var_offset</span><span class="p">,</span> <span class="n">collections_lib</span><span class="o">.</span><span class="n">Sequence</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">var_offset</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;`var_offset` must be a sequence (like tuple or list) instead of &quot;</span> <span class="o">+</span>
          <span class="nb">type</span><span class="p">(</span><span class="n">var_offset</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_offset</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Expected equal length, but `var_offset` is of length </span><span class="si">{}</span><span class="s2"> while &quot;</span>
          <span class="s2">&quot;full_shape is of length </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="nb">len</span><span class="p">(</span><span class="n">var_offset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_shape</span><span class="p">)))</span>

    <span class="k">for</span> <span class="n">offset</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">var_offset</span><span class="p">,</span> <span class="n">full_shape</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">offset</span> <span class="o">&gt;=</span> <span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Expected 0 &lt;= offset &lt; shape but found offset=</span><span class="si">{}</span><span class="s2">, shape=</span><span class="si">{}</span><span class="s2"> for &quot;</span>
            <span class="s2">&quot;var_offset=</span><span class="si">{}</span><span class="s2">, full_shape=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">var_offset</span><span class="p">,</span>
                                                  <span class="n">full_shape</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_full_shape</span> <span class="o">=</span> <span class="n">full_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_var_offset</span> <span class="o">=</span> <span class="n">var_offset</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">full_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">var_offset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_var_offset</span>

  <span class="k">def</span> <span class="nf">single_offset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the offset when the variable is partitioned in at most one dim.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: Tuple or list of `int` indicating the shape of one specific</span>
<span class="sd">        variable partition.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `int` representing the offset in the dimension along which the variable is</span>
<span class="sd">       partitioned. Returns 0 if the variable is not being partitioned.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: Depending on self.single_slice_dim().</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">single_slice_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_slice_dim</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># If this variable is not being partitioned at all, single_slice_dim() could</span>
    <span class="c1"># return None.</span>
    <span class="k">if</span> <span class="n">single_slice_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_offset</span><span class="p">[</span><span class="n">single_slice_dim</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">single_slice_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the slice dim when the variable is partitioned only in one dim.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: Tuple or list of `int` indicating the shape of one specific</span>
<span class="sd">        variable partition.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `int` representing the dimension that the variable is partitioned in, or</span>
<span class="sd">      `None` if the variable doesn&#39;t seem to be partitioned at all.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `shape` is not a sequence.</span>
<span class="sd">      ValueError: If `shape` is not the same length as `self.full_shape`. If</span>
<span class="sd">        the variable is partitioned in more than one dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">collections_lib</span><span class="o">.</span><span class="n">Sequence</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;`shape` must be a sequence (like tuple or list) instead of &quot;</span> <span class="o">+</span>
          <span class="nb">type</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Expected equal length, but received shape=</span><span class="si">{}</span><span class="s2"> of length </span><span class="si">{}</span><span class="s2"> while &quot;</span>
          <span class="s2">&quot;self.full_shape=</span><span class="si">{}</span><span class="s2"> is of length </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">,</span>
                                                       <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">)))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_offset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;With self.var_offset=</span><span class="si">{}</span><span class="s2">, a partition of shape=</span><span class="si">{}</span><span class="s2"> would exceed &quot;</span>
            <span class="s2">&quot;self.full_shape=</span><span class="si">{}</span><span class="s2"> in dimension </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">var_offset</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

    <span class="n">slice_dim</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)):</span>
      <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="n">slice_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot use single_slice_dim() with shape=</span><span class="si">{}</span><span class="s2"> and &quot;</span>
            <span class="s2">&quot;self.full_shape=</span><span class="si">{}</span><span class="s2"> since slice dim could be either dimension </span><span class="si">{}</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;or </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">slice_dim</span><span class="p">))</span>
      <span class="n">slice_dim</span> <span class="o">=</span> <span class="n">i</span>

    <span class="k">return</span> <span class="n">slice_dim</span>


<span class="k">class</span> <span class="nc">_ReuseMode</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Mode for variable access within a variable scope.&quot;&quot;&quot;</span>

  <span class="c1"># Indicates that variables are to be fetched if they already exist or</span>
  <span class="c1"># otherwise created.</span>
  <span class="n">AUTO_REUSE</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="c1"># TODO(alive): For TensorFlow 2.0, Deprecate True/False/None API in favor of</span>
  <span class="c1">#              enum values.</span>
  <span class="c1"># REUSE_FALSE = 2</span>
  <span class="c1"># REUSE_TRUE = 3</span>


<span class="c1"># TODO(apassos) remove these forwarding symbols.</span>
<span class="n">VariableSynchronization</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">VariableSynchronization</span>  <span class="c1"># pylint: disable=invalid-name</span>
<span class="n">VariableAggregation</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">VariableAggregation</span>  <span class="c1"># pylint: disable=invalid-name</span>

<span class="n">AUTO_REUSE</span> <span class="o">=</span> <span class="n">_ReuseMode</span><span class="o">.</span><span class="n">AUTO_REUSE</span>
<span class="n">tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;AUTO_REUSE&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">export_constant</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;AUTO_REUSE&quot;</span><span class="p">)</span>
<span class="n">AUTO_REUSE</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">When passed in as the value for the `reuse` flag, AUTO_REUSE indicates that</span>
<span class="s2">get_variable() should create the requested variable if it doesn&#39;t exist or, if</span>
<span class="s2">it does exist, simply return it.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">_DEFAULT_USE_RESOURCE</span> <span class="o">=</span> <span class="n">tf2</span><span class="o">.</span><span class="n">enabled</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;enable_resource_variables&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">enable_resource_variables</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Creates resource variables by default.</span>

<span class="sd">  Resource variables are improved versions of TensorFlow variables with a</span>
<span class="sd">  well-defined memory model. Accessing a resource variable reads its value, and</span>
<span class="sd">  all ops which access a specific read value of the variable are guaranteed to</span>
<span class="sd">  see the same value for that tensor. Writes which happen after a read (by</span>
<span class="sd">  having a control or data dependency on the read) are guaranteed not to affect</span>
<span class="sd">  the value of the read tensor, and similarly writes which happen before a read</span>
<span class="sd">  are guaranteed to affect the value. No guarantees are made about unordered</span>
<span class="sd">  read/write pairs.</span>

<span class="sd">  Calling tf.enable_resource_variables() lets you opt-in to this TensorFlow 2.0</span>
<span class="sd">  feature.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">global</span> <span class="n">_DEFAULT_USE_RESOURCE</span>
  <span class="n">_DEFAULT_USE_RESOURCE</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">_api_usage_gauge</span><span class="o">.</span><span class="n">get_cell</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;resource_variables_enabled&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">resource_variables_enabled</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns `True` if resource variables are enabled.</span>

<span class="sd">  Resource variables are improved versions of TensorFlow variables with a</span>
<span class="sd">  well-defined memory model. Accessing a resource variable reads its value, and</span>
<span class="sd">  all ops which access a specific read value of the variable are guaranteed to</span>
<span class="sd">  see the same value for that tensor. Writes which happen after a read (by</span>
<span class="sd">  having a control or data dependency on the read) are guaranteed not to affect</span>
<span class="sd">  the value of the read tensor, and similarly writes which happen before a read</span>
<span class="sd">  are guaranteed to affect the value. No guarantees are made about unordered</span>
<span class="sd">  read/write pairs.</span>

<span class="sd">  Calling tf.enable_resource_variables() lets you opt-in to this TensorFlow 2.0</span>
<span class="sd">  feature.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">global</span> <span class="n">_DEFAULT_USE_RESOURCE</span>
  <span class="k">return</span> <span class="n">_DEFAULT_USE_RESOURCE</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;non-resource variables are not supported in the long term&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;disable_resource_variables&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">disable_resource_variables</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Opts out of resource variables.</span>

<span class="sd">  If your code needs tf.disable_resource_variables() to be called to work</span>
<span class="sd">  properly please file a bug.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">global</span> <span class="n">_DEFAULT_USE_RESOURCE</span>
  <span class="n">_DEFAULT_USE_RESOURCE</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">_api_usage_gauge</span><span class="o">.</span><span class="n">get_cell</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_VariableStore</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Variable store that carries a number of named Variables.</span>

<span class="sd">  New variable names and new variables can be created; all stored</span>
<span class="sd">  variables are initialized with the initializer passed to __init__.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    vars: a dictionary with string names (same as passed in GetVar) as keys and</span>
<span class="sd">      the corresponding TensorFlow Variables as values.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create a variable store.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># A dictionary of the stored TensorFlow variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioned_vars</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># A dict of the stored PartitionedVariables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_store_eager_variables</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">get_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">name</span><span class="p">,</span>
                   <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                   <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                   <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                   <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets an existing variable with these parameters or create a new one.</span>

<span class="sd">    If a variable with the given name is already stored, we return the stored</span>
<span class="sd">    variable. Otherwise, we create a new one.</span>

<span class="sd">    Set `reuse` to `True` when you only want to reuse existing Variables.</span>
<span class="sd">    Set `reuse` to `False` when you only want to create new Variables.</span>
<span class="sd">    Set `reuse` to None (the default) or tf.compat.v1.AUTO_REUSE when you want</span>
<span class="sd">    variables to be created if they don&#39;t exist or returned if they do.</span>

<span class="sd">    If initializer is `None` (the default), the default initializer passed in</span>
<span class="sd">    the constructor is used. If that one is `None` too, we use a new</span>
<span class="sd">    `glorot_uniform_initializer`. If initializer is a Tensor, we use</span>
<span class="sd">    it as a value and derive the shape from the initializer.</span>

<span class="sd">    If a partitioner is provided, a `PartitionedVariable` is returned.</span>
<span class="sd">    Accessing this object as a `Tensor` returns the shards concatenated along</span>
<span class="sd">    the partition axis.</span>

<span class="sd">    Some useful partitioners are available.  See, e.g.,</span>
<span class="sd">    `variable_axis_size_partitioner` and `min_max_variable_partitioner`.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the new or existing variable.</span>
<span class="sd">      shape: Shape of the new or existing variable.</span>
<span class="sd">      dtype: Type of the new or existing variable (defaults to `DT_FLOAT`).</span>
<span class="sd">      initializer: Initializer for the variable.</span>
<span class="sd">      regularizer: A (Tensor -&gt; Tensor or None) function; the result of applying</span>
<span class="sd">        it on a newly created variable will be added to the collection</span>
<span class="sd">        GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.</span>
<span class="sd">      reuse: a Boolean, None, or tf.AUTO_REUSE. Controls reuse or creation of</span>
<span class="sd">        variables. When eager execution is enabled  this argument is always</span>
<span class="sd">        forced to be False.</span>
<span class="sd">      trainable: If `True` also add the variable to the graph collection</span>
<span class="sd">        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`). `trainable`</span>
<span class="sd">        defaults to `True`, unless `synchronization` is set to `ON_READ`, in</span>
<span class="sd">        which case it defaults to `False`.</span>
<span class="sd">      collections: List of graph collections keys to add the `Variable` to.</span>
<span class="sd">        Defaults to `[GraphKeys.GLOBAL_VARIABLES]` (see `tf.Variable`).</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the `Variable` reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      partitioner: Optional callable that accepts a fully defined `TensorShape`</span>
<span class="sd">        and dtype of the `Variable` to be created, and returns a list of</span>
<span class="sd">        partitions for each axis (currently only one axis can be partitioned).</span>
<span class="sd">      validate_shape: If False, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If True, the default, the shape of initial_value</span>
<span class="sd">        must be known.</span>
<span class="sd">      use_resource: If False, creates a regular Variable. If True, creates</span>
<span class="sd">        instead an experimental ResourceVariable which has well-defined</span>
<span class="sd">        semantics. Defaults to False (will later change to True). When eager</span>
<span class="sd">        execution is enabled this argument is always forced to be true.</span>
<span class="sd">      custom_getter: Callable that takes as a first argument the true getter,</span>
<span class="sd">        and allows overwriting the internal get_variable method. The signature</span>
<span class="sd">        of `custom_getter` should match that of this method,</span>
<span class="sd">        but the most future-proof version will allow for changes: `def</span>
<span class="sd">          custom_getter(getter, *args, **kwargs)`.  Direct access to</span>
<span class="sd">        all `get_variable` parameters is also allowed: `def</span>
<span class="sd">          custom_getter(getter, name, *args, **kwargs)`.  A simple identity</span>
<span class="sd">        custom getter that simply creates variables with modified names is:</span>
<span class="sd">          ```python</span>
<span class="sd">        def custom_getter(getter, name, *args, **kwargs): return getter(name +</span>
<span class="sd">          &#39;_suffix&#39;, *args, **kwargs) ```</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses when to</span>
<span class="sd">        synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The created or existing `Variable` (or `PartitionedVariable`, if a</span>
<span class="sd">      partitioner was used).</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: when creating a new variable and shape is not declared,</span>
<span class="sd">        when reusing a variable and specifying a conflicting shape,</span>
<span class="sd">        or when violating reuse during variable creation.</span>
<span class="sd">      RuntimeError: when eager execution is enabled and not called from an</span>
<span class="sd">        EagerVariableStore.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">custom_getter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">custom_getter</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Passed a custom_getter which is not callable: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                       <span class="n">custom_getter</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="c1"># Variable creation and initialization takes place in `init_scope`s;</span>
        <span class="c1"># as such, if an `init_scope` lifts us into the eager context, then we</span>
        <span class="c1"># need to use `ResourceVariable`s.</span>
        <span class="n">use_resource</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Note that it&#39;s fine to reuse eager variables whose initialization was</span>
    <span class="c1"># lifted from a function-building graph into the eager context (that&#39;s why</span>
    <span class="c1"># the following clause is not wrapped in an `init_scope`); lifted variables</span>
    <span class="c1"># are tracked by the graph&#39;s `VariableStore`.</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store_eager_variables</span> <span class="ow">and</span> <span class="n">reuse</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;When eager execution is enabled variable reuse is only supported&quot;</span>
            <span class="s2">&quot; when an EagerVariableStore is active. See the documentation on&quot;</span>
            <span class="s2">&quot; EagerVariableStore for example usage.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store_eager_variables</span><span class="p">:</span>
        <span class="n">reuse</span> <span class="o">=</span> <span class="n">AUTO_REUSE</span>

    <span class="c1"># If a *_ref type is passed in an error would be triggered further down the</span>
    <span class="c1"># stack. We prevent this using base_dtype to get a non-ref version of the</span>
    <span class="c1"># type, before doing anything else. When _ref types are removed in favor of</span>
    <span class="c1"># resources, this line can be removed.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="c1"># .base_dtype not existing means that we will try and use the raw dtype</span>
      <span class="c1"># which was passed in - this might be a NumPy type which is valid.</span>
      <span class="k">pass</span>

    <span class="c1"># This is the main logic of get_variable.  However, custom_getter</span>
    <span class="c1"># may override this logic.  So we save it as a callable and pass</span>
    <span class="c1"># it to custom_getter.</span>
    <span class="c1"># Note: the parameters of _true_getter, and their documentation, match</span>
    <span class="c1"># *exactly* item-for-item with the docstring of this method.</span>
    <span class="k">def</span> <span class="nf">_true_getter</span><span class="p">(</span>  <span class="c1"># pylint: disable=missing-docstring</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
      <span class="n">is_scalar</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">collections_lib</span><span class="o">.</span><span class="n">Sequence</span><span class="p">)</span> <span class="ow">and</span>
          <span class="ow">not</span> <span class="n">shape</span><span class="p">)</span>
      <span class="c1"># Partitioned variable case</span>
      <span class="k">if</span> <span class="n">partitioner</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_scalar</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">partitioner</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Partitioner must be callable, but received: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                           <span class="n">partitioner</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_partitioned_variable</span><span class="p">(</span>
              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
              <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
              <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
              <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
              <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span>
              <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
              <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
              <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
              <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
              <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
              <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
              <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
              <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
              <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

      <span class="c1"># Special case for partitioned variable to allow reuse without having to</span>
      <span class="c1"># specify partitioner.</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">reuse</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="n">partitioner</span> <span class="ow">is</span> <span class="kc">None</span>
          <span class="ow">and</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioned_vars</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_partitioned_variable</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
            <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
            <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
            <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
            <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
            <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
            <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
            <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
            <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

      <span class="c1"># Single variable case</span>
      <span class="k">if</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/part_0&quot;</span> <span class="o">%</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;No partitioner was provided, but a partitioned version of the &quot;</span>
            <span class="s2">&quot;variable was found: </span><span class="si">%s</span><span class="s2">/part_0. Perhaps a variable of the same &quot;</span>
            <span class="s2">&quot;name was already created with partitioning?&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>

      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_single_variable</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
          <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">custom_getter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Handle backwards compatibility with getter arguments that were added</span>
      <span class="c1"># to the API after users started writing custom getters.</span>
      <span class="n">custom_getter_kwargs</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s2">&quot;getter&quot;</span><span class="p">:</span> <span class="n">_true_getter</span><span class="p">,</span>
          <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
          <span class="s2">&quot;shape&quot;</span><span class="p">:</span> <span class="n">shape</span><span class="p">,</span>
          <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span>
          <span class="s2">&quot;initializer&quot;</span><span class="p">:</span> <span class="n">initializer</span><span class="p">,</span>
          <span class="s2">&quot;regularizer&quot;</span><span class="p">:</span> <span class="n">regularizer</span><span class="p">,</span>
          <span class="s2">&quot;reuse&quot;</span><span class="p">:</span> <span class="n">reuse</span><span class="p">,</span>
          <span class="s2">&quot;trainable&quot;</span><span class="p">:</span> <span class="n">trainable</span><span class="p">,</span>
          <span class="s2">&quot;collections&quot;</span><span class="p">:</span> <span class="n">collections</span><span class="p">,</span>
          <span class="s2">&quot;caching_device&quot;</span><span class="p">:</span> <span class="n">caching_device</span><span class="p">,</span>
          <span class="s2">&quot;partitioner&quot;</span><span class="p">:</span> <span class="n">partitioner</span><span class="p">,</span>
          <span class="s2">&quot;validate_shape&quot;</span><span class="p">:</span> <span class="n">validate_shape</span><span class="p">,</span>
          <span class="s2">&quot;use_resource&quot;</span><span class="p">:</span> <span class="n">use_resource</span><span class="p">,</span>
          <span class="s2">&quot;synchronization&quot;</span><span class="p">:</span> <span class="n">synchronization</span><span class="p">,</span>
          <span class="s2">&quot;aggregation&quot;</span><span class="p">:</span> <span class="n">aggregation</span><span class="p">,</span>
      <span class="p">}</span>
      <span class="c1"># `fn_args` and `has_kwargs` can handle functions, `functools.partial`,</span>
      <span class="c1"># `lambda`.</span>
      <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;constraint&quot;</span> <span class="ow">in</span> <span class="n">function_utils</span><span class="o">.</span><span class="n">fn_args</span><span class="p">(</span><span class="n">custom_getter</span><span class="p">)</span> <span class="ow">or</span>
          <span class="n">function_utils</span><span class="o">.</span><span class="n">has_kwargs</span><span class="p">(</span><span class="n">custom_getter</span><span class="p">)):</span>
        <span class="n">custom_getter_kwargs</span><span class="p">[</span><span class="s2">&quot;constraint&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">constraint</span>
      <span class="k">return</span> <span class="n">custom_getter</span><span class="p">(</span><span class="o">**</span><span class="n">custom_getter_kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">_true_getter</span><span class="p">(</span>
          <span class="n">name</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
          <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
          <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_partitioned_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">name</span><span class="p">,</span>
                                <span class="n">partitioner</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                                <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets or creates a sharded variable list with these parameters.</span>

<span class="sd">    The `partitioner` must be a callable that accepts a fully defined</span>
<span class="sd">    `TensorShape` and returns a sequence of integers (the `partitions`).</span>
<span class="sd">    These integers describe how to partition the given sharded `Variable`</span>
<span class="sd">    along the given dimension.  That is, `partitions[1] = 3` means split</span>
<span class="sd">    the `Variable` into 3 shards along dimension 1.  Currently, sharding along</span>
<span class="sd">    only one axis is supported.</span>

<span class="sd">    If the list of variables with the given name (prefix) is already stored,</span>
<span class="sd">    we return the stored variables. Otherwise, we create a new one.</span>

<span class="sd">    Set `reuse` to `True` when you only want to reuse existing Variables.</span>
<span class="sd">    Set `reuse` to `False` when you only want to create new Variables.</span>
<span class="sd">    Set `reuse` to None (the default) or tf.compat.v1.AUTO_REUSE when you want</span>
<span class="sd">    variables to be created if they don&#39;t exist or returned if they do.</span>

<span class="sd">    If initializer is `None` (the default), the default initializer passed in</span>
<span class="sd">    the constructor is used. If that one is `None` too, we use a new</span>
<span class="sd">    `glorot_uniform_initializer`. If initializer is a Tensor, we use</span>
<span class="sd">    it as a value and derive the shape from the initializer.</span>

<span class="sd">    If the initializer is a callable, then it will be called for each</span>
<span class="sd">    shard.  Otherwise the initializer should match the shape of the entire</span>
<span class="sd">    sharded Variable, and it will be sliced accordingly for each shard.</span>

<span class="sd">    Some useful partitioners are available.  See, e.g.,</span>
<span class="sd">    `variable_axis_size_partitioner` and `min_max_variable_partitioner`.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: the name of the new or existing sharded variable.</span>
<span class="sd">      partitioner: Optional callable that accepts a fully defined `TensorShape`</span>
<span class="sd">        and `dtype` of the Variable to be created, and returns a list of</span>
<span class="sd">        partitions for each axis (currently only one axis can be partitioned).</span>
<span class="sd">      shape: shape of the new or existing sharded variable.</span>
<span class="sd">      dtype: type of the new or existing sharded variable (defaults to</span>
<span class="sd">        `DT_FLOAT`).</span>
<span class="sd">      initializer: initializer for the sharded variable.</span>
<span class="sd">      regularizer: a (Tensor -&gt; Tensor or None) function; the result of applying</span>
<span class="sd">        it on a newly created variable will be added to the collection</span>
<span class="sd">        GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.</span>
<span class="sd">      reuse: a Boolean, None, or tf.AUTO_REUSE. Controls reuse or creation of</span>
<span class="sd">        variables.</span>
<span class="sd">      trainable: If `True` also add the variable to the graph collection</span>
<span class="sd">        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).</span>
<span class="sd">      collections: List of graph collections keys to add the Variable to.</span>
<span class="sd">        Defaults to `[GraphKeys.GLOBAL_VARIABLES]` (see `tf.Variable`).</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      validate_shape: If False, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If True, the default, the shape of initial_value</span>
<span class="sd">        must be known.</span>
<span class="sd">      use_resource: If False, creates a regular Variable. If True, creates an</span>
<span class="sd">        experimental ResourceVariable which has well-defined semantics. Defaults</span>
<span class="sd">        to False (will later change to True).</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses when to</span>
<span class="sd">        synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `PartitionedVariable` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: when creating a new variable and shape is not declared,</span>
<span class="sd">        when reusing a variable and specifying a conflicting shape,</span>
<span class="sd">        when violating reuse during variable creation, or if an existing</span>
<span class="sd">        sharded variable exists for the given name but with different sharding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">initializing_from_value</span> <span class="o">=</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">initializer</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;A partitioner was provided, but an unpartitioned version of the &quot;</span>
          <span class="s2">&quot;variable was found: </span><span class="si">%s</span><span class="s2">.  Perhaps a variable of the same name was &quot;</span>
          <span class="s2">&quot;already created without partitioning?&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">initializing_from_value</span><span class="p">:</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">initializer</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>

    <span class="n">partitions</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">reuse</span> <span class="ow">or</span> <span class="n">partitioner</span><span class="p">:</span>
      <span class="n">partitions</span> <span class="o">=</span> <span class="n">_call_partitioner</span><span class="p">(</span><span class="n">partitioner</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioned_vars</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">reuse</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Partitioned variable with name </span><span class="si">%s</span><span class="s2"> already exists. Did you mean to &quot;</span>
            <span class="s2">&quot;set reuse=True or reuse=tf.AUTO_REUSE in VarScope?&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>

      <span class="n">existing_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioned_vars</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">existing_var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Trying to reuse partitioned variable </span><span class="si">%s</span><span class="s2">, but specified shape </span><span class="si">%s</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;and found shape </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">existing_var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()))</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">existing_var</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Trying to reuse partitioned variable </span><span class="si">%s</span><span class="s2">, but specified dtype </span><span class="si">%s</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;and found dtype </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">existing_var</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">partitions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
          <span class="n">existing_var</span><span class="o">.</span><span class="n">_get_partitions</span><span class="p">()</span> <span class="o">!=</span> <span class="n">partitions</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Trying to reuse partitioned variable </span><span class="si">%s</span><span class="s2">, but specified partitions &quot;</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> and found partitions </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
            <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">partitions</span><span class="p">,</span> <span class="n">existing_var</span><span class="o">.</span><span class="n">_get_partitions</span><span class="p">()))</span>
      <span class="c1"># pylint: enable=protected-access</span>

      <span class="k">return</span> <span class="n">existing_var</span>

    <span class="k">if</span> <span class="n">reuse</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;PartitionedVariable </span><span class="si">%s</span><span class="s2"> does not exist, or was not &quot;</span>
                       <span class="s2">&quot;created with tf.get_variable(). Did you mean to set &quot;</span>
                       <span class="s2">&quot;reuse=False or reuse=tf.AUTO_REUSE in VarScope?&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>

    <span class="n">slice_dim</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">=</span> <span class="n">_get_slice_dim_and_num_slices</span><span class="p">(</span><span class="n">partitions</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/part_0&quot;</span> <span class="o">%</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">:</span>
      <span class="k">if</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/part_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Partitioner returned a different partitioning than what was &quot;</span>
            <span class="s2">&quot;already found.  Partitioner returned </span><span class="si">%d</span><span class="s2"> shards, and shard &quot;</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/part_0 was found, but </span><span class="si">%s</span><span class="s2">/part_</span><span class="si">%d</span><span class="s2"> was not.&quot;</span> <span class="o">%</span>
            <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
      <span class="k">if</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/part_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Partitioner returned a different partitioning than what was &quot;</span>
            <span class="s2">&quot;already found.  Partitioner returned </span><span class="si">%d</span><span class="s2"> shards, and shard &quot;</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/part_0 was found, but so was the extra shard </span><span class="si">%s</span><span class="s2">/part_</span><span class="si">%d</span><span class="s2">.&quot;</span> <span class="o">%</span>
            <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">))</span>

    <span class="n">vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">var_offset</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
        <span class="n">_iter_slices</span><span class="p">(</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span> <span class="n">num_slices</span><span class="p">,</span> <span class="n">slice_dim</span><span class="p">)):</span>
      <span class="n">partition_info</span> <span class="o">=</span> <span class="n">_PartitionInfo</span><span class="p">(</span>
          <span class="n">full_shape</span><span class="o">=</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span> <span class="n">var_offset</span><span class="o">=</span><span class="n">var_offset</span><span class="p">)</span>
      <span class="n">var_full_name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/part_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
          <span class="n">var_full_name</span> <span class="o">+</span> <span class="s2">&quot;/PartitionedInitializer&quot;</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># Create the tensor to initialize the variable with default value.</span>
        <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">init</span><span class="p">,</span> <span class="n">initializing_from_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_default_initializer</span><span class="p">(</span>
              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">initializing_from_value</span><span class="p">:</span>
            <span class="n">init_shape</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">init_shape</span> <span class="o">=</span> <span class="n">var_shape</span>
        <span class="k">elif</span> <span class="n">callable</span><span class="p">(</span><span class="n">initializer</span><span class="p">):</span>
          <span class="n">init</span> <span class="o">=</span> <span class="n">initializer</span>
          <span class="n">init_shape</span> <span class="o">=</span> <span class="n">var_shape</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initializer</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
          <span class="n">init</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">initializer</span><span class="p">,</span> <span class="n">var_offset</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">)</span>
          <span class="c1"># Use the dtype of the given tensor.</span>
          <span class="n">dtype</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
          <span class="n">init_shape</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">init</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">initializer</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
          <span class="n">init</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">var_offset</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">)</span>
          <span class="n">init_shape</span> <span class="o">=</span> <span class="kc">None</span>

      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_single_variable</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">var_full_name</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">init_shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
            <span class="n">partition_info</span><span class="o">=</span><span class="n">partition_info</span><span class="p">,</span>
            <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
            <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
            <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
            <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
            <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
            <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
            <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
            <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">var</span><span class="o">.</span><span class="n">_set_save_slice_info</span><span class="p">(</span>
          <span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">SaveSliceInfo</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span> <span class="n">var_offset</span><span class="p">,</span>
                                           <span class="n">var_shape</span><span class="p">))</span>
      <span class="n">vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>

    <span class="n">partitioned_var</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">PartitionedVariable</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">variable_list</span><span class="o">=</span><span class="n">vs</span><span class="p">,</span>
        <span class="n">partitions</span><span class="o">=</span><span class="n">partitions</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store_eager_variables</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_partitioned_vars</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">partitioned_var</span>
    <span class="k">return</span> <span class="n">partitioned_var</span>

  <span class="k">def</span> <span class="nf">_get_single_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">name</span><span class="p">,</span>
                           <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                           <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">partition_info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                           <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                           <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get or create a single Variable (e.g.</span>

<span class="sd">    a shard or entire variable).</span>

<span class="sd">    See the documentation of get_variable above (ignore partitioning components)</span>
<span class="sd">    for details.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: see get_variable.</span>
<span class="sd">      shape: see get_variable.</span>
<span class="sd">      dtype: see get_variable.</span>
<span class="sd">      initializer: see get_variable.</span>
<span class="sd">      regularizer: see get_variable.</span>
<span class="sd">      partition_info: _PartitionInfo object.</span>
<span class="sd">      reuse: see get_variable.</span>
<span class="sd">      trainable: see get_variable.</span>
<span class="sd">      collections: see get_variable.</span>
<span class="sd">      caching_device: see get_variable.</span>
<span class="sd">      validate_shape: see get_variable.</span>
<span class="sd">      use_resource: see get_variable.</span>
<span class="sd">      constraint: see get_variable.</span>
<span class="sd">      synchronization: see get_variable.</span>
<span class="sd">      aggregation: see get_variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Variable.  See documentation of get_variable above.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: See documentation of get_variable above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set to true if initializer is a constant.</span>
    <span class="n">initializing_from_value</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">initializer</span><span class="p">):</span>
      <span class="n">initializing_from_value</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">initializing_from_value</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If initializer is a constant, do not specify shape.&quot;</span><span class="p">)</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">:</span>
      <span class="c1"># Here we handle the case when returning an existing variable.</span>
      <span class="k">if</span> <span class="n">reuse</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Variable </span><span class="si">%s</span><span class="s2"> already exists, disallowed.&quot;</span>
                   <span class="s2">&quot; Did you mean to set reuse=True or &quot;</span>
                   <span class="s2">&quot;reuse=tf.AUTO_REUSE in VarScope?&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
        <span class="c1"># ResourceVariables don&#39;t have an op associated with so no traceback</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">ResourceVariable</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>
        <span class="n">tb</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">traceback</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Throw away internal tf entries and only take a few lines. In some</span>
        <span class="c1"># cases the traceback can be longer (e.g. if someone uses factory</span>
        <span class="c1"># functions to create variables) so we take more than needed in the</span>
        <span class="c1"># default case.</span>
        <span class="n">tb</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tb</span> <span class="k">if</span> <span class="s2">&quot;tensorflow/python&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]][:</span><span class="mi">5</span><span class="p">]</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Originally defined at:</span><span class="se">\n\n</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">err_msg</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">traceback</span><span class="o">.</span><span class="n">format_list</span><span class="p">(</span><span class="n">tb</span><span class="p">))))</span>
      <span class="n">found_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">found_var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trying to share variable </span><span class="si">%s</span><span class="s2">, but specified shape </span><span class="si">%s</span><span class="s2">&quot;</span>
                         <span class="s2">&quot; and found shape </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">found_var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()))</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">found_var</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">dtype_str</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">name</span>
        <span class="n">found_type_str</span> <span class="o">=</span> <span class="n">found_var</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trying to share variable </span><span class="si">%s</span><span class="s2">, but specified dtype </span><span class="si">%s</span><span class="s2">&quot;</span>
                         <span class="s2">&quot; and found dtype </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype_str</span><span class="p">,</span> <span class="n">found_type_str</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">found_var</span>

    <span class="c1"># The code below handles only the case of creating a new variable.</span>
    <span class="k">if</span> <span class="n">reuse</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Variable </span><span class="si">%s</span><span class="s2"> does not exist, or was not created with &quot;</span>
                       <span class="s2">&quot;tf.get_variable(). Did you mean to set &quot;</span>
                       <span class="s2">&quot;reuse=tf.AUTO_REUSE in VarScope?&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># Create the tensor to initialize the variable with default value.</span>
    <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">initializer</span><span class="p">,</span> <span class="n">initializing_from_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_default_initializer</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># Enter an init scope when creating the initializer.</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">initializing_from_value</span><span class="p">:</span>
        <span class="n">init_val</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="n">variable_dtype</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Instantiate initializer if provided initializer is a type object.</span>
        <span class="k">if</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">initializer</span><span class="p">):</span>
          <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
          <span class="k">if</span> <span class="s2">&quot;partition_info&quot;</span> <span class="ow">in</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span><span class="o">.</span><span class="n">args</span><span class="p">:</span>
            <span class="n">init_val</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">initializer</span><span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
                <span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">partition_info</span><span class="o">=</span><span class="n">partition_info</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">init_val</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">initializer</span><span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
                <span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
          <span class="n">variable_dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf_inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span><span class="o">.</span><span class="n">defaults</span> <span class="ow">or</span> <span class="p">[]):</span>
          <span class="n">init_val</span> <span class="o">=</span> <span class="n">initializer</span>
          <span class="n">variable_dtype</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The initializer passed is not valid. It should &quot;</span>
                           <span class="s2">&quot;be a callable with no arguments and the &quot;</span>
                           <span class="s2">&quot;shape should not be provided or an instance of &quot;</span>
                           <span class="s2">&quot;`tf.keras.initializers.*&#39; and `shape` should be &quot;</span>
                           <span class="s2">&quot;fully defined.&quot;</span><span class="p">)</span>

    <span class="c1"># Create the variable.</span>
    <span class="k">if</span> <span class="n">use_resource</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Set the default value if unspecified.</span>
      <span class="n">use_resource</span> <span class="o">=</span> <span class="n">_DEFAULT_USE_RESOURCE</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">VariableV1</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">init_val</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">variable_dtype</span><span class="p">,</span>
        <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store_eager_variables</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">collections</span><span class="p">:</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store_eager_variables</span><span class="p">:</span>
      <span class="c1"># In eager mode we do not want to keep default references to Variable</span>
      <span class="c1"># objects as this will prevent their memory from being released.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_vars</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Created variable </span><span class="si">%s</span><span class="s2"> with shape </span><span class="si">%s</span><span class="s2"> and init </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                 <span class="nb">format</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">initializer</span><span class="p">)</span>

    <span class="c1"># Run the regularizer if requested and save the resulting loss.</span>
    <span class="k">if</span> <span class="n">regularizer</span><span class="p">:</span>
      <span class="k">def</span> <span class="nf">make_regularizer_op</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/Regularizer/&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">regularizer</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">regularizer</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lazy_eval_tensor</span> <span class="o">=</span> <span class="n">_LazyEvalTensor</span><span class="p">(</span><span class="n">make_regularizer_op</span><span class="p">)</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">REGULARIZATION_LOSSES</span><span class="p">,</span>
                              <span class="n">lazy_eval_tensor</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">v</span>

  <span class="c1"># Initialize variable when no initializer provided</span>
  <span class="k">def</span> <span class="nf">_get_default_initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Provide a default initializer and a corresponding value.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: see get_variable.</span>
<span class="sd">      shape: see get_variable.</span>
<span class="sd">      dtype: see get_variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      initializer and initializing_from_value. See get_variable above.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: When giving unsupported dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">shape</span>
    <span class="c1"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span>
    <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
      <span class="n">initializer</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">glorot_uniform_initializer</span><span class="p">()</span>
      <span class="n">initializing_from_value</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span>
    <span class="c1"># If dtype is DT_BOOL, provide a default value `FALSE`</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_unsigned</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_bool</span> <span class="ow">or</span>
          <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">string</span><span class="p">):</span>
      <span class="n">initializer</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()</span>
      <span class="n">initializing_from_value</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;An initializer for variable </span><span class="si">%s</span><span class="s2"> of </span><span class="si">%s</span><span class="s2"> is required&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">initializer</span><span class="p">,</span> <span class="n">initializing_from_value</span>


<span class="k">class</span> <span class="nc">_LazyEvalTensor</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A Tensor-like object that only evaluates its thunk when used.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thunk</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes a _LazyEvalTensor object.</span>

<span class="sd">    Args:</span>
<span class="sd">      thunk: A callable. A thunk which computes the value of the tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thunk</span> <span class="o">=</span> <span class="n">thunk</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_master_tensor</span> <span class="o">=</span> <span class="n">thunk</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">name</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">as_ref</span>
    <span class="k">assert</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thunk</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_make_master_property</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">prop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_master_tensor</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">return</span> <span class="n">prop</span>

<span class="n">_master_property_list</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;op&quot;</span><span class="p">,</span> <span class="s2">&quot;shape&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;value_index&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_name</span> <span class="ow">in</span> <span class="n">_master_property_list</span><span class="p">:</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">_LazyEvalTensor</span><span class="p">,</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_make_master_property</span><span class="p">(</span><span class="n">_name</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_make_master_method</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_master_tensor</span><span class="p">,</span> <span class="n">name</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">return</span> <span class="n">method</span>

<span class="n">_master_method_list</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;get_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;__str__&quot;</span><span class="p">,</span> <span class="s2">&quot;shape_as_list&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_name</span> <span class="ow">in</span> <span class="n">_master_method_list</span><span class="p">:</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">_LazyEvalTensor</span><span class="p">,</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_make_master_method</span><span class="p">(</span><span class="n">_name</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_make_op_method</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_as_tensor</span><span class="p">(),</span> <span class="n">name</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">return</span> <span class="n">method</span>

<span class="n">_op_list</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;__abs__&quot;</span><span class="p">,</span> <span class="s2">&quot;__add__&quot;</span><span class="p">,</span> <span class="s2">&quot;__and__&quot;</span><span class="p">,</span> <span class="s2">&quot;__bool__&quot;</span><span class="p">,</span> <span class="s2">&quot;__div__&quot;</span><span class="p">,</span> <span class="s2">&quot;__eq__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__floordiv__&quot;</span><span class="p">,</span> <span class="s2">&quot;__ge__&quot;</span><span class="p">,</span> <span class="s2">&quot;__getitem__&quot;</span><span class="p">,</span> <span class="s2">&quot;__gt__&quot;</span><span class="p">,</span> <span class="s2">&quot;__invert__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__iter__&quot;</span><span class="p">,</span> <span class="s2">&quot;__le__&quot;</span><span class="p">,</span> <span class="s2">&quot;__len__&quot;</span><span class="p">,</span> <span class="s2">&quot;__lt__&quot;</span><span class="p">,</span> <span class="s2">&quot;__matmul__&quot;</span><span class="p">,</span> <span class="s2">&quot;__mod__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__mul__&quot;</span><span class="p">,</span> <span class="s2">&quot;__ne__&quot;</span><span class="p">,</span> <span class="s2">&quot;__neg__&quot;</span><span class="p">,</span> <span class="s2">&quot;__nonzero__&quot;</span><span class="p">,</span> <span class="s2">&quot;__or__&quot;</span><span class="p">,</span> <span class="s2">&quot;__pow__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__radd__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rand__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rdiv__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rfloordiv__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rmatmul__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__rmod__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rmul__&quot;</span><span class="p">,</span> <span class="s2">&quot;__ror__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rpow__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rsub__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__rtruediv__&quot;</span><span class="p">,</span> <span class="s2">&quot;__rxor__&quot;</span><span class="p">,</span> <span class="s2">&quot;__sub__&quot;</span><span class="p">,</span> <span class="s2">&quot;__truediv__&quot;</span><span class="p">,</span> <span class="s2">&quot;__xor__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;eval&quot;</span><span class="p">,</span> <span class="s2">&quot;numpy&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_name</span> <span class="ow">in</span> <span class="n">_op_list</span><span class="p">:</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">_LazyEvalTensor</span><span class="p">,</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_make_op_method</span><span class="p">(</span><span class="n">_name</span><span class="p">))</span>


<span class="n">ops</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">(</span>
    <span class="n">_LazyEvalTensor</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="p">:</span> <span class="n">val</span><span class="o">.</span><span class="n">_as_tensor</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="p">)</span>

<span class="n">session</span><span class="o">.</span><span class="n">register_session_run_conversion_functions</span><span class="p">(</span>
    <span class="n">_LazyEvalTensor</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">fetch</span><span class="p">:</span> <span class="p">([</span><span class="n">fetch</span><span class="o">.</span><span class="n">_master_tensor</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">fetched_vals</span><span class="p">:</span> <span class="n">fetched_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="p">)</span>

<span class="n">ops</span><span class="o">.</span><span class="n">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">_LazyEvalTensor</span><span class="p">)</span>


<span class="c1"># To stop regularization, use this regularizer</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;no_regularizer&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">no_regularizer</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Use this function to prevent regularization of variables.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="kc">None</span>


<span class="c1"># TODO(alive): support caching devices and partitioned variables in Eager mode.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;VariableScope&quot;</span><span class="p">])</span>
<span class="k">class</span> <span class="nc">VariableScope</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Variable scope object to carry defaults to provide to `get_variable`.</span>

<span class="sd">  Many of the arguments we need for `get_variable` in a variable store are most</span>
<span class="sd">  easily handled with a context. This object is used for the defaults.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    name: name of the current scope, used as prefix in get_variable.</span>
<span class="sd">    initializer: default initializer passed to get_variable.</span>
<span class="sd">    regularizer: default regularizer passed to get_variable.</span>
<span class="sd">    reuse: Boolean, None, or tf.compat.v1.AUTO_REUSE, setting the reuse in</span>
<span class="sd">      get_variable. When eager execution is enabled this argument is always</span>
<span class="sd">      forced to be False.</span>
<span class="sd">    caching_device: string, callable, or None: the caching device passed to</span>
<span class="sd">      get_variable.</span>
<span class="sd">    partitioner: callable or `None`: the partitioner passed to `get_variable`.</span>
<span class="sd">    custom_getter: default custom getter passed to get_variable.</span>
<span class="sd">    name_scope: The name passed to `tf.name_scope`.</span>
<span class="sd">    dtype: default type passed to get_variable (defaults to DT_FLOAT).</span>
<span class="sd">    use_resource: if False, create a normal Variable; if True create an</span>
<span class="sd">      experimental ResourceVariable with well-defined semantics. Defaults to</span>
<span class="sd">      False (will later change to True). When eager execution is enabled this</span>
<span class="sd">      argument is always forced to be True.</span>
<span class="sd">    constraint: An optional projection function to be applied to the variable</span>
<span class="sd">      after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">      constraints or value constraints for layer weights). The function must</span>
<span class="sd">      take as input the unprojected Tensor representing the value of the</span>
<span class="sd">      variable and return the Tensor for the projected value (which must have</span>
<span class="sd">      the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">      distributed training.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">reuse</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
               <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name_scope</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
               <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new VariableScope with the given properties.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span> <span class="o">=</span> <span class="n">regularizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="n">caching_device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span> <span class="o">=</span> <span class="n">partitioner</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span> <span class="o">=</span> <span class="n">custom_getter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span> <span class="o">=</span> <span class="n">name_scope</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span> <span class="o">=</span> <span class="n">use_resource</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="n">constraint</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Caching devices is not yet supported &quot;</span>
                                  <span class="s2">&quot;when eager execution is enabled.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">AUTO_REUSE</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">original_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">reuse</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_resource</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">caching_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioner</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">custom_getter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span>

  <span class="k">def</span> <span class="nf">reuse_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reuse variables in this scope.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">set_initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initializer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set initializer for this scope.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>

  <span class="k">def</span> <span class="nf">set_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set data type for this scope.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>

  <span class="k">def</span> <span class="nf">set_use_resource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_resource</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets whether to use ResourceVariables for this scope.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">use_resource</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;When eager execution is enabled, &quot;</span>
                       <span class="s2">&quot;use_resource cannot be set to false.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span> <span class="o">=</span> <span class="n">use_resource</span>

  <span class="k">def</span> <span class="nf">set_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">regularizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set regularizer for this scope.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span> <span class="o">=</span> <span class="n">regularizer</span>

  <span class="k">def</span> <span class="nf">set_caching_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">caching_device</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set caching_device for this scope.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Caching devices are not yet supported &quot;</span>
                                <span class="s2">&quot;when eager execution is enabled.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="n">caching_device</span>

  <span class="k">def</span> <span class="nf">set_partitioner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partitioner</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set partitioner for this scope.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span> <span class="o">=</span> <span class="n">partitioner</span>

  <span class="k">def</span> <span class="nf">set_custom_getter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">custom_getter</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set custom getter for this scope.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span> <span class="o">=</span> <span class="n">custom_getter</span>

  <span class="k">def</span> <span class="nf">get_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get this scope&#39;s variables.&quot;&quot;&quot;</span>
    <span class="n">scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get this scope&#39;s trainable variables.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">global_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get this scope&#39;s global variables.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">local_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get this scope&#39;s local variables.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">var_store</span><span class="p">,</span>
                   <span class="n">name</span><span class="p">,</span>
                   <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                   <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                   <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets an existing variable with this name or create a new one.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">regularizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">regularizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span>
    <span class="k">if</span> <span class="n">caching_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">caching_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span>
    <span class="k">if</span> <span class="n">partitioner</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">partitioner</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span>
    <span class="k">if</span> <span class="n">custom_getter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">custom_getter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="n">reuse</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">use_resource</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">reuse</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reuse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span>
      <span class="k">if</span> <span class="n">use_resource</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_resource</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span>

    <span class="n">full_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="k">else</span> <span class="n">name</span>
    <span class="c1"># Variable names only depend on variable_scope (full_name here),</span>
    <span class="c1"># not name_scope, so we reset it below for the time of variable creation.</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
      <span class="c1"># Check that `initializer` dtype and `dtype` are consistent before</span>
      <span class="c1"># replacing them with defaults.</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
          <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">initializer</span><span class="p">)):</span>
        <span class="n">init_dtype</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
        <span class="k">if</span> <span class="n">init_dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Initializer type &#39;</span><span class="si">%s</span><span class="s2">&#39; and explicit dtype &#39;</span><span class="si">%s</span><span class="s2">&#39; &quot;</span>
                           <span class="s2">&quot;don&#39;t match.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">init_dtype</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span>
      <span class="k">if</span> <span class="n">constraint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">constraint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span>
      <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
      <span class="k">return</span> <span class="n">var_store</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="n">full_name</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
          <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
          <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_partitioned_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">var_store</span><span class="p">,</span>
                                <span class="n">name</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                                <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets an existing variable with this name or create a new one.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">initializer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span>
    <span class="k">if</span> <span class="n">regularizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">regularizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span>
    <span class="k">if</span> <span class="n">constraint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">constraint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span>
    <span class="k">if</span> <span class="n">caching_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">caching_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span>
    <span class="k">if</span> <span class="n">partitioner</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">partitioner</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
    <span class="k">if</span> <span class="n">use_resource</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">use_resource</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Private access to _get_partitioned_variable is not allowed when &quot;</span>
          <span class="s2">&quot;a custom getter is set.  Current custom getter: </span><span class="si">%s</span><span class="s2">.  &quot;</span>
          <span class="s2">&quot;It is likely that you&#39;re using create_partitioned_variables.  &quot;</span>
          <span class="s2">&quot;If so, consider instead using get_variable with a non-empty &quot;</span>
          <span class="s2">&quot;partitioner parameter instead.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">partitioner</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No partitioner was specified&quot;</span><span class="p">)</span>

    <span class="c1"># This allows the variable scope name to be used as the variable name if</span>
    <span class="c1"># this function is invoked with an empty name arg, for backward</span>
    <span class="c1"># compatibility with create_partitioned_variables().</span>
    <span class="n">full_name_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="n">full_name_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">full_name_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">full_name</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">full_name_list</span><span class="p">)</span>

    <span class="c1"># Variable names only depend on variable_scope (full_name here),</span>
    <span class="c1"># not name_scope, so we reset it below for the time of variable creation.</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">var_store</span><span class="o">.</span><span class="n">_get_partitioned_variable</span><span class="p">(</span>
          <span class="n">full_name</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
          <span class="n">reuse</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
          <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>


<span class="n">_VARSTORE_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;__variable_store&quot;</span><span class="p">,)</span>
<span class="n">_VARSCOPESTORE_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;__varscope&quot;</span><span class="p">,)</span>


<span class="k">class</span> <span class="nc">_VariableScopeStore</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A thread local store for the current variable scope and scope counts.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_VariableScopeStore</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_scope</span> <span class="o">=</span> <span class="n">VariableScope</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variable_scopes_count</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="k">def</span> <span class="nf">open_variable_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">scope_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable_scopes_count</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">variable_scopes_count</span><span class="p">[</span><span class="n">scope_name</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">variable_scopes_count</span><span class="p">[</span><span class="n">scope_name</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="k">def</span> <span class="nf">close_variable_subscopes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope_name</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variable_scopes_count</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
      <span class="k">if</span> <span class="n">scope_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">scope_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variable_scopes_count</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">def</span> <span class="nf">variable_scope_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable_scopes_count</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope_name</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_variable_scope_store</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the variable scope store for current thread.&quot;&quot;&quot;</span>
  <span class="n">scope_store</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">_VARSCOPESTORE_KEY</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">scope_store</span><span class="p">:</span>
    <span class="n">scope_store</span> <span class="o">=</span> <span class="n">_VariableScopeStore</span><span class="p">()</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">_VARSCOPESTORE_KEY</span><span class="p">,</span> <span class="n">scope_store</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">scope_store</span> <span class="o">=</span> <span class="n">scope_store</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">return</span> <span class="n">scope_store</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;get_variable_scope&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">get_variable_scope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the current variable scope.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_variable_scope_store</span><span class="p">()</span><span class="o">.</span><span class="n">current_scope</span>


<span class="k">def</span> <span class="nf">_get_default_variable_store</span><span class="p">():</span>
  <span class="n">store</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">_VARSTORE_KEY</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">store</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">store</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">store</span> <span class="o">=</span> <span class="n">_VariableStore</span><span class="p">()</span>
  <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">_VARSTORE_KEY</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">store</span>


<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">with_variable_store</span><span class="p">(</span><span class="n">store</span><span class="p">):</span>
  <span class="n">store_collection</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="n">_VARSTORE_KEY</span><span class="p">)</span>
  <span class="n">old</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">store_collection</span><span class="p">)</span>
  <span class="n">store_collection</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">store</span><span class="p">]</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">store_collection</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">old</span>


<span class="k">class</span> <span class="nc">EagerVariableStore</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper allowing functional layers to be used with eager execution.</span>

<span class="sd">  When eager execution is enabled Variables get deleted when they go out of</span>
<span class="sd">  scope, and are not stored in global collections by default. A lot of code</span>
<span class="sd">  (mostly the functional layers in tf.layers) assumes that variables are kept in</span>
<span class="sd">  a global list.</span>

<span class="sd">  EagerVariableStore can be used in conjunction with this code to make it</span>
<span class="sd">  eager-friendly. For example, to create a dense layer, use:</span>

<span class="sd">  ```</span>
<span class="sd">    container = tfe.EagerVariableStore()</span>
<span class="sd">    for input in dataset_iterator:</span>
<span class="sd">      with container.as_default():</span>
<span class="sd">        x = tf.compat.v1.layers.dense(input, name=&quot;l1&quot;)</span>
<span class="sd">    print(container.variables)  # Should print the variables used in the layer.</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">store</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">store</span><span class="o">.</span><span class="n">_store_eager_variables</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot construct EagerVariableStore from a &quot;</span>
                         <span class="s2">&quot;VariableStore object that does not hold eager &quot;</span>
                         <span class="s2">&quot;variables.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_store</span> <span class="o">=</span> <span class="n">store</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_store</span> <span class="o">=</span> <span class="n">_VariableStore</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_store</span><span class="o">.</span><span class="n">_store_eager_variables</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">as_default</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">with_variable_store</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_store</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_store</span><span class="o">.</span><span class="n">_vars</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store</span><span class="o">.</span><span class="n">_vars</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">trainable</span><span class="p">],</span>
                  <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">non_trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store</span><span class="o">.</span><span class="n">_vars</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">trainable</span><span class="p">],</span>
                  <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Copy this variable store and all of its contents.</span>

<span class="sd">    Variables contained in this store will be copied over to the new variable</span>
<span class="sd">    store, meaning that they can be modified without affecting the variables in</span>
<span class="sd">    this store.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A new EagerVariableStore instance containing copied variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">new_store</span> <span class="o">=</span> <span class="n">EagerVariableStore</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_store</span><span class="o">.</span><span class="n">_vars</span><span class="p">):</span>
      <span class="c1"># Strip device out of variable name.</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="n">stripped_var_name</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">stripped_var_name</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">[:</span><span class="n">index</span><span class="p">]</span>

      <span class="c1"># Create new variable with same value, name, and &quot;trainable&quot; flag.</span>
      <span class="n">new_var</span> <span class="o">=</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">ResourceVariable</span><span class="p">(</span>
          <span class="n">var</span><span class="o">.</span><span class="n">read_value</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="n">stripped_var_name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">trainable</span><span class="p">)</span>
      <span class="n">new_store</span><span class="o">.</span><span class="n">_store</span><span class="o">.</span><span class="n">_vars</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_var</span>
    <span class="k">return</span> <span class="n">new_store</span>
    <span class="c1"># pylint: enable=protected-access</span>


<span class="c1"># The argument list for get_variable must match arguments to get_local_variable.</span>
<span class="c1"># So, if you are updating the arguments, also update arguments to</span>
<span class="c1"># get_local_variable below.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;get_variable&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">get_variable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                 <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                 <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">get_variable_scope</span><span class="p">()</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
      <span class="n">_get_default_variable_store</span><span class="p">(),</span>
      <span class="n">name</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
      <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
      <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
      <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
      <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
      <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
      <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>


<span class="n">get_variable_or_local_docstring</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;&quot;&quot;</span><span class="si">%s</span><span class="s2"></span>

<span class="si">%s</span><span class="s2">This function prefixes the name with the current variable scope</span>
<span class="s2">and performs reuse checks. See the</span>
<span class="s2">[Variable Scope How To](https://tensorflow.org/guide/variables)</span>
<span class="s2">for an extensive description of how reusing works. Here is a basic example:</span>

<span class="s2">```python</span>
<span class="s2">def foo():</span>
<span class="s2">  with tf.variable_scope(&quot;foo&quot;, reuse=tf.AUTO_REUSE):</span>
<span class="s2">    v = tf.get_variable(&quot;v&quot;, [1])</span>
<span class="s2">  return v</span>

<span class="s2">v1 = foo()  # Creates v.</span>
<span class="s2">v2 = foo()  # Gets the same, existing v.</span>
<span class="s2">assert v1 == v2</span>
<span class="s2">```</span>

<span class="s2">If initializer is `None` (the default), the default initializer passed in</span>
<span class="s2">the variable scope will be used. If that one is `None` too, a</span>
<span class="s2">`glorot_uniform_initializer` will be used. The initializer can also be</span>
<span class="s2">a Tensor, in which case the variable is initialized to this value and shape.</span>

<span class="s2">Similarly, if the regularizer is `None` (the default), the default regularizer</span>
<span class="s2">passed in the variable scope will be used (if that is `None` too,</span>
<span class="s2">then by default no regularization is performed).</span>

<span class="s2">If a partitioner is provided, a `PartitionedVariable` is returned.</span>
<span class="s2">Accessing this object as a `Tensor` returns the shards concatenated along</span>
<span class="s2">the partition axis.</span>

<span class="s2">Some useful partitioners are available.  See, e.g.,</span>
<span class="s2">`variable_axis_size_partitioner` and `min_max_variable_partitioner`.</span>

<span class="s2">Args:</span>
<span class="s2">  name: The name of the new or existing variable.</span>
<span class="s2">  shape: Shape of the new or existing variable.</span>
<span class="s2">  dtype: Type of the new or existing variable (defaults to `DT_FLOAT`).</span>
<span class="s2">  initializer: Initializer for the variable if one is created. Can either be</span>
<span class="s2">    an initializer object or a Tensor. If it&#39;s a Tensor, its shape must be known</span>
<span class="s2">    unless validate_shape is False.</span>
<span class="s2">  regularizer: A (Tensor -&gt; Tensor or None) function; the result of</span>
<span class="s2">    applying it on a newly created variable will be added to the collection</span>
<span class="s2">    `tf.GraphKeys.REGULARIZATION_LOSSES` and can be used for regularization.</span>
<span class="s2">  </span><span class="si">%s</span><span class="s2">collections: List of graph collections keys to add the Variable to.</span>
<span class="s2">    Defaults to `[</span><span class="si">%s</span><span class="s2">]` (see `tf.Variable`).</span>
<span class="s2">  caching_device: Optional device string or function describing where the</span>
<span class="s2">    Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="s2">    device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="s2">    cache on the device where the Ops using the Variable reside, to</span>
<span class="s2">    deduplicate copying through `Switch` and other conditional statements.</span>
<span class="s2">  partitioner: Optional callable that accepts a fully defined `TensorShape`</span>
<span class="s2">    and `dtype` of the Variable to be created, and returns a list of</span>
<span class="s2">    partitions for each axis (currently only one axis can be partitioned).</span>
<span class="s2">  validate_shape: If False, allows the variable to be initialized with a</span>
<span class="s2">      value of unknown shape. If True, the default, the shape of initial_value</span>
<span class="s2">      must be known. For this to be used the initializer must be a Tensor and</span>
<span class="s2">      not an initializer object.</span>
<span class="s2">  use_resource: If False, creates a regular Variable. If true, creates an</span>
<span class="s2">    experimental ResourceVariable instead with well-defined semantics.</span>
<span class="s2">    Defaults to False (will later change to True). When eager execution is</span>
<span class="s2">    enabled this argument is always forced to be True.</span>
<span class="s2">  custom_getter: Callable that takes as a first argument the true getter, and</span>
<span class="s2">    allows overwriting the internal get_variable method.</span>
<span class="s2">    The signature of `custom_getter` should match that of this method,</span>
<span class="s2">    but the most future-proof version will allow for changes:</span>
<span class="s2">    `def custom_getter(getter, *args, **kwargs)`.  Direct access to</span>
<span class="s2">    all `get_variable` parameters is also allowed:</span>
<span class="s2">    `def custom_getter(getter, name, *args, **kwargs)`.  A simple identity</span>
<span class="s2">    custom getter that simply creates variables with modified names is:</span>
<span class="s2">    ```python</span>
<span class="s2">    def custom_getter(getter, name, *args, **kwargs):</span>
<span class="s2">      return getter(name + &#39;_suffix&#39;, *args, **kwargs)</span>
<span class="s2">    ```</span>
<span class="s2">  constraint: An optional projection function to be applied to the variable</span>
<span class="s2">    after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="s2">    constraints or value constraints for layer weights). The function must</span>
<span class="s2">    take as input the unprojected Tensor representing the value of the</span>
<span class="s2">    variable and return the Tensor for the projected value</span>
<span class="s2">    (which must have the same shape). Constraints are not safe to</span>
<span class="s2">    use when doing asynchronous distributed training.</span>
<span class="s2">  synchronization: Indicates when a distributed a variable will be</span>
<span class="s2">    aggregated. Accepted values are constants defined in the class</span>
<span class="s2">    `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="s2">    `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="s2">    when to synchronize.</span>
<span class="s2">  aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="s2">    Accepted values are constants defined in the class</span>
<span class="s2">    `tf.VariableAggregation`.</span>

<span class="s2">Returns:</span>
<span class="s2">  The created or existing `Variable` (or `PartitionedVariable`, if a</span>
<span class="s2">  partitioner was used).</span>

<span class="s2">Raises:</span>
<span class="s2">  ValueError: when creating a new variable and shape is not declared,</span>
<span class="s2">    when violating reuse during variable creation, or when `initializer` dtype</span>
<span class="s2">    and `dtype` don&#39;t match. Reuse is set inside `variable_scope`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
<span class="n">get_variable</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">get_variable_or_local_docstring</span> <span class="o">%</span> <span class="p">(</span>
    <span class="s2">&quot;Gets an existing variable with these parameters or create a new one.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="s2">&quot;trainable: If `True` also add the variable to the graph collection</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;    `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).</span><span class="se">\n</span><span class="s2">  &quot;</span><span class="p">,</span>
    <span class="s2">&quot;GraphKeys.GLOBAL_VARIABLES&quot;</span><span class="p">)</span>


<span class="c1"># The argument list for get_local_variable must match arguments to get_variable.</span>
<span class="c1"># So, if you are updating the arguments, also update arguments to get_variable.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;get_local_variable&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">get_local_variable</span><span class="p">(</span>  <span class="c1"># pylint: disable=missing-docstring</span>
    <span class="n">name</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># pylint: disable=unused-argument</span>
    <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">collections</span><span class="p">:</span>
    <span class="n">collections</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">get_variable</span><span class="p">(</span>
      <span class="n">name</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
      <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
      <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
      <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
      <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
      <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">)</span>


<span class="n">get_local_variable</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">get_variable_or_local_docstring</span> <span class="o">%</span> <span class="p">(</span>
    <span class="s2">&quot;Gets an existing *local* variable or creates a new one.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Behavior is the same as in `get_variable`, except that variables are</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;added to the `LOCAL_VARIABLES` collection and `trainable` is set to</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;`False`.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;GraphKeys.LOCAL_VARIABLES&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_partitioned_variable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                              <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gets or creates a sharded variable list with these parameters.</span>

<span class="sd">  The `partitioner` must be a callable that accepts a fully defined</span>
<span class="sd">  `TensorShape` and returns a sequence of integers (the `partitions`).</span>
<span class="sd">  These integers describe how to partition the given sharded `Variable`</span>
<span class="sd">  along the given dimension.  That is, `partitions[1] = 3` means split</span>
<span class="sd">  the `Variable` into 3 shards along dimension 1.  Currently, sharding along</span>
<span class="sd">  only one axis is supported.</span>

<span class="sd">  If the list of variables with the given name (prefix) is already stored,</span>
<span class="sd">  we return the stored variables. Otherwise, we create a new one.</span>

<span class="sd">  If initializer is `None` (the default), the default initializer passed in</span>
<span class="sd">  the constructor is used. If that one is `None` too, we use a new</span>
<span class="sd">  `glorot_uniform_initializer`. If initializer is a Tensor, we use</span>
<span class="sd">  it as a value and derive the shape from the initializer.</span>

<span class="sd">  If the initializer is a callable, then it will be called for each</span>
<span class="sd">  shard.  Otherwise the initializer should match the shape of the entire</span>
<span class="sd">  sharded Variable, and it will be sliced accordingly for each shard.</span>

<span class="sd">  Some useful partitioners are available.  See, e.g.,</span>
<span class="sd">  `variable_axis_size_partitioner` and `min_max_variable_partitioner`.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: The name of the new or existing variable.</span>
<span class="sd">    shape: Shape of the new or existing variable.</span>
<span class="sd">    dtype: Type of the new or existing variable (defaults to `DT_FLOAT`).</span>
<span class="sd">    initializer: Initializer for the variable if one is created.</span>
<span class="sd">    regularizer: A (Tensor -&gt; Tensor or None) function; the result of applying</span>
<span class="sd">      it on a newly created variable will be added to the collection</span>
<span class="sd">      GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.</span>
<span class="sd">    trainable: If `True` also add the variable to the graph collection</span>
<span class="sd">      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).</span>
<span class="sd">    collections: List of graph collections keys to add the Variable to. Defaults</span>
<span class="sd">      to `[GraphKeys.GLOBAL_VARIABLES]` (see `tf.Variable`).</span>
<span class="sd">    caching_device: Optional device string or function describing where the</span>
<span class="sd">      Variable should be cached for reading.  Defaults to the Variable&#39;s device.</span>
<span class="sd">      If not `None`, caches on another device.  Typical use is to cache on the</span>
<span class="sd">      device where the Ops using the Variable reside, to deduplicate copying</span>
<span class="sd">      through `Switch` and other conditional statements.</span>
<span class="sd">    partitioner: Optional callable that accepts a fully defined `TensorShape`</span>
<span class="sd">      and `dtype` of the Variable to be created, and returns a list of</span>
<span class="sd">      partitions for each axis (currently only one axis can be partitioned).</span>
<span class="sd">    validate_shape: If False, allows the variable to be initialized with a value</span>
<span class="sd">      of unknown shape. If True, the default, the shape of initial_value must be</span>
<span class="sd">      known.</span>
<span class="sd">    use_resource: If False, creates a regular Variable. If True, creates an</span>
<span class="sd">      experimental ResourceVariable instead which has well-defined semantics.</span>
<span class="sd">      Defaults to False (will later change to True).</span>
<span class="sd">    constraint: An optional projection function to be applied to the variable</span>
<span class="sd">      after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">      constraints or value constraints for layer weights). The function must</span>
<span class="sd">      take as input the unprojected Tensor representing the value of the</span>
<span class="sd">      variable and return the Tensor for the projected value (which must have</span>
<span class="sd">      the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">      distributed training.</span>
<span class="sd">    synchronization: Indicates when a distributed a variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class</span>
<span class="sd">      `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">      `AUTO` and the current `DistributionStrategy` chooses when to synchronize.</span>
<span class="sd">    aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class</span>
<span class="sd">      `tf.VariableAggregation`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple `(shards, partitions)` where `shards` is the list of `Variable`</span>
<span class="sd">    shards and `partitions` is the output of the partitioner on the input</span>
<span class="sd">    shape.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: when creating a new variable and shape is not declared,</span>
<span class="sd">      or when violating reuse during variable creation. Reuse is set inside</span>
<span class="sd">      `variable_scope`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">scope</span> <span class="o">=</span> <span class="n">get_variable_scope</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">scope</span><span class="o">.</span><span class="n">custom_getter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Private access to _get_partitioned_variable is not allowed when &quot;</span>
        <span class="s2">&quot;a custom getter is set.  Current custom getter: </span><span class="si">%s</span><span class="s2">.  &quot;</span>
        <span class="s2">&quot;It is likely that you&#39;re using create_partitioned_variables.  &quot;</span>
        <span class="s2">&quot;If so, consider instead using get_variable with a non-empty &quot;</span>
        <span class="s2">&quot;partitioner parameter instead.&quot;</span> <span class="o">%</span> <span class="n">scope</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">scope</span><span class="o">.</span><span class="n">_get_partitioned_variable</span><span class="p">(</span>
      <span class="n">_get_default_variable_store</span><span class="p">(),</span>
      <span class="n">name</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
      <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
      <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
      <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
      <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
      <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>
  <span class="c1"># pylint: enable=protected-access</span>


<span class="c1"># Named like a function for compatibility with the previous</span>
<span class="c1"># @tf_contextlib.contextmanager definition.</span>
<span class="k">class</span> <span class="nc">_pure_variable_scope</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;A context for the variable_scope, see `variable_scope` for docs.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">name_or_scope</span><span class="p">,</span>
               <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">old_name_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
               <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a context for the variable_scope, see `variable_scope` for docs.</span>

<span class="sd">    Note: this does not create a name scope.</span>

<span class="sd">    Args:</span>
<span class="sd">      name_or_scope: `string` or `VariableScope`: the scope to open.</span>
<span class="sd">      reuse: `True` or None, or tf.compat.v1.AUTO_REUSE; if `None`, we inherit</span>
<span class="sd">        the parent scope&#39;s reuse flag.</span>
<span class="sd">      initializer: default initializer for variables within this scope.</span>
<span class="sd">      regularizer: default regularizer for variables within this scope.</span>
<span class="sd">      caching_device: default caching device for variables within this scope.</span>
<span class="sd">      partitioner: default partitioner for variables within this scope.</span>
<span class="sd">      custom_getter: default custom getter for variables within this scope.</span>
<span class="sd">      old_name_scope: the original name scope when re-entering a variable scope.</span>
<span class="sd">      dtype: type of the variables within this scope (defaults to `DT_FLOAT`).</span>
<span class="sd">      use_resource: If False, variables in this scope will be regular Variables.</span>
<span class="sd">        If True, experimental ResourceVariables will be creates instead, with</span>
<span class="sd">        well-defined semantics. Defaults to False (will later change to True).</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span> <span class="o">=</span> <span class="n">name_or_scope</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span> <span class="o">=</span> <span class="n">regularizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="n">caching_device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span> <span class="o">=</span> <span class="n">partitioner</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span> <span class="o">=</span> <span class="n">custom_getter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_old_name_scope</span> <span class="o">=</span> <span class="n">old_name_scope</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span> <span class="o">=</span> <span class="n">use_resource</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="n">constraint</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_var_store</span> <span class="o">=</span> <span class="n">_get_default_variable_store</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span> <span class="o">=</span> <span class="n">get_variable_scope_store</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_variable_scope_object</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span> <span class="n">VariableScope</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_new_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">name</span>
      <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">_name_scope</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="c1"># Handler for the case when we jump to a shared scope.  We create a new</span>
      <span class="c1">#   VariableScope (self._var_scope_object) that contains a copy of the</span>
      <span class="c1">#   provided shared scope, possibly with changed reuse and initializer, if</span>
      <span class="c1">#   the user requested this.</span>
      <span class="n">variable_scope_object</span> <span class="o">=</span> <span class="n">VariableScope</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">reuse</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_new_name</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">regularizer</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">partitioner</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">,</span>
          <span class="n">name_scope</span><span class="o">=</span><span class="n">name_scope</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">use_resource</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_caching_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_partitioner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_custom_getter</span><span class="p">(</span>
            <span class="n">_maybe_wrap_custom_getter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">))</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_use_resource</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cached_variable_scope_object</span> <span class="o">=</span> <span class="n">variable_scope_object</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Begins the scope block.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A VariableScope.</span>
<span class="sd">    Raises:</span>
<span class="sd">      ValueError: when trying to reuse within a create scope, or create within</span>
<span class="sd">        a reuse scope, or if reuse is not `None` or `True`.</span>
<span class="sd">      TypeError: when the types of some arguments are not appropriate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_old</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">current_scope</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span> <span class="n">VariableScope</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">open_variable_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_new_name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_old_subscopes</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">variable_scopes_count</span><span class="p">)</span>
      <span class="n">variable_scope_object</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_variable_scope_object</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Handler for the case when we just prolong current variable scope.</span>
      <span class="c1">#   VariableScope with name extended by the provided one, and inherited</span>
      <span class="c1">#   reuse and initializer (except if the user provided values to set).</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_new_name</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">name</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="ow">or</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">reuse</span><span class="p">)</span>  <span class="c1"># Re-using is inherited by sub-scopes.</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_name_scope</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_name_scope</span>
      <span class="n">variable_scope_object</span> <span class="o">=</span> <span class="n">VariableScope</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_new_name</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">regularizer</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">partitioner</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">use_resource</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">,</span>
          <span class="n">name_scope</span><span class="o">=</span><span class="n">name_scope</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_caching_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_partitioner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_custom_getter</span><span class="p">(</span>
            <span class="n">_maybe_wrap_custom_getter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">_old</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">))</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variable_scope_object</span><span class="o">.</span><span class="n">set_use_resource</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">open_variable_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_new_name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">current_scope</span> <span class="o">=</span> <span class="n">variable_scope_object</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_variable_scope_object</span> <span class="o">=</span> <span class="n">variable_scope_object</span>
    <span class="k">return</span> <span class="n">variable_scope_object</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">current_scope</span> <span class="ow">is</span>
        <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_variable_scope_object</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Improper nesting of variable_scope.&quot;</span><span class="p">)</span>
    <span class="c1"># If jumping out from a non-prolonged scope, restore counts.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span> <span class="n">VariableScope</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">variable_scopes_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_subscopes</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">close_variable_subscopes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_new_name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_var_scope_store</span><span class="o">.</span><span class="n">current_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old</span>


<span class="k">def</span> <span class="nf">_maybe_wrap_custom_getter</span><span class="p">(</span><span class="n">custom_getter</span><span class="p">,</span> <span class="n">old_getter</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrap a call to a custom_getter to use the old_getter internally.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">old_getter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">custom_getter</span>

  <span class="c1"># The new custom_getter should call the old one</span>
  <span class="k">def</span> <span class="nf">wrapped_custom_getter</span><span class="p">(</span><span class="n">getter</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Call:</span>
    <span class="c1">#  custom_getter(</span>
    <span class="c1">#    lambda: old_getter(true_getter, ...), *args, **kwargs)</span>
    <span class="c1"># which means custom_getter will call old_getter, which</span>
    <span class="c1"># will call the true_getter, perform any intermediate</span>
    <span class="c1"># processing, and return the results to the current</span>
    <span class="c1"># getter, which will also perform additional processing.</span>
    <span class="k">return</span> <span class="n">custom_getter</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">old_getter</span><span class="p">,</span> <span class="n">getter</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">wrapped_custom_getter</span>


<span class="k">def</span> <span class="nf">_get_unique_variable_scope</span><span class="p">(</span><span class="n">prefix</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get a name with the given prefix unique in the current variable scope.&quot;&quot;&quot;</span>
  <span class="n">var_scope_store</span> <span class="o">=</span> <span class="n">get_variable_scope_store</span><span class="p">()</span>
  <span class="n">current_scope</span> <span class="o">=</span> <span class="n">get_variable_scope</span><span class="p">()</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">current_scope</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">prefix</span> <span class="k">if</span> <span class="n">current_scope</span><span class="o">.</span><span class="n">name</span> <span class="k">else</span> <span class="n">prefix</span>
  <span class="k">if</span> <span class="n">var_scope_store</span><span class="o">.</span><span class="n">variable_scope_count</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">prefix</span>
  <span class="n">idx</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="k">while</span> <span class="n">var_scope_store</span><span class="o">.</span><span class="n">variable_scope_count</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">)</span>


<span class="c1"># Named like a function for backwards compatibility with the</span>
<span class="c1"># @tf_contextlib.contextmanager version, which was switched to a class to avoid</span>
<span class="c1"># some object creation overhead.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;variable_scope&quot;</span><span class="p">])</span>  <span class="c1"># pylint: disable=invalid-name</span>
<span class="k">class</span> <span class="nc">variable_scope</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A context manager for defining ops that creates variables (layers).</span>

<span class="sd">  This context manager validates that the (optional) `values` are from the same</span>
<span class="sd">  graph, ensures that graph is the default graph, and pushes a name scope and a</span>
<span class="sd">  variable scope.</span>

<span class="sd">  If `name_or_scope` is not None, it is used as is. If `name_or_scope` is None,</span>
<span class="sd">  then `default_name` is used.  In that case, if the same name has been</span>
<span class="sd">  previously used in the same scope, it will be made unique by appending `_N`</span>
<span class="sd">  to it.</span>

<span class="sd">  Variable scope allows you to create new variables and to share already created</span>
<span class="sd">  ones while providing checks to not create or share by accident. For details,</span>
<span class="sd">  see the [Variable Scope How To](https://tensorflow.org/guide/variables), here</span>
<span class="sd">  we present only a few basic examples.</span>

<span class="sd">  Simple example of how to create a new variable:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;):</span>
<span class="sd">      with tf.compat.v1.variable_scope(&quot;bar&quot;):</span>
<span class="sd">          v = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">          assert v.name == &quot;foo/bar/v:0&quot;</span>
<span class="sd">  ```</span>

<span class="sd">  Simple example of how to reenter a premade variable scope safely:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;) as vs:</span>
<span class="sd">    pass</span>

<span class="sd">  # Re-enter the variable scope.</span>
<span class="sd">  with tf.compat.v1.variable_scope(vs,</span>
<span class="sd">                         auxiliary_name_scope=False) as vs1:</span>
<span class="sd">    # Restore the original name_scope.</span>
<span class="sd">    with tf.name_scope(vs1.original_name_scope):</span>
<span class="sd">        v = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">        assert v.name == &quot;foo/v:0&quot;</span>
<span class="sd">        c = tf.constant([1], name=&quot;c&quot;)</span>
<span class="sd">        assert c.name == &quot;foo/c:0&quot;</span>
<span class="sd">  ```</span>

<span class="sd">  Keep in mind that the counters for `default_name` are discarded once the</span>
<span class="sd">  parent scope is exited. Therefore when the code re-enters the scope (for</span>
<span class="sd">  instance by saving it), all nested default_name counters will be restarted.</span>

<span class="sd">  For instance:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;) as vs:</span>
<span class="sd">    with tf.compat.v1.variable_scope(None, default_name=&quot;bar&quot;):</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;a&quot;, [1])</span>
<span class="sd">      assert v.name == &quot;foo/bar/a:0&quot;, v.name</span>
<span class="sd">    with tf.compat.v1.variable_scope(None, default_name=&quot;bar&quot;):</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;b&quot;, [1])</span>
<span class="sd">      assert v.name == &quot;foo/bar_1/b:0&quot;</span>

<span class="sd">  with tf.compat.v1.variable_scope(vs):</span>
<span class="sd">    with tf.compat.v1.variable_scope(None, default_name=&quot;bar&quot;):</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;c&quot;, [1])</span>
<span class="sd">      assert v.name == &quot;foo/bar/c:0&quot;   # Uses bar instead of bar_2!</span>
<span class="sd">  ```</span>

<span class="sd">  Basic example of sharing a variable AUTO_REUSE:</span>

<span class="sd">  ```python</span>
<span class="sd">  def foo():</span>
<span class="sd">    with tf.compat.v1.variable_scope(&quot;foo&quot;, reuse=tf.compat.v1.AUTO_REUSE):</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">    return v</span>

<span class="sd">  v1 = foo()  # Creates v.</span>
<span class="sd">  v2 = foo()  # Gets the same, existing v.</span>
<span class="sd">  assert v1 == v2</span>
<span class="sd">  ```</span>

<span class="sd">  Basic example of sharing a variable with reuse=True:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;):</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;, reuse=True):</span>
<span class="sd">      v1 = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">  assert v1 == v</span>
<span class="sd">  ```</span>

<span class="sd">  Sharing a variable by capturing a scope and setting reuse:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;) as scope:</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">      scope.reuse_variables()</span>
<span class="sd">      v1 = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">  assert v1 == v</span>
<span class="sd">  ```</span>

<span class="sd">  To prevent accidental sharing of variables, we raise an exception when getting</span>
<span class="sd">  an existing variable in a non-reusing scope.</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;):</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">      v1 = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">      #  Raises ValueError(&quot;... v already exists ...&quot;).</span>
<span class="sd">  ```</span>

<span class="sd">  Similarly, we raise an exception when trying to get a variable that does not</span>
<span class="sd">  exist in reuse mode.</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.compat.v1.variable_scope(&quot;foo&quot;, reuse=True):</span>
<span class="sd">      v = tf.compat.v1.get_variable(&quot;v&quot;, [1])</span>
<span class="sd">      #  Raises ValueError(&quot;... v does not exists ...&quot;).</span>
<span class="sd">  ```</span>

<span class="sd">  Note that the `reuse` flag is inherited: if we open a reusing scope, then all</span>
<span class="sd">  its sub-scopes become reusing as well.</span>

<span class="sd">  A note about name scoping: Setting `reuse` does not impact the naming of other</span>
<span class="sd">  ops such as mult. See related discussion on</span>
<span class="sd">  [github#6189](https://github.com/tensorflow/tensorflow/issues/6189)</span>

<span class="sd">  Note that up to and including version 1.0, it was allowed (though explicitly</span>
<span class="sd">  discouraged) to pass False to the reuse argument, yielding undocumented</span>
<span class="sd">  behaviour slightly different from None. Starting at 1.1.0 passing None and</span>
<span class="sd">  False as reuse has exactly the same effect.</span>

<span class="sd">  A note about using variable scopes in multi-threaded environment: Variable</span>
<span class="sd">  scopes are thread local, so one thread will not see another thread&#39;s current</span>
<span class="sd">  scope. Also, when using `default_name`, unique scopes names are also generated</span>
<span class="sd">  only on a per thread basis. If the same name was used within a different</span>
<span class="sd">  thread, that doesn&#39;t prevent a new thread from creating the same scope.</span>
<span class="sd">  However, the underlying variable store is shared across threads (within the</span>
<span class="sd">  same graph). As such, if another thread tries to create a new variable with</span>
<span class="sd">  the same name as a variable created by a previous thread, it will fail unless</span>
<span class="sd">  reuse is True.</span>

<span class="sd">  Further, each thread starts with an empty variable scope. So if you wish to</span>
<span class="sd">  preserve name prefixes from a scope from the main thread, you should capture</span>
<span class="sd">  the main thread&#39;s scope and re-enter it in each thread. For e.g.</span>

<span class="sd">  ```</span>
<span class="sd">  main_thread_scope = variable_scope.get_variable_scope()</span>

<span class="sd">  # Thread&#39;s target function:</span>
<span class="sd">  def thread_target_fn(captured_scope):</span>
<span class="sd">    with variable_scope.variable_scope(captured_scope):</span>
<span class="sd">      # .... regular code for this thread</span>


<span class="sd">  thread = threading.Thread(target=thread_target_fn, args=(main_thread_scope,))</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">name_or_scope</span><span class="p">,</span>
               <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">auxiliary_name_scope</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the context manager.</span>

<span class="sd">    Args:</span>
<span class="sd">      name_or_scope: `string` or `VariableScope`: the scope to open.</span>
<span class="sd">      default_name: The default name to use if the `name_or_scope` argument is</span>
<span class="sd">        `None`, this name will be uniquified. If name_or_scope is provided it</span>
<span class="sd">        won&#39;t be used and therefore it is not required and can be None.</span>
<span class="sd">      values: The list of `Tensor` arguments that are passed to the op function.</span>
<span class="sd">      initializer: default initializer for variables within this scope.</span>
<span class="sd">      regularizer: default regularizer for variables within this scope.</span>
<span class="sd">      caching_device: default caching device for variables within this scope.</span>
<span class="sd">      partitioner: default partitioner for variables within this scope.</span>
<span class="sd">      custom_getter: default custom getter for variables within this scope.</span>
<span class="sd">      reuse: `True`, None, or tf.compat.v1.AUTO_REUSE; if `True`, we go into</span>
<span class="sd">        reuse mode for this scope as well as all sub-scopes; if</span>
<span class="sd">        tf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and</span>
<span class="sd">        return them otherwise; if None, we inherit the parent scope&#39;s reuse</span>
<span class="sd">        flag. When eager execution is enabled, new variables are always created</span>
<span class="sd">        unless an EagerVariableStore or template is currently active.</span>
<span class="sd">      dtype: type of variables created in this scope (defaults to the type in</span>
<span class="sd">        the passed scope, or inherited from parent scope).</span>
<span class="sd">      use_resource: If False, all variables will be regular Variables. If True,</span>
<span class="sd">        experimental ResourceVariables with well-defined semantics will be used</span>
<span class="sd">        instead. Defaults to False (will later change to True). When eager</span>
<span class="sd">        execution is enabled this argument is always forced to be True.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">      auxiliary_name_scope: If `True`, we create an auxiliary name scope with</span>
<span class="sd">        the scope. If `False`, we don&#39;t create it. Note that the argument is not</span>
<span class="sd">        inherited, and it only takes effect for once when creating. You should</span>
<span class="sd">        only use it for re-entering a premade variable scope.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A scope that can be captured and reused.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: when trying to reuse within a create scope, or create within</span>
<span class="sd">        a reuse scope.</span>
<span class="sd">      TypeError: when the types of some arguments are not appropriate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span> <span class="o">=</span> <span class="n">name_or_scope</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span> <span class="o">=</span> <span class="n">default_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="n">values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span> <span class="o">=</span> <span class="n">regularizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="n">caching_device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span> <span class="o">=</span> <span class="n">partitioner</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span> <span class="o">=</span> <span class="n">custom_getter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span> <span class="o">=</span> <span class="n">use_resource</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="n">constraint</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;If default_name is None then name_or_scope is required&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
      <span class="c1"># We don&#39;t allow non-inheriting scopes, False = None here.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="ow">is</span> <span class="kc">True</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="ow">is</span> <span class="n">AUTO_REUSE</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The reuse parameter must be True or False or None.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">_get_graph_from_inputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cached_pure_variable_scope</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auxiliary_name_scope</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The auxiliary_name_scope must be `True` or `False`, &quot;</span>
                      <span class="s2">&quot;while get </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">auxiliary_name_scope</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_auxiliary_name_scope</span> <span class="o">=</span> <span class="n">auxiliary_name_scope</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># If the default graph is building a function, then we should not replace it</span>
    <span class="c1"># with the cached graph.</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_context_manager</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_context_manager</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_pure_variable_scope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Fast path for re-entering variable_scopes. We&#39;ve held on to the pure</span>
      <span class="c1"># variable scope from a previous successful __enter__, so we avoid some</span>
      <span class="c1"># overhead by re-using that object.</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_pure_variable_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enter_scope_uncached</span><span class="p">()</span>
    <span class="k">except</span><span class="p">:</span>
      <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span> <span class="ow">and</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_graph_context_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_context_manager</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
      <span class="k">raise</span>

  <span class="k">def</span> <span class="nf">_enter_scope_uncached</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enters the context manager when there is no cached scope yet.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The entered variable scope.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: A wrong type is passed as `scope` at __init__().</span>
<span class="sd">      ValueError: `reuse` is incorrectly set at __init__().</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auxiliary_name_scope</span><span class="p">:</span>
      <span class="c1"># Create a new name scope later</span>
      <span class="n">current_name_scope</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Reenter the current name scope</span>
      <span class="n">name_scope</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">name_scope</span><span class="p">:</span>
        <span class="c1"># Hack to reenter</span>
        <span class="n">name_scope</span> <span class="o">+=</span> <span class="s2">&quot;/&quot;</span>
        <span class="n">current_name_scope</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name_scope</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Root scope</span>
        <span class="n">current_name_scope</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name_scope</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># IMPORTANT: Only assign to self._cached_pure_variable_scope and</span>
    <span class="c1"># self._current_name_scope after successful __enter__() calls.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">VariableScope</span><span class="p">,)</span> <span class="o">+</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;VariableScope: name_or_scope must be a string or &quot;</span>
                        <span class="s2">&quot;VariableScope.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
        <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">name_scope</span> <span class="ow">or</span> <span class="n">current_name_scope</span><span class="p">:</span>
        <span class="n">current_name_scope</span> <span class="o">=</span> <span class="n">current_name_scope</span> <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
            <span class="n">name_scope</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">current_name_scope_name</span> <span class="o">=</span> <span class="n">current_name_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
        <span class="k">except</span><span class="p">:</span>
          <span class="n">current_name_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
          <span class="k">raise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span> <span class="o">=</span> <span class="n">current_name_scope</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
          <span class="n">old_name_scope</span> <span class="o">=</span> <span class="n">current_name_scope_name</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">old_name_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="o">.</span><span class="n">original_name_scope</span>
        <span class="n">pure_variable_scope</span> <span class="o">=</span> <span class="n">_pure_variable_scope</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span>
            <span class="n">reuse</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">,</span>
            <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span><span class="p">,</span>
            <span class="n">caching_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span><span class="p">,</span>
            <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span><span class="p">,</span>
            <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
            <span class="n">old_name_scope</span><span class="o">=</span><span class="n">old_name_scope</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
            <span class="n">use_resource</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span><span class="p">,</span>
            <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">entered_pure_variable_scope</span> <span class="o">=</span> <span class="n">pure_variable_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
        <span class="k">except</span><span class="p">:</span>
          <span class="n">pure_variable_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
          <span class="k">raise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cached_pure_variable_scope</span> <span class="o">=</span> <span class="n">pure_variable_scope</span>
        <span class="k">return</span> <span class="n">entered_pure_variable_scope</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># This can only happen if someone is entering the root variable scope.</span>
        <span class="n">pure_variable_scope</span> <span class="o">=</span> <span class="n">_pure_variable_scope</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_name_or_scope</span><span class="p">,</span>
            <span class="n">reuse</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">,</span>
            <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span><span class="p">,</span>
            <span class="n">caching_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span><span class="p">,</span>
            <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span><span class="p">,</span>
            <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
            <span class="n">use_resource</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span><span class="p">,</span>
            <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">entered_pure_variable_scope</span> <span class="o">=</span> <span class="n">pure_variable_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
        <span class="k">except</span><span class="p">:</span>
          <span class="n">pure_variable_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
          <span class="k">raise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cached_pure_variable_scope</span> <span class="o">=</span> <span class="n">pure_variable_scope</span>
        <span class="k">return</span> <span class="n">entered_pure_variable_scope</span>

    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Here name_or_scope is None. Using default name, but made unique.</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;reuse=True cannot be used without a name_or_scope&quot;</span><span class="p">)</span>
      <span class="n">current_name_scope</span> <span class="o">=</span> <span class="n">current_name_scope</span> <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">current_name_scope_name</span> <span class="o">=</span> <span class="n">current_name_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
      <span class="k">except</span><span class="p">:</span>
        <span class="n">current_name_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
        <span class="k">raise</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span> <span class="o">=</span> <span class="n">current_name_scope</span>
      <span class="n">unique_default_name</span> <span class="o">=</span> <span class="n">_get_unique_variable_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span><span class="p">)</span>
      <span class="n">pure_variable_scope</span> <span class="o">=</span> <span class="n">_pure_variable_scope</span><span class="p">(</span>
          <span class="n">unique_default_name</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">,</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizer</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span><span class="p">,</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioner</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
          <span class="n">old_name_scope</span><span class="o">=</span><span class="n">current_name_scope_name</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_resource</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">entered_pure_variable_scope</span> <span class="o">=</span> <span class="n">pure_variable_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
      <span class="k">except</span><span class="p">:</span>
        <span class="n">pure_variable_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
        <span class="k">raise</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cached_pure_variable_scope</span> <span class="o">=</span> <span class="n">pure_variable_scope</span>
      <span class="k">return</span> <span class="n">entered_pure_variable_scope</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cached_pure_variable_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span>
                                                <span class="n">traceback_arg</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_current_name_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span>
                                            <span class="n">traceback_arg</span><span class="p">)</span>
      <span class="k">finally</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_graph_context_manager</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span>
                                               <span class="n">traceback_arg</span><span class="p">)</span>


<span class="c1"># pylint: disable=g-doc-return-or-yield</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;variable_op_scope&quot;</span><span class="p">])</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">variable_op_scope</span><span class="p">(</span><span class="n">values</span><span class="p">,</span>
                      <span class="n">name_or_scope</span><span class="p">,</span>
                      <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Deprecated: context manager for defining an op that creates variables.&quot;&quot;&quot;</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;tf.variable_op_scope(values, name, default_name) is deprecated,&quot;</span>
               <span class="s2">&quot; use tf.variable_scope(name, default_name, values)&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">variable_scope</span><span class="p">(</span>
      <span class="n">name_or_scope</span><span class="p">,</span>
      <span class="n">default_name</span><span class="o">=</span><span class="n">default_name</span><span class="p">,</span>
      <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span>
      <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
      <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
      <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
      <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
      <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="k">yield</span> <span class="n">scope</span>


<span class="k">def</span> <span class="nf">_call_partitioner</span><span class="p">(</span><span class="n">partitioner</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Call partitioner validating its inputs/output.</span>

<span class="sd">  Args:</span>
<span class="sd">    partitioner: a function mapping `Tensor` shape and dtype to a list of</span>
<span class="sd">      partitions.</span>
<span class="sd">    shape: shape of the `Tensor` to partition, must have at least two</span>
<span class="sd">      dimensions.</span>
<span class="sd">    dtype: dtype of the elements in the `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list with elements &gt;=1 and exactly one &gt;1. The index of that</span>
<span class="sd">    element corresponds to the partitioning axis.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Shape of a new partitioned variable must be &quot;</span>
                     <span class="s2">&quot;fully defined, but instead was </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,))</span>
  <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;A partitioned Variable must have rank at least 1, &quot;</span>
                     <span class="s2">&quot;shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">shape</span><span class="p">)</span>

  <span class="n">slicing</span> <span class="o">=</span> <span class="n">partitioner</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slicing</span><span class="p">,</span> <span class="n">collections_lib</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Partitioner must return a sequence, but saw: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                     <span class="n">slicing</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">slicing</span><span class="p">)</span> <span class="o">!=</span> <span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Partitioner returned a partition list that does not match the &quot;</span>
        <span class="s2">&quot;Variable&#39;s rank: </span><span class="si">%s</span><span class="s2"> vs. </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">slicing</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
  <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">slicing</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Partitioner returned zero partitions for some axes: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                     <span class="n">slicing</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">slicing</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can only slice a variable along one dimension: &quot;</span>
                     <span class="s2">&quot;shape: </span><span class="si">%s</span><span class="s2">, partitioning: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">slicing</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">slicing</span>


<span class="c1"># TODO(slebedev): could be inlined, but</span>
<span class="c1"># `_VariableStore._get_partitioned_variable` is too complex even</span>
<span class="c1"># without this logic.</span>
<span class="k">def</span> <span class="nf">_get_slice_dim_and_num_slices</span><span class="p">(</span><span class="n">slicing</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get slicing dimension and number of slices from the partitioner output.&quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">slice_dim</span><span class="p">,</span> <span class="n">num_slices</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">slicing</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">num_slices</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">break</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Degenerate case: no partitioning applied.</span>
    <span class="n">slice_dim</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_slices</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">slice_dim</span><span class="p">,</span> <span class="n">num_slices</span>


<span class="k">def</span> <span class="nf">_iter_slices</span><span class="p">(</span><span class="n">full_shape</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">,</span> <span class="n">slice_dim</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Slices a given a shape along the specified dimension.&quot;&quot;&quot;</span>
  <span class="n">num_slices_with_excess</span> <span class="o">=</span> <span class="n">full_shape</span><span class="p">[</span><span class="n">slice_dim</span><span class="p">]</span> <span class="o">%</span> <span class="n">num_slices</span>
  <span class="n">offset</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_shape</span><span class="p">)</span>
  <span class="n">min_slice_len</span> <span class="o">=</span> <span class="n">full_shape</span><span class="p">[</span><span class="n">slice_dim</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_slices</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">num_slices</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">full_shape</span><span class="p">[:]</span>
    <span class="n">shape</span><span class="p">[</span><span class="n">slice_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_slice_len</span> <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_slices_with_excess</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">offset</span><span class="p">[:],</span> <span class="n">shape</span>
    <span class="n">offset</span><span class="p">[</span><span class="n">slice_dim</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shape</span><span class="p">[</span><span class="n">slice_dim</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">default_variable_creator</span><span class="p">(</span><span class="n">next_creator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Default variable creator.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">next_creator</span> <span class="ow">is</span> <span class="kc">None</span>
  <span class="n">initial_value</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;initial_value&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">trainable</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;trainable&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">collections</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;collections&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">validate_shape</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;validate_shape&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
  <span class="n">caching_device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;caching_device&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">variable_def</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;variable_def&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">expected_shape</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;expected_shape&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">import_scope</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;import_scope&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">constraint</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;constraint&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">use_resource</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_resource&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">synchronization</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;synchronization&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">aggregation</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;aggregation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">use_resource</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">use_resource</span> <span class="o">=</span> <span class="n">get_variable_scope</span><span class="p">()</span><span class="o">.</span><span class="n">use_resource</span>
  <span class="k">if</span> <span class="n">use_resource</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">use_resource</span> <span class="o">=</span> <span class="n">_DEFAULT_USE_RESOURCE</span>
  <span class="n">use_resource</span> <span class="o">=</span> <span class="n">use_resource</span> <span class="ow">or</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">use_resource</span><span class="p">:</span>
    <span class="n">distribute_strategy</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;distribute_strategy&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">ResourceVariable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
        <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span>
        <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">,</span>
        <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">variables</span><span class="o">.</span><span class="n">RefVariable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
        <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span>
        <span class="n">expected_shape</span><span class="o">=</span><span class="n">expected_shape</span><span class="p">,</span>
        <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">default_variable_creator_v2</span><span class="p">(</span><span class="n">next_creator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Default variable creator.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">next_creator</span> <span class="ow">is</span> <span class="kc">None</span>
  <span class="n">initial_value</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;initial_value&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">trainable</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;trainable&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">validate_shape</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;validate_shape&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
  <span class="n">caching_device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;caching_device&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">variable_def</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;variable_def&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">import_scope</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;import_scope&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">constraint</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;constraint&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">distribute_strategy</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;distribute_strategy&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">synchronization</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;synchronization&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">aggregation</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;aggregation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">ResourceVariable</span><span class="p">(</span>
      <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
      <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
      <span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span>
      <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">,</span>
      <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>


<span class="n">variables</span><span class="o">.</span><span class="n">default_variable_creator</span> <span class="o">=</span> <span class="n">default_variable_creator</span>
<span class="n">variables</span><span class="o">.</span><span class="n">default_variable_creator_v2</span> <span class="o">=</span> <span class="n">default_variable_creator_v2</span>


<span class="k">def</span> <span class="nf">_make_getter</span><span class="p">(</span><span class="n">captured_getter</span><span class="p">,</span> <span class="n">captured_previous</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gets around capturing loop variables in python being broken.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="k">lambda</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">captured_getter</span><span class="p">(</span><span class="n">captured_previous</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="c1"># TODO(apassos) remove forwarding symbol</span>
<span class="n">variable</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">VariableV1</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;variable_creator_scope&quot;</span><span class="p">])</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">variable_creator_scope_v1</span><span class="p">(</span><span class="n">variable_creator</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Scope which defines a variable creation function to be used by variable().</span>

<span class="sd">  variable_creator is expected to be a function with the following signature:</span>

<span class="sd">  ```</span>
<span class="sd">    def variable_creator(next_creator, **kwargs)</span>
<span class="sd">  ```</span>

<span class="sd">  The creator is supposed to eventually call the next_creator to create a</span>
<span class="sd">  variable if it does want to create a variable and not call Variable or</span>
<span class="sd">  ResourceVariable directly. This helps make creators composable. A creator may</span>
<span class="sd">  choose to create multiple variables, return already existing variables, or</span>
<span class="sd">  simply register that a variable was created and defer to the next creators in</span>
<span class="sd">  line. Creators can also modify the keyword arguments seen by the next</span>
<span class="sd">  creators.</span>

<span class="sd">  Custom getters in the variable scope will eventually resolve down to these</span>
<span class="sd">  custom creators when they do create variables.</span>

<span class="sd">  The valid keyword arguments in kwds are:</span>

<span class="sd">   * initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called. In</span>
<span class="sd">        that case, `dtype` must be specified. (Note that initializer functions</span>
<span class="sd">        from init_ops.py must first be bound to a shape before being used here.)</span>
<span class="sd">   * trainable: If `True`, the default, also adds the variable to the graph</span>
<span class="sd">        collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as</span>
<span class="sd">        the default list of variables to use by the `Optimizer` classes.</span>
<span class="sd">        `trainable` defaults to `True`, unless `synchronization` is</span>
<span class="sd">        set to `ON_READ`, in which case it defaults to `False`.</span>
<span class="sd">   * collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">   * validate_shape: If `False`, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If `True`, the default, the shape of</span>
<span class="sd">        `initial_value` must be known.</span>
<span class="sd">   * caching_device: Optional device string describing where the Variable</span>
<span class="sd">        should be cached for reading.  Defaults to the Variable&#39;s device.</span>
<span class="sd">        If not `None`, caches on another device.  Typical use is to cache</span>
<span class="sd">        on the device where the Ops using the Variable reside, to deduplicate</span>
<span class="sd">        copying through `Switch` and other conditional statements.</span>
<span class="sd">   * name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">   * dtype: If set, initial_value will be converted to the given type.</span>
<span class="sd">        If `None`, either the datatype will be kept (if `initial_value` is</span>
<span class="sd">        a Tensor), or `convert_to_tensor` will decide.</span>
<span class="sd">   * constraint: A constraint function to be applied to the variable after</span>
<span class="sd">        updates by some algorithms.</span>
<span class="sd">   * use_resource: if True, a ResourceVariable is always created.</span>
<span class="sd">   * synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">   * aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>

<span class="sd">  This set may grow over time, so it&#39;s important the signature of creators is as</span>
<span class="sd">  mentioned above.</span>

<span class="sd">  Args:</span>
<span class="sd">    variable_creator: the passed creator</span>

<span class="sd">  Yields:</span>
<span class="sd">    A scope in which the creator is active</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_variable_creator_scope</span><span class="p">(</span><span class="n">variable_creator</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">yield</span>


<span class="c1"># Note: only the docstrings differ between this and v1.</span>
<div class="viewcode-block" id="variable_creator_scope"><a class="viewcode-back" href="../../../../index.html#tensorflow.variable_creator_scope">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;variable_creator_scope&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">variable_creator_scope</span><span class="p">(</span><span class="n">variable_creator</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Scope which defines a variable creation function to be used by variable().</span>

<span class="sd">  variable_creator is expected to be a function with the following signature:</span>

<span class="sd">  ```</span>
<span class="sd">    def variable_creator(next_creator, **kwargs)</span>
<span class="sd">  ```</span>

<span class="sd">  The creator is supposed to eventually call the next_creator to create a</span>
<span class="sd">  variable if it does want to create a variable and not call Variable or</span>
<span class="sd">  ResourceVariable directly. This helps make creators composable. A creator may</span>
<span class="sd">  choose to create multiple variables, return already existing variables, or</span>
<span class="sd">  simply register that a variable was created and defer to the next creators in</span>
<span class="sd">  line. Creators can also modify the keyword arguments seen by the next</span>
<span class="sd">  creators.</span>

<span class="sd">  Custom getters in the variable scope will eventually resolve down to these</span>
<span class="sd">  custom creators when they do create variables.</span>

<span class="sd">  The valid keyword arguments in kwds are:</span>

<span class="sd">   * initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called. In</span>
<span class="sd">        that case, `dtype` must be specified. (Note that initializer functions</span>
<span class="sd">        from init_ops.py must first be bound to a shape before being used here.)</span>
<span class="sd">   * trainable: If `True`, the default, GradientTapes automatically watch</span>
<span class="sd">        uses of this Variable.</span>
<span class="sd">   * validate_shape: If `False`, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If `True`, the default, the shape of</span>
<span class="sd">        `initial_value` must be known.</span>
<span class="sd">   * caching_device: Optional device string describing where the Variable</span>
<span class="sd">        should be cached for reading.  Defaults to the Variable&#39;s device.</span>
<span class="sd">        If not `None`, caches on another device.  Typical use is to cache</span>
<span class="sd">        on the device where the Ops using the Variable reside, to deduplicate</span>
<span class="sd">        copying through `Switch` and other conditional statements.</span>
<span class="sd">   * name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type.</span>
<span class="sd">        If `None`, either the datatype will be kept (if `initial_value` is</span>
<span class="sd">        a Tensor), or `convert_to_tensor` will decide.</span>
<span class="sd">   * constraint: A constraint function to be applied to the variable after</span>
<span class="sd">        updates by some algorithms.</span>
<span class="sd">   * synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">   * aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>

<span class="sd">  This set may grow over time, so it&#39;s important the signature of creators is as</span>
<span class="sd">  mentioned above.</span>

<span class="sd">  Args:</span>
<span class="sd">    variable_creator: the passed creator</span>

<span class="sd">  Yields:</span>
<span class="sd">    A scope in which the creator is active</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_variable_creator_scope</span><span class="p">(</span><span class="n">variable_creator</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">yield</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>