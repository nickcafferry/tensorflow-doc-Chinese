

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.map_fn &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.map_fn</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.map_fn</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># =============================================================================</span>

<span class="sd">&quot;&quot;&quot;Functional operations.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>


<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">sparse_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">tensor_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">variable_scope</span> <span class="k">as</span> <span class="n">vs</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>


<div class="viewcode-block" id="map_fn"><a class="viewcode-back" href="../../../../index.html#tensorflow.map_fn">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;map_fn&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">map_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">elems</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">back_prop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">swap_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;map on the list of tensors unpacked from `elems` on dimension 0.</span>

<span class="sd">  The simplest version of `map_fn` repeatedly applies the callable `fn` to a</span>
<span class="sd">  sequence of elements from first to last. The elements are made of the</span>
<span class="sd">  tensors unpacked from `elems`. `dtype` is the data type of the return</span>
<span class="sd">  value of `fn`. Users must provide `dtype` if it is different from</span>
<span class="sd">  the data type of `elems`.</span>

<span class="sd">  Suppose that `elems` is unpacked into `values`, a list of tensors. The shape</span>
<span class="sd">  of the result tensor is `[values.shape[0]] + fn(values[0]).shape`.</span>

<span class="sd">  This method also allows multi-arity `elems` and output of `fn`.  If `elems`</span>
<span class="sd">  is a (possibly nested) list or tuple of tensors, then each of these tensors</span>
<span class="sd">  must have a matching first (unpack) dimension.  The signature of `fn` may</span>
<span class="sd">  match the structure of `elems`.  That is, if `elems` is</span>
<span class="sd">  `(t1, [t2, t3, [t4, t5]])`, then an appropriate signature for `fn` is:</span>
<span class="sd">  `fn = lambda (t1, [t2, t3, [t4, t5]]):`.</span>

<span class="sd">  Furthermore, `fn` may emit a different structure than its input.  For example,</span>
<span class="sd">  `fn` may look like: `fn = lambda t1: return (t1 + 1, t1 - 1)`.  In this case,</span>
<span class="sd">  the `dtype` parameter is not optional: `dtype` must be a type or (possibly</span>
<span class="sd">  nested) tuple of types matching the output of `fn`.</span>

<span class="sd">  To apply a functional operation to the nonzero elements of a SparseTensor</span>
<span class="sd">  one of the following methods is recommended. First, if the function is</span>
<span class="sd">  expressible as TensorFlow ops, use</span>

<span class="sd">  ```python</span>
<span class="sd">    result = SparseTensor(input.indices, fn(input.values), input.dense_shape)</span>
<span class="sd">  ```</span>

<span class="sd">  If, however, the function is not expressible as a TensorFlow op, then use</span>

<span class="sd">  ```python</span>
<span class="sd">  result = SparseTensor(</span>
<span class="sd">    input.indices, map_fn(fn, input.values), input.dense_shape)</span>
<span class="sd">  ```</span>

<span class="sd">  instead.</span>

<span class="sd">  When executing eagerly, map_fn does not execute in parallel even if</span>
<span class="sd">  `parallel_iterations` is set to a value &gt; 1. You can still get the</span>
<span class="sd">  performance benefits of running a function in parallel by using the</span>
<span class="sd">  `tf.function` decorator,</span>

<span class="sd">  ```python</span>
<span class="sd">  # Assume the function being used in map_fn is fn.</span>
<span class="sd">  # To ensure map_fn calls fn in parallel, use the tf.function decorator.</span>
<span class="sd">  @tf.function</span>
<span class="sd">  def func(tensor):</span>
<span class="sd">    return tf.map_fn(fn, tensor)</span>
<span class="sd">  ```</span>

<span class="sd">  Note that if you use the `tf.function` decorator, any non-TensorFlow Python</span>
<span class="sd">  code that you may have written in your function won&#39;t get executed. See</span>
<span class="sd">  [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) for</span>
<span class="sd">  more  details. The recommendation would be to debug without `tf.function` but</span>
<span class="sd">  switch to it to get performance benefits of running `map_fn` in parallel.</span>

<span class="sd">  Args:</span>
<span class="sd">    fn: The callable to be performed.  It accepts one argument, which will</span>
<span class="sd">      have the same (possibly nested) structure as `elems`.  Its output</span>
<span class="sd">      must have the same structure as `dtype` if one is provided, otherwise</span>
<span class="sd">      it must have the same structure as `elems`.</span>
<span class="sd">    elems: A tensor or (possibly nested) sequence of tensors, each of which</span>
<span class="sd">      will be unpacked along their first dimension.  The nested sequence</span>
<span class="sd">      of the resulting slices will be applied to `fn`.</span>
<span class="sd">    dtype: (optional) The output type(s) of `fn`.  If `fn` returns a structure</span>
<span class="sd">      of Tensors differing from the structure of `elems`, then `dtype` is not</span>
<span class="sd">      optional and must have the same structure as the output of `fn`.</span>
<span class="sd">    parallel_iterations: (optional) The number of iterations allowed to run</span>
<span class="sd">      in parallel. When graph building, the default value is 10. While executing</span>
<span class="sd">      eagerly, the default value is set to 1.</span>
<span class="sd">    back_prop: (optional) True enables support for back propagation.</span>
<span class="sd">    swap_memory: (optional) True enables GPU-CPU memory swapping.</span>
<span class="sd">    infer_shape: (optional) False disables tests for consistent output shapes.</span>
<span class="sd">    name: (optional) Name prefix for the returned tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor or (possibly nested) sequence of tensors.  Each tensor packs the</span>
<span class="sd">    results of applying `fn` to tensors unpacked from `elems` along the first</span>
<span class="sd">    dimension, from first to last.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `fn` is not callable or the structure of the output of</span>
<span class="sd">      `fn` and `dtype` do not match, or if elems is a SparseTensor.</span>
<span class="sd">    ValueError: if the lengths of the output of `fn` and `dtype` do not match.</span>

<span class="sd">  Examples:</span>
<span class="sd">    ```python</span>
<span class="sd">    elems = np.array([1, 2, 3, 4, 5, 6])</span>
<span class="sd">    squares = map_fn(lambda x: x * x, elems)</span>
<span class="sd">    # squares == [1, 4, 9, 16, 25, 36]</span>
<span class="sd">    ```</span>

<span class="sd">    ```python</span>
<span class="sd">    elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))</span>
<span class="sd">    alternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)</span>
<span class="sd">    # alternate == [-1, 2, -3]</span>
<span class="sd">    ```</span>

<span class="sd">    ```python</span>
<span class="sd">    elems = np.array([1, 2, 3])</span>
<span class="sd">    alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))</span>
<span class="sd">    # alternates[0] == [1, 2, 3]</span>
<span class="sd">    # alternates[1] == [-1, -2, -3]</span>
<span class="sd">    ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;fn must be callable.&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elems</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s2">&quot;To perform a map on the values of a sparse tensor use either &quot;</span>
        <span class="s2">&quot; SparseTensor(input.indices, fn(input.values), input.dense_shape) or &quot;</span>
        <span class="s2">&quot; SparseTensor(input.indices, map_fn(fn, input.values), &quot;</span>
        <span class="s2">&quot;input.dense_shape)&quot;</span><span class="p">)</span>

  <span class="n">in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
  <span class="c1"># Set the default number of parallel_iterations depending on graph/eager mode.</span>
  <span class="k">if</span> <span class="n">in_graph_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">parallel_iterations</span><span class="p">:</span>
    <span class="n">parallel_iterations</span> <span class="o">=</span> <span class="mi">10</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">in_graph_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">parallel_iterations</span><span class="p">:</span>
    <span class="n">parallel_iterations</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">in_graph_mode</span> <span class="ow">and</span> <span class="n">parallel_iterations</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span> <span class="s2">&quot;Setting parallel_iterations &gt; 1 has no &quot;</span>
        <span class="s2">&quot;effect when executing eagerly. Consider calling map_fn&quot;</span>
        <span class="s2">&quot; with tf.function to execute fn in &quot;</span>
        <span class="s2">&quot;parallel.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">parallel_iterations</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="n">input_is_sequence</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">elems</span><span class="p">)</span>
  <span class="n">input_flatten</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_is_sequence</span> <span class="k">else</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
  <span class="k">def</span> <span class="nf">input_pack</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">elems</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_is_sequence</span> <span class="k">else</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">output_is_sequence</span> <span class="o">=</span> <span class="n">input_is_sequence</span>
    <span class="n">output_flatten</span> <span class="o">=</span> <span class="n">input_flatten</span>
    <span class="n">output_pack</span> <span class="o">=</span> <span class="n">input_pack</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">output_is_sequence</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">output_flatten</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_is_sequence</span> <span class="k">else</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">output_pack</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
              <span class="k">if</span> <span class="n">output_is_sequence</span> <span class="k">else</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

  <span class="n">elems_flat</span> <span class="o">=</span> <span class="n">input_flatten</span><span class="p">(</span><span class="n">elems</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;map&quot;</span><span class="p">,</span> <span class="n">elems_flat</span><span class="p">):</span>
    <span class="c1"># TODO(akshayka): Remove the in_graph_mode check once caching devices are</span>
    <span class="c1"># supported in Eager</span>
    <span class="k">if</span> <span class="n">in_graph_mode</span><span class="p">:</span>
      <span class="c1"># Any get_variable calls in fn will cache the first call locally</span>
      <span class="c1"># and not issue repeated network I/O requests for each iteration.</span>
      <span class="n">varscope</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
      <span class="n">varscope_caching_device_was_none</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">if</span> <span class="n">varscope</span><span class="o">.</span><span class="n">caching_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># TODO(ebrevdo): Change to using colocate_with here and in other</span>
        <span class="c1"># methods.</span>
        <span class="n">varscope</span><span class="o">.</span><span class="n">set_caching_device</span><span class="p">(</span><span class="k">lambda</span> <span class="n">op</span><span class="p">:</span> <span class="n">op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">varscope_caching_device_was_none</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">elems_flat</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;elem&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">elems_flat</span><span class="p">]</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span> <span class="ow">or</span> <span class="n">input_pack</span><span class="p">([</span><span class="n">elem</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">elems_flat</span><span class="p">])</span>
    <span class="n">dtype_flat</span> <span class="o">=</span> <span class="n">output_flatten</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Convert elems to tensor array. n may be known statically.</span>
    <span class="n">static_shape</span> <span class="o">=</span> <span class="n">elems_flat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">static_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">static_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">elems_flat</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;elems must be a 1+ dimensional Tensor, not a scalar&quot;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;elements in elems must be 1+ dimensional Tensors, not scalars&quot;</span>
        <span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">static_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
         <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">elems_flat</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># TensorArrays are always flat</span>
    <span class="n">elems_ta</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                     <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
                                     <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                     <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">elems_flat</span><span class="p">]</span>
    <span class="c1"># Unpack elements</span>
    <span class="n">elems_ta</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">elem_ta</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem_ta</span><span class="p">,</span> <span class="n">elem</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">elems_ta</span><span class="p">,</span> <span class="n">elems_flat</span><span class="p">)]</span>

    <span class="n">i</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">accs_ta</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span>
                                     <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
                                     <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                     <span class="n">infer_shape</span><span class="o">=</span><span class="n">infer_shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">dtype_flat</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tas</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;The loop body of map_fn.</span>

<span class="sd">      Args:</span>
<span class="sd">        i: the loop counter</span>
<span class="sd">        tas: the flat TensorArray accumulator list</span>

<span class="sd">      Returns:</span>
<span class="sd">        (i + 1, tas): the updated counter + updated TensorArrays</span>

<span class="sd">      Raises:</span>
<span class="sd">        TypeError: if dtype and packed_fn_values structure do not match</span>
<span class="sd">        ValueType: if dtype and packed_fn_values lengths do not match</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">packed_values</span> <span class="o">=</span> <span class="n">input_pack</span><span class="p">([</span><span class="n">elem_ta</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem_ta</span> <span class="ow">in</span> <span class="n">elems_ta</span><span class="p">])</span>
      <span class="n">packed_fn_values</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">packed_values</span><span class="p">)</span>
      <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">elems</span><span class="p">,</span> <span class="n">packed_fn_values</span><span class="p">)</span>
      <span class="n">flat_fn_values</span> <span class="o">=</span> <span class="n">output_flatten</span><span class="p">(</span><span class="n">packed_fn_values</span><span class="p">)</span>
      <span class="n">tas</span> <span class="o">=</span> <span class="p">[</span><span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">ta</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tas</span><span class="p">,</span> <span class="n">flat_fn_values</span><span class="p">)]</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tas</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">r_a</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">,</span> <span class="n">compute</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">accs_ta</span><span class="p">),</span>
        <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">,</span>
        <span class="n">back_prop</span><span class="o">=</span><span class="n">back_prop</span><span class="p">,</span>
        <span class="n">swap_memory</span><span class="o">=</span><span class="n">swap_memory</span><span class="p">,</span>
        <span class="n">maximum_iterations</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">results_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">r_a</span><span class="p">]</span>

    <span class="n">n_static</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">Dimension</span><span class="p">(</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span>
        <span class="n">elems_flat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank_at_least</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">elems_flat</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
      <span class="n">n_static</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">Dimension</span><span class="p">(</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span>
          <span class="n">elem</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank_at_least</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">])))</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results_flat</span><span class="p">:</span>
      <span class="n">r</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">n_static</span><span class="p">)</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
          <span class="n">r</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]))</span>

    <span class="c1"># TODO(akshayka): Remove the in_graph_mode check once caching devices are</span>
    <span class="c1"># supported in Eager</span>
    <span class="k">if</span> <span class="n">in_graph_mode</span> <span class="ow">and</span> <span class="n">varscope_caching_device_was_none</span><span class="p">:</span>
      <span class="n">varscope</span><span class="o">.</span><span class="n">set_caching_device</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output_pack</span><span class="p">(</span><span class="n">results_flat</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;map_fn&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_arg_values</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span>
    <span class="sd">&quot;&quot;&quot;back_prop=False is deprecated. Consider using tf.stop_gradient instead.</span>
<span class="sd">Instead of:</span>
<span class="sd">results = tf.map_fn(fn, elems, back_prop=False)</span>
<span class="sd">Use:</span>
<span class="sd">results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">warn_once</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">back_prop</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">map_fn_v2</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span>
              <span class="n">elems</span><span class="p">,</span>
              <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">back_prop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">swap_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">infer_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;map on the list of tensors unpacked from `elems` on dimension 0.</span>

<span class="sd">  The simplest version of `map_fn` repeatedly applies the callable `fn` to a</span>
<span class="sd">  sequence of elements from first to last. The elements are made of the</span>
<span class="sd">  tensors unpacked from `elems`. `dtype` is the data type of the return</span>
<span class="sd">  value of `fn`. Users must provide `dtype` if it is different from</span>
<span class="sd">  the data type of `elems`.</span>

<span class="sd">  Suppose that `elems` is unpacked into `values`, a list of tensors. The shape</span>
<span class="sd">  of the result tensor is `[values.shape[0]] + fn(values[0]).shape`.</span>

<span class="sd">  This method also allows multi-arity `elems` and output of `fn`.  If `elems`</span>
<span class="sd">  is a (possibly nested) list or tuple of tensors, then each of these tensors</span>
<span class="sd">  must have a matching first (unpack) dimension.  The signature of `fn` may</span>
<span class="sd">  match the structure of `elems`.  That is, if `elems` is</span>
<span class="sd">  `(t1, [t2, t3, [t4, t5]])`, then an appropriate signature for `fn` is:</span>
<span class="sd">  `fn = lambda (t1, [t2, t3, [t4, t5]]):`.</span>

<span class="sd">  Furthermore, `fn` may emit a different structure than its input.  For example,</span>
<span class="sd">  `fn` may look like: `fn = lambda t1: return (t1 + 1, t1 - 1)`.  In this case,</span>
<span class="sd">  the `dtype` parameter is not optional: `dtype` must be a type or (possibly</span>
<span class="sd">  nested) tuple of types matching the output of `fn`.</span>

<span class="sd">  To apply a functional operation to the nonzero elements of a SparseTensor</span>
<span class="sd">  one of the following methods is recommended. First, if the function is</span>
<span class="sd">  expressible as TensorFlow ops, use</span>

<span class="sd">  ```python</span>
<span class="sd">    result = SparseTensor(input.indices, fn(input.values), input.dense_shape)</span>
<span class="sd">  ```</span>

<span class="sd">  If, however, the function is not expressible as a TensorFlow op, then use</span>

<span class="sd">  ```python</span>
<span class="sd">  result = SparseTensor(</span>
<span class="sd">    input.indices, map_fn(fn, input.values), input.dense_shape)</span>
<span class="sd">  ```</span>

<span class="sd">  instead.</span>

<span class="sd">  When executing eagerly, map_fn does not execute in parallel even if</span>
<span class="sd">  `parallel_iterations` is set to a value &gt; 1. You can still get the</span>
<span class="sd">  performance benefits of running a function in parallel by using the</span>
<span class="sd">  `tf.function` decorator,</span>

<span class="sd">  ```python</span>
<span class="sd">  # Assume the function being used in map_fn is fn.</span>
<span class="sd">  # To ensure map_fn calls fn in parallel, use the tf.function decorator.</span>
<span class="sd">  @tf.function</span>
<span class="sd">  def func(tensor):</span>
<span class="sd">    return tf.map_fn(fn, tensor)</span>
<span class="sd">  ```</span>

<span class="sd">  Note that if you use the `tf.function` decorator, any non-TensorFlow Python</span>
<span class="sd">  code that you may have written in your function won&#39;t get executed. See</span>
<span class="sd">  [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) for</span>
<span class="sd">  more  details. The recommendation would be to debug without `tf.function` but</span>
<span class="sd">  switch to it to get performance benefits of running `map_fn` in parallel.</span>

<span class="sd">  Args:</span>
<span class="sd">    fn: The callable to be performed.  It accepts one argument, which will have</span>
<span class="sd">      the same (possibly nested) structure as `elems`.  Its output must have the</span>
<span class="sd">      same structure as `dtype` if one is provided, otherwise it must have the</span>
<span class="sd">      same structure as `elems`.</span>
<span class="sd">    elems: A tensor or (possibly nested) sequence of tensors, each of which will</span>
<span class="sd">      be unpacked along their first dimension.  The nested sequence of the</span>
<span class="sd">      resulting slices will be applied to `fn`.</span>
<span class="sd">    dtype: (optional) The output type(s) of `fn`.  If `fn` returns a structure</span>
<span class="sd">      of Tensors differing from the structure of `elems`, then `dtype` is not</span>
<span class="sd">      optional and must have the same structure as the output of `fn`.</span>
<span class="sd">    parallel_iterations: (optional) The number of iterations allowed to run in</span>
<span class="sd">      parallel. When graph building, the default value is 10. While executing</span>
<span class="sd">      eagerly, the default value is set to 1.</span>
<span class="sd">    back_prop: (optional) Deprecated. False disables support for back</span>
<span class="sd">      propagation. Prefer using `tf.stop_gradient` instead.</span>
<span class="sd">    swap_memory: (optional) True enables GPU-CPU memory swapping.</span>
<span class="sd">    infer_shape: (optional) False disables tests for consistent output shapes.</span>
<span class="sd">    name: (optional) Name prefix for the returned tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor or (possibly nested) sequence of tensors.  Each tensor packs the</span>
<span class="sd">    results of applying `fn` to tensors unpacked from `elems` along the first</span>
<span class="sd">    dimension, from first to last.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `fn` is not callable or the structure of the output of</span>
<span class="sd">      `fn` and `dtype` do not match, or if elems is a SparseTensor.</span>
<span class="sd">    ValueError: if the lengths of the output of `fn` and `dtype` do not match.</span>

<span class="sd">  Examples:</span>
<span class="sd">    ```python</span>
<span class="sd">    elems = np.array([1, 2, 3, 4, 5, 6])</span>
<span class="sd">    squares = map_fn(lambda x: x * x, elems)</span>
<span class="sd">    # squares == [1, 4, 9, 16, 25, 36]</span>
<span class="sd">    ```</span>

<span class="sd">    ```python</span>
<span class="sd">    elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))</span>
<span class="sd">    alternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)</span>
<span class="sd">    # alternate == [-1, 2, -3]</span>
<span class="sd">    ```</span>

<span class="sd">    ```python</span>
<span class="sd">    elems = np.array([1, 2, 3])</span>
<span class="sd">    alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))</span>
<span class="sd">    # alternates[0] == [1, 2, 3]</span>
<span class="sd">    # alternates[1] == [-1, -2, -3]</span>
<span class="sd">    ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">map_fn</span><span class="p">(</span>
      <span class="n">fn</span><span class="o">=</span><span class="n">fn</span><span class="p">,</span>
      <span class="n">elems</span><span class="o">=</span><span class="n">elems</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">,</span>
      <span class="n">back_prop</span><span class="o">=</span><span class="n">back_prop</span><span class="p">,</span>
      <span class="n">swap_memory</span><span class="o">=</span><span class="n">swap_memory</span><span class="p">,</span>
      <span class="n">infer_shape</span><span class="o">=</span><span class="n">infer_shape</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>