

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.gradients_impl &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.gradients_impl</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.gradients_impl</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Implements the graph generation for computation of gradients.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">check_ops</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gradients_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">image_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">linalg_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">linalg_ops</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">logging_ops</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">manip_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">optional_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">random_grad</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">tensor_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.unconnected_gradients</span> <span class="k">import</span> <span class="n">UnconnectedGradients</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>


<div class="viewcode-block" id="gradients"><a class="viewcode-back" href="../../../../index.html#tensorflow.gradients">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gradients&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span>
              <span class="n">xs</span><span class="p">,</span>
              <span class="n">grad_ys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gradients&quot;</span><span class="p">,</span>
              <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">aggregation_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">stop_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.</span>

<span class="sd">  `ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`</span>
<span class="sd">  is a list of `Tensor`, holding the gradients received by the</span>
<span class="sd">  `ys`. The list must be the same length as `ys`.</span>

<span class="sd">  `gradients()` adds ops to the graph to output the derivatives of `ys` with</span>
<span class="sd">  respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where</span>
<span class="sd">  each tensor is the `sum(dy/dx)` for y in `ys` and for x in `xs`.</span>

<span class="sd">  `grad_ys` is a list of tensors of the same length as `ys` that holds</span>
<span class="sd">  the initial gradients for each y in `ys`.  When `grad_ys` is None,</span>
<span class="sd">  we fill in a tensor of &#39;1&#39;s of the shape of y for each y in `ys`.  A</span>
<span class="sd">  user can provide their own initial `grad_ys` to compute the</span>
<span class="sd">  derivatives using a different initial gradient for each y (e.g., if</span>
<span class="sd">  one wanted to weight the gradient differently for each value in</span>
<span class="sd">  each y).</span>

<span class="sd">  `stop_gradients` is a `Tensor` or a list of tensors to be considered constant</span>
<span class="sd">  with respect to all `xs`. These tensors will not be backpropagated through,</span>
<span class="sd">  as though they had been explicitly disconnected using `stop_gradient`.  Among</span>
<span class="sd">  other things, this allows computation of partial derivatives as opposed to</span>
<span class="sd">  total derivatives. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.constant(0.)</span>
<span class="sd">  b = 2 * a</span>
<span class="sd">  g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])</span>
<span class="sd">  ```</span>

<span class="sd">  Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the</span>
<span class="sd">  total derivatives `tf.gradients(a + b, [a, b])`, which take into account the</span>
<span class="sd">  influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is</span>
<span class="sd">  equivalent to:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.stop_gradient(tf.constant(0.))</span>
<span class="sd">  b = tf.stop_gradient(2 * a)</span>
<span class="sd">  g = tf.gradients(a + b, [a, b])</span>
<span class="sd">  ```</span>

<span class="sd">  `stop_gradients` provides a way of stopping gradient after the graph has</span>
<span class="sd">  already been constructed, as compared to `tf.stop_gradient` which is used</span>
<span class="sd">  during graph construction.  When the two approaches are combined,</span>
<span class="sd">  backpropagation stops at both `tf.stop_gradient` nodes and nodes in</span>
<span class="sd">  `stop_gradients`, whichever is encountered first.</span>

<span class="sd">  All integer tensors are considered constant with respect to all `xs`, as if</span>
<span class="sd">  they were included in `stop_gradients`.</span>

<span class="sd">  `unconnected_gradients` determines the value returned for each x in xs if it</span>
<span class="sd">  is unconnected in the graph to ys. By default this is None to safeguard</span>
<span class="sd">  against errors. Mathematically these gradients are zero which can be requested</span>
<span class="sd">  using the `&#39;zero&#39;` option. `tf.UnconnectedGradients` provides the</span>
<span class="sd">  following options and behaviors:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.ones([1, 2])</span>
<span class="sd">  b = tf.ones([3, 1])</span>
<span class="sd">  g1 = tf.gradients([b], [a], unconnected_gradients=&#39;none&#39;)</span>
<span class="sd">  sess.run(g1)  # [None]</span>

<span class="sd">  g2 = tf.gradients([b], [a], unconnected_gradients=&#39;zero&#39;)</span>
<span class="sd">  sess.run(g2)  # [array([[0., 0.]], dtype=float32)]</span>
<span class="sd">  ```</span>

<span class="sd">  Let us take one practical example which comes during the back propogation</span>
<span class="sd">  phase. This function is used to evaluate the derivatives of the cost function</span>
<span class="sd">  with respect to Weights `Ws` and Biases `bs`. Below sample implementation</span>
<span class="sd">  provides the exaplantion of what it is actually used for :</span>

<span class="sd">  ```python</span>
<span class="sd">  Ws = tf.constant(0.)</span>
<span class="sd">  bs = 2 * Ws</span>
<span class="sd">  cost = Ws + bs  # This is just an example. So, please ignore the formulas.</span>
<span class="sd">  g = tf.gradients(cost, [Ws, bs])</span>
<span class="sd">  dCost_dW, dCost_db = g</span>
<span class="sd">  ```</span>


<span class="sd">  Args:</span>
<span class="sd">    ys: A `Tensor` or list of tensors to be differentiated.</span>
<span class="sd">    xs: A `Tensor` or list of tensors to be used for differentiation.</span>
<span class="sd">    grad_ys: Optional. A `Tensor` or list of tensors the same size as</span>
<span class="sd">      `ys` and holding the gradients computed for each y in `ys`.</span>
<span class="sd">    name: Optional name to use for grouping all the gradient ops together.</span>
<span class="sd">      defaults to &#39;gradients&#39;.</span>
<span class="sd">    colocate_gradients_with_ops: If True, try colocating gradients with</span>
<span class="sd">      the corresponding op.</span>
<span class="sd">    gate_gradients: If True, add a tuple around the gradients returned</span>
<span class="sd">      for an operations.  This avoids some race conditions.</span>
<span class="sd">    aggregation_method: Specifies the method used to combine gradient terms.</span>
<span class="sd">      Accepted values are constants defined in the class `AggregationMethod`.</span>
<span class="sd">    stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate</span>
<span class="sd">      through.</span>
<span class="sd">    unconnected_gradients: Optional. Specifies the gradient value returned when</span>
<span class="sd">      the given input tensors are unconnected. Accepted values are constants</span>
<span class="sd">      defined in the class `tf.UnconnectedGradients` and the default value is</span>
<span class="sd">      `none`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` of length `len(xs)` where each tensor is the `sum(dy/dx)`</span>
<span class="sd">    for y in `ys` and for x in `xs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    LookupError: if one of the operations between `x` and `y` does not</span>
<span class="sd">      have a registered gradient function.</span>
<span class="sd">    ValueError: if the arguments are invalid.</span>
<span class="sd">    RuntimeError: if called in Eager mode.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Creating the gradient graph for control flow mutates Operations.</span>
  <span class="c1"># _mutation_lock ensures a Session.run call cannot occur between creating and</span>
  <span class="c1"># mutating new ops.</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_mutation_lock</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">gradients_util</span><span class="o">.</span><span class="n">_GradientsHelper</span><span class="p">(</span>
        <span class="n">ys</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">grad_ys</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
        <span class="n">gate_gradients</span><span class="p">,</span> <span class="n">aggregation_method</span><span class="p">,</span> <span class="n">stop_gradients</span><span class="p">,</span>
        <span class="n">unconnected_gradients</span><span class="p">)</span></div>
  <span class="c1"># pylint: enable=protected-access</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;gradients&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">gradients_v2</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span>  <span class="c1"># pylint: disable=invalid-name</span>
                 <span class="n">xs</span><span class="p">,</span>
                 <span class="n">grad_ys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gradients&quot;</span><span class="p">,</span>
                 <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">aggregation_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">stop_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.</span>

<span class="sd">  `ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`</span>
<span class="sd">  is a list of `Tensor`, holding the gradients received by the</span>
<span class="sd">  `ys`. The list must be the same length as `ys`.</span>

<span class="sd">  `gradients()` adds ops to the graph to output the derivatives of `ys` with</span>
<span class="sd">  respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where</span>
<span class="sd">  each tensor is the `sum(dy/dx)` for y in `ys` and for x in `xs`.</span>

<span class="sd">  `grad_ys` is a list of tensors of the same length as `ys` that holds</span>
<span class="sd">  the initial gradients for each y in `ys`.  When `grad_ys` is None,</span>
<span class="sd">  we fill in a tensor of &#39;1&#39;s of the shape of y for each y in `ys`.  A</span>
<span class="sd">  user can provide their own initial `grad_ys` to compute the</span>
<span class="sd">  derivatives using a different initial gradient for each y (e.g., if</span>
<span class="sd">  one wanted to weight the gradient differently for each value in</span>
<span class="sd">  each y).</span>

<span class="sd">  `stop_gradients` is a `Tensor` or a list of tensors to be considered constant</span>
<span class="sd">  with respect to all `xs`. These tensors will not be backpropagated through,</span>
<span class="sd">  as though they had been explicitly disconnected using `stop_gradient`.  Among</span>
<span class="sd">  other things, this allows computation of partial derivatives as opposed to</span>
<span class="sd">  total derivatives. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.constant(0.)</span>
<span class="sd">  b = 2 * a</span>
<span class="sd">  g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])</span>
<span class="sd">  ```</span>

<span class="sd">  Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the</span>
<span class="sd">  total derivatives `tf.gradients(a + b, [a, b])`, which take into account the</span>
<span class="sd">  influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is</span>
<span class="sd">  equivalent to:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.stop_gradient(tf.constant(0.))</span>
<span class="sd">  b = tf.stop_gradient(2 * a)</span>
<span class="sd">  g = tf.gradients(a + b, [a, b])</span>
<span class="sd">  ```</span>

<span class="sd">  `stop_gradients` provides a way of stopping gradient after the graph has</span>
<span class="sd">  already been constructed, as compared to `tf.stop_gradient` which is used</span>
<span class="sd">  during graph construction.  When the two approaches are combined,</span>
<span class="sd">  backpropagation stops at both `tf.stop_gradient` nodes and nodes in</span>
<span class="sd">  `stop_gradients`, whichever is encountered first.</span>

<span class="sd">  All integer tensors are considered constant with respect to all `xs`, as if</span>
<span class="sd">  they were included in `stop_gradients`.</span>

<span class="sd">  `unconnected_gradients` determines the value returned for each x in xs if it</span>
<span class="sd">  is unconnected in the graph to ys. By default this is None to safeguard</span>
<span class="sd">  against errors. Mathematically these gradients are zero which can be requested</span>
<span class="sd">  using the `&#39;zero&#39;` option. `tf.UnconnectedGradients` provides the</span>
<span class="sd">  following options and behaviors:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.ones([1, 2])</span>
<span class="sd">  b = tf.ones([3, 1])</span>
<span class="sd">  g1 = tf.gradients([b], [a], unconnected_gradients=&#39;none&#39;)</span>
<span class="sd">  sess.run(g1)  # [None]</span>

<span class="sd">  g2 = tf.gradients([b], [a], unconnected_gradients=&#39;zero&#39;)</span>
<span class="sd">  sess.run(g2)  # [array([[0., 0.]], dtype=float32)]</span>
<span class="sd">  ```</span>

<span class="sd">  Let us take one practical example which comes during the back propogation</span>
<span class="sd">  phase. This function is used to evaluate the derivatives of the cost function</span>
<span class="sd">  with respect to Weights `Ws` and Biases `bs`. Below sample implementation</span>
<span class="sd">  provides the exaplantion of what it is actually used for :</span>

<span class="sd">  ```python</span>
<span class="sd">  Ws = tf.constant(0.)</span>
<span class="sd">  bs = 2 * Ws</span>
<span class="sd">  cost = Ws + bs  # This is just an example. So, please ignore the formulas.</span>
<span class="sd">  g = tf.gradients(cost, [Ws, bs])</span>
<span class="sd">  dCost_dW, dCost_db = g</span>
<span class="sd">  ```</span>


<span class="sd">  Args:</span>
<span class="sd">    ys: A `Tensor` or list of tensors to be differentiated.</span>
<span class="sd">    xs: A `Tensor` or list of tensors to be used for differentiation.</span>
<span class="sd">    grad_ys: Optional. A `Tensor` or list of tensors the same size as</span>
<span class="sd">      `ys` and holding the gradients computed for each y in `ys`.</span>
<span class="sd">    name: Optional name to use for grouping all the gradient ops together.</span>
<span class="sd">      defaults to &#39;gradients&#39;.</span>
<span class="sd">    gate_gradients: If True, add a tuple around the gradients returned</span>
<span class="sd">      for an operations.  This avoids some race conditions.</span>
<span class="sd">    aggregation_method: Specifies the method used to combine gradient terms.</span>
<span class="sd">      Accepted values are constants defined in the class `AggregationMethod`.</span>
<span class="sd">    stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate</span>
<span class="sd">      through.</span>
<span class="sd">    unconnected_gradients: Optional. Specifies the gradient value returned when</span>
<span class="sd">      the given input tensors are unconnected. Accepted values are constants</span>
<span class="sd">      defined in the class `tf.UnconnectedGradients` and the default value is</span>
<span class="sd">      `none`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` of length `len(xs)` where each tensor is the `sum(dy/dx)`</span>
<span class="sd">    for y in `ys` and for x in `xs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    LookupError: if one of the operations between `x` and `y` does not</span>
<span class="sd">      have a registered gradient function.</span>
<span class="sd">    ValueError: if the arguments are invalid.</span>
<span class="sd">    RuntimeError: if called in Eager mode.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Creating the gradient graph for control flow mutates Operations.</span>
  <span class="c1"># _mutation_lock ensures a Session.run call cannot occur between creating and</span>
  <span class="c1"># mutating new ops.</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_mutation_lock</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">gradients_util</span><span class="o">.</span><span class="n">_GradientsHelper</span><span class="p">(</span>
        <span class="n">ys</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">grad_ys</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="p">,</span>
        <span class="n">aggregation_method</span><span class="p">,</span> <span class="n">stop_gradients</span><span class="p">,</span>
        <span class="n">unconnected_gradients</span><span class="p">)</span>
  <span class="c1"># pylint: enable=protected-access</span>


<span class="c1"># TODO(vrv): Make this available when we want to make it public.</span>
<span class="k">def</span> <span class="nf">_hessian_vector_product</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiply the Hessian of `ys` wrt `xs` by `v`.</span>

<span class="sd">  This is an efficient construction that uses a backprop-like approach</span>
<span class="sd">  to compute the product between the Hessian and another vector. The</span>
<span class="sd">  Hessian is usually too large to be explicitly computed or even</span>
<span class="sd">  represented, but this method allows us to at least multiply by it</span>
<span class="sd">  for the same big-O cost as backprop.</span>

<span class="sd">  Implicit Hessian-vector products are the main practical, scalable way</span>
<span class="sd">  of using second derivatives with neural networks. They allow us to</span>
<span class="sd">  do things like construct Krylov subspaces and approximate conjugate</span>
<span class="sd">  gradient descent.</span>

<span class="sd">  Example: if `y` = 1/2 `x`^T A `x`, then `hessian_vector_product(y,</span>
<span class="sd">  x, v)` will return an expression that evaluates to the same values</span>
<span class="sd">  as (A + A.T) `v`.</span>

<span class="sd">  Args:</span>
<span class="sd">    ys: A scalar value, or a tensor or list of tensors to be summed to</span>
<span class="sd">        yield a scalar.</span>
<span class="sd">    xs: A list of tensors that we should construct the Hessian over.</span>
<span class="sd">    v: A list of tensors, with the same shapes as xs, that we want to</span>
<span class="sd">       multiply by the Hessian.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of tensors (or if the list would be length 1, a single tensor)</span>
<span class="sd">    containing the product between the Hessian and `v`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: `xs` and `v` have different length.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Validate the input</span>
  <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">!=</span> <span class="n">length</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;xs and v must have the same length.&quot;</span><span class="p">)</span>

  <span class="c1"># First backprop</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>

  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">==</span> <span class="n">length</span>
  <span class="n">elemwise_products</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">grad_elem</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">v_elem</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">grad_elem</span><span class="p">,</span> <span class="n">v_elem</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">grad_elem</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="p">]</span>

  <span class="c1"># Second backprop</span>
  <span class="k">return</span> <span class="n">gradients</span><span class="p">(</span><span class="n">elemwise_products</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>


<div class="viewcode-block" id="hessians"><a class="viewcode-back" href="../../../../index.html#tensorflow.hessians">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hessians&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">hessians</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span>
             <span class="n">xs</span><span class="p">,</span>
             <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hessians&quot;</span><span class="p">,</span>
             <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">aggregation_method</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Constructs the Hessian of sum of `ys` with respect to `x` in `xs`.</span>

<span class="sd">  `hessians()` adds ops to the graph to output the Hessian matrix of `ys`</span>
<span class="sd">  with respect to `xs`.  It returns a list of `Tensor` of length `len(xs)`</span>
<span class="sd">  where each tensor is the Hessian of `sum(ys)`.</span>

<span class="sd">  The Hessian is a matrix of second-order partial derivatives of a scalar</span>
<span class="sd">  tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).</span>

<span class="sd">  Args:</span>
<span class="sd">    ys: A `Tensor` or list of tensors to be differentiated.</span>
<span class="sd">    xs: A `Tensor` or list of tensors to be used for differentiation.</span>
<span class="sd">    name: Optional name to use for grouping all the gradient ops together.</span>
<span class="sd">      defaults to &#39;hessians&#39;.</span>
<span class="sd">    colocate_gradients_with_ops: See `gradients()` documentation for details.</span>
<span class="sd">    gate_gradients: See `gradients()` documentation for details.</span>
<span class="sd">    aggregation_method: See `gradients()` documentation for details.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of Hessian matrices of `sum(ys)` for each `x` in `xs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    LookupError: if one of the operations between `xs` and `ys` does not</span>
<span class="sd">      have a registered gradient function.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">xs</span> <span class="o">=</span> <span class="n">gradients_util</span><span class="o">.</span><span class="n">_AsList</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&quot;colocate_gradients_with_ops&quot;</span><span class="p">:</span> <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
      <span class="s2">&quot;gate_gradients&quot;</span><span class="p">:</span> <span class="n">gate_gradients</span><span class="p">,</span>
      <span class="s2">&quot;aggregation_method&quot;</span><span class="p">:</span> <span class="n">aggregation_method</span>
  <span class="p">}</span>
  <span class="c1"># Compute first-order derivatives and iterate for each x in xs.</span>
  <span class="n">hessians</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">_gradients</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">_gradients</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
    <span class="c1"># change shape to one-dimension without graph branching</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Declare an iterator and tensor array loop variables for the gradients.</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loop_vars</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">array_ops</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="c1"># Iterate over all elements of the gradient and compute second order</span>
    <span class="c1"># derivatives.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">hessian</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">j</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">,</span>
        <span class="k">lambda</span> <span class="n">j</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                           <span class="n">result</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">gradients</span><span class="p">(</span><span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])),</span>
        <span class="n">loop_vars</span>
    <span class="p">)</span>

    <span class="n">_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_reshaped_hessian</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hessian</span><span class="o">.</span><span class="n">stack</span><span class="p">(),</span>
                                          <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">_shape</span><span class="p">,</span> <span class="n">_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">hessians</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_reshaped_hessian</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">hessians</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;hessians&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">HessiansV2</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span>
               <span class="n">xs</span><span class="p">,</span>
               <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">aggregation_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hessians&quot;</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">hessians</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="o">=</span><span class="n">gate_gradients</span><span class="p">,</span>
                  <span class="n">aggregation_method</span><span class="o">=</span><span class="n">aggregation_method</span><span class="p">)</span>


<span class="n">HessiansV2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">hessians</span><span class="o">.</span><span class="vm">__doc__</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>