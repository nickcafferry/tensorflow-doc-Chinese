

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.math_ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.math_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.math_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Math Operations.</span>

<span class="sd">Note: Functions taking `Tensor` arguments can also take anything accepted by</span>
<span class="sd">`tf.convert_to_tensor`.</span>

<span class="sd">Note: Elementwise binary operations in TensorFlow follow [numpy-style</span>
<span class="sd">broadcasting](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).</span>

<span class="sd">TensorFlow provides a variety of math functions including:</span>

<span class="sd">* Basic arithmetic operators and trigonometric functions.</span>
<span class="sd">* Special math functions (like: `tf.math.igamma` and `tf.math.zeta`)</span>
<span class="sd">* Complex number functions (like: `tf.math.imag` and `tf.math.angle`)</span>
<span class="sd">* Reductions and scans (like: `tf.math.reduce_mean` and `tf.math.cumsum`)</span>
<span class="sd">* Segment functions (like: `tf.math.segment_sum`)</span>

<span class="sd">See: `tf.linalg` for matrix and tensor functions.</span>

<span class="sd">&lt;a id=Segmentation&gt;&lt;/a&gt;</span>

<span class="sd">## About Segmentation</span>

<span class="sd">TensorFlow provides several operations that you can use to perform common</span>
<span class="sd">math computations on tensor segments.</span>
<span class="sd">Here a segmentation is a partitioning of a tensor along</span>
<span class="sd">the first dimension, i.e. it  defines a mapping from the first dimension onto</span>
<span class="sd">`segment_ids`. The `segment_ids` tensor should be the size of</span>
<span class="sd">the first dimension, `d0`, with consecutive IDs in the range `0` to `k`,</span>
<span class="sd">where `k&lt;d0`.</span>
<span class="sd">In particular, a segmentation of a matrix tensor is a mapping of rows to</span>
<span class="sd">segments.</span>

<span class="sd">For example:</span>

<span class="sd">```python</span>
<span class="sd">c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])</span>
<span class="sd">tf.math.segment_sum(c, tf.constant([0, 0, 1]))</span>
<span class="sd">#  ==&gt;  [[0 0 0 0]</span>
<span class="sd">#        [5 6 7 8]]</span>
<span class="sd">```</span>

<span class="sd">The standard `segment_*` functions assert that the segment indices are sorted.</span>
<span class="sd">If you have unsorted indices use the equivalent `unsorted_segment_` function.</span>
<span class="sd">These functions take an additional argument `num_segments` so that the output</span>
<span class="sd">tensor can be efficiently allocated.</span>

<span class="sd">``` python</span>
<span class="sd">c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])</span>
<span class="sd">tf.math.unsorted_segment_sum(c, tf.constant([0, 1, 0]), num_segments=2)</span>
<span class="sd"># ==&gt; [[ 6,  8, 10, 12],</span>
<span class="sd">#       [-1, -2, -3, -4]]</span>
<span class="sd">```</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">builtins</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">graph_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">sparse_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_data_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_nn_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_sparse_ops</span>
<span class="c1"># go/tf-wildcard-import</span>
<span class="c1"># pylint: disable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.gen_math_ops</span> <span class="k">import</span> <span class="o">*</span>
<span class="c1"># pylint: enable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">dispatch</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="c1"># Aliases for some automatically-generated names.</span>
<span class="n">linspace</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">lin_space</span>
<span class="n">nextafter</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">next_after</span>

<span class="n">arg_max</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use `tf.math.argmax` instead&quot;</span><span class="p">)(</span><span class="n">arg_max</span><span class="p">)</span>  <span class="c1"># pylint: disable=used-before-assignment</span>
<span class="n">arg_min</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use `tf.math.argmin` instead&quot;</span><span class="p">)(</span><span class="n">arg_min</span><span class="p">)</span>  <span class="c1"># pylint: disable=used-before-assignment</span>
<span class="n">tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;arg_max&quot;</span><span class="p">])(</span><span class="n">arg_max</span><span class="p">)</span>
<span class="n">tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;arg_min&quot;</span><span class="p">])(</span><span class="n">arg_min</span><span class="p">)</span>


<span class="c1"># This is set by resource_variable_ops.py. It is included in this way since</span>
<span class="c1"># there is a circular dependency between math_ops and resource_variable_ops</span>
<span class="n">_resource_variable_type</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_set_doc</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">_decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
    <span class="k">return</span> <span class="n">func</span>

  <span class="k">return</span> <span class="n">_decorator</span>


<span class="c1"># pylint: disable=redefined-builtin</span>
<div class="viewcode-block" id="argmax"><a class="viewcode-back" href="../../../../index.html#tensorflow.argmax">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.argmax&quot;</span><span class="p">,</span> <span class="s2">&quot;argmax&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use the `axis` argument instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;dimension&quot;</span><span class="p">)</span>
<span class="nd">@_set_doc</span><span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_max</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;dimensions&quot;</span><span class="p">,</span>
                                         <span class="s2">&quot;axes&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;dimension&quot;</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
           <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">dimension</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span>
                                                <span class="n">dimension</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">argmax_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.argmax&quot;</span><span class="p">,</span> <span class="s2">&quot;argmax&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">argmax_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the index with the largest value across axes of a tensor.</span>

<span class="sd">  Note that in case of ties the identity of the return value is not guaranteed.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; A = tf.constant([2, 20, 30, 3, 6])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.argmax(A)  # A[2] is maximum in tensor A</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt;</span>
<span class="sd">  &gt;&gt;&gt; B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],</span>
<span class="sd">  ...                  [14, 45, 23, 5, 27]])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.argmax(B, 0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.math.argmax(B, 1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`.</span>
<span class="sd">    axis: An integer, the axis to reduce across. Default to 0.</span>
<span class="sd">    output_type: An optional output dtype (`tf.int32` or `tf.int64`). Defaults</span>
<span class="sd">      to `tf.int64`.</span>
<span class="sd">    name: An optional name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `output_type`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">)</span>


<div class="viewcode-block" id="argmin"><a class="viewcode-back" href="../../../../index.html#tensorflow.argmin">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.argmin&quot;</span><span class="p">,</span> <span class="s2">&quot;argmin&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use the `axis` argument instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;dimension&quot;</span><span class="p">)</span>
<span class="nd">@_set_doc</span><span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_min</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;dimensions&quot;</span><span class="p">,</span>
                                         <span class="s2">&quot;axes&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;dimension&quot;</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
           <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">dimension</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span>
                                                <span class="n">dimension</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">argmin_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.argmin&quot;</span><span class="p">,</span> <span class="s2">&quot;argmin&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">argmin_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the index with the smallest value across axes of a tensor.</span>

<span class="sd">  Note that in case of ties the identity of the return value is not guaranteed.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span>
<span class="sd">      `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`,</span>
<span class="sd">      `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`,</span>
<span class="sd">      `uint64`.</span>
<span class="sd">    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.</span>
<span class="sd">      int32 or int64, must be in the range `-rank(input), rank(input))`.</span>
<span class="sd">      Describes which axis of the input Tensor to reduce across. For vectors,</span>
<span class="sd">      use axis = 0.</span>
<span class="sd">    output_type: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to</span>
<span class="sd">      `tf.int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `output_type`.</span>

<span class="sd">  Usage:</span>
<span class="sd">  ```python</span>
<span class="sd">  import tensorflow as tf</span>
<span class="sd">  a = [1, 10, 26.9, 2.8, 166.32, 62.3]</span>
<span class="sd">  b = tf.math.argmin(input = a)</span>
<span class="sd">  c = tf.keras.backend.eval(b)</span>
<span class="sd">  # c = 0</span>
<span class="sd">  # here a[0] = 1 which is the smallest element of a across axis 0</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_min</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">)</span>


<span class="c1"># pylint: enable=redefined-builtin</span>


<span class="c1"># pylint: disable=anomalous-backslash-in-string,protected-access</span>
<span class="c1"># pylint: disable=g-docstring-has-escape</span>
<div class="viewcode-block" id="abs"><a class="viewcode-back" href="../../../../index.html#tensorflow.abs">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.abs&quot;</span><span class="p">,</span> <span class="s2">&quot;abs&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the absolute value of a tensor.</span>

<span class="sd">  Given a tensor of integer or floating-point values, this operation returns a</span>
<span class="sd">  tensor of the same type, where each element contains the absolute value of the</span>
<span class="sd">  corresponding element in the input.</span>

<span class="sd">  Given a tensor `x` of complex numbers, this operation returns a tensor of type</span>
<span class="sd">  `float32` or `float64` that is the absolute value of each element in `x`. For</span>
<span class="sd">  a complex number \\(a + bj\\), its absolute value is computed as \\(\sqrt{a^2</span>
<span class="sd">  + b^2}\\).  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])</span>
<span class="sd">  &gt;&gt;&gt; tf.abs(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=</span>
<span class="sd">  array([[5.25594901],</span>
<span class="sd">         [6.60492241]])&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,</span>
<span class="sd">      `int32`, `int64`, `complex64` or `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,</span>
<span class="sd">      with absolute values. Note, for `complex64` or `complex128` input, the</span>
<span class="sd">      returned `Tensor` will be of type `float32` or `float64`, respectively.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Abs&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">complex_abs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_abs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="c1"># pylint: enable=g-docstring-has-escape</span>


<span class="c1"># pylint: disable=redefined-builtin</span>
<span class="k">def</span> <span class="nf">_bucketize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">bucketize</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">boundaries</span><span class="o">=</span><span class="n">boundaries</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: enable=redefined-builtin</span>


<span class="k">class</span> <span class="nc">DivideDelegateWithName</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Use Python2/Python3 division delegation to implement divide for tensors.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct DivideDelegateWithName.</span>

<span class="sd">    Args:</span>
<span class="sd">      x: Tensor to use as left operand in operator overloads</span>
<span class="sd">      name: The name that is preferred for the op created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

  <span class="k">def</span> <span class="nf">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_truediv_python3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">floordiv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__div__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_div_python2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="divide"><a class="viewcode-back" href="../../../../index.html#tensorflow.divide">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.divide&quot;</span><span class="p">,</span> <span class="s2">&quot;divide&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes Python style division of `x` by `y`.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([16, 12, 11])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([4, 6, 2])</span>
<span class="sd">  &gt;&gt;&gt; tf.divide(x,y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3,), dtype=float64,</span>
<span class="sd">  numpy=array([4. , 2. , 5.5])&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`</span>
<span class="sd">    y: A `Tensor`</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with same shape as input</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Cannot use tensors operator overload, because it has no way to track</span>
    <span class="c1"># override names. Use a dummy class to track the runtime division behavior</span>
    <span class="k">return</span> <span class="n">DivideDelegateWithName</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="o">/</span> <span class="n">y</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span></div>


<div class="viewcode-block" id="multiply"><a class="viewcode-back" href="../../../../index.html#tensorflow.multiply">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.multiply&quot;</span><span class="p">,</span> <span class="s2">&quot;multiply&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns an element-wise x * y.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant(([1, 2, 3, 4]))</span>
<span class="sd">  &gt;&gt;&gt; tf.math.multiply(x, x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)&gt;</span>

<span class="sd">  Since `tf.math.multiply` will convert its arguments to `Tensor`s, you can also</span>
<span class="sd">  pass in non-`Tensor` arguments:</span>

<span class="sd">  &gt;&gt;&gt; tf.math.multiply(7,6)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt;</span>

<span class="sd">  If `x.shape` is not thes same as `y.shape`, they will be broadcast to a</span>
<span class="sd">  compatible shape. (More about broadcasting</span>
<span class="sd">  [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).)</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.ones([1, 2]);</span>
<span class="sd">  &gt;&gt;&gt; y = tf.ones([2, 1]);</span>
<span class="sd">  &gt;&gt;&gt; x * y  # Taking advantage of operator overriding</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span>
<span class="sd">  array([[1., 1.],</span>
<span class="sd">       [1., 1.]], dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor. Must be one of the following types: `bfloat16`,</span>
<span class="sd">      `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`,</span>
<span class="sd">      `int16`, `int32`, `int64`, `complex64`, `complex128`.</span>
<span class="sd">    y: A `Tensor`. Must have the same type as `x`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>

<span class="sd">  A `Tensor`.  Has the same type as `x`.</span>

<span class="sd">  Raises:</span>

<span class="sd">   * InvalidArgumentError: When `x` and `y` have incomptatible shapes or types.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="c1"># TODO(aselle): put deprecation in after another round of global code changes</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2016-12-30&quot;</span><span class="p">,</span>
    <span class="s2">&quot;`tf.mul(x, y)` is deprecated; use `tf.math.multiply(x, y)` or `x * y`&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">_mul</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">_mul</span><span class="o">.</span><span class="vm">__doc__</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_mul</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">))</span>


<div class="viewcode-block" id="subtract"><a class="viewcode-back" href="../../../../index.html#tensorflow.subtract">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.subtract&quot;</span><span class="p">,</span> <span class="s2">&quot;subtract&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">subtract</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="n">subtract</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;`Sub`&quot;</span><span class="p">,</span> <span class="s2">&quot;`tf.subtract`&quot;</span><span class="p">)</span>


<span class="c1"># TODO(aselle): put deprecation in after another round of global code changes</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2016-12-30&quot;</span><span class="p">,</span>
    <span class="s2">&quot;`tf.sub(x, y)` is deprecated, please use `tf.subtract(x, y)` or `x - y`&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">_sub</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">_sub</span><span class="o">.</span><span class="vm">__doc__</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_sub</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">))</span>

<span class="n">negative</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">neg</span>


<span class="c1"># pylint: disable=g-docstring-has-escape</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2016-12-30&quot;</span><span class="p">,</span>
    <span class="s2">&quot;`tf.neg(x)` is deprecated, please use `tf.negative(x)` or `-x`&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_neg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes numerical negative value element-wise.</span>

<span class="sd">  I.e., \\(y = -x\\).</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span>
<span class="sd">      `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">negative</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: enable=g-docstring-has-escape</span>


<div class="viewcode-block" id="scalar_mul"><a class="viewcode-back" href="../../../../index.html#tensorflow.scalar_mul">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.scalar_mul&quot;</span><span class="p">,</span> <span class="s2">&quot;scalar_mul&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">scalar_mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiplies a scalar times a `Tensor` or `IndexedSlices` object.</span>

<span class="sd">  Intended for use in gradient code which might deal with `IndexedSlices`</span>
<span class="sd">  objects, which are easy to multiply by a scalar but more expensive to</span>
<span class="sd">  multiply with arbitrary tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    scalar: A 0-D scalar `Tensor`. Must have known shape.</span>
<span class="sd">    x: A `Tensor` or `IndexedSlices` to be scaled.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `scalar * x` of the same type (`Tensor` or `IndexedSlices`) as `x`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if scalar is not a 0-D `scalar`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">scalar</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
      <span class="n">scalar</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scalar&quot;</span><span class="p">)</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">scalar</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
          <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only scalar multiply works, got shape </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">shape</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.scalar_mul&quot;</span><span class="p">,</span> <span class="s2">&quot;scalar_mul&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@_set_doc</span><span class="p">(</span><span class="n">scalar_mul</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">scalar_mul_v2</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;scalar_mul&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">scalar_mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="pow"><a class="viewcode-back" href="../../../../index.html#tensorflow.pow">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.pow&quot;</span><span class="p">,</span> <span class="s2">&quot;pow&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the power of one value to another.</span>

<span class="sd">  Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for</span>
<span class="sd">  corresponding elements in `x` and `y`. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[2, 2], [3, 3]])</span>
<span class="sd">  y = tf.constant([[8, 16], [2, 3]])</span>
<span class="sd">  tf.pow(x, y)  # [[256, 65536], [9, 27]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,</span>
<span class="sd">      `complex64`, or `complex128`.</span>
<span class="sd">    y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,</span>
<span class="sd">      `complex64`, or `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Pow&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="c1"># pylint: disable=redefined-builtin,redefined-outer-name</span>
<div class="viewcode-block" id="complex"><a class="viewcode-back" href="../../../../index.html#tensorflow.complex">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;dtypes.complex&quot;</span><span class="p">,</span> <span class="s2">&quot;complex&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">complex</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Converts two real numbers to a complex number.</span>

<span class="sd">  Given a tensor `real` representing the real part of a complex number, and a</span>
<span class="sd">  tensor `imag` representing the imaginary part of a complex number, this</span>
<span class="sd">  operation returns complex numbers elementwise of the form \\(a + bj\\), where</span>
<span class="sd">  *a* represents the `real` part and *b* represents the `imag` part.</span>

<span class="sd">  The input tensors `real` and `imag` must have the same shape.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  real = tf.constant([2.25, 3.25])</span>
<span class="sd">  imag = tf.constant([4.75, 5.75])</span>
<span class="sd">  tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    real: A `Tensor`. Must be one of the following types: `float32`, `float64`.</span>
<span class="sd">    imag: A `Tensor`. Must have the same type as `real`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `complex64` or `complex128`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: Real and imag must be correct types</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">real</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;real&quot;</span><span class="p">)</span>
  <span class="n">imag</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">imag</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;imag&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Complex&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">input_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">real</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">imag</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_types</span> <span class="o">==</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
      <span class="n">Tout</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex128</span>
    <span class="k">elif</span> <span class="n">input_types</span> <span class="o">==</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
      <span class="n">Tout</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;real and imag have incorrect types: &quot;</span>
                      <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">real</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">imag</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_complex</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="sign"><a class="viewcode-back" href="../../../../index.html#tensorflow.sign">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.sign&quot;</span><span class="p">,</span> <span class="s2">&quot;sign&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">sign</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns an element-wise indication of the sign of a number.</span>

<span class="sd">  y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0.</span>

<span class="sd">  For complex numbers, y = sign(x) = x / |x| if x != 0, otherwise y = 0.</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; tf.math.sign([0., 2., -3.])</span>
<span class="sd">  &lt;tf.Tensor: ... numpy=array([ 0.,  1., -1.], dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">   x: A Tensor. Must be one of the following types: bfloat16, half, float32,</span>
<span class="sd">      float64, int32, int64, complex64, complex128.</span>
<span class="sd">   name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">   A Tensor. Has the same type as x.</span>

<span class="sd">   If x is a SparseTensor, returns SparseTensor(x.indices,</span>
<span class="sd">     tf.math.sign(x.values, ...), x.dense_shape).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">div_no_nan</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">cast</span><span class="p">(</span>
            <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">complex_abs</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">Tout</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span> <span class="k">else</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.real&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.real&quot;</span><span class="p">,</span> <span class="s2">&quot;real&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;real&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">real</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the real part of a complex (or real) tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of type `float` that</span>
<span class="sd">  is the real part of each element in `input` considered as a complex number.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])</span>
<span class="sd">  tf.math.real(x)  # [-2.25, 3.25]</span>
<span class="sd">  ```</span>

<span class="sd">  If `input` is already real, it is returned unchanged.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must have numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `float32` or `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Real&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="n">real_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">input</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.imag&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.imag&quot;</span><span class="p">,</span> <span class="s2">&quot;imag&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;imag&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">imag</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the imaginary part of a complex (or real) tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of type `float` that</span>
<span class="sd">  is the imaginary part of each element in `input` considered as a complex</span>
<span class="sd">  number. If `input` is real, a tensor of all zeros is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])</span>
<span class="sd">  tf.math.imag(x)  # [4.75, 5.75]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float`, `double`,</span>
<span class="sd">      `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `float32` or `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Imag&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.angle&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.angle&quot;</span><span class="p">,</span> <span class="s2">&quot;angle&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;angle&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">angle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the element-wise argument of a complex (or real) tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of type `float` that</span>
<span class="sd">  is the argument of each element in `input` considered as a complex number.</span>

<span class="sd">  The elements in `input` are considered to be complex numbers of the form</span>
<span class="sd">  \\(a + bj\\), where *a* is the real part and *b* is the imaginary part.</span>
<span class="sd">  If `input` is real then *b* is zero by definition.</span>

<span class="sd">  The argument returned by this function is of the form \\(atan2(b, a)\\).</span>
<span class="sd">  If `input` is real, a tensor of all zeros is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```</span>
<span class="sd">  input = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j], dtype=tf.complex64)</span>
<span class="sd">  tf.math.angle(input).numpy()</span>
<span class="sd">  # ==&gt; array([2.0131705, 1.056345 ], dtype=float32)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float`, `double`,</span>
<span class="sd">      `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `float32` or `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Angle&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">angle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="c1"># pylint: enable=redefined-outer-name,redefined-builtin</span>


<div class="viewcode-block" id="round"><a class="viewcode-back" href="../../../../index.html#tensorflow.round">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.round&quot;</span><span class="p">,</span> <span class="s2">&quot;round&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Rounds the values of a tensor to the nearest integer, element-wise.</span>

<span class="sd">  Rounds half to even.  Also known as bankers rounding. If you want to round</span>
<span class="sd">  according to the current system rounding mode use tf::cint.</span>
<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])</span>
<span class="sd">  tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, or `int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of same shape and type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="cast"><a class="viewcode-back" href="../../../../index.html#tensorflow.cast">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;cast&quot;</span><span class="p">,</span> <span class="s2">&quot;dtypes.cast&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to a new type.</span>

<span class="sd">  The operation casts `x` (in case of `Tensor`) or `x.values`</span>
<span class="sd">  (in case of `SparseTensor` or `IndexedSlices`) to `dtype`.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([1.8, 2.2], dtype=tf.float32)</span>
<span class="sd">  &gt;&gt;&gt; tf.dtypes.cast(x, tf.int32)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span>

<span class="sd">  The operation supports data types (for `x` and `dtype`) of</span>
<span class="sd">  `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`,</span>
<span class="sd">  `float16`, `float32`, `float64`, `complex64`, `complex128`, `bfloat16`.</span>
<span class="sd">  In case of casting from complex types (`complex64`, `complex128`) to real</span>
<span class="sd">  types, only the real part of `x` is returned. In case of casting from real</span>
<span class="sd">  types to complex types (`complex64`, `complex128`), the imaginary part of the</span>
<span class="sd">  returned value is set to `0`. The handling of complex types here matches the</span>
<span class="sd">  behavior of numpy.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices` of numeric type. It could</span>
<span class="sd">      be `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`,</span>
<span class="sd">      `int64`, `float16`, `float32`, `float64`, `complex64`, `complex128`,</span>
<span class="sd">      `bfloat16`.</span>
<span class="sd">    dtype: The destination type. The list of supported dtypes is the same as</span>
<span class="sd">      `x`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` and</span>
<span class="sd">      same type as `dtype`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `dtype`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">base_type</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_resource_variable_type</span><span class="p">))</span> <span class="ow">and</span> <span class="n">base_type</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Cast&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">):</span>
      <span class="n">values_cast</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">values_cast</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">values_cast</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">values_cast</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># TODO(josh11b): If x is not already a Tensor, we could return</span>
      <span class="c1"># ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that</span>
      <span class="c1"># allows some conversions that cast() can&#39;t do, e.g. casting numbers to</span>
      <span class="c1"># strings.</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">!=</span> <span class="n">base_type</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span> <span class="ow">and</span> <span class="n">base_type</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Casting complex to real discards imaginary part.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="saturate_cast"><a class="viewcode-back" href="../../../../index.html#tensorflow.saturate_cast">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;dtypes.saturate_cast&quot;</span><span class="p">,</span> <span class="s2">&quot;saturate_cast&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">saturate_cast</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Performs a safe saturating cast of `value` to `dtype`.</span>

<span class="sd">  This function casts the input to `dtype` without applying any scaling.  If</span>
<span class="sd">  there is a danger that values would over or underflow in the cast, this op</span>
<span class="sd">  applies the appropriate clamping before the cast.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A `Tensor`.</span>
<span class="sd">    dtype: The desired output `DType`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `value` safely cast to `dtype`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># When casting to a type with smaller representable range, clamp.</span>
  <span class="c1"># Note that this covers casting to unsigned types as well.</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;saturate_cast&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">value</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">min</span> <span class="o">&lt;</span> <span class="n">dtype</span><span class="o">.</span><span class="n">min</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">min</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">&gt;</span> <span class="n">dtype</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
          <span class="n">value</span><span class="p">,</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use `tf.cast` instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_float&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_float</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToFloat&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `float32`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `float32`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `float32`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use `tf.cast` instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_double&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_double</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToDouble&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `float64`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `float64`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use `tf.cast` instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_int32&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_int32</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToInt32&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `int32`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `int32`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `int32`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use `tf.cast` instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_int64&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_int64</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToInt64&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `int64`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `int64`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `int64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use `tf.cast` instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_bfloat16&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_bfloat16</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToBFloat16&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `bfloat16`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `bfloat16`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `bfloat16`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use `tf.cast` instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_complex64&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_complex64</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToComplex64&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `complex64`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `complex64`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `complex64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use `tf.cast` instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_complex128&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_complex128</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToComplex128&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `complex128`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `complex128`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `complex128`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex128</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__neg__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">neg</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__abs__&quot;</span><span class="p">,</span> <span class="nb">abs</span><span class="p">)</span>
<span class="c1"># __invert__ corresponds to the ~ operator.  Here we follow the numpy convention</span>
<span class="c1"># ~ marks an elementwise bit-wise inverse.  This is only implemented for boolean</span>
<span class="c1"># tensors and will throw a TypeError if used on nonboolean arrays</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__invert__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">clazz_object</span><span class="o">=</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Register operators with different tensor and scalar versions.</span>

<span class="sd">  If `clazz_object` is `SparseTensor`, assumes `func` takes `(sp_indices,</span>
<span class="sd">  sp_values, sp_shape, dense)` and outputs `(new_sp_values)`.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the operator</span>
<span class="sd">    op_name: name of the operator being overridden</span>
<span class="sd">    clazz_object: class to override for.  Either `Tensor` or `SparseTensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">binary_op_wrapper</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor_v2</span><span class="p">(</span>
              <span class="n">y</span><span class="p">,</span> <span class="n">dtype_hint</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
          <span class="c1"># If the RHS is not a tensor, it might be a tensor aware object</span>
          <span class="c1"># that can implement the operator with knowledge of itself</span>
          <span class="c1"># and the tensor.</span>
          <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="s2">&quot;__r</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span>
      <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">binary_op_wrapper_sparse</span><span class="p">(</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="p">[</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sp_x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span>
          <span class="n">sp_x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
          <span class="n">func</span><span class="p">(</span><span class="n">sp_x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">sp_x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">sp_x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">),</span>
          <span class="n">sp_x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">r_binary_op_wrapper</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="c1"># Propagate func.__doc__ to the wrappers</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">binary_op_wrapper</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
  <span class="n">r_binary_op_wrapper</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
  <span class="n">binary_op_wrapper_sparse</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>

  <span class="k">if</span> <span class="n">clazz_object</span> <span class="ow">is</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">clazz_object</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">binary_op_wrapper</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">binary_op_wrapper</span>
    <span class="n">clazz_object</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__r</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">r_binary_op_wrapper</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">r_binary_op_wrapper</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">clazz_object</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">,</span>
                                    <span class="n">binary_op_wrapper_sparse</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">binary_op_wrapper_sparse</span>


<span class="c1"># Conversion table for __truediv__.  None entries mean no conversion required.</span>
<span class="n">_TRUEDIV_TABLE</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">uint16</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>


<span class="c1"># NOTE: the support of &quot;sparse (true)div dense&quot; is currently not baked in into</span>
<span class="c1"># &quot;tf.(true_)div()&quot;.  Until such an API decision is made, the supported usage is</span>
<span class="c1"># to explicitly use the &quot;/&quot; operator to invoke either truediv or div.</span>
<span class="k">def</span> <span class="nf">_sparse_dense_truediv</span><span class="p">(</span><span class="n">sp_indices</span><span class="p">,</span> <span class="n">sp_values</span><span class="p">,</span> <span class="n">sp_shape</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Internal helper function for &#39;sp_t / dense_t&#39;.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">,</span>
                      <span class="p">[</span><span class="n">sp_indices</span><span class="p">,</span> <span class="n">sp_values</span><span class="p">,</span> <span class="n">sp_shape</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">sp_values</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sp_values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sp_values&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">sp_values</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">_TRUEDIV_TABLE</span><span class="p">[</span><span class="n">x_dtype</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid dtype </span><span class="si">%r</span><span class="s2"> in __truediv__&quot;</span> <span class="o">%</span> <span class="n">x_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">sp_values</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">sp_values</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_div</span><span class="p">(</span>
        <span class="n">sp_indices</span><span class="p">,</span> <span class="n">sp_values</span><span class="p">,</span> <span class="n">sp_shape</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_truediv_python3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">_TRUEDIV_TABLE</span><span class="p">[</span><span class="n">x_dtype</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid dtype </span><span class="si">%r</span><span class="s2"> in __truediv__&quot;</span> <span class="o">%</span> <span class="n">x_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_div_python2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divide two values using Python 2 semantics.</span>

<span class="sd">  Used for Tensor.__div__.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of real numeric type.</span>
<span class="sd">    y: `Tensor` denominator of real numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `x / y` returns the quotient of x and y.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;div&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">or</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="truediv"><a class="viewcode-back" href="../../../../index.html#tensorflow.truediv">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.truediv&quot;</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">truediv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divides x / y elementwise (using Python 3 division operator semantics).</span>

<span class="sd">  NOTE: Prefer using the Tensor operator or tf.divide which obey Python</span>
<span class="sd">  division operator semantics.</span>

<span class="sd">  This function forces Python 3 division operator semantics where all integer</span>
<span class="sd">  arguments are cast to floating types first.   This op is generated by normal</span>
<span class="sd">  `x / y` division in Python 3 and in Python 2.7 with</span>
<span class="sd">  `from __future__ import division`.  If you want integer division that rounds</span>
<span class="sd">  down, use `x // y` or `tf.math.floordiv`.</span>

<span class="sd">  `x` and `y` must have the same numeric type.  If the inputs are floating</span>
<span class="sd">  point, the output will have the same type.  If the inputs are integral, the</span>
<span class="sd">  inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`</span>
<span class="sd">  and `int64` (matching the behavior of Numpy).</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of numeric type.</span>
<span class="sd">    y: `Tensor` denominator of numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `x / y` evaluated in floating point.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` and `y` have different dtypes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_truediv_python3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Deprecated in favor of operator or tf.math.divide.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;div&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divides x / y elementwise (using Python 2 division operator semantics).</span>

<span class="sd">  NOTE: Prefer using the Tensor division operator or tf.divide which obey Python</span>
<span class="sd">  3 division operator semantics.</span>

<span class="sd">  This function divides `x` and `y`, forcing Python 2 semantics. That is, if `x`</span>
<span class="sd">  and `y` are both integers then the result will be an integer. This is in</span>
<span class="sd">  contrast to Python 3, where division with `/` is always a float while division</span>
<span class="sd">  with `//` is always an integer.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of real numeric type.</span>
<span class="sd">    y: `Tensor` denominator of real numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `x / y` returns the quotient of x and y.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_div_python2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.divide_no_nan&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.divide_no_nan&quot;</span><span class="p">,</span> <span class="s2">&quot;div_no_nan&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;div_no_nan&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">div_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes a safe divide which returns 0 if the y is zero.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`.</span>
<span class="sd">    y: A `Tensor` whose dtype is compatible with `x`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The element-wise value of the x divided by y.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;div_no_nan&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">div_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.multiply_no_nan&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">multiply_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the product of x and y and returns 0 if the y is zero, even if x is NaN or infinite.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`.</span>
<span class="sd">    y: A `Tensor` whose dtype is compatible with `x`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The element-wise value of the x times y.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;multiply_no_nan&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># TODO(aselle): This should be removed</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_mod</span>


<span class="c1"># TODO(aselle): Deprecate this once all internal functionality uses</span>
<span class="c1"># tf.truncatediv</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.floordiv&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.floordiv&quot;</span><span class="p">,</span> <span class="s2">&quot;floordiv&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;floordiv&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">floordiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divides `x / y` elementwise, rounding toward the most negative integer.</span>

<span class="sd">  The same as `tf.compat.v1.div(x,y)` for integers, but uses</span>
<span class="sd">  `tf.floor(tf.compat.v1.div(x,y))` for</span>
<span class="sd">  floating point arguments so that the result is always an integer (though</span>
<span class="sd">  possibly an integer represented as floating point).  This op is generated by</span>
<span class="sd">  `x // y` floor division in Python 3 and in Python 2.7 with</span>
<span class="sd">  `from __future__ import division`.</span>

<span class="sd">  `x` and `y` must have the same type, and the result will have the same type</span>
<span class="sd">  as well.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of real numeric type.</span>
<span class="sd">    y: `Tensor` denominator of real numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `x / y` rounded down.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If the inputs are complex.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;floordiv&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">realdiv</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real_div</span>
<span class="n">truncatediv</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">truncate_div</span>
<span class="c1"># TODO(aselle): Rename this to floordiv when we can.</span>
<span class="n">floor_div</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_div</span>
<span class="n">truncatemod</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">truncate_mod</span>
<span class="n">floormod</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_mod</span>


<span class="k">def</span> <span class="nf">_add_dispatch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Dispatches to add for strings and add_v2 for all other types.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">string</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add_v2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_mul_dispatch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Dispatches cwise mul for &quot;Dense*Dense&quot; and &quot;Dense*Sparse&quot;.&quot;&quot;&quot;</span>
  <span class="n">is_tensor_y</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">is_tensor_y</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>  <span class="c1"># Case: Dense * Sparse.</span>
    <span class="n">new_vals</span> <span class="o">=</span> <span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_mul</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                                     <span class="n">y</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">new_vals</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>


<span class="c1"># NOTE(aselle): When integer division is added for sparse_dense_cwise,</span>
<span class="c1"># div, truediv, and floordiv should be delegated appropriately for</span>
<span class="c1"># Python semantics, analogous to dense cwise tensor operations.</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_div</span><span class="p">,</span> <span class="s2">&quot;div&quot;</span><span class="p">,</span>
                              <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_sparse_dense_truediv</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">,</span>
                              <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_mul</span><span class="p">,</span> <span class="s2">&quot;mul&quot;</span><span class="p">,</span>
                              <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>

<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_add_dispatch</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">,</span> <span class="s2">&quot;sub&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_mul_dispatch</span><span class="p">,</span> <span class="s2">&quot;mul&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_div_python2</span><span class="p">,</span> <span class="s2">&quot;div&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_truediv_python3</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">floordiv</span><span class="p">,</span> <span class="s2">&quot;floordiv&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_mod</span><span class="p">,</span> <span class="s2">&quot;mod&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="nb">pow</span><span class="p">,</span> <span class="s2">&quot;pow&quot;</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.logical_xor&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.logical_xor&quot;</span><span class="p">,</span> <span class="s2">&quot;logical_xor&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;logical_xor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">logical_xor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;LogicalXor&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Logical XOR function.</span>

<span class="sd">  x ^ y = (x | y) &amp; ~(x &amp; y)</span>

<span class="sd">  The operation works for the following input types:</span>

<span class="sd">  - Two single elements of type `bool`</span>
<span class="sd">  - One `tf.Tensor` of type `bool` and one single `bool`, where the result will</span>
<span class="sd">    be calculated by applying logical XOR with the single element to each</span>
<span class="sd">    element in the larger Tensor.</span>
<span class="sd">  - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,</span>
<span class="sd">    the result will be the element-wise logical XOR of the two input tensors.</span>

<span class="sd">  Usage:</span>

<span class="sd">  &gt;&gt;&gt; a = tf.constant([True])</span>
<span class="sd">  &gt;&gt;&gt; b = tf.constant([False])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.logical_xor(a, b)</span>
<span class="sd">  &lt;tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])&gt;</span>

<span class="sd">  &gt;&gt;&gt; c = tf.constant([True])</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([False, True, True, False])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.logical_xor(c, x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])&gt;</span>

<span class="sd">  &gt;&gt;&gt; y = tf.constant([False, False, True, True])</span>
<span class="sd">  &gt;&gt;&gt; z = tf.constant([False, True, False, True])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.logical_xor(y, z)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">      x: A `tf.Tensor` type bool.</span>
<span class="sd">      y: A `tf.Tensor` of type bool.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` of type bool with the same size as that of x or y.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(alemi) Make this a cwise op if people end up relying on it.</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="logical_and"><a class="viewcode-back" href="../../../../index.html#tensorflow.logical_and">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.logical_and&quot;</span><span class="p">,</span> <span class="s2">&quot;logical_and&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">logical_and</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Logical AND function.</span>

<span class="sd">  The operation works for the following input types:</span>

<span class="sd">  - Two single elements of type `bool`</span>
<span class="sd">  - One `tf.Tensor` of type `bool` and one single `bool`, where the result will</span>
<span class="sd">    be calculated by applying logical AND with the single element to each</span>
<span class="sd">    element in the larger Tensor.</span>
<span class="sd">  - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,</span>
<span class="sd">    the result will be the element-wise logical AND of the two input tensors.</span>

<span class="sd">  Usage:</span>

<span class="sd">  &gt;&gt;&gt; a = tf.constant([True])</span>
<span class="sd">  &gt;&gt;&gt; b = tf.constant([False])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.logical_and(a, b)</span>
<span class="sd">  &lt;tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])&gt;</span>

<span class="sd">  &gt;&gt;&gt; c = tf.constant([True])</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([False, True, True, False])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.logical_and(c, x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])&gt;</span>

<span class="sd">  &gt;&gt;&gt; y = tf.constant([False, False, True, True])</span>
<span class="sd">  &gt;&gt;&gt; z = tf.constant([False, True, False, True])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.logical_and(y, z)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">      x: A `tf.Tensor` type bool.</span>
<span class="sd">      y: A `tf.Tensor` of type bool.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` of type bool with the same size as that of x or y.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">,</span> <span class="s2">&quot;and&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_or</span><span class="p">,</span> <span class="s2">&quot;or&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">logical_xor</span><span class="p">,</span> <span class="s2">&quot;xor&quot;</span><span class="p">)</span>

<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__lt__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">less</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__le__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">less_equal</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__gt__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">greater</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__ge__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">)</span>


<div class="viewcode-block" id="equal"><a class="viewcode-back" href="../../../../index.html#tensorflow.equal">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.equal&quot;</span><span class="p">,</span> <span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the truth value of (x == y) element-wise.</span>

<span class="sd">  Performs a [broadcast](</span>
<span class="sd">  https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) with the</span>
<span class="sd">  arguments and then an element-wise equality comparison, returning a Tensor of</span>
<span class="sd">  boolean values.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant(2)</span>
<span class="sd">  &gt;&gt;&gt; tf.math.equal(x, y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])&gt;</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([2, 4])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.equal(x, y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.</span>
<span class="sd">    y: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` of type bool with the same size as that of x or y.</span>

<span class="sd">  Raises:</span>
<span class="sd">    `tf.errors.InvalidArgumentError`: If shapes of arguments are incompatible</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="not_equal"><a class="viewcode-back" href="../../../../index.html#tensorflow.not_equal">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.not_equal&quot;</span><span class="p">,</span> <span class="s2">&quot;not_equal&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">not_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the truth value of (x != y) element-wise.</span>

<span class="sd">  Performs a [broadcast](</span>
<span class="sd">  https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) with the</span>
<span class="sd">  arguments and then an element-wise inequality comparison, returning a Tensor</span>
<span class="sd">  of boolean values.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant(2)</span>
<span class="sd">  &gt;&gt;&gt; tf.math.not_equal(x, y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])&gt;</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([2, 4])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.not_equal(x, y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  False])&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.</span>
<span class="sd">    y: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` of type bool with the same size as that of x or y.</span>

<span class="sd">  Raises:</span>
<span class="sd">    `tf.errors.InvalidArgumentError`: If shapes of arguments are incompatible</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">tensor_equals</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compares two tensors element-wise for equality.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">other</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">False</span>
  <span class="n">g</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">()</span> <span class="ow">and</span>
      <span class="p">(</span><span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">g</span><span class="o">.</span><span class="n">building_function</span><span class="p">)):</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">incompatible_shape_error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># In legacy graph mode, tensor equality is object equality</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">other</span>


<span class="k">def</span> <span class="nf">tensor_not_equals</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compares two tensors element-wise for equality.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">other</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">incompatible_shape_error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># In legacy graph mode, tensor equality is object equality</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">other</span>


<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__eq__&quot;</span><span class="p">,</span> <span class="n">tensor_equals</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__ne__&quot;</span><span class="p">,</span> <span class="n">tensor_not_equals</span><span class="p">)</span>


<div class="viewcode-block" id="range"><a class="viewcode-back" href="../../../../index.html#tensorflow.range">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;range&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;range&quot;</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Creates a sequence of numbers.</span>

<span class="sd">  Creates a sequence of numbers that begins at `start` and extends by</span>
<span class="sd">  increments of `delta` up to but not including `limit`.</span>

<span class="sd">  The dtype of the resulting tensor is inferred from the inputs unless</span>
<span class="sd">  it is provided explicitly.</span>

<span class="sd">  Like the Python builtin `range`, `start` defaults to 0, so that</span>
<span class="sd">  `range(n) = range(0, n)`.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; start = 3</span>
<span class="sd">  &gt;&gt;&gt; limit = 18</span>
<span class="sd">  &gt;&gt;&gt; delta = 3</span>
<span class="sd">  &gt;&gt;&gt; tf.range(start, limit, delta)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5,), dtype=int32,</span>
<span class="sd">  numpy=array([ 3,  6,  9, 12, 15], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; start = 3</span>
<span class="sd">  &gt;&gt;&gt; limit = 1</span>
<span class="sd">  &gt;&gt;&gt; delta = -0.5</span>
<span class="sd">  &gt;&gt;&gt; tf.range(start, limit, delta)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=float32,</span>
<span class="sd">  numpy=array([3. , 2.5, 2. , 1.5], dtype=float32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; limit = 5</span>
<span class="sd">  &gt;&gt;&gt; tf.range(limit)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5,), dtype=int32,</span>
<span class="sd">  numpy=array([0, 1, 2, 3, 4], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    start: A 0-D `Tensor` (scalar). Acts as first entry in the range if `limit`</span>
<span class="sd">      is not None; otherwise, acts as range limit and first entry defaults to 0.</span>
<span class="sd">    limit: A 0-D `Tensor` (scalar). Upper limit of sequence, exclusive. If None,</span>
<span class="sd">      defaults to the value of `start` while the first entry of the range</span>
<span class="sd">      defaults to 0.</span>
<span class="sd">    delta: A 0-D `Tensor` (scalar). Number that increments `start`. Defaults to</span>
<span class="sd">      1.</span>
<span class="sd">    dtype: The type of the elements of the resulting tensor.</span>
<span class="sd">    name: A name for the operation. Defaults to &quot;range&quot;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An 1-D `Tensor` of type `dtype`.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.arange</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">limit</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">start</span><span class="p">,</span> <span class="n">limit</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">start</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Range&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">start</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;start&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">limit</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;limit&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">delta</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;delta&quot;</span><span class="p">)</span>

    <span class="c1"># infer dtype if not explicitly provided</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">dtype_hierarchy</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span>
      <span class="p">]</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">dtype_hierarchy</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="p">[</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">])</span>
      <span class="n">inferred_dtype</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">arg</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="p">[</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">]],</span>
                           <span class="n">key</span><span class="o">=</span><span class="n">dtype_hierarchy</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">inferred_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="c1"># Always try perform a cast even start/limit/delta are already tensors.</span>
    <span class="c1"># This will revole the case where start/limit/delta&#39;s original&#39;s dtype</span>
    <span class="c1"># is different from provided dtype.</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">inferred_dtype</span><span class="p">)</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">inferred_dtype</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">inferred_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_range_tensor_conversion_function</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">as_ref</span>
  <span class="k">return</span> <span class="nb">range</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">if</span> <span class="ow">not</span> <span class="n">six</span><span class="o">.</span><span class="n">PY2</span><span class="p">:</span>
  <span class="n">ops</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">(</span><span class="n">builtins</span><span class="o">.</span><span class="n">range</span><span class="p">,</span>
                                          <span class="n">_range_tensor_conversion_function</span><span class="p">)</span>

<span class="c1"># Reduction operations</span>
<span class="k">def</span> <span class="nf">_ReductionDims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;Returns range(0, rank(x)) if reduction_indices is None.&quot;&quot;&quot;</span>
  <span class="c1"># TODO(aselle): Remove this after deprecation</span>
  <span class="k">if</span> <span class="n">reduction_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t specify both axis&#39; and &#39;reduction_indices&#39;.&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">reduction_indices</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">axis</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Fast path: avoid creating Rank and Range ops if ndims is known.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">rank</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
      <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="k">elif</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span> <span class="ow">and</span>
          <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()):</span>
      <span class="n">rank</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>  <span class="c1"># sparse.dense_shape is 1-D.</span>
      <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

    <span class="c1"># Otherwise, we rely on Range and Rank to do the right thing at run-time.</span>
    <span class="k">return</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_has_fully_defined_shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns true if tensor has a fully defined shape.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_may_reduce_to_scalar</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set a reduction&#39;s output shape to be a scalar if we are certain.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_fully_defined_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">keepdims</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
      <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">output</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(())</span>
  <span class="k">return</span> <span class="n">output</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_sum&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_sum&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_sum_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the sum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 1, 1], [1, 1, 1]])</span>
<span class="sd">  tf.reduce_sum(x)  # 6</span>
<span class="sd">  tf.reduce_sum(x, 0)  # [2, 2, 2]</span>
<span class="sd">  tf.reduce_sum(x, 1)  # [3, 3]</span>
<span class="sd">  tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]</span>
<span class="sd">  tf.reduce_sum(x, [0, 1])  # 6</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to</span>
<span class="sd">  int64 while tensorflow returns the same dtype as the input.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_sum"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_sum">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_sum&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_sum&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_sum</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the sum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 1, 1], [1, 1, 1]])</span>
<span class="sd">  tf.reduce_sum(x)  # 6</span>
<span class="sd">  tf.reduce_sum(x, 0)  # [2, 2, 2]</span>
<span class="sd">  tf.reduce_sum(x, 1)  # [3, 3]</span>
<span class="sd">  tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]</span>
<span class="sd">  tf.reduce_sum(x, [0, 1])  # 6</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to</span>
<span class="sd">  int64 while tensorflow returns the same dtype as the input.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">return</span> <span class="n">reduce_sum_with_dims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                              <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span></div>


<span class="k">def</span> <span class="nf">reduce_sum_with_dims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                         <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_sum</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_euclidean_norm&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_euclidean_norm</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the Euclidean norm of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 2, 3], [1, 1, 1]]) # x.dtype is tf.int32</span>
<span class="sd">  tf.math.reduce_euclidean_norm(x)  # returns 4 as dtype is tf.int32</span>
<span class="sd">  y = tf.constant([[1, 2, 3], [1, 1, 1]], dtype = tf.float32)</span>
<span class="sd">  tf.math.reduce_euclidean_norm(y)  # returns 4.1231055 which is sqrt(17)</span>
<span class="sd">  tf.math.reduce_euclidean_norm(y, 0)  # [sqrt(2), sqrt(5), sqrt(10)]</span>
<span class="sd">  tf.math.reduce_euclidean_norm(y, 1)  # [sqrt(14), sqrt(3)]</span>
<span class="sd">  tf.math.reduce_euclidean_norm(y, 1, keepdims=True)  # [[sqrt(14)], [sqrt(3)]]</span>
<span class="sd">  tf.math.reduce_euclidean_norm(y, [0, 1])  # sqrt(17)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">euclidean_norm</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.count_nonzero&quot;</span><span class="p">,</span> <span class="s2">&quot;count_nonzero&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;reduction_indices is deprecated, use axis instead&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reduction_indices&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">count_nonzero</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Computes number of nonzero elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  **NOTE** Floating point comparison to zero is done by exact floating point</span>
<span class="sd">  equality check.  Small values are **not** rounded to zero for purposes of</span>
<span class="sd">  the nonzero check.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0, 1, 0], [1, 1, 0]])</span>
<span class="sd">  tf.math.count_nonzero(x)  # 3</span>
<span class="sd">  tf.math.count_nonzero(x, 0)  # [1, 2, 0]</span>
<span class="sd">  tf.math.count_nonzero(x, 1)  # [1, 2]</span>
<span class="sd">  tf.math.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]</span>
<span class="sd">  tf.math.count_nonzero(x, [0, 1])  # 3</span>
<span class="sd">  ```</span>

<span class="sd">  **NOTE** Strings are compared against zero-length empty string `&quot;&quot;`. Any</span>
<span class="sd">  string with a size greater than zero is already considered as nonzero.</span>

<span class="sd">  For example:</span>
<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([&quot;&quot;, &quot;a&quot;, &quot;  &quot;, &quot;b&quot;, &quot;&quot;])</span>
<span class="sd">  tf.math.count_nonzero(x) # 3, with &quot;a&quot;, &quot;  &quot;, and &quot;b&quot; as nonzero strings.</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should be of numeric type, `bool`, or</span>
<span class="sd">      `string`.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    dtype: The output dtype; defaults to `tf.int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>
<span class="sd">    input: Overrides input_tensor. For compatibility.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor (number of nonzero values).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span>
                                                        <span class="s2">&quot;input_tensor&quot;</span><span class="p">,</span>
                                                        <span class="n">input_tensor</span><span class="p">)</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">count_nonzero_v2</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.count_nonzero&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">count_nonzero_v2</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes number of nonzero elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  **NOTE** Floating point comparison to zero is done by exact floating point</span>
<span class="sd">  equality check.  Small values are **not** rounded to zero for purposes of</span>
<span class="sd">  the nonzero check.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0, 1, 0], [1, 1, 0]])</span>
<span class="sd">  tf.math.count_nonzero(x)  # 3</span>
<span class="sd">  tf.math.count_nonzero(x, 0)  # [1, 2, 0]</span>
<span class="sd">  tf.math.count_nonzero(x, 1)  # [1, 2]</span>
<span class="sd">  tf.math.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]</span>
<span class="sd">  tf.math.count_nonzero(x, [0, 1])  # 3</span>
<span class="sd">  ```</span>

<span class="sd">  **NOTE** Strings are compared against zero-length empty string `&quot;&quot;`. Any</span>
<span class="sd">  string with a size greater than zero is already considered as nonzero.</span>

<span class="sd">  For example:</span>
<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([&quot;&quot;, &quot;a&quot;, &quot;  &quot;, &quot;b&quot;, &quot;&quot;])</span>
<span class="sd">  tf.math.count_nonzero(x) # 3, with &quot;a&quot;, &quot;  &quot;, and &quot;b&quot; as nonzero strings.</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: The tensor to reduce. Should be of numeric type, `bool`, or `string`.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input), rank(input))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    dtype: The output dtype; defaults to `tf.int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor (number of nonzero values).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;count_nonzero&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]):</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input&quot;</span><span class="p">)</span>
    <span class="c1"># A scalar of &#39;zero&#39; is enough as `not_equal` will broadcast.</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span>
        <span class="n">reduce_sum</span><span class="p">(</span>
            <span class="c1"># int64 reduction happens on GPU</span>
            <span class="n">cast</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">zero</span><span class="p">),</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
            <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_mean&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">reduce_mean_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                   <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the mean of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis` by computing the</span>
<span class="sd">  mean of elements across the dimensions in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a tensor with a single</span>
<span class="sd">  element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([[1., 1.], [2., 2.]])</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.5&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x, 0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x, 1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.mean</span>

<span class="sd">  Please note that `np.mean` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,</span>
<span class="sd">  for example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([1, 0, 1, 0])</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([1., 0., 1., 0.])</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5&gt;</span>

<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_mean"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_mean">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_mean&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the mean of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis` by computing the</span>
<span class="sd">  mean of elements across the dimensions in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions are retained</span>
<span class="sd">  with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a tensor with a single</span>
<span class="sd">  element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([[1., 1.], [2., 2.]])</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.5&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x, 0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x, 1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.mean</span>

<span class="sd">  Please note that `np.mean` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,</span>
<span class="sd">  for example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([1, 0, 1, 0])</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([1., 0., 1., 0.])</span>
<span class="sd">  &gt;&gt;&gt; tf.reduce_mean(y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5&gt;</span>

<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_variance&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_variance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the variance of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1., 2.], [3., 4.]])</span>
<span class="sd">  tf.reduce_variance(x)  # 1.25</span>
<span class="sd">  tf.reduce_variance(x, 0)  # [1., 1.]</span>
<span class="sd">  tf.reduce_variance(x, 1)  # [0.25,  0.25]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name scope for the associated operations (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.var</span>

<span class="sd">  Please note that `np.var` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_variance` has an aggressive type inference from</span>
<span class="sd">  `input_tensor`,</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="k">if</span> <span class="n">name</span> <span class="k">else</span> <span class="s2">&quot;reduce_variance&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">squared_deviations</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">input_tensor</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">squared_deviations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_std&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_std</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the standard deviation of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1., 2.], [3., 4.]])</span>
<span class="sd">  tf.reduce_std(x)  # 1.1180339887498949</span>
<span class="sd">  tf.reduce_std(x, 0)  # [1., 1.]</span>
<span class="sd">  tf.reduce_std(x, 1)  # [0.5,  0.5]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name scope for the associated operations (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.std</span>

<span class="sd">  Please note that `np.std` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_std` has an aggressive type inference from `input_tensor`,</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="k">if</span> <span class="n">name</span> <span class="k">else</span> <span class="s2">&quot;reduce_std&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">reduce_variance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_prod"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_prod">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_prod&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_prod&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_prod</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the product of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.prod</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_prod&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_prod&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_prod_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                   <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the product of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.prod</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_min&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_min&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_min_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the minimum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.min</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_min</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_min"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_min">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_min&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_min&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_min</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the minimum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  For example:</span>
<span class="sd">    &gt;&gt;&gt; a = tf.constant([[1, 2], [3, 4]])</span>
<span class="sd">    &gt;&gt;&gt; tf.reduce_min(a)</span>
<span class="sd">    &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.min</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_min</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_max&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_max&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_max_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the maximum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.max</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_max"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_max">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_max&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_max&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_max</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the maximum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Usage example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([5, 1, 2, 4])</span>
<span class="sd">  &gt;&gt;&gt; print(tf.reduce_max(x))</span>
<span class="sd">  tf.Tensor(5, shape=(), dtype=int32)</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([-5, -1, -2, -4])</span>
<span class="sd">  &gt;&gt;&gt; print(tf.reduce_max(x))</span>
<span class="sd">  tf.Tensor(-1, shape=(), dtype=int32)</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([4, float(&#39;nan&#39;)])</span>
<span class="sd">  &gt;&gt;&gt; print(tf.reduce_max(x))</span>
<span class="sd">  tf.Tensor(4.0, shape=(), dtype=float32)</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([float(&#39;nan&#39;), float(&#39;nan&#39;)])</span>
<span class="sd">  &gt;&gt;&gt; print(tf.reduce_max(x))</span>
<span class="sd">  tf.Tensor(-inf, shape=(), dtype=float32)</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([float(&#39;-inf&#39;), float(&#39;inf&#39;)])</span>
<span class="sd">  &gt;&gt;&gt; print(tf.reduce_max(x))</span>
<span class="sd">  tf.Tensor(inf, shape=(), dtype=float32)</span>

<span class="sd">  See the numpy docs for `np.amax` and `np.nanmax` behavior.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">reduce_max_with_dims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                              <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span></div>


<span class="k">def</span> <span class="nf">reduce_max_with_dims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                         <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_max</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_all&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_all&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_all_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical and&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_all(x)  # False</span>
<span class="sd">  tf.reduce_all(x, 0)  # [False, False]</span>
<span class="sd">  tf.reduce_all(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.all</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_all</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_all"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_all">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;reduce_all&quot;</span><span class="p">,</span> <span class="s2">&quot;math.reduce_all&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_all</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical and&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_all(x)  # False</span>
<span class="sd">  tf.reduce_all(x, 0)  # [False, False]</span>
<span class="sd">  tf.reduce_all(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.all</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_all</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_any&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_any&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_any_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical or&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_any(x)  # True</span>
<span class="sd">  tf.reduce_any(x, 0)  # [True, True]</span>
<span class="sd">  tf.reduce_any(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.any</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_any</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_any"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_any">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_any&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_any&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_any</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical or&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_any(x)  # True</span>
<span class="sd">  tf.reduce_any(x, 0)  # [True, True]</span>
<span class="sd">  tf.reduce_any(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.any</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_any</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_logsumexp&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_logsumexp&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_logsumexp_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                        <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes log(sum(exp(elements across dimensions of a tensor))).</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  This function is more numerically stable than log(sum(exp(input))). It avoids</span>
<span class="sd">  overflows caused by taking the exp of large inputs and underflows caused by</span>
<span class="sd">  taking the log of small inputs.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0., 0., 0.], [0., 0., 0.]])</span>
<span class="sd">  tf.reduce_logsumexp(x)  # log(6)</span>
<span class="sd">  tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]</span>
<span class="sd">  tf.reduce_logsumexp(x, [0, 1])  # log(6)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                                <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span>
                                                <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_logsumexp"><a class="viewcode-back" href="../../../../index.html#tensorflow.reduce_logsumexp">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_logsumexp&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_logsumexp&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">reduce_logsumexp</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes log(sum(exp(elements across dimensions of a tensor))).</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  This function is more numerically stable than log(sum(exp(input))). It avoids</span>
<span class="sd">  overflows caused by taking the exp of large inputs and underflows caused by</span>
<span class="sd">  taking the log of small inputs.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0., 0., 0.], [0., 0., 0.]])</span>
<span class="sd">  tf.reduce_logsumexp(x)  # log(6)</span>
<span class="sd">  tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]</span>
<span class="sd">  tf.reduce_logsumexp(x, [0, 1])  # log(6)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;ReduceLogSumExp&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_tensor</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">reduce_dim</span> <span class="o">=</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">raw_max</span> <span class="o">=</span> <span class="n">reduce_max_with_dims</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="n">reduce_dim</span><span class="p">)</span>
    <span class="n">my_max</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span>
        <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
            <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">is_finite</span><span class="p">(</span><span class="n">raw_max</span><span class="p">),</span> <span class="n">raw_max</span><span class="p">,</span>
            <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">raw_max</span><span class="p">)))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="n">reduce_sum_with_dims</span><span class="p">(</span>
            <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">my_max</span><span class="p">)),</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
            <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
            <span class="n">dims</span><span class="o">=</span><span class="n">reduce_dim</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdims</span><span class="p">:</span>
      <span class="n">my_max</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">my_max</span><span class="p">,</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">my_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.trace&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;linalg.trace&quot;</span><span class="p">,</span> <span class="s2">&quot;trace&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;trace&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the trace of a tensor `x`.</span>

<span class="sd">  `trace(x)` returns the sum along the main diagonal of each inner-most matrix</span>
<span class="sd">  in x. If x is of rank `k` with shape `[I, J, K, ..., L, M, N]`, then output</span>
<span class="sd">  is a tensor of rank `k-2` with dimensions `[I, J, K, ..., L]` where</span>

<span class="sd">  `output[i, j, k, ..., l] = trace(x[i, j, i, ..., l, :, :])`</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 2], [3, 4]])</span>
<span class="sd">  tf.linalg.trace(x)  # 5</span>

<span class="sd">  x = tf.constant([[1, 2, 3],</span>
<span class="sd">                   [4, 5, 6],</span>
<span class="sd">                   [7, 8, 9]])</span>
<span class="sd">  tf.linalg.trace(x)  # 15</span>

<span class="sd">  x = tf.constant([[[1, 2, 3],</span>
<span class="sd">                    [4, 5, 6],</span>
<span class="sd">                    [7, 8, 9]],</span>
<span class="sd">                   [[-1, -2, -3],</span>
<span class="sd">                    [-4, -5, -6],</span>
<span class="sd">                    [-7, -8, -9]]])</span>
<span class="sd">  tf.linalg.trace(x)  # [15, -15]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: tensor.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The trace of input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Trace&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">matrix_diag_part</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="matmul"><a class="viewcode-back" href="../../../../index.html#tensorflow.matmul">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.matmul&quot;</span><span class="p">,</span> <span class="s2">&quot;matmul&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span>
           <span class="n">b</span><span class="p">,</span>
           <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">adjoint_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">adjoint_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">a_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">b_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiplies matrix `a` by matrix `b`, producing `a` * `b`.</span>

<span class="sd">  The inputs must, following any transpositions, be tensors of rank &gt;= 2</span>
<span class="sd">  where the inner 2 dimensions specify valid matrix multiplication dimensions,</span>
<span class="sd">  and any further outer dimensions specify matching batch size.</span>

<span class="sd">  Both matrices must be of the same type. The supported types are:</span>
<span class="sd">  `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.</span>

<span class="sd">  Either matrix can be transposed or adjointed (conjugated and transposed) on</span>
<span class="sd">  the fly by setting one of the corresponding flag to `True`. These are `False`</span>
<span class="sd">  by default.</span>

<span class="sd">  If one or both of the matrices contain a lot of zeros, a more efficient</span>
<span class="sd">  multiplication algorithm can be used by setting the corresponding</span>
<span class="sd">  `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.</span>
<span class="sd">  This optimization is only available for plain matrices (rank-2 tensors) with</span>
<span class="sd">  datatypes `bfloat16` or `float32`.</span>

<span class="sd">  A simple 2-D tensor matrix multiplication:</span>

<span class="sd">  &gt;&gt;&gt; a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])</span>
<span class="sd">  &gt;&gt;&gt; a  # 2-D tensor</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 2, 3],</span>
<span class="sd">         [4, 5, 6]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])</span>
<span class="sd">  &gt;&gt;&gt; b  # 2-D tensor</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[ 7,  8],</span>
<span class="sd">         [ 9, 10],</span>
<span class="sd">         [11, 12]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; c = tf.matmul(a, b)</span>
<span class="sd">  &gt;&gt;&gt; c  # `a` * `b`</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[ 58,  64],</span>
<span class="sd">         [139, 154]], dtype=int32)&gt;</span>

<span class="sd">  A batch matrix multiplication with batch shape [2]:</span>

<span class="sd">  &gt;&gt;&gt; a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])</span>
<span class="sd">  &gt;&gt;&gt; a  # 3-D tensor</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=</span>
<span class="sd">  array([[[ 1,  2,  3],</span>
<span class="sd">          [ 4,  5,  6]],</span>
<span class="sd">         [[ 7,  8,  9],</span>
<span class="sd">          [10, 11, 12]]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])</span>
<span class="sd">  &gt;&gt;&gt; b  # 3-D tensor</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[[13, 14],</span>
<span class="sd">          [15, 16],</span>
<span class="sd">          [17, 18]],</span>
<span class="sd">         [[19, 20],</span>
<span class="sd">          [21, 22],</span>
<span class="sd">          [23, 24]]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; c = tf.matmul(a, b)</span>
<span class="sd">  &gt;&gt;&gt; c  # `a` * `b`</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[[ 94, 100],</span>
<span class="sd">          [229, 244]],</span>
<span class="sd">         [[508, 532],</span>
<span class="sd">          [697, 730]]], dtype=int32)&gt;</span>

<span class="sd">  Since python &gt;= 3.5 the @ operator is supported</span>
<span class="sd">  (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,</span>
<span class="sd">  it simply calls the `tf.matmul()` function, so the following lines are</span>
<span class="sd">  equivalent:</span>

<span class="sd">  &gt;&gt;&gt; d = a @ b @ [[10], [11]]</span>
<span class="sd">  &gt;&gt;&gt; d = tf.matmul(tf.matmul(a, b), [[10], [11]])</span>

<span class="sd">  Args:</span>
<span class="sd">    a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,</span>
<span class="sd">      `complex64`, `complex128` and rank &gt; 1.</span>
<span class="sd">    b: `tf.Tensor` with same type and rank as `a`.</span>
<span class="sd">    transpose_a: If `True`, `a` is transposed before multiplication.</span>
<span class="sd">    transpose_b: If `True`, `b` is transposed before multiplication.</span>
<span class="sd">    adjoint_a: If `True`, `a` is conjugated and transposed before</span>
<span class="sd">      multiplication.</span>
<span class="sd">    adjoint_b: If `True`, `b` is conjugated and transposed before</span>
<span class="sd">      multiplication.</span>
<span class="sd">    a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this</span>
<span class="sd">      **does not support `tf.sparse.SparseTensor`**, it just makes optimizations</span>
<span class="sd">      that assume most values in `a` are zero.</span>
<span class="sd">      See `tf.sparse.sparse_dense_matmul`</span>
<span class="sd">      for some support for `tf.SparseTensor` multiplication.</span>
<span class="sd">    b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this</span>
<span class="sd">      **does not support `tf.sparse.SparseTensor`**, it just makes optimizations</span>
<span class="sd">      that assume most values in `a` are zero.</span>
<span class="sd">      See `tf.sparse.sparse_dense_matmul`</span>
<span class="sd">      for some support for `tf.SparseTensor` multiplication.</span>
<span class="sd">    name: Name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix</span>
<span class="sd">    is the product of the corresponding matrices in `a` and `b`, e.g. if all</span>
<span class="sd">    transpose or adjoint attributes are `False`:</span>

<span class="sd">    `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,</span>
<span class="sd">    for all indices `i`, `j`.</span>

<span class="sd">    Note: This is matrix product, not element-wise product.</span>


<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and</span>
<span class="sd">      `adjoint_b` are both set to `True`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;MatMul&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">transpose_a</span> <span class="ow">and</span> <span class="n">adjoint_a</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only one of transpose_a and adjoint_a can be True.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transpose_b</span> <span class="ow">and</span> <span class="n">adjoint_b</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only one of transpose_b and adjoint_b can be True.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">,</span> <span class="n">_resource_variable_type</span><span class="p">)):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">,</span> <span class="n">_resource_variable_type</span><span class="p">)):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>

    <span class="c1"># TODO(apassos) remove _shape_tuple here when it is not needed.</span>
    <span class="n">a_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">b_shape</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="n">output_may_have_non_empty_batch_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">a_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">or</span>
        <span class="p">(</span><span class="n">b_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">a_is_sparse</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">b_is_sparse</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output_may_have_non_empty_batch_shape</span><span class="p">:</span>
      <span class="c1"># BatchMatmul does not support transpose, so we conjugate the matrix and</span>
      <span class="c1"># use adjoint instead. Conj() is a noop for real matrices.</span>
      <span class="k">if</span> <span class="n">transpose_a</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">adjoint_a</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">if</span> <span class="n">transpose_b</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
        <span class="n">adjoint_b</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">batch_mat_mul_v2</span><span class="p">(</span>
          <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">adj_x</span><span class="o">=</span><span class="n">adjoint_a</span><span class="p">,</span> <span class="n">adj_y</span><span class="o">=</span><span class="n">adjoint_b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># Neither matmul nor sparse_matmul support adjoint, so we conjugate</span>
    <span class="c1"># the matrix and use transpose instead. Conj() is a noop for real</span>
    <span class="c1"># matrices.</span>
    <span class="k">if</span> <span class="n">adjoint_a</span><span class="p">:</span>
      <span class="n">a</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="n">transpose_a</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">adjoint_b</span><span class="p">:</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
      <span class="n">transpose_b</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">use_sparse_matmul</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">a_is_sparse</span> <span class="ow">or</span> <span class="n">b_is_sparse</span><span class="p">:</span>
      <span class="n">sparse_matmul_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
      <span class="n">use_sparse_matmul</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">sparse_matmul_types</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">sparse_matmul_types</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">or</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="c1"># matmul currently doesn&#39;t handle mixed-precision inputs.</span>
      <span class="n">use_sparse_matmul</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">use_sparse_matmul</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">sparse_matmul</span><span class="p">(</span>
          <span class="n">a</span><span class="p">,</span>
          <span class="n">b</span><span class="p">,</span>
          <span class="n">transpose_a</span><span class="o">=</span><span class="n">transpose_a</span><span class="p">,</span>
          <span class="n">transpose_b</span><span class="o">=</span><span class="n">transpose_b</span><span class="p">,</span>
          <span class="n">a_is_sparse</span><span class="o">=</span><span class="n">a_is_sparse</span><span class="p">,</span>
          <span class="n">b_is_sparse</span><span class="o">=</span><span class="n">b_is_sparse</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="c1"># sparse_matmul always returns float32, even with</span>
      <span class="c1"># bfloat16 inputs. This prevents us from configuring bfloat16 training.</span>
      <span class="c1"># casting to bfloat16 also matches non-sparse matmul behavior better.</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">ret</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mat_mul</span><span class="p">(</span>
          <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="n">transpose_a</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="n">transpose_b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.matvec&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matvec</span><span class="p">(</span><span class="n">a</span><span class="p">,</span>
           <span class="n">b</span><span class="p">,</span>
           <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">adjoint_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">a_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">b_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiplies matrix `a` by vector `b`, producing `a` * `b`.</span>

<span class="sd">  The matrix `a` must, following any transpositions, be a tensor of rank &gt;= 2,</span>
<span class="sd">  with `shape(a)[-1] == shape(b)[-1]`, and `shape(a)[:-2]` able to broadcast</span>
<span class="sd">  with `shape(b)[:-1]`.</span>

<span class="sd">  Both `a` and `b` must be of the same type. The supported types are:</span>
<span class="sd">  `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.</span>

<span class="sd">  Matrix `a` can be transposed or adjointed (conjugated and transposed) on</span>
<span class="sd">  the fly by setting one of the corresponding flag to `True`. These are `False`</span>
<span class="sd">  by default.</span>

<span class="sd">  If one or both of the inputs contain a lot of zeros, a more efficient</span>
<span class="sd">  multiplication algorithm can be used by setting the corresponding</span>
<span class="sd">  `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.</span>
<span class="sd">  This optimization is only available for plain matrices/vectors (rank-2/1</span>
<span class="sd">  tensors) with datatypes `bfloat16` or `float32`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  # 2-D tensor `a`</span>
<span class="sd">  # [[1, 2, 3],</span>
<span class="sd">  #  [4, 5, 6]]</span>
<span class="sd">  a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])</span>

<span class="sd">  # 1-D tensor `b`</span>
<span class="sd">  # [7, 9, 11]</span>
<span class="sd">  b = tf.constant([7, 9, 11], shape=[3])</span>

<span class="sd">  # `a` * `b`</span>
<span class="sd">  # [ 58,  64]</span>
<span class="sd">  c = tf.linalg.matvec(a, b)</span>


<span class="sd">  # 3-D tensor `a`</span>
<span class="sd">  # [[[ 1,  2,  3],</span>
<span class="sd">  #   [ 4,  5,  6]],</span>
<span class="sd">  #  [[ 7,  8,  9],</span>
<span class="sd">  #   [10, 11, 12]]]</span>
<span class="sd">  a = tf.constant(np.arange(1, 13, dtype=np.int32),</span>
<span class="sd">                  shape=[2, 2, 3])</span>

<span class="sd">  # 2-D tensor `b`</span>
<span class="sd">  # [[13, 14, 15],</span>
<span class="sd">  #  [16, 17, 18]]</span>
<span class="sd">  b = tf.constant(np.arange(13, 19, dtype=np.int32),</span>
<span class="sd">                  shape=[2, 3])</span>

<span class="sd">  # `a` * `b`</span>
<span class="sd">  # [[ 86, 212],</span>
<span class="sd">  #  [410, 563]]</span>
<span class="sd">  c = tf.linalg.matvec(a, b)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,</span>
<span class="sd">      `complex128` and rank &gt; 1.</span>
<span class="sd">    b: `Tensor` with same type as `a` and compatible dimensions.</span>
<span class="sd">    transpose_a: If `True`, `a` is transposed before multiplication.</span>
<span class="sd">    adjoint_a: If `True`, `a` is conjugated and transposed before</span>
<span class="sd">      multiplication.</span>
<span class="sd">    a_is_sparse: If `True`, `a` is treated as a sparse matrix.</span>
<span class="sd">    b_is_sparse: If `True`, `b` is treated as a sparse matrix.</span>
<span class="sd">    name: Name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of the same type as `a` and `b` where each inner-most vector is</span>
<span class="sd">    the product of the corresponding matrices in `a` and vectors in `b`, e.g. if</span>
<span class="sd">    all transpose or adjoint attributes are `False`:</span>

<span class="sd">    `output`[..., i] = sum_k (`a`[..., i, k] * `b`[..., k]), for all indices i.</span>

<span class="sd">    Note: This is matrix-vector product, not element-wise product.</span>


<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If transpose_a and adjoint_a are both set to True.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;MatVec&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span>
        <span class="n">a</span><span class="p">,</span>
        <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">transpose_a</span><span class="o">=</span><span class="n">transpose_a</span><span class="p">,</span>
        <span class="n">adjoint_a</span><span class="o">=</span><span class="n">adjoint_a</span><span class="p">,</span>
        <span class="n">a_is_sparse</span><span class="o">=</span><span class="n">a_is_sparse</span><span class="p">,</span>
        <span class="n">b_is_sparse</span><span class="o">=</span><span class="n">b_is_sparse</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">matmul</span><span class="p">,</span> <span class="s2">&quot;matmul&quot;</span><span class="p">)</span>

<span class="n">sparse_matmul</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use `tf.linalg.matmul` instead&quot;</span><span class="p">)(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_mat_mul</span><span class="p">)</span>
<span class="n">tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse_matmul&quot;</span><span class="p">])(</span><span class="n">sparse_matmul</span><span class="p">)</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterStatistics</span><span class="p">(</span><span class="s2">&quot;MatMul&quot;</span><span class="p">,</span> <span class="s2">&quot;flops&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_calc_mat_mul_flops</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculates the compute resources needed for MatMul.&quot;&quot;&quot;</span>
  <span class="n">transpose_a</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;transpose_a&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">b</span>
  <span class="n">a_shape</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">tensor_shape_from_node_def_name</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">a_shape</span><span class="o">.</span><span class="n">assert_is_fully_defined</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">transpose_a</span><span class="p">:</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">tensor_shape_from_node_def_name</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
  <span class="n">output_shape</span><span class="o">.</span><span class="n">assert_is_fully_defined</span><span class="p">()</span>
  <span class="n">output_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">output_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">OpStats</span><span class="p">(</span><span class="s2">&quot;flops&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">output_count</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterStatistics</span><span class="p">(</span><span class="s2">&quot;BatchMatMul&quot;</span><span class="p">,</span> <span class="s2">&quot;flops&quot;</span><span class="p">)</span>
<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterStatistics</span><span class="p">(</span><span class="s2">&quot;BatchMatMulV2&quot;</span><span class="p">,</span> <span class="s2">&quot;flops&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_calc_batch_mat_mul_flops</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculates the compute resources needed for BatchMatMul.&quot;&quot;&quot;</span>
  <span class="n">transpose_a</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;transpose_a&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">b</span>
  <span class="n">a_shape</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">tensor_shape_from_node_def_name</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">a_shape</span><span class="o">.</span><span class="n">assert_is_fully_defined</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">transpose_a</span><span class="p">:</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">tensor_shape_from_node_def_name</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
  <span class="n">output_shape</span><span class="o">.</span><span class="n">assert_is_fully_defined</span><span class="p">()</span>
  <span class="n">output_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">output_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">OpStats</span><span class="p">(</span><span class="s2">&quot;flops&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">output_count</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_as_indexed_slices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert &#39;x&#39; to IndexedSlices.</span>

<span class="sd">  Convert a dense Tensor to a block-sparse IndexedSlices.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: Either a Tensor object, or an IndexedSlices object.</span>
<span class="sd">    optimize: if true, attempt to optimize the conversion of &#39;x&#39;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An IndexedSlices object.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If &#39;x&#39; is not a Tensor or an IndexedSlices object.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(touts): op_scope</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Not a Tensor or IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="n">x_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_as_indexed_slices_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert all elements of &#39;inputs&#39; to IndexedSlices.</span>

<span class="sd">  Additionally, homogenize the types of all the indices to</span>
<span class="sd">  either int32 or int64.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: List containing either Tensor or IndexedSlices objects.</span>
<span class="sd">    optimize: if true, attempt to optimize the conversion of each input.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of IndexedSlices objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If &#39;inputs&#39; is not a list or a tuple.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected a list or tuple, not a </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">_as_indexed_slices</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
  <span class="n">with_int32_index</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">o</span><span class="o">.</span><span class="n">indices</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span>
  <span class="p">]</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">with_int32_index</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">with_int32_index</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">outputs</span>
  <span class="n">casted_outputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
      <span class="n">casted_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">cast</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
                            <span class="n">o</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">casted_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">casted_outputs</span>


<div class="viewcode-block" id="add_n"><a class="viewcode-back" href="../../../../index.html#tensorflow.add_n">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.add_n&quot;</span><span class="p">,</span> <span class="s2">&quot;add_n&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds all input tensors element-wise.</span>

<span class="sd">  `tf.math.add_n` performs the same operation as `tf.math.accumulate_n`, but it</span>
<span class="sd">  waits for all of its inputs to be ready before beginning to sum.</span>
<span class="sd">  This buffering can result in higher memory consumption when inputs are ready</span>
<span class="sd">  at different times, since the minimum temporary storage required is</span>
<span class="sd">  proportional to the input size rather than the output size.</span>

<span class="sd">  This op does not [broadcast](</span>
<span class="sd">  https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html)</span>
<span class="sd">  its inputs. If you need broadcasting, use `tf.math.add` (or the `+` operator)</span>
<span class="sd">  instead.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; a = tf.constant([[3, 5], [4, 8]])</span>
<span class="sd">  &gt;&gt;&gt; b = tf.constant([[1, 6], [2, 9]])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.add_n([a, b, a])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[ 7, 16],</span>
<span class="sd">         [10, 25]], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A list of `tf.Tensor` or `tf.IndexedSlices` objects, each with the</span>
<span class="sd">      same shape and type. `tf.IndexedSlices` objects will be converted into</span>
<span class="sd">      dense tensors prior to adding.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` of the same shape and type as the elements of `inputs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `inputs` don&#39;t all have same shape and dtype or the shape</span>
<span class="sd">    cannot be inferred.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inputs must be a list of at least one &quot;</span>
                     <span class="s2">&quot;Tensor/IndexedSlices with the same dtype and shape&quot;</span><span class="p">)</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inputs must be a list of at least one &quot;</span>
                     <span class="s2">&quot;Tensor/IndexedSlices with the same dtype and shape&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">values</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">values</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.accumulate_n&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.accumulate_n&quot;</span><span class="p">,</span> <span class="s2">&quot;accumulate_n&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;accumulate_n&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">accumulate_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tensor_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the element-wise sum of a list of tensors.</span>

<span class="sd">  Optionally, pass `shape` and `tensor_dtype` for shape and type checking,</span>
<span class="sd">  otherwise, these are inferred.</span>

<span class="sd">  `accumulate_n` performs the same operation as `tf.math.add_n`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.constant([[1, 2], [3, 4]])</span>
<span class="sd">  b = tf.constant([[5, 0], [0, 6]])</span>
<span class="sd">  tf.math.accumulate_n([a, b, a])  # [[7, 4], [6, 14]]</span>

<span class="sd">  # Explicitly pass shape and type</span>
<span class="sd">  tf.math.accumulate_n([a, b, a], shape=[2, 2], tensor_dtype=tf.int32)</span>
<span class="sd">                                                                 # [[7,  4],</span>
<span class="sd">                                                                 #  [6, 14]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A list of `Tensor` objects, each with same shape and type.</span>
<span class="sd">    shape: Expected shape of elements of `inputs` (optional). Also controls the</span>
<span class="sd">      output shape of this op, which may affect type inference in other ops. A</span>
<span class="sd">      value of `None` means &quot;infer the input shape from the shapes in `inputs`&quot;.</span>
<span class="sd">    tensor_dtype: Expected data type of `inputs` (optional). A value of `None`</span>
<span class="sd">      means &quot;infer the input dtype from `inputs[0]`&quot;.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of same shape and type as the elements of `inputs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `inputs` don&#39;t all have same shape and dtype or the shape</span>
<span class="sd">    cannot be inferred.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_input_error</span><span class="p">():</span>
    <span class="k">return</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inputs must be a list of at least one Tensor with the &quot;</span>
                      <span class="s2">&quot;same dtype and shape&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="n">_input_error</span><span class="p">()</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">raise</span> <span class="n">_input_error</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">raise</span> <span class="n">_input_error</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">input_tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>

  <span class="c1"># tensor_dtype is for safety only; operator&#39;s output type computed in C++</span>
  <span class="k">if</span> <span class="n">tensor_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tensor_dtype</span> <span class="o">!=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;tensor_dtype is </span><span class="si">{}</span><span class="s2">, but input is of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">tensor_dtype</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;AccumulateNV2&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_accumulate_n_grad</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Same as gradient for AddN. Copies the gradient to all inputs.&quot;&quot;&quot;</span>
  <span class="c1"># Not broadcasting.</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">grad</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../../index.html#tensorflow.sigmoid">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;nn.sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes sigmoid of `x` element-wise.</span>

<span class="sd">  Formula for calculating sigmoid(x): `y = 1 / (1 + exp(-x))`.</span>

<span class="sd">  For x \in (-inf, inf) =&gt; sigmoid(x) \in (0, 1)</span>

<span class="sd">  Example Usage:</span>

<span class="sd">  If a positive number is large, then its sigmoid will approach to 1 since the</span>
<span class="sd">  formula will be `y = &lt;large_num&gt; / (1 + &lt;large_num&gt;)`</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([0.0, 1.0, 50.0, 100.0])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.sigmoid(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=float32,</span>
<span class="sd">  numpy=array([0.5      , 0.7310586, 1.       , 1.       ], dtype=float32)&gt;</span>

<span class="sd">  If a negative number is large, its sigmoid will approach to 0 since the</span>
<span class="sd">  formula will be `y = 1 / (1 + &lt;large_num&gt;)`</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([-100.0, -50.0, -1.0, 0.0])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.sigmoid(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=</span>
<span class="sd">  array([0.0000000e+00, 1.9287499e-22, 2.6894143e-01, 0.5],</span>
<span class="sd">        dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor with type `float16`, `float32`, `float64`, `complex64`, or</span>
<span class="sd">      `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same type as `x`.</span>
<span class="sd">  </span>
<span class="sd">  Usage Example:</span>
<span class="sd">  </span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([-128.0, 0.0, 128.0], dtype=tf.float32)</span>
<span class="sd">  &gt;&gt;&gt; tf.sigmoid(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3,), dtype=float32,</span>
<span class="sd">  numpy=array([0. , 0.5, 1. ], dtype=float32)&gt;</span>

<span class="sd">  @compatibility(scipy)</span>
<span class="sd">  Equivalent to scipy.special.expit</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Sigmoid&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.log_sigmoid&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.log_sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;log_sigmoid&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;log_sigmoid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">log_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes log sigmoid of `x` element-wise.</span>

<span class="sd">  Specifically, `y = log(1 / (1 + exp(-x)))`.  For numerical stability,</span>
<span class="sd">  we use `y = -tf.nn.softplus(-x)`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor with type `float32` or `float64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;LogSigmoid&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">gen_nn_ops</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.bincount&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">bincount</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span>
             <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">minlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">maxlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
             <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Counts the number of occurrences of each value in an integer array.</span>

<span class="sd">  If `minlength` and `maxlength` are not given, returns a vector with length</span>
<span class="sd">  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.</span>
<span class="sd">  If `weights` are non-None, then index `i` of the output stores the sum of the</span>
<span class="sd">  value in `weights` at each index where the corresponding value in `arr` is</span>
<span class="sd">  `i`.</span>

<span class="sd">  ```python</span>
<span class="sd">  values = tf.constant([1,1,2,3,2,4,4,5])</span>
<span class="sd">  tf.math.bincount(values) #[0 2 2 1 2 1]</span>
<span class="sd">  ```</span>
<span class="sd">  Vector length = Maximum element in vector `values` is 5. Adding 1, which is 6</span>
<span class="sd">                  will be the vector length.</span>

<span class="sd">  Each bin value in the output indicates number of occurrences of the particular</span>
<span class="sd">  index. Here, index 1 in output has a value 2. This indicates value 1 occurs</span>
<span class="sd">  two times in `values`.</span>

<span class="sd">  ```python</span>
<span class="sd">  values = tf.constant([1,1,2,3,2,4,4,5])</span>
<span class="sd">  weights = tf.constant([1,5,0,1,0,5,4,5])</span>
<span class="sd">  tf.math.bincount(values, weights=weights) #[0 6 0 1 9 5]</span>
<span class="sd">  ```</span>
<span class="sd">  Bin will be incremented by the corresponding weight instead of 1.</span>
<span class="sd">  Here, index 1 in output has a value 6. This is the summation of weights</span>
<span class="sd">  corresponding to the value in `values`.</span>

<span class="sd">  Args:</span>
<span class="sd">    arr: An int32 tensor of non-negative values.</span>
<span class="sd">    weights: If non-None, must be the same shape as arr. For each value in</span>
<span class="sd">      `arr`, the bin will be incremented by the corresponding weight instead of</span>
<span class="sd">      1.</span>
<span class="sd">    minlength: If given, ensures the output has length at least `minlength`,</span>
<span class="sd">      padding with zeros at the end if necessary.</span>
<span class="sd">    maxlength: If given, skips values in `arr` that are equal or greater than</span>
<span class="sd">      `maxlength`, ensuring that the output has length at most `maxlength`.</span>
<span class="sd">    dtype: If `weights` is None, determines the type of the output bins.</span>
<span class="sd">    name: A name scope for the associated operations (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A vector with the same dtype as `weights` or the given `dtype`. The bin</span>
<span class="sd">    values.</span>

<span class="sd">  Raises:</span>
<span class="sd">    `InvalidArgumentError` if negative values are provided as an input.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;bincount&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;arr&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">array_is_nonempty</span> <span class="o">=</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">array_is_nonempty</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">minlength</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">minlength</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">minlength</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;minlength&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">output_size</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">minlength</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">maxlength</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">maxlength</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">maxlength</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;maxlength&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">output_size</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">maxlength</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weights&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">arr</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">([],</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.bincount&quot;</span><span class="p">,</span> <span class="s2">&quot;bincount&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;bincount&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">bincount_v1</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">minlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">maxlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Counts the number of occurrences of each value in an integer array.</span>

<span class="sd">  If `minlength` and `maxlength` are not given, returns a vector with length</span>
<span class="sd">  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.</span>
<span class="sd">  If `weights` are non-None, then index `i` of the output stores the sum of the</span>
<span class="sd">  value in `weights` at each index where the corresponding value in `arr` is</span>
<span class="sd">  `i`.</span>

<span class="sd">  Args:</span>
<span class="sd">    arr: An int32 tensor of non-negative values.</span>
<span class="sd">    weights: If non-None, must be the same shape as arr. For each value in</span>
<span class="sd">      `arr`, the bin will be incremented by the corresponding weight instead of</span>
<span class="sd">      1.</span>
<span class="sd">    minlength: If given, ensures the output has length at least `minlength`,</span>
<span class="sd">      padding with zeros at the end if necessary.</span>
<span class="sd">    maxlength: If given, skips values in `arr` that are equal or greater than</span>
<span class="sd">      `maxlength`, ensuring that the output has length at most `maxlength`.</span>
<span class="sd">    dtype: If `weights` is None, determines the type of the output bins.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A vector with the same dtype as `weights` or the given `dtype`. The bin</span>
<span class="sd">    values.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">bincount</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">minlength</span><span class="p">,</span> <span class="n">maxlength</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="cumsum"><a class="viewcode-back" href="../../../../index.html#tensorflow.cumsum">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.cumsum&quot;</span><span class="p">,</span> <span class="s2">&quot;cumsum&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the cumulative sum of the tensor `x` along `axis`.</span>

<span class="sd">  By default, this op performs an inclusive cumsum, which means that the first</span>
<span class="sd">  element of the input is identical to the first element of the output:</span>
<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; # tf.cumsum([a, b, c])   # [a, a + b, a + b + c]</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4, 6, 8])</span>
<span class="sd">  &gt;&gt;&gt; tf.cumsum(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="sd">  numpy=array([ 2,  6, 12, 20], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; # using varying `axis` values</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([[2, 4, 6, 8], [1,3,5,7]])</span>
<span class="sd">  &gt;&gt;&gt; tf.cumsum(y, axis=0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=</span>
<span class="sd">  array([[ 2,  4,  6,  8],</span>
<span class="sd">         [ 3,  7, 11, 15]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.cumsum(y, axis=1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=</span>
<span class="sd">  array([[ 2,  6, 12, 20],</span>
<span class="sd">         [ 1,  4,  9, 16]], dtype=int32)&gt;</span>

<span class="sd">  By setting the `exclusive` kwarg to `True`, an exclusive cumsum is performed</span>
<span class="sd">  instead:</span>

<span class="sd">  &gt;&gt;&gt; # tf.cumsum([a, b, c], exclusive=True)  =&gt; [0, a, a + b]</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4, 6, 8])</span>
<span class="sd">  &gt;&gt;&gt; tf.cumsum(x, exclusive=True)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="sd">  numpy=array([ 0,  2,  6, 12], dtype=int32)&gt;</span>

<span class="sd">  By setting the `reverse` kwarg to `True`, the cumsum is performed in the</span>
<span class="sd">  opposite direction:</span>

<span class="sd">  &gt;&gt;&gt; # tf.cumsum([a, b, c], reverse=True)  # [a + b + c, b + c, c]</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4, 6, 8])</span>
<span class="sd">  &gt;&gt;&gt; tf.cumsum(x, reverse=True)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="sd">  numpy=array([20, 18, 14,  8], dtype=int32)&gt;</span>

<span class="sd">  This is more efficient than using separate `tf.reverse` ops.</span>
<span class="sd">  The `reverse` and `exclusive` kwargs can also be combined:</span>

<span class="sd">  &gt;&gt;&gt; # tf.cumsum([a, b, c], exclusive=True, reverse=True)  # [b + c, c, 0]</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 4, 6, 8])</span>
<span class="sd">  &gt;&gt;&gt; tf.cumsum(x, exclusive=True, reverse=True)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="sd">  numpy=array([18, 14,  8,  0], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span>
<span class="sd">      `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,</span>
<span class="sd">      `complex128`, `qint8`, `quint8`, `qint32`, `half`.</span>
<span class="sd">    axis: A `Tensor` of type `int32` (default: 0). Must be in the range</span>
<span class="sd">      `[-rank(x), rank(x))`.</span>
<span class="sd">    exclusive: If `True`, perform exclusive cumsum.</span>
<span class="sd">    reverse: A `bool` (default: False).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Cumsum&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.cumprod&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.cumprod&quot;</span><span class="p">,</span> <span class="s2">&quot;cumprod&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;cumprod&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the cumulative product of the tensor `x` along `axis`.</span>

<span class="sd">  By default, this op performs an inclusive cumprod, which means that the</span>
<span class="sd">  first element of the input is identical to the first element of the output:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c])  # [a, a * b, a * b * c]</span>
<span class="sd">  ```</span>

<span class="sd">  By setting the `exclusive` kwarg to `True`, an exclusive cumprod is</span>
<span class="sd">  performed</span>
<span class="sd">  instead:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c], exclusive=True)  # [1, a, a * b]</span>
<span class="sd">  ```</span>

<span class="sd">  By setting the `reverse` kwarg to `True`, the cumprod is performed in the</span>
<span class="sd">  opposite direction:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c], reverse=True)  # [a * b * c, b * c, c]</span>
<span class="sd">  ```</span>

<span class="sd">  This is more efficient than using separate `tf.reverse` ops.</span>
<span class="sd">  The `reverse` and `exclusive` kwargs can also be combined:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c], exclusive=True, reverse=True)  # [b * c, c, 1]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span>
<span class="sd">      `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,</span>
<span class="sd">      `complex128`, `qint8`, `quint8`, `qint32`, `half`.</span>
<span class="sd">    axis: A `Tensor` of type `int32` (default: 0). Must be in the range</span>
<span class="sd">      `[-rank(x), rank(x))`.</span>
<span class="sd">    exclusive: If `True`, perform exclusive cumprod.</span>
<span class="sd">    reverse: A `bool` (default: False).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Cumprod&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.cumulative_logsumexp&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.cumulative_logsumexp&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">cumulative_logsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the cumulative log-sum-exp of the tensor `x` along `axis`.</span>

<span class="sd">  By default, this op performs an inclusive cumulative log-sum-exp, which means</span>
<span class="sd">  that the first element of the input is identical to the first element of</span>
<span class="sd">  the output.</span>

<span class="sd">  This operation is significantly more numerically stable than the equivalent</span>
<span class="sd">  tensorflow operation `tf.math.log(tf.math.cumsum(tf.math.exp(x)))`, although</span>
<span class="sd">  computes the same result given infinite numerical precision. However, note</span>
<span class="sd">  that in some cases, it may be less stable than `tf.math.reduce_logsumexp`</span>
<span class="sd">  for a given element, as it applies the &quot;log-sum-exp trick&quot; in a different</span>
<span class="sd">  way.</span>

<span class="sd">  More precisely, where `tf.math.reduce_logsumexp` uses the following trick:</span>

<span class="sd">  ```</span>
<span class="sd">  log(sum(exp(x))) == log(sum(exp(x - max(x)))) + max(x)</span>
<span class="sd">  ```</span>

<span class="sd">  it cannot be directly used here as there is no fast way of applying it</span>
<span class="sd">  to each prefix `x[:i]`. Instead, this function implements a prefix</span>
<span class="sd">  scan using pairwise log-add-exp, which is a commutative and associative</span>
<span class="sd">  (up to floating point precision) operator:</span>

<span class="sd">  ```</span>
<span class="sd">  log_add_exp(x, y) = log(exp(x) + exp(y))</span>
<span class="sd">                    = log(1 + exp(min(x, y) - max(x, y))) + max(x, y)</span>
<span class="sd">  ```</span>

<span class="sd">  However, reducing using the above operator leads to a different computation</span>
<span class="sd">  tree (logs are taken repeatedly instead of only at the end), and the maximum</span>
<span class="sd">  is only computed pairwise instead of over the entire prefix. In general, this</span>
<span class="sd">  leads to a different and slightly less precise computation.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float16`, `float32`,</span>
<span class="sd">      `float64`.</span>
<span class="sd">    axis: A `Tensor` of type `int32` or `int64` (default: 0). Must be in the</span>
<span class="sd">      range `[-rank(x), rank(x))`.</span>
<span class="sd">    exclusive: If `True`, perform exclusive cumulative log-sum-exp.</span>
<span class="sd">    reverse: If `True`, performs the cumulative log-sum-exp in the reverse</span>
<span class="sd">      direction.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same shape and type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;CumulativeLogsumexp&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cumulative_logsumexp</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.conj&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.conj&quot;</span><span class="p">,</span> <span class="s2">&quot;conj&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;conj&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">conj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the complex conjugate of a complex number.</span>

<span class="sd">  Given a tensor `input` of complex numbers, this operation returns a tensor of</span>
<span class="sd">  complex numbers that are the complex conjugate of each element in `input`. The</span>
<span class="sd">  complex numbers in `input` must be of the form \\(a + bj\\), where *a* is the</span>
<span class="sd">  real part and *b* is the imaginary part.</span>

<span class="sd">  The complex conjugate returned by this operation is of the form \\(a - bj\\).</span>

<span class="sd">  For example:</span>

<span class="sd">      # tensor &#39;input&#39; is [-2.25 + 4.75j, 3.25 + 5.75j]</span>
<span class="sd">      tf.math.conj(input) ==&gt; [-2.25 - 4.75j, 3.25 - 5.75j]</span>

<span class="sd">  If `x` is real, it is returned unchanged.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` to conjugate.  Must have numeric or variant type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` that is the conjugate of `x` (with the same type).</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` is not a numeric tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">dt</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">or</span> <span class="n">dt</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Conj&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected numeric or variant tensor, got dtype </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduced_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper function for reduction ops.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_shape: 1-D Tensor, the shape of the Tensor being reduced.</span>
<span class="sd">    axes: 1-D Tensor, the reduction axes.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A 1-D Tensor, the output shape as if keepdims were set to True.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">input_shape</span><span class="p">[</span><span class="n">axes</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">input_shape</span>

  <span class="c1"># Example:</span>
  <span class="c1"># cast needed for SparseTensor reductions</span>
  <span class="n">input_shape</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [2, 3, 5, 7]</span>
  <span class="n">axes</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [1, 2]</span>

  <span class="n">input_rank</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>  <span class="c1"># 4</span>
  <span class="n">axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">axes</span> <span class="o">+</span> <span class="n">input_rank</span><span class="p">)</span> <span class="o">%</span> <span class="n">input_rank</span>
  <span class="n">axes_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>  <span class="c1"># [2]</span>
  <span class="k">return</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">dynamic_stitch</span><span class="p">(</span>  <span class="c1"># [2, 1, 1, 7]</span>
      <span class="p">[</span>
          <span class="nb">range</span><span class="p">(</span><span class="n">input_rank</span><span class="p">),</span>  <span class="c1"># [0, 1, 2, 3]</span>
          <span class="n">axes</span>
      <span class="p">],</span>  <span class="c1"># [1, 2]</span>
      <span class="p">[</span>
          <span class="n">input_shape</span><span class="p">,</span>  <span class="c1"># [2, 3, 5, 7]</span>
          <span class="n">array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">axes_shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="p">])</span>  <span class="c1"># [1, 1]</span>


<span class="k">def</span> <span class="nf">_unsorted_segment_N</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Helper function for unsorted_segment_mean/_sqrtN.</span>

<span class="sd">  Computes the number</span>
<span class="sd">      of segment entries with 0-entries set to 1 to allow division by N.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">num_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">num_segments</span><span class="p">)</span>
  <span class="c1"># bincount doesn&#39;t support negative indices so we use unsorted_segment_sum</span>
  <span class="n">segment_ids_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
  <span class="n">ones_tensor</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">ones_tensor</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
  <span class="c1"># add dimensions for all non-reduced axes</span>
  <span class="n">broadcastable_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
      <span class="p">[</span><span class="n">num_segments</span><span class="p">[</span><span class="n">array_ops</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
       <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                       <span class="o">-</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)],</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="n">num_segments</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span>
      <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">broadcastable_shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span>
    <span class="s2">&quot;math.unsorted_segment_mean&quot;</span><span class="p">,</span>
    <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.unsorted_segment_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;unsorted_segment_mean&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;unsorted_segment_mean&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">unsorted_segment_mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the mean along segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  This operator is similar to the unsorted segment sum operator found</span>
<span class="sd">  [here](../../../api_docs/python/math_ops.md#UnsortedSegmentSum).</span>
<span class="sd">  Instead of computing the sum over segments, it computes the mean of all</span>
<span class="sd">  entries belonging to a segment such that:</span>

<span class="sd">  \\(output_i = 1/N_i \sum_{j...} data[j...]\\) where the sum is over tuples</span>
<span class="sd">  `j...` such that `segment_ids[j...] == i` with \\N_i\\ being the number of</span>
<span class="sd">  occurrences of id \\i\\.</span>

<span class="sd">  If there is no entry for a given segment ID `i`, it outputs 0.</span>

<span class="sd">  If the given segment ID `i` is negative, the value is dropped and will not</span>
<span class="sd">  be added to the sum of the segment.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with floating point or complex dtype.</span>
<span class="sd">    segment_ids: An integer tensor whose shape is a prefix of `data.shape`.</span>
<span class="sd">    num_segments: An integer scalar `Tensor`.  The number of distinct segment</span>
<span class="sd">      IDs.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`.  Has same shape as data, except for the first `segment_ids.rank`</span>
<span class="sd">    dimensions, which are replaced with a single dimension which has size</span>
<span class="sd">   `num_segments`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;UnsortedSegmentMean&quot;</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">_unsorted_segment_N</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="n">summed</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">summed</span> <span class="o">/</span> <span class="n">N</span>


<span class="nd">@tf_export</span><span class="p">(</span>
    <span class="s2">&quot;math.unsorted_segment_sqrt_n&quot;</span><span class="p">,</span>
    <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.unsorted_segment_sqrt_n&quot;</span><span class="p">,</span> <span class="s2">&quot;unsorted_segment_sqrt_n&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;unsorted_segment_sqrt_n&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">unsorted_segment_sqrt_n</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along segments of a tensor divided by the sqrt(N).</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  This operator is similar to the unsorted segment sum operator found</span>
<span class="sd">  [here](../../../api_docs/python/math_ops.md#UnsortedSegmentSum).</span>
<span class="sd">  Additionally to computing the sum over segments, it divides the results by</span>
<span class="sd">  sqrt(N).</span>

<span class="sd">  \\(output_i = 1/sqrt(N_i) \sum_{j...} data[j...]\\) where the sum is over</span>
<span class="sd">  tuples `j...` such that `segment_ids[j...] == i` with \\N_i\\ being the</span>
<span class="sd">  number of occurrences of id \\i\\.</span>

<span class="sd">  If there is no entry for a given segment ID `i`, it outputs 0.</span>

<span class="sd">  Note that this op only supports floating point and complex dtypes,</span>
<span class="sd">  due to tf.sqrt only supporting these types.</span>

<span class="sd">  If the given segment ID `i` is negative, the value is dropped and will not</span>
<span class="sd">  be added to the sum of the segment.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with floating point or complex dtype.</span>
<span class="sd">    segment_ids: An integer tensor whose shape is a prefix of `data.shape`.</span>
<span class="sd">    num_segments: An integer scalar `Tensor`.  The number of distinct segment</span>
<span class="sd">      IDs.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`.  Has same shape as data, except for the first `segment_ids.rank`</span>
<span class="sd">    dimensions, which are replaced with a single dimension which has size</span>
<span class="sd">   `num_segments`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;UnsortedSegmentSqrtN&quot;</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">_unsorted_segment_N</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="n">summed</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">summed</span> <span class="o">/</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.segment_sum&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_segment_sum&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_segment_sum&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_segment_sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                       <span class="n">indices</span><span class="p">,</span>
                       <span class="n">segment_ids</span><span class="p">,</span>
                       <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along sparse segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `tf.math.segment_sum`, but `segment_ids` can have rank less than `data`&#39;s</span>
<span class="sd">  first dimension, selecting a subset of dimension 0, specified by `indices`.</span>
<span class="sd">  `segment_ids` is allowed to have missing ids, in which case the output will</span>
<span class="sd">  be zeros at those indices. In those cases `num_segments` is used to determine</span>
<span class="sd">  the size of the output.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])</span>

<span class="sd">  # Select two rows, one segment.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))</span>
<span class="sd">  # =&gt; [[0 0 0 0]]</span>

<span class="sd">  # Select two rows, two segment.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))</span>
<span class="sd">  # =&gt; [[ 1  2  3  4]</span>
<span class="sd">  #     [-1 -2 -3 -4]]</span>

<span class="sd">  # With missing segment ids.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),</span>
<span class="sd">                        num_segments=4)</span>
<span class="sd">  # =&gt; [[ 1  2  3  4]</span>
<span class="sd">  #     [ 0  0  0  0]</span>
<span class="sd">  #     [-1 -2 -3 -4]</span>
<span class="sd">  #     [ 0  0  0  0]]</span>

<span class="sd">  # Select all rows, two segments.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))</span>
<span class="sd">  # =&gt; [[0 0 0 0]</span>
<span class="sd">  #     [5 6 7 8]]</span>

<span class="sd">  # Which is equivalent to:</span>
<span class="sd">  tf.math.segment_sum(c, tf.constant([0, 0, 1]))</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">num_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sum_with_num_segments</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sum</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sparse.segment_sum&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">sparse_segment_sum_v2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                          <span class="n">indices</span><span class="p">,</span>
                          <span class="n">segment_ids</span><span class="p">,</span>
                          <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along sparse segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `tf.math.segment_sum`, but `segment_ids` can have rank less than `data`&#39;s</span>
<span class="sd">  first dimension, selecting a subset of dimension 0, specified by `indices`.</span>
<span class="sd">  `segment_ids` is allowed to have missing ids, in which case the output will</span>
<span class="sd">  be zeros at those indices. In those cases `num_segments` is used to determine</span>
<span class="sd">  the size of the output.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])</span>

<span class="sd">  # Select two rows, one segment.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))</span>
<span class="sd">  # =&gt; [[0 0 0 0]]</span>

<span class="sd">  # Select two rows, two segment.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))</span>
<span class="sd">  # =&gt; [[ 1  2  3  4]</span>
<span class="sd">  #     [-1 -2 -3 -4]]</span>

<span class="sd">  # With missing segment ids.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),</span>
<span class="sd">                        num_segments=4)</span>
<span class="sd">  # =&gt; [[ 1  2  3  4]</span>
<span class="sd">  #     [ 0  0  0  0]</span>
<span class="sd">  #     [-1 -2 -3 -4]</span>
<span class="sd">  #     [ 0  0  0  0]]</span>

<span class="sd">  # Select all rows, two segments.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))</span>
<span class="sd">  # =&gt; [[0 0 0 0]</span>
<span class="sd">  #     [5 6 7 8]]</span>

<span class="sd">  # Which is equivalent to:</span>
<span class="sd">  tf.math.segment_sum(c, tf.constant([0, 0, 1]))</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">sparse_segment_sum</span><span class="p">(</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.segment_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_segment_mean&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_segment_mean&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_segment_mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                        <span class="n">indices</span><span class="p">,</span>
                        <span class="n">segment_ids</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the mean along sparse segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `tf.math.segment_mean`, but `segment_ids` can have rank less than</span>
<span class="sd">  `data`&#39;s first dimension, selecting a subset of dimension 0, specified by</span>
<span class="sd">  `indices`.</span>
<span class="sd">  `segment_ids` is allowed to have missing ids, in which case the output will</span>
<span class="sd">  be zeros at those indices. In those cases `num_segments` is used to determine</span>
<span class="sd">  the size of the output.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">num_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_mean_with_num_segments</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_mean</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sparse.segment_mean&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">sparse_segment_mean_v2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                           <span class="n">indices</span><span class="p">,</span>
                           <span class="n">segment_ids</span><span class="p">,</span>
                           <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the mean along sparse segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `tf.math.segment_mean`, but `segment_ids` can have rank less than</span>
<span class="sd">  `data`&#39;s first dimension, selecting a subset of dimension 0, specified by</span>
<span class="sd">  `indices`.</span>
<span class="sd">  `segment_ids` is allowed to have missing ids, in which case the output will</span>
<span class="sd">  be zeros at those indices. In those cases `num_segments` is used to determine</span>
<span class="sd">  the size of the output.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">sparse_segment_mean</span><span class="p">(</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.segment_sqrt_n&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_segment_sqrt_n&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_segment_sqrt_n&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_segment_sqrt_n</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                          <span class="n">indices</span><span class="p">,</span>
                          <span class="n">segment_ids</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along sparse segments of a tensor divided by the sqrt(N).</span>

<span class="sd">  `N` is the size of the segment being reduced.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">num_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sqrt_n_with_num_segments</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sqrt_n</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sparse.segment_sqrt_n&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">sparse_segment_sqrt_n_v2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                             <span class="n">indices</span><span class="p">,</span>
                             <span class="n">segment_ids</span><span class="p">,</span>
                             <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along sparse segments of a tensor divided by the sqrt(N).</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `tf.sparse.segment_mean`, but instead of dividing by the size of the</span>
<span class="sd">  segment, `N`, divide by `sqrt(N)` instead.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">sparse_segment_sqrt_n</span><span class="p">(</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">)</span>


<div class="viewcode-block" id="tensordot"><a class="viewcode-back" href="../../../../index.html#tensorflow.tensordot">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;tensordot&quot;</span><span class="p">,</span> <span class="s2">&quot;linalg.tensordot&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Tensor contraction of a and b along specified axes and outer product.</span>

<span class="sd">  Tensordot (also known as tensor contraction) sums the product of elements</span>
<span class="sd">  from `a` and `b` over the indices specified by `a_axes` and `b_axes`.</span>
<span class="sd">  The lists `a_axes` and `b_axes` specify those pairs of axes along which to</span>
<span class="sd">  contract the tensors. The axis `a_axes[i]` of `a` must have the same dimension</span>
<span class="sd">  as axis `b_axes[i]` of `b` for all `i` in `range(0, len(a_axes))`. The lists</span>
<span class="sd">  `a_axes` and `b_axes` must have identical length and consist of unique</span>
<span class="sd">  integers that specify valid axes for each of the tensors. Additionally</span>
<span class="sd">  outer product is supported by passing `axes=0`.</span>

<span class="sd">  This operation corresponds to `numpy.tensordot(a, b, axes)`.</span>

<span class="sd">  Example 1: When `a` and `b` are matrices (order 2), the case `axes = 1`</span>
<span class="sd">  is equivalent to matrix multiplication.</span>

<span class="sd">  Example 2: When `a` and `b` are matrices (order 2), the case</span>
<span class="sd">  `axes = [[1], [0]]` is equivalent to matrix multiplication.</span>

<span class="sd">  Example 3: When `a` and `b` are matrices (order 2), the case `axes=0` gives</span>
<span class="sd">  the outer product, a tensor of order 4.</span>

<span class="sd">  Example 4: Suppose that \\(a_{ijk}\\) and \\(b_{lmn}\\) represent two</span>
<span class="sd">  tensors of order 3. Then, `contract(a, b, [[0], [2]])` is the order 4 tensor</span>
<span class="sd">  \\(c_{jklm}\\) whose entry</span>
<span class="sd">  corresponding to the indices \\((j,k,l,m)\\) is given by:</span>

<span class="sd">  \\( c_{jklm} = \sum_i a_{ijk} b_{lmi} \\).</span>

<span class="sd">  In general, `order(c) = order(a) + order(b) - 2*len(axes[0])`.</span>

<span class="sd">  Args:</span>
<span class="sd">    a: `Tensor` of type `float32` or `float64`.</span>
<span class="sd">    b: `Tensor` with the same type as `a`.</span>
<span class="sd">    axes: Either a scalar `N`, or a list or an `int32` `Tensor` of shape [2, k].</span>
<span class="sd">      If axes is a scalar, sum over the last N axes of a and the first N axes of</span>
<span class="sd">      b in order. If axes is a list or `Tensor` the first and second row contain</span>
<span class="sd">      the set of unique integers specifying axes along which the contraction is</span>
<span class="sd">      computed, for `a` and `b`, respectively. The number of axes for `a` and</span>
<span class="sd">      `b` must be equal. If `axes=0`, computes the outer product between `a` and</span>
<span class="sd">      `b`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with the same type as `a`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the shapes of `a`, `b`, and `axes` are incompatible.</span>
<span class="sd">    IndexError: If the values in axes exceed the rank of the corresponding</span>
<span class="sd">      tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_tensordot_reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">flipped</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Helper method to perform transpose and reshape for contraction op.</span>

<span class="sd">    This method is helpful in reducing `math_ops.tensordot` to `math_ops.matmul`</span>
<span class="sd">    using `array_ops.transpose` and `array_ops.reshape`. The method takes a</span>
<span class="sd">    tensor and performs the correct transpose and reshape operation for a given</span>
<span class="sd">    set of indices. It returns the reshaped tensor as well as a list of indices</span>
<span class="sd">    necessary to reshape the tensor again after matrix multiplication.</span>

<span class="sd">    Args:</span>
<span class="sd">      a: `Tensor`.</span>
<span class="sd">      axes: List or `int32` `Tensor` of unique indices specifying valid axes of</span>
<span class="sd">        `a`.</span>
<span class="sd">      flipped: An optional `bool`. Defaults to `False`. If `True`, the method</span>
<span class="sd">        assumes that `a` is the second argument in the contraction operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is</span>
<span class="sd">      the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is</span>
<span class="sd">      either a list of integers or an `int32` `Tensor`, depending on whether</span>
<span class="sd">      the shape of a is fully specified, and free_dims_static is either a list</span>
<span class="sd">      of integers and None values, or None, representing the inferred</span>
<span class="sd">      static shape of the free dimensions</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="n">shape_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
      <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
      <span class="n">free</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
      <span class="n">free_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">free</span><span class="p">]</span>
      <span class="n">prod_free</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">free</span><span class="p">]))</span>
      <span class="n">prod_axes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]))</span>
      <span class="n">perm</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">+</span> <span class="n">free</span> <span class="k">if</span> <span class="n">flipped</span> <span class="k">else</span> <span class="n">free</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
      <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">prod_axes</span><span class="p">,</span> <span class="n">prod_free</span><span class="p">]</span> <span class="k">if</span> <span class="n">flipped</span> <span class="k">else</span> <span class="p">[</span><span class="n">prod_free</span><span class="p">,</span> <span class="n">prod_axes</span><span class="p">]</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">perm</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">)))</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="n">a_trans</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">a_trans</span> <span class="o">=</span> <span class="n">a</span>
      <span class="k">if</span> <span class="n">a_trans</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span> <span class="o">!=</span> <span class="n">new_shape</span><span class="p">:</span>
        <span class="n">reshaped_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a_trans</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">reshaped_a</span> <span class="o">=</span> <span class="n">a_trans</span>
      <span class="k">return</span> <span class="n">reshaped_a</span><span class="p">,</span> <span class="n">free_dims</span><span class="p">,</span> <span class="n">free_dims</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">shape_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">free</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">axes_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">free_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">free</span><span class="p">]</span>
        <span class="n">free_dims_static</span> <span class="o">=</span> <span class="n">free_dims</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;axes&quot;</span><span class="p">)</span>
        <span class="n">free</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">free</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;free&quot;</span><span class="p">)</span>
        <span class="n">shape_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">free_dims_static</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">shape_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">rank_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;axes&quot;</span><span class="p">)</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">axes</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">axes</span> <span class="o">+</span> <span class="n">rank_a</span><span class="p">)</span>
        <span class="n">free</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">rank_a</span><span class="p">),</span> <span class="n">axes</span><span class="p">)</span>
      <span class="n">free_dims</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">free</span><span class="p">)</span>
      <span class="n">axes_dims</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
      <span class="n">prod_free_dims</span> <span class="o">=</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">free_dims</span><span class="p">)</span>
      <span class="n">prod_axes_dims</span> <span class="o">=</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">axes_dims</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">flipped</span><span class="p">:</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">axes</span><span class="p">,</span> <span class="n">free</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">prod_axes_dims</span><span class="p">,</span> <span class="n">prod_free_dims</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">free</span><span class="p">,</span> <span class="n">axes</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">prod_free_dims</span><span class="p">,</span> <span class="n">prod_axes_dims</span><span class="p">])</span>
      <span class="n">reshaped_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="n">new_shape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">reshaped_a</span><span class="p">,</span> <span class="n">free_dims</span><span class="p">,</span> <span class="n">free_dims_static</span>

  <span class="k">def</span> <span class="nf">_tensordot_axes</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates two sets of contraction axes for the two tensor arguments.&quot;&quot;&quot;</span>
    <span class="n">a_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">integral_types</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">axes</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;axes&#39; must be at least 0.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">axes</span> <span class="o">&gt;</span> <span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;axes&#39; must not be larger than the number of &quot;</span>
                           <span class="s2">&quot;dimensions of tensor </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">xrange</span><span class="p">(</span><span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">-</span> <span class="n">axes</span><span class="p">,</span>
                            <span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">)),</span> <span class="nb">list</span><span class="p">(</span><span class="n">xrange</span><span class="p">(</span><span class="n">axes</span><span class="p">)))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="n">axes</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="nb">range</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;axes&#39; must be an integer or have length 2.&quot;</span><span class="p">)</span>
      <span class="n">a_axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">b_axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a_axes</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">integral_types</span><span class="p">)</span> <span class="ow">and</span> \
          <span class="nb">isinstance</span><span class="p">(</span><span class="n">b_axes</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">integral_types</span><span class="p">):</span>
        <span class="n">a_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">a_axes</span><span class="p">]</span>
        <span class="n">b_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">b_axes</span><span class="p">]</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_axes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_axes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Different number of contraction axes &#39;a&#39; and &#39;b&#39;, </span><span class="si">%s</span><span class="s2"> != </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
            <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a_axes</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_axes</span><span class="p">)))</span>
      <span class="k">return</span> <span class="n">a_axes</span><span class="p">,</span> <span class="n">b_axes</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">axes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;axes&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Tensordot&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
    <span class="n">a_axes</span><span class="p">,</span> <span class="n">b_axes</span> <span class="o">=</span> <span class="n">_tensordot_axes</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">a_reshape</span><span class="p">,</span> <span class="n">a_free_dims</span><span class="p">,</span> <span class="n">a_free_dims_static</span> <span class="o">=</span> <span class="n">_tensordot_reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a_axes</span><span class="p">)</span>
    <span class="n">b_reshape</span><span class="p">,</span> <span class="n">b_free_dims</span><span class="p">,</span> <span class="n">b_free_dims_static</span> <span class="o">=</span> <span class="n">_tensordot_reshape</span><span class="p">(</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">b_axes</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">ab_matmul</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">a_reshape</span><span class="p">,</span> <span class="n">b_reshape</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a_free_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b_free_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">ab_matmul</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span> <span class="ow">and</span>
          <span class="n">ab_matmul</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span> <span class="o">==</span> <span class="n">a_free_dims</span> <span class="o">+</span> <span class="n">b_free_dims</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ab_matmul</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">ab_matmul</span><span class="p">,</span> <span class="n">a_free_dims</span> <span class="o">+</span> <span class="n">b_free_dims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">a_free_dims</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a_free_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">b_free_dims</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b_free_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">product</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">ab_matmul</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">a_free_dims</span><span class="p">,</span> <span class="n">b_free_dims</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">a_free_dims_static</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">b_free_dims_static</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">product</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">a_free_dims_static</span> <span class="o">+</span> <span class="n">b_free_dims_static</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">product</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.polyval&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">polyval</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the elementwise value of a polynomial.</span>

<span class="sd">  If `x` is a tensor and `coeffs` is a list n + 1 tensors,</span>
<span class="sd">  this function returns the value of the n-th order polynomial</span>

<span class="sd">     p(x) = coeffs[n-1] + coeffs[n-2] * x + ...  + coeffs[0] * x**(n-1)</span>

<span class="sd">  evaluated using Horner&#39;s method, i.e.</span>

<span class="sd">     p(x) = coeffs[n-1] + x * (coeffs[n-2] + ... + x * (coeffs[1] +</span>
<span class="sd">            x * coeffs[0]))</span>
<span class="sd">            </span>
<span class="sd">  Usage Example:</span>
<span class="sd">  </span>
<span class="sd">  &gt;&gt;&gt; coefficients = [1.0, 2.5, -4.2]</span>
<span class="sd">  &gt;&gt;&gt; x = 5.0</span>
<span class="sd">  &gt;&gt;&gt; y = tf.math.polyval(coefficients, x)</span>
<span class="sd">  &gt;&gt;&gt; y</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=33.3&gt;</span>

<span class="sd">  Usage Example:</span>

<span class="sd">  &gt;&gt;&gt; tf.math.polyval([2, 1, 0], 3) # evaluates 2 * (3**2) + 1 * (3**1) + 0 * (3**0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=int32, numpy=21&gt;</span>

<span class="sd">  `tf.math.polyval` can also be used in polynomial regression. Taking</span>
<span class="sd">  advantage of this function can facilitate writing a polynomial equation</span>
<span class="sd">  as compared to explicitly writing it out, especially for higher degree</span>
<span class="sd">  polynomials.</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant(3)</span>
<span class="sd">  &gt;&gt;&gt; theta1 = tf.Variable(2)</span>
<span class="sd">  &gt;&gt;&gt; theta2 = tf.Variable(1)</span>
<span class="sd">  &gt;&gt;&gt; theta3 = tf.Variable(0)</span>
<span class="sd">  &gt;&gt;&gt; tf.math.polyval([theta1, theta2, theta3], x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=int32, numpy=21&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    coeffs: A list of `Tensor` representing the coefficients of the polynomial.</span>
<span class="sd">    x: A `Tensor` representing the variable of the polynomial.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as the expression p(x) with usual broadcasting</span>
<span class="sd">    rules for element-wise addition and multiplication applied.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to numpy.polyval.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument coeffs must be list type &quot;</span>
                     <span class="s2">&quot;found </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)))</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;polyval&quot;</span><span class="p">,</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">coeff</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;coeff_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">index</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">coeff</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
      <span class="n">p</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">p</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reciprocal_no_nan&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reciprocal_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Performs a safe reciprocal operation, element wise.</span>

<span class="sd">  If a particular element is zero, the reciprocal for that element is</span>
<span class="sd">  also set to zero.</span>

<span class="sd">  For example:</span>
<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([2.0, 0.5, 0, 1], dtype=tf.float32)</span>
<span class="sd">  tf.math.reciprocal_no_nan(x)  # [ 0.5, 2, 0.0, 1.0 ]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` of type `float16`, `float32`, `float64` `complex64` or</span>
<span class="sd">      `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of same shape and type as `x`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: x must be of a valid dtype.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;reciprocal_no_nan&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">one</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;one&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">div_no_nan</span><span class="p">(</span><span class="n">one</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.xlog1py&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">xlog1py</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute x * log1p(y).</span>

<span class="sd">  Given `x` and `y`, compute `x * log1p(y)`. This function safely returns</span>
<span class="sd">  zero when `x = 0`, no matter what the value of `y` is.</span>

<span class="sd">  Example:</span>

<span class="sd">  &gt;&gt;&gt; tf.math.xlog1py(0., 1.)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.math.xlog1py(1., 1.)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.6931472&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.math.xlog1py(2., 2.)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.1972246&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.math.xlog1py(0., -1.)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `tf.Tensor` of type `bfloat16`, `half`, `float32`, `float64`,</span>
<span class="sd">      `complex64`, `complex128`</span>
<span class="sd">    y: A `tf.Tensor` of type `bfloat16`, `half`, `float32`, `float64`,</span>
<span class="sd">      `complex64`, `complex128`</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `x * log1p(y)`.</span>

<span class="sd">  @compatibility(scipy)</span>
<span class="sd">  Equivalent to scipy.special.xlog1py</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;xlog1py&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">xlog1py</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.erfinv&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">erfinv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute inverse error function.</span>

<span class="sd">  Given `x`, compute the inverse error function of `x`. This function</span>
<span class="sd">  is the inverse of `tf.math.erf`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` with type `float` or `double`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">  Returns:</span>
<span class="sd">    Inverse error function of `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;erfinv&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">erfinv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.ndtri&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">ndtri</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute quantile of Standard Normal.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` with type `float` or `double`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">  Returns:</span>
<span class="sd">    Inverse error function of `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;ndtri&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">ndtri</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.ceil&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.ceil&quot;</span><span class="p">,</span> <span class="s2">&quot;ceil&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;ceil&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return the ceiling of the input, element-wise.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])</span>
<span class="sd">  &lt;tf.Tensor: shape=(7,), dtype=float32,</span>
<span class="sd">  numpy=array([-1., -1., -0.,  1.,  2.,  2.,  2.], dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `tf.Tensor`. Must be one of the following types: `bfloat16`, `half`,</span>
<span class="sd">      `float32`, `float64`. `int32`</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor`. Has the same type as `x`.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.ceil</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="sqrt"><a class="viewcode-back" href="../../../../index.html#tensorflow.sqrt">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.sqrt&quot;</span><span class="p">,</span> <span class="s2">&quot;sqrt&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes element-wise square root of the input tensor.</span>

<span class="sd">  Note: This operation does not support integer types.</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([[4.0], [16.0]])</span>
<span class="sd">  &gt;&gt;&gt; tf.sqrt(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=</span>
<span class="sd">    array([[2.],</span>
<span class="sd">           [4.]], dtype=float32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([[-4.0], [16.0]])</span>
<span class="sd">  &gt;&gt;&gt; tf.sqrt(y)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=</span>
<span class="sd">    array([[nan],</span>
<span class="sd">           [ 4.]], dtype=float32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)</span>
<span class="sd">  &gt;&gt;&gt; tf.sqrt(z)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 1), dtype=complex128, numpy=</span>
<span class="sd">    array([[0.0+1.j],</span>
<span class="sd">           [4.0+0.j]])&gt;</span>

<span class="sd">  Note: In order to support complex complex, please provide an input tensor</span>
<span class="sd">  of `complex64` or `complex128`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `tf.Tensor` of type `bfloat16`, `half`, `float32`, `float64`,</span>
<span class="sd">      `complex64`, `complex128`</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` of same size, type and sparsity as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="c1"># pylint: disable=g-docstring-has-escape</span>
<div class="viewcode-block" id="exp"><a class="viewcode-back" href="../../../../index.html#tensorflow.exp">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.exp&quot;</span><span class="p">,</span> <span class="s2">&quot;exp&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes exponential of x element-wise.  \\(y = e^x\\).</span>

<span class="sd">  This function computes the exponential of the input tensor element-wise.</span>
<span class="sd">  i.e. `math.exp(x)` or \\(e^x\\), where `x` is the input tensor.</span>
<span class="sd">  \\(e\\) denotes Euler&#39;s number and is approximately equal to 2.718281.</span>
<span class="sd">  Output is positive for any real input.</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant(2.0)</span>
<span class="sd">  &gt;&gt;&gt; tf.math.exp(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=7.389056&gt;</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([2.0, 8.0])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.exp(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2,), dtype=float32,</span>
<span class="sd">  numpy=array([   7.389056, 2980.958   ], dtype=float32)&gt;</span>

<span class="sd">  For complex numbers, the exponential value is calculated as</span>
<span class="sd">  \\(e^{x+iy}={e^x}{e^{iy}}={e^x}{\\cos(y)+i\\sin(y)}\\)</span>

<span class="sd">  For `1+1j` the value would be computed as:</span>
<span class="sd">  \\(e^1{\\cos(1)+i\\sin(1)} = 2.7182817 \\times (0.5403023+0.84147096j)\\)</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant(1 + 1j)</span>
<span class="sd">  &gt;&gt;&gt; tf.math.exp(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=complex128,</span>
<span class="sd">  numpy=(1.4686939399158851+2.2873552871788423j)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `tf.Tensor`. Must be one of the following types: `bfloat16`, `half`,</span>
<span class="sd">      `float32`, `float64`, `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor`. Has the same type as `x`.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.exp</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="c1"># pylint: enable=g-docstring-has-escape</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.sobol_sample&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sobol_sample</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_results</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates points from the Sobol sequence.</span>

<span class="sd">  Creates a Sobol sequence with `num_results` samples. Each sample has dimension</span>
<span class="sd">  `dim`. Skips the first `skip` samples.</span>

<span class="sd">  Args:</span>
<span class="sd">    dim: Positive scalar `Tensor` representing each sample&#39;s dimension.</span>
<span class="sd">    num_results: Positive scalar `Tensor` of dtype int32. The number of Sobol</span>
<span class="sd">        points to return in the output.</span>
<span class="sd">    skip: (Optional) Positive scalar `Tensor` of dtype int32. The number of</span>
<span class="sd">        initial points of the Sobol sequence to skip. Default value is 0.</span>
<span class="sd">    dtype: (Optional) The `tf.Dtype` of the sample. One of: `tf.float32` or</span>
<span class="sd">        `tf.float64`. Defaults to `tf.float32`.</span>
<span class="sd">    name: (Optional) Python `str` name prefixed to ops created by this function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    `Tensor` of samples from Sobol sequence with `shape` [num_results, dim].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;sobol&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_results</span><span class="p">,</span> <span class="n">skip</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sobol_sample</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_results</span><span class="p">,</span> <span class="n">skip</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.rsqrt&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.rsqrt&quot;</span><span class="p">,</span> <span class="s2">&quot;rsqrt&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;rsqrt&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">rsqrt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes reciprocal of square root of x element-wise.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([2., 0., -2.])</span>
<span class="sd">  &gt;&gt;&gt; tf.math.rsqrt(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3,), dtype=float32,</span>
<span class="sd">  numpy=array([0.707, inf, nan], dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `tf.Tensor`. Must be one of the following types: `bfloat16`, `half`,</span>
<span class="sd">      `float32`, `float64`. `int32`</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor`. Has the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>