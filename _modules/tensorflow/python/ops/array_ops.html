

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.array_ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.array_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.array_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="c1"># Tests for this file live in python/kernel_tests/array_ops_test.py</span>
<span class="sd">&quot;&quot;&quot;Support for manipulating tensors.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">common_shapes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">composite_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">sparse_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_util</span>
<span class="c1"># &#39;Constant&#39; gets imported in the module &#39;array_ops&#39;.</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework.constant_op</span> <span class="k">import</span> <span class="n">constant</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_math_ops</span>
<span class="c1"># go/tf-wildcard-import</span>
<span class="c1"># pylint: disable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.gen_array_ops</span> <span class="k">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.gen_array_ops</span> <span class="k">import</span> <span class="n">reverse_v2</span> <span class="k">as</span> <span class="n">reverse</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">dispatch</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_decorator</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>
<span class="c1"># pylint: enable=wildcard-import</span>

<span class="c1"># Used for slicing to specify a new 1 size dimension</span>
<span class="n">newaxis</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;newaxis&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">export_constant</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;newaxis&quot;</span><span class="p">)</span>

<span class="c1"># We override the &#39;slice&#39; for the &quot;slice&quot; op, so we keep python&#39;s</span>
<span class="c1"># existing &#39;slice&#39; for later use in this module.</span>
<span class="n">_BaseSlice</span> <span class="o">=</span> <span class="nb">slice</span>


<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../../../index.html#tensorflow.reshape">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;reshape&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;reshape&quot;</span><span class="p">,</span> <span class="s2">&quot;manip.reshape&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reshapes a tensor.</span>

<span class="sd">  Given `tensor`, this operation returns a new `tf.Tensor` that has the same</span>
<span class="sd">  values as `tensor` in the same order, except with a new shape given by</span>
<span class="sd">  `shape`.</span>

<span class="sd">  &gt;&gt;&gt; t1 = [[1, 2, 3],</span>
<span class="sd">  ...       [4, 5, 6]]</span>
<span class="sd">  &gt;&gt;&gt; print(tf.shape(t1).numpy())</span>
<span class="sd">  [2 3]</span>
<span class="sd">  &gt;&gt;&gt; t2 = tf.reshape(t1, [6])</span>
<span class="sd">  &gt;&gt;&gt; t2</span>
<span class="sd">  &lt;tf.Tensor: shape=(6,), dtype=int32,</span>
<span class="sd">    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t2, [3, 2])</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 2],</span>
<span class="sd">           [3, 4],</span>
<span class="sd">           [5, 6]], dtype=int32)&gt;</span>

<span class="sd">  The `tf.reshape` does not change the order of or the total number of elements</span>
<span class="sd">  in the tensor, and so it can reuse the underlying data buffer. This makes it</span>
<span class="sd">  a fast operation independent of how big of a tensor it is operating on.</span>

<span class="sd">  &gt;&gt;&gt; tf.reshape([1, 2, 3], [2, 2])</span>
<span class="sd">  Traceback (most recent call last):</span>
<span class="sd">  ...</span>
<span class="sd">  InvalidArgumentError: Input to reshape is a tensor with 3 values, but the</span>
<span class="sd">  requested shape has 4</span>

<span class="sd">  To instead reorder the data to rearrange the dimensions of a tensor, see</span>
<span class="sd">  `tf.transpose`.</span>

<span class="sd">  &gt;&gt;&gt; t = [[1, 2, 3],</span>
<span class="sd">  ...      [4, 5, 6]]</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [3, 2]).numpy()</span>
<span class="sd">  array([[1, 2],</span>
<span class="sd">         [3, 4],</span>
<span class="sd">         [5, 6]], dtype=int32)</span>
<span class="sd">  &gt;&gt;&gt; tf.transpose(t, perm=[1, 0]).numpy()</span>
<span class="sd">  array([[1, 4],</span>
<span class="sd">         [2, 5],</span>
<span class="sd">         [3, 6]], dtype=int32)</span>

<span class="sd">  If one component of `shape` is the special value -1, the size of that</span>
<span class="sd">  dimension is computed so that the total size remains constant.  In particular,</span>
<span class="sd">  a `shape` of `[-1]` flattens into 1-D.  At most one component of `shape` can</span>
<span class="sd">  be -1.</span>

<span class="sd">  &gt;&gt;&gt; t = [[1, 2, 3],</span>
<span class="sd">  ...      [4, 5, 6]]</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [-1])</span>
<span class="sd">  &lt;tf.Tensor: shape=(6,), dtype=int32,</span>
<span class="sd">    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [3, -1])</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 2],</span>
<span class="sd">           [3, 4],</span>
<span class="sd">           [5, 6]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [-1, 2])</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 2],</span>
<span class="sd">           [3, 4],</span>
<span class="sd">           [5, 6]], dtype=int32)&gt;</span>

<span class="sd">  `tf.reshape(t, [])` reshapes a tensor `t` with one element to a scalar.</span>

<span class="sd">  &gt;&gt;&gt; tf.reshape([7], []).numpy()</span>
<span class="sd">  7</span>

<span class="sd">  More examples:</span>

<span class="sd">  &gt;&gt;&gt; t = [1, 2, 3, 4, 5, 6, 7, 8, 9]</span>
<span class="sd">  &gt;&gt;&gt; print(tf.shape(t).numpy())</span>
<span class="sd">  [9]</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [3, 3])</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 2, 3],</span>
<span class="sd">           [4, 5, 6],</span>
<span class="sd">           [7, 8, 9]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; t = [[[1, 1], [2, 2]],</span>
<span class="sd">  ...      [[3, 3], [4, 4]]]</span>
<span class="sd">  &gt;&gt;&gt; print(tf.shape(t).numpy())</span>
<span class="sd">  [2 2 2]</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [2, 4])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 1, 2, 2],</span>
<span class="sd">           [3, 3, 4, 4]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; t = [[[1, 1, 1],</span>
<span class="sd">  ...       [2, 2, 2]],</span>
<span class="sd">  ...      [[3, 3, 3],</span>
<span class="sd">  ...       [4, 4, 4]],</span>
<span class="sd">  ...      [[5, 5, 5],</span>
<span class="sd">  ...       [6, 6, 6]]]</span>
<span class="sd">  &gt;&gt;&gt; print(tf.shape(t).numpy())</span>
<span class="sd">  [3 2 3]</span>
<span class="sd">  &gt;&gt;&gt; # Pass &#39;[-1]&#39; to flatten &#39;t&#39;.</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [-1])</span>
<span class="sd">  &lt;tf.Tensor: shape=(18,), dtype=int32,</span>
<span class="sd">    numpy=array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],</span>
<span class="sd">    dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; # -- Using -1 to infer the shape --</span>
<span class="sd">  &gt;&gt;&gt; # Here -1 is inferred to be 9:</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [2, -1])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 9), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 1, 1, 2, 2, 2, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; # -1 is inferred to be 2:</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [-1, 9])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 9), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 1, 1, 2, 2, 2, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; # -1 is inferred to be 3:</span>
<span class="sd">  &gt;&gt;&gt; tf.reshape(t, [ 2, -1, 3])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=</span>
<span class="sd">    array([[[1, 1, 1],</span>
<span class="sd">            [2, 2, 2],</span>
<span class="sd">            [3, 3, 3]],</span>
<span class="sd">           [[4, 4, 4],</span>
<span class="sd">            [5, 5, 5],</span>
<span class="sd">            [6, 6, 6]]], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A `Tensor`.</span>
<span class="sd">    shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.</span>
<span class="sd">      Defines the shape of the output tensor.</span>
<span class="sd">    name: Optional string. A name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `tensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="n">tensor_util</span><span class="o">.</span><span class="n">maybe_set_static_shape</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="fill"><a class="viewcode-back" href="../../../../index.html#tensorflow.fill">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;fill&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fill</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a tensor filled with a scalar value.</span>

<span class="sd">  This operation creates a tensor of shape `dims` and fills it with `value`.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; tf.fill([2, 3], 9)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="sd">  array([[9, 9, 9],</span>
<span class="sd">         [9, 9, 9]], dtype=int32)&gt;</span>

<span class="sd">  `tf.fill` evaluates at graph runtime and supports dynamic shapes based on</span>
<span class="sd">  other runtime `tf.Tensors`, unlike `tf.constant(value, shape=dims)`, which</span>
<span class="sd">  embeds the value as a `Const` node.</span>

<span class="sd">  Args:</span>
<span class="sd">    dims: A 1-D sequence of non-negative numbers. Represents the shape of the</span>
<span class="sd">      output `tf.Tensor`. Entries should be of type: `int32`, `int64`.</span>
<span class="sd">    value: A value to fill the returned `tf.Tensor`.</span>
<span class="sd">    name: Optional string. The name of the output `tf.Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` with shape `dims` and the same dtype as `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    InvalidArgumentError: `dims` contains negative entries.</span>
<span class="sd">    NotFoundError: `dims` contains non-integer entries.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Similar to `np.full`. In `numpy`, more parameters are supported. Passing a</span>
<span class="sd">  number argument as the shape (`np.full(5, value)`) is valid in `numpy` for</span>
<span class="sd">  specifying a 1-D shaped result, while TensorFlow does not support this syntax.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">tensor_util</span><span class="o">.</span><span class="n">maybe_set_static_shape</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="identity"><a class="viewcode-back" href="../../../../index.html#tensorflow.identity">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;identity&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return a Tensor with the same shape and contents as input.</span>

<span class="sd">  The return value is not the same Tensor as the original, but contains the same</span>
<span class="sd">  values.  This operation is fast when used on the same device.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; a = tf.constant([0.78])</span>
<span class="sd">  &gt;&gt;&gt; a_identity = tf.identity(a)</span>
<span class="sd">  &gt;&gt;&gt; a.numpy()</span>
<span class="sd">  array([0.78], dtype=float32)</span>
<span class="sd">  &gt;&gt;&gt; a_identity.numpy()</span>
<span class="sd">  array([0.78], dtype=float32)</span>

<span class="sd">  Calling `tf.identity` on a variable will make a Tensor that represents the</span>
<span class="sd">  value of that variable at the time it is called. This is equivalent to calling</span>
<span class="sd">  `&lt;variable&gt;.read_value()`.</span>

<span class="sd">  &gt;&gt;&gt; a = tf.Variable(5)</span>
<span class="sd">  &gt;&gt;&gt; a_identity = tf.identity(a)</span>
<span class="sd">  &gt;&gt;&gt; a.assign_add(1)</span>
<span class="sd">  &lt;tf.Variable ... shape=() dtype=int32, numpy=6&gt;</span>
<span class="sd">  &gt;&gt;&gt; a.numpy()</span>
<span class="sd">  6</span>
<span class="sd">  &gt;&gt;&gt; a_identity.numpy()</span>
<span class="sd">  5</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">identity</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">expand_composites</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">):</span>
    <span class="c1"># Make sure we get an input with handle data attached from resource</span>
    <span class="c1"># variables. Variables have correct handle data when graph building.</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="c1"># Propagate handle data for happier shape inference for resource variables.</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;_handle_data&quot;</span><span class="p">):</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">_handle_data</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">return</span> <span class="n">ret</span></div>


<span class="c1"># pylint: disable=redefined-builtin,protected-access</span>
<div class="viewcode-block" id="expand_dims"><a class="viewcode-back" href="../../../../index.html#tensorflow.expand_dims">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;expand_dims&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use the `axis` argument instead&quot;</span><span class="p">,</span> <span class="s2">&quot;dim&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Inserts a dimension of 1 into a tensor&#39;s shape.</span>

<span class="sd">  Given a tensor `input`, this operation inserts a dimension of 1 at the</span>
<span class="sd">  dimension index `axis` of `input`&#39;s shape. The dimension index `axis` starts</span>
<span class="sd">  at zero; if you specify a negative number for `axis` it is counted backward</span>
<span class="sd">  from the end.</span>

<span class="sd">  This operation is useful if you want to add a batch dimension to a single</span>
<span class="sd">  element. For example, if you have a single image of shape `[height, width,</span>
<span class="sd">  channels]`, you can make it a batch of 1 image with `expand_dims(image, 0)`,</span>
<span class="sd">  which will make the shape `[1, height, width, channels]`.</span>

<span class="sd">  Other examples:</span>

<span class="sd">  ```python</span>
<span class="sd">  # &#39;t&#39; is a tensor of shape [2]</span>
<span class="sd">  tf.shape(tf.expand_dims(t, 0))  # [1, 2]</span>
<span class="sd">  tf.shape(tf.expand_dims(t, 1))  # [2, 1]</span>
<span class="sd">  tf.shape(tf.expand_dims(t, -1))  # [2, 1]</span>

<span class="sd">  # &#39;t2&#39; is a tensor of shape [2, 3, 5]</span>
<span class="sd">  tf.shape(tf.expand_dims(t2, 0))  # [1, 2, 3, 5]</span>
<span class="sd">  tf.shape(tf.expand_dims(t2, 2))  # [2, 3, 1, 5]</span>
<span class="sd">  tf.shape(tf.expand_dims(t2, 3))  # [2, 3, 5, 1]</span>
<span class="sd">  ```</span>

<span class="sd">  This operation requires that:</span>

<span class="sd">  `-1-input.dims() &lt;= dim &lt;= input.dims()`</span>

<span class="sd">  This operation is related to `squeeze()`, which removes dimensions of</span>
<span class="sd">  size 1.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`.</span>
<span class="sd">    axis: 0-D (scalar). Specifies the dimension index at which to expand the</span>
<span class="sd">      shape of `input`. Must be in the range `[-rank(input) - 1, rank(input)]`.</span>
<span class="sd">    name: The name of the output `Tensor` (optional).</span>
<span class="sd">    dim: 0-D (scalar). Equivalent to `axis`, to be deprecated.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with the same data as `input`, but its shape has an additional</span>
<span class="sd">    dimension of size 1 added.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if either both or neither of `dim` and `axis` are specified.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must specify an axis argument to tf.expand_dims()&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">expand_dims_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;expand_dims&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">expand_dims_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a tensor with an additional dimension inserted at index `axis`.</span>

<span class="sd">  Given a tensor `input`, this operation inserts a dimension of size 1 at the</span>
<span class="sd">  dimension index `axis` of `input`&#39;s shape. The dimension index `axis` starts</span>
<span class="sd">  at zero; if you specify a negative number for `axis` it is counted backward</span>
<span class="sd">  from the end.</span>

<span class="sd">  This operation is useful if you want to add a batch dimension to a single</span>
<span class="sd">  element. For example, if you have a single image of shape `[height, width,</span>
<span class="sd">  channels]`, you can make it a batch of one image with `expand_dims(image, 0)`,</span>
<span class="sd">  which will make the shape `[1, height, width, channels]`.</span>

<span class="sd">  Examples:</span>

<span class="sd">  &gt;&gt;&gt; t = [[1, 2, 3],[4, 5, 6]] # shape [2, 3]</span>

<span class="sd">  &gt;&gt;&gt; tf.expand_dims(t, 0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=</span>
<span class="sd">  array([[[1, 2, 3],</span>
<span class="sd">          [4, 5, 6]]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; tf.expand_dims(t, 1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 1, 3), dtype=int32, numpy=</span>
<span class="sd">  array([[[1, 2, 3]],</span>
<span class="sd">         [[4, 5, 6]]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; tf.expand_dims(t, 2)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3, 1), dtype=int32, numpy=</span>
<span class="sd">  array([[[1],</span>
<span class="sd">          [2],</span>
<span class="sd">          [3]],</span>
<span class="sd">         [[4],</span>
<span class="sd">          [5],</span>
<span class="sd">          [6]]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; tf.expand_dims(t, -1) # Last dimension index. In this case, same as 2.</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3, 1), dtype=int32, numpy=</span>
<span class="sd">  array([[[1],</span>
<span class="sd">          [2],</span>
<span class="sd">          [3]],</span>
<span class="sd">         [[4],</span>
<span class="sd">          [5],</span>
<span class="sd">          [6]]], dtype=int32)&gt;</span>

<span class="sd">  This operation is related to:</span>

<span class="sd">  *   `tf.squeeze`, which removes dimensions of size 1.</span>
<span class="sd">  *   `tf.reshape`, which provides more flexible reshaping capability</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`.</span>
<span class="sd">    axis: Integer specifying the dimension index at which to expand the</span>
<span class="sd">      shape of `input`. Given an input of D dimensions, `axis` must be in range</span>
<span class="sd">      `[-(D+1), D]` (inclusive).</span>
<span class="sd">    name: Optional string. The name of the output `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor with the same data as `input`, with an additional dimension</span>
<span class="sd">    inserted at the index specified by `axis`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `axis` is not specified.</span>
<span class="sd">    InvalidArgumentError: If `axis` is out of range `[-(D+1), D]`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: enable=redefined-builtin,protected-access</span>


<span class="c1"># Aliases for some automatically-generated names.</span>
<span class="c1"># pylint: disable=protected-access</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="s2">&quot;2016-11-30&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;This op will be removed after the deprecation date. &quot;</span>
                        <span class="s2">&quot;Please switch to tf.setdiff1d().&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">listdiff</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">list_diff</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out_idx</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">listdiff</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">list_diff</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">listdiff</span><span class="o">.</span><span class="vm">__doc__</span>

<span class="c1"># pylint: enable=protected-access</span>


<span class="c1"># pylint: disable=undefined-variable</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="s2">&quot;2018-11-30&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;This op will be removed after the deprecation date. &quot;</span>
                        <span class="s2">&quot;Please switch to tf.sets.difference().&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;setdiff1d&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">setdiff1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">index_dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the difference between two lists of numbers or strings.</span>

<span class="sd">  Given a list x and a list y, this operation returns a list out that</span>
<span class="sd">  represents all values that are in x but not in y. The returned list</span>
<span class="sd">  out is sorted in the same order that the numbers appear in x</span>
<span class="sd">  (duplicates are preserved). This operation also returns a list idx</span>
<span class="sd">  that represents the position of each out element in x.</span>

<span class="sd">  In other words:</span>

<span class="sd">  ```python</span>
<span class="sd">  out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1]</span>
<span class="sd">  ```</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; x = [1, 2, 3, 4, 5, 6]</span>
<span class="sd">  &gt;&gt;&gt; y = [1, 3, 5]</span>
<span class="sd">  &gt;&gt;&gt; setdiff1d(x,y)</span>
<span class="sd">  ListDiff(out=&lt;tf.Tensor: id=2, shape=(3,), dtype=int32,</span>
<span class="sd">  numpy=array([2, 4, 6], dtype=int32)&gt;, idx=&lt;tf.Tensor: id=3,</span>
<span class="sd">  shape=(3,), dtype=int32, numpy=array([1, 3, 5], dtype=int32)&gt;)</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor. 1-D. Values to keep.</span>
<span class="sd">    y: A Tensor. Must have the same type as x. 1-D. Values to remove.</span>
<span class="sd">    out_idx: An optional tf.DType from: tf.int32, tf.int64. Defaults to</span>
<span class="sd">      tf.int32.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of Tensor objects (out, idx).</span>
<span class="sd">    out: A Tensor. Has the same type as x.</span>
<span class="sd">    idx: A Tensor of type out_idx.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">list_diff</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">index_dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">setdiff1d</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">list_diff</span><span class="o">.</span><span class="vm">__doc__</span>


<div class="viewcode-block" id="broadcast_dynamic_shape"><a class="viewcode-back" href="../../../../index.html#tensorflow.broadcast_dynamic_shape">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;broadcast_dynamic_shape&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">broadcast_dynamic_shape</span><span class="p">(</span><span class="n">shape_x</span><span class="p">,</span> <span class="n">shape_y</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the shape of a broadcast given symbolic shapes.</span>

<span class="sd">  When shape_x and shape_y are Tensors representing shapes (i.e. the result of</span>
<span class="sd">  calling tf.shape on another Tensor) this computes a Tensor which is the shape</span>
<span class="sd">  of the result of a broadcasting op applied in tensors of shapes shape_x and</span>
<span class="sd">  shape_y.</span>

<span class="sd">  For example, if shape_x is [1, 2, 3] and shape_y is [5, 1, 3], the result is a</span>
<span class="sd">  Tensor whose value is [5, 2, 3].</span>

<span class="sd">  This is useful when validating the result of a broadcasting operation when the</span>
<span class="sd">  tensors do not have statically known shapes.</span>

<span class="sd">  Args:</span>
<span class="sd">    shape_x: A rank 1 integer `Tensor`, representing the shape of x.</span>
<span class="sd">    shape_y: A rank 1 integer `Tensor`, representing the shape of y.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A rank 1 integer `Tensor` representing the broadcasted shape.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">broadcast_args</span><span class="p">(</span><span class="n">shape_x</span><span class="p">,</span> <span class="n">shape_y</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_static_shape"><a class="viewcode-back" href="../../../../index.html#tensorflow.broadcast_static_shape">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;broadcast_static_shape&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">broadcast_static_shape</span><span class="p">(</span><span class="n">shape_x</span><span class="p">,</span> <span class="n">shape_y</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the shape of a broadcast given known shapes.</span>

<span class="sd">  When shape_x and shape_y are fully known TensorShapes this computes a</span>
<span class="sd">  TensorShape which is the shape of the result of a broadcasting op applied in</span>
<span class="sd">  tensors of shapes shape_x and shape_y.</span>

<span class="sd">  For example, if shape_x is [1, 2, 3] and shape_y is [5, 1, 3], the result is a</span>
<span class="sd">  TensorShape whose value is [5, 2, 3].</span>

<span class="sd">  This is useful when validating the result of a broadcasting operation when the</span>
<span class="sd">  tensors have statically known shapes.</span>

<span class="sd">  Args:</span>
<span class="sd">    shape_x: A `TensorShape`</span>
<span class="sd">    shape_y: A `TensorShape`</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `TensorShape` representing the broadcasted shape.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the two shapes can not be broadcasted.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">common_shapes</span><span class="o">.</span><span class="n">broadcast_shape</span><span class="p">(</span><span class="n">shape_x</span><span class="p">,</span> <span class="n">shape_y</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">shape_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns the shape of a tensor.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.size`.</span>

<span class="sd">  This operation returns a 1-D integer tensor representing the shape of `input`.</span>
<span class="sd">  This represents the minimal set of known information at definition time.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])</span>
<span class="sd">  &gt;&gt;&gt; tf.shape(t)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 2, 3], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.shape(t).numpy()</span>
<span class="sd">  array([2, 2, 3], dtype=int32)</span>

<span class="sd">  Note: When using symbolic tensors, such as when using the Keras functional</span>
<span class="sd">  API, tf.shape() will return the shape of the symbolic tensor.</span>

<span class="sd">  &gt;&gt;&gt; a = tf.keras.layers.Input((None, 10))</span>
<span class="sd">  &gt;&gt;&gt; tf.shape(a)</span>
<span class="sd">  &lt;tf.Tensor ... shape=(3,) dtype=int32&gt;</span>

<span class="sd">  In these cases, using `tf.Tensor.shape` will return more informative results.</span>

<span class="sd">  &gt;&gt;&gt; a.shape</span>
<span class="sd">  TensorShape([None, None, 10])</span>

<span class="sd">  `tf.shape` and `Tensor.shape` should be identical in eager mode.  Within</span>
<span class="sd">  `tf.function` or within a `compat.v1` context, not all dimensions may be</span>
<span class="sd">  known until execution time.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    out_type: (Optional) The specified output type of the operation (`int32` or</span>
<span class="sd">      `int64`). Defaults to `tf.int32`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `out_type`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">out_type</span><span class="p">)</span>


<div class="viewcode-block" id="shape"><a class="viewcode-back" href="../../../../index.html#tensorflow.shape">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;shape&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns the shape of a tensor.</span>

<span class="sd">  This operation returns a 1-D integer tensor representing the shape of `input`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])</span>
<span class="sd">  tf.shape(t)  # [2, 2, 3]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    out_type: (Optional) The specified output type of the operation (`int32`</span>
<span class="sd">    or `int64`). Defaults to `tf.int32`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `out_type`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">shape_internal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">shape_internal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns the shape of a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    optimize: if true, encode the shape as a constant when possible.</span>
<span class="sd">    out_type: (Optional) The specified output type of the operation (`int32` or</span>
<span class="sd">      `int64`). Defaults to tf.int32.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `out_type`.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Shape&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensorValue</span><span class="p">)):</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span> <span class="n">out_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">optimize</span> <span class="ow">and</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
          <span class="k">return</span> <span class="n">constant</span><span class="p">(</span><span class="n">input_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>


<div class="viewcode-block" id="shape_n"><a class="viewcode-back" href="../../../../index.html#tensorflow.shape_n">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;shape_n&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shape_n</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns shape of tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A list of at least 1 `Tensor` object with the same type.</span>
<span class="sd">    out_type: The specified output type of the operation (`int32` or `int64`).</span>
<span class="sd">      Defaults to `tf.int32`(optional).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list with the same length as `input` of `Tensor` objects with</span>
<span class="sd">      type `out_type`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">shape_n</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">size_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns the size of a tensor.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.shape`.</span>

<span class="sd">  Returns a 0-D `Tensor` representing the number of elements in `input`</span>
<span class="sd">  of type `out_type`. Defaults to tf.int32.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])</span>
<span class="sd">  &gt;&gt;&gt; tf.size(t)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=int32, numpy=12&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    out_type: (Optional) The specified non-quantized numeric output type of the</span>
<span class="sd">      operation. Defaults to `tf.int32`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `out_type`. Defaults to `tf.int32`.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.size()</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">return</span> <span class="n">size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">out_type</span><span class="p">)</span>


<div class="viewcode-block" id="size"><a class="viewcode-back" href="../../../../index.html#tensorflow.size">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;size&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns the size of a tensor.</span>

<span class="sd">  Returns a 0-D `Tensor` representing the number of elements in `input`</span>
<span class="sd">  of type `out_type`. Defaults to tf.int32.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])</span>
<span class="sd">  tf.size(t)  # 12</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    out_type: (Optional) The specified non-quantized numeric output type of the</span>
<span class="sd">      operation. Defaults to `tf.int32`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `out_type`. Defaults to `tf.int32`.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.size()</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">size_internal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">size_internal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin,protected-access</span>
  <span class="sd">&quot;&quot;&quot;Returns the size of a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    optimize: if true, encode the size as a constant when possible.</span>
<span class="sd">    out_type: (Optional) The specified non-quantized numeric output type of the</span>
<span class="sd">      operation. Defaults to `tf.int32`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `out_type`. Defaults to `tf.int32`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">)</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
          <span class="nb">input</span><span class="p">,</span>
          <span class="p">(</span><span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensorValue</span><span class="p">))):</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">np_out_type</span> <span class="o">=</span> <span class="n">out_type</span><span class="o">.</span><span class="n">as_numpy_dtype</span>
    <span class="n">num_elements</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np_out_type</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">num_elements</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Size&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensorValue</span><span class="p">)):</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span>
          <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span> <span class="n">out_type</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
      <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">optimize</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
          <span class="k">return</span> <span class="n">constant</span><span class="p">(</span><span class="n">input_shape</span><span class="o">.</span><span class="n">num_elements</span><span class="p">(),</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">dims</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>


<div class="viewcode-block" id="rank"><a class="viewcode-back" href="../../../../index.html#tensorflow.rank">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;rank&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">rank</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns the rank of a tensor.</span>

<span class="sd">  Returns a 0-D `int32` `Tensor` representing the rank of `input`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  # shape of tensor &#39;t&#39; is [2, 2, 3]</span>
<span class="sd">  t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])</span>
<span class="sd">  tf.rank(t)  # 3</span>
<span class="sd">  ```</span>

<span class="sd">  **Note**: The rank of a tensor is not the same as the rank of a matrix. The</span>
<span class="sd">  rank of a tensor is the number of indices required to uniquely select each</span>
<span class="sd">  element of the tensor. Rank is also known as &quot;order&quot;, &quot;degree&quot;, or &quot;ndims.&quot;</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `int32`.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.ndim</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">rank_internal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">rank_internal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Returns the rank of a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or `SparseTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    optimize: if true, encode the rank as a constant when possible.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `int32`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Rank&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensorValue</span><span class="p">)):</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
      <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">optimize</span> <span class="ow">and</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">constant</span><span class="p">(</span><span class="n">input_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">_SLICE_TYPE_ERROR</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;Only integers, slices (`:`), ellipsis (`...`), &quot;</span>
    <span class="s2">&quot;tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid &quot;</span>
    <span class="s2">&quot;indices&quot;</span><span class="p">)</span>

<span class="n">_SUPPORTED_SLICE_DTYPES</span> <span class="o">=</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32_ref</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                           <span class="n">dtypes</span><span class="o">.</span><span class="n">int64_ref</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_index</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Check if a given value is a valid index into a tensor.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)):</span>
    <span class="k">return</span>

  <span class="c1"># Optimistic check. Assumptions:</span>
  <span class="c1"># * any object with a dtype is supported</span>
  <span class="c1"># * any object with a dtype has a sizeable shape attribute.</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_SLICE_DTYPES</span> <span class="ow">or</span>
      <span class="n">idx</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># TODO(slebedev): IndexError seems more appropriate here, but it</span>
    <span class="c1"># will break `_slice_helper` contract.</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">_SLICE_TYPE_ERROR</span> <span class="o">+</span> <span class="s2">&quot;, got </span><span class="si">{!r}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_is_undefined_dimension</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)</span> <span class="ow">and</span> <span class="n">d</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_slice_helper</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">slice_spec</span><span class="p">,</span> <span class="n">var</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Overload for Tensor.__getitem__.</span>

<span class="sd">  This operation extracts the specified region from the tensor.</span>
<span class="sd">  The notation is similar to NumPy with the restriction that</span>
<span class="sd">  currently only support basic indexing. That means that</span>
<span class="sd">  using a non-scalar tensor as input is not currently allowed.</span>

<span class="sd">  Some useful examples:</span>

<span class="sd">  ```python</span>
<span class="sd">  # Strip leading and trailing 2 elements</span>
<span class="sd">  foo = tf.constant([1,2,3,4,5,6])</span>
<span class="sd">  print(foo[2:-2].eval())  # =&gt; [3,4]</span>

<span class="sd">  # Skip every other row and reverse the order of the columns</span>
<span class="sd">  foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">  print(foo[::2,::-1].eval())  # =&gt; [[3,2,1], [9,8,7]]</span>

<span class="sd">  # Use scalar tensors as indices on both dimensions</span>
<span class="sd">  print(foo[tf.constant(0), tf.constant(2)].eval())  # =&gt; 3</span>

<span class="sd">  # Insert another dimension</span>
<span class="sd">  foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">  print(foo[tf.newaxis, :, :].eval()) # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]</span>
<span class="sd">  print(foo[:, tf.newaxis, :].eval()) # =&gt; [[[1,2,3]], [[4,5,6]], [[7,8,9]]]</span>
<span class="sd">  print(foo[:, :, tf.newaxis].eval()) # =&gt; [[[1],[2],[3]], [[4],[5],[6]],</span>
<span class="sd">  [[7],[8],[9]]]</span>

<span class="sd">  # Ellipses (3 equivalent operations)</span>
<span class="sd">  foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">  print(foo[tf.newaxis, :, :].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]</span>
<span class="sd">  print(foo[tf.newaxis, ...].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]</span>
<span class="sd">  print(foo[tf.newaxis].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]</span>

<span class="sd">  # Masks</span>
<span class="sd">  foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">  print(foo[foo &gt; 2].eval())  # =&gt; [3, 4, 5, 6, 7, 8, 9]</span>
<span class="sd">  ```</span>

<span class="sd">  Notes:</span>
<span class="sd">    - `tf.newaxis` is `None` as in NumPy.</span>
<span class="sd">    - An implicit ellipsis is placed at the end of the `slice_spec`</span>
<span class="sd">    - NumPy advanced indexing is currently not supported.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: An ops.Tensor object.</span>
<span class="sd">    slice_spec: The arguments to Tensor.__getitem__.</span>
<span class="sd">    var: In the case of variable slice assignment, the Variable object to slice</span>
<span class="sd">      (i.e. tensor is the read-only view of this variable).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The appropriate slice of &quot;tensor&quot;, based on &quot;slice_spec&quot;.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If a slice range is negative size.</span>
<span class="sd">    TypeError: If the slice indices aren&#39;t int, slice, ellipsis,</span>
<span class="sd">      tf.newaxis or scalar int32/int64 tensors.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_spec</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">or</span> \
  <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_spec</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">slice_spec</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span> <span class="ow">or</span> \
  <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_spec</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">slice_spec</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">boolean_mask</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">slice_spec</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_spec</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">slice_spec</span> <span class="o">=</span> <span class="p">[</span><span class="n">slice_spec</span><span class="p">]</span>

  <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="n">new_axis_mask</span><span class="p">,</span> <span class="n">shrink_axis_mask</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
  <span class="n">begin_mask</span><span class="p">,</span> <span class="n">end_mask</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
  <span class="n">ellipsis_mask</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">slice_spec</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">_BaseSlice</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">s</span><span class="o">.</span><span class="n">start</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_undefined_dimension</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">start</span><span class="p">):</span>
        <span class="n">_check_index</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">start</span><span class="p">)</span>
        <span class="n">begin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">start</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">begin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">begin_mask</span> <span class="o">|=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">index</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">s</span><span class="o">.</span><span class="n">stop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_undefined_dimension</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">stop</span><span class="p">):</span>
        <span class="n">_check_index</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">stop</span><span class="p">)</span>
        <span class="n">end</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">stop</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">end</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">end_mask</span> <span class="o">|=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">index</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">s</span><span class="o">.</span><span class="n">step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_undefined_dimension</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
        <span class="n">_check_index</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
        <span class="n">strides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">strides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">s</span> <span class="ow">is</span> <span class="bp">Ellipsis</span><span class="p">:</span>
      <span class="n">begin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">end</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">strides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">ellipsis_mask</span> <span class="o">|=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">index</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">s</span> <span class="ow">is</span> <span class="n">newaxis</span><span class="p">:</span>
      <span class="n">begin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">end</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">strides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">new_axis_mask</span> <span class="o">|=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">index</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">_check_index</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
      <span class="n">begin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
      <span class="n">end</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">strides</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">shrink_axis_mask</span> <span class="o">|=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">index</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="c1"># stack possibly involves no tensors, so we must use op_scope correct graph.</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
      <span class="kc">None</span><span class="p">,</span>
      <span class="s2">&quot;strided_slice&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span> <span class="o">+</span> <span class="n">begin</span> <span class="o">+</span> <span class="n">end</span> <span class="o">+</span> <span class="n">strides</span><span class="p">,</span>
      <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">begin</span><span class="p">:</span>
      <span class="n">packed_begin</span><span class="p">,</span> <span class="n">packed_end</span><span class="p">,</span> <span class="n">packed_strides</span> <span class="o">=</span> <span class="p">(</span><span class="n">stack</span><span class="p">(</span><span class="n">begin</span><span class="p">),</span> <span class="n">stack</span><span class="p">(</span><span class="n">end</span><span class="p">),</span>
                                                  <span class="n">stack</span><span class="p">(</span><span class="n">strides</span><span class="p">))</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">packed_begin</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span> <span class="ow">or</span>
          <span class="n">packed_end</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span> <span class="ow">or</span>
          <span class="n">packed_strides</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">packed_begin</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
          <span class="n">packed_begin</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">packed_begin</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">packed_end</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
          <span class="n">packed_end</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">packed_end</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">packed_strides</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
          <span class="n">packed_strides</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">packed_strides</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">var_empty</span> <span class="o">=</span> <span class="n">constant</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">packed_begin</span> <span class="o">=</span> <span class="n">packed_end</span> <span class="o">=</span> <span class="n">packed_strides</span> <span class="o">=</span> <span class="n">var_empty</span>
    <span class="k">return</span> <span class="n">strided_slice</span><span class="p">(</span>
        <span class="n">tensor</span><span class="p">,</span>
        <span class="n">packed_begin</span><span class="p">,</span>
        <span class="n">packed_end</span><span class="p">,</span>
        <span class="n">packed_strides</span><span class="p">,</span>
        <span class="n">begin_mask</span><span class="o">=</span><span class="n">begin_mask</span><span class="p">,</span>
        <span class="n">end_mask</span><span class="o">=</span><span class="n">end_mask</span><span class="p">,</span>
        <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="n">shrink_axis_mask</span><span class="p">,</span>
        <span class="n">new_axis_mask</span><span class="o">=</span><span class="n">new_axis_mask</span><span class="p">,</span>
        <span class="n">ellipsis_mask</span><span class="o">=</span><span class="n">ellipsis_mask</span><span class="p">,</span>
        <span class="n">var</span><span class="o">=</span><span class="n">var</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: disable=undefined-variable,protected-access,redefined-outer-name</span>
<div class="viewcode-block" id="slice"><a class="viewcode-back" href="../../../../index.html#tensorflow.slice">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;slice&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">slice</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Extracts a slice from a tensor.</span>

<span class="sd">  This operation extracts a slice of size `size` from a tensor `input_` starting</span>
<span class="sd">  at the location specified by `begin`. The slice `size` is represented as a</span>
<span class="sd">  tensor shape, where `size[i]` is the number of elements of the &#39;i&#39;th dimension</span>
<span class="sd">  of `input_` that you want to slice. The starting location (`begin`) for the</span>
<span class="sd">  slice is represented as an offset in each dimension of `input_`. In other</span>
<span class="sd">  words, `begin[i]` is the offset into the i&#39;th dimension of `input_` that you</span>
<span class="sd">  want to slice from.</span>

<span class="sd">  Note that `tf.Tensor.__getitem__` is typically a more pythonic way to</span>
<span class="sd">  perform slices, as it allows you to write `foo[3:7, :-2]` instead of</span>
<span class="sd">  `tf.slice(foo, [3, 0], [4, foo.get_shape()[1]-2])`.</span>

<span class="sd">  `begin` is zero-based; `size` is one-based. If `size[i]` is -1,</span>
<span class="sd">  all remaining elements in dimension i are included in the</span>
<span class="sd">  slice. In other words, this is equivalent to setting:</span>

<span class="sd">  `size[i] = input_.dim_size(i) - begin[i]`</span>

<span class="sd">  This operation requires that:</span>

<span class="sd">  `0 &lt;= begin[i] &lt;= begin[i] + size[i] &lt;= Di  for i in [0, n]`</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  t = tf.constant([[[1, 1, 1], [2, 2, 2]],</span>
<span class="sd">                   [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">                   [[5, 5, 5], [6, 6, 6]]])</span>
<span class="sd">  tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]</span>
<span class="sd">  tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],</span>
<span class="sd">                                     #   [4, 4, 4]]]</span>
<span class="sd">  tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],</span>
<span class="sd">                                     #  [[5, 5, 5]]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_: A `Tensor`.</span>
<span class="sd">    begin: An `int32` or `int64` `Tensor`.</span>
<span class="sd">    size: An `int32` or `int64` `Tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` the same type as `input_`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">_slice</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="c1"># pylint: disable=invalid-name</span>
<div class="viewcode-block" id="strided_slice"><a class="viewcode-back" href="../../../../index.html#tensorflow.strided_slice">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;strided_slice&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">strided_slice</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span>
                  <span class="n">begin</span><span class="p">,</span>
                  <span class="n">end</span><span class="p">,</span>
                  <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">begin_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">end_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">ellipsis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">new_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">var</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Extracts a strided slice of a tensor (generalized python array indexing).</span>

<span class="sd">  **Instead of calling this op directly most users will want to use the</span>
<span class="sd">  NumPy-style slicing syntax (e.g. `tensor[..., 3:4:-1, tf.newaxis, 3]`), which</span>
<span class="sd">  is supported via `tf.Tensor.__getitem__` and `tf.Variable.__getitem__`.**</span>
<span class="sd">  The interface of this op is a low-level encoding of the slicing syntax.</span>

<span class="sd">  Roughly speaking, this op extracts a slice of size `(end-begin)/stride`</span>
<span class="sd">  from the given `input_` tensor. Starting at the location specified by `begin`</span>
<span class="sd">  the slice continues by adding `stride` to the index until all dimensions are</span>
<span class="sd">  not less than `end`.</span>
<span class="sd">  Note that a stride can be negative, which causes a reverse slice.</span>

<span class="sd">  Given a Python slice `input[spec0, spec1, ..., specn]`,</span>
<span class="sd">  this function will be called as follows.</span>

<span class="sd">  `begin`, `end`, and `strides` will be vectors of length n.</span>
<span class="sd">  n in general is not equal to the rank of the `input_` tensor.</span>

<span class="sd">  In each mask field (`begin_mask`, `end_mask`, `ellipsis_mask`,</span>
<span class="sd">  `new_axis_mask`, `shrink_axis_mask`) the ith bit will correspond to</span>
<span class="sd">  the ith spec.</span>

<span class="sd">  If the ith bit of `begin_mask` is set, `begin[i]` is ignored and</span>
<span class="sd">  the fullest possible range in that dimension is used instead.</span>
<span class="sd">  `end_mask` works analogously, except with the end range.</span>

<span class="sd">  `foo[5:,:,:3]` on a 7x8x9 tensor is equivalent to `foo[5:7,0:8,0:3]`.</span>
<span class="sd">  `foo[::-1]` reverses a tensor with shape 8.</span>

<span class="sd">  If the ith bit of `ellipsis_mask` is set, as many unspecified dimensions</span>
<span class="sd">  as needed will be inserted between other dimensions. Only one</span>
<span class="sd">  non-zero bit is allowed in `ellipsis_mask`.</span>

<span class="sd">  For example `foo[3:5,...,4:5]` on a shape 10x3x3x10 tensor is</span>
<span class="sd">  equivalent to `foo[3:5,:,:,4:5]` and</span>
<span class="sd">  `foo[3:5,...]` is equivalent to `foo[3:5,:,:,:]`.</span>

<span class="sd">  If the ith bit of `new_axis_mask` is set, then `begin`,</span>
<span class="sd">  `end`, and `stride` are ignored and a new length 1 dimension is</span>
<span class="sd">  added at this point in the output tensor.</span>

<span class="sd">  For example,</span>
<span class="sd">  `foo[:4, tf.newaxis, :2]` would produce a shape `(4, 1, 2)` tensor.</span>

<span class="sd">  If the ith bit of `shrink_axis_mask` is set, it implies that the ith</span>
<span class="sd">  specification shrinks the dimensionality by 1, taking on the value at index</span>
<span class="sd">  `begin[i]`. `end[i]` and `strides[i]` are ignored in this case. For example in</span>
<span class="sd">  Python one might do `foo[:, 3, :]` which would result in `shrink_axis_mask`</span>
<span class="sd">  equal to 2.</span>


<span class="sd">  NOTE: `begin` and `end` are zero-indexed.</span>
<span class="sd">  `strides` entries must be non-zero.</span>


<span class="sd">  ```python</span>
<span class="sd">  t = tf.constant([[[1, 1, 1], [2, 2, 2]],</span>
<span class="sd">                   [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">                   [[5, 5, 5], [6, 6, 6]]])</span>
<span class="sd">  tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])  # [[[3, 3, 3]]]</span>
<span class="sd">  tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])  # [[[3, 3, 3],</span>
<span class="sd">                                                        #   [4, 4, 4]]]</span>
<span class="sd">  tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])  # [[[4, 4, 4],</span>
<span class="sd">                                                           #   [3, 3, 3]]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_: A `Tensor`.</span>
<span class="sd">    begin: An `int32` or `int64` `Tensor`.</span>
<span class="sd">    end: An `int32` or `int64` `Tensor`.</span>
<span class="sd">    strides: An `int32` or `int64` `Tensor`.</span>
<span class="sd">    begin_mask: An `int32` mask.</span>
<span class="sd">    end_mask: An `int32` mask.</span>
<span class="sd">    ellipsis_mask: An `int32` mask.</span>
<span class="sd">    new_axis_mask: An `int32` mask.</span>
<span class="sd">    shrink_axis_mask: An `int32` mask.</span>
<span class="sd">    var: The variable corresponding to `input_` or None</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">strides</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">ones_like</span><span class="p">(</span><span class="n">begin</span><span class="p">)</span>

  <span class="n">op</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span>
      <span class="nb">input</span><span class="o">=</span><span class="n">input_</span><span class="p">,</span>
      <span class="n">begin</span><span class="o">=</span><span class="n">begin</span><span class="p">,</span>
      <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">,</span>
      <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">begin_mask</span><span class="o">=</span><span class="n">begin_mask</span><span class="p">,</span>
      <span class="n">end_mask</span><span class="o">=</span><span class="n">end_mask</span><span class="p">,</span>
      <span class="n">ellipsis_mask</span><span class="o">=</span><span class="n">ellipsis_mask</span><span class="p">,</span>
      <span class="n">new_axis_mask</span><span class="o">=</span><span class="n">new_axis_mask</span><span class="p">,</span>
      <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="n">shrink_axis_mask</span><span class="p">)</span>

  <span class="n">parent_name</span> <span class="o">=</span> <span class="n">name</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">var</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">)):</span>

    <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Closure that holds all the arguments to create an assignment.&quot;&quot;&quot;</span>

      <span class="k">if</span> <span class="n">var</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Sliced assignment is only supported for variables&quot;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">name</span> <span class="o">=</span> <span class="n">parent_name</span> <span class="o">+</span> <span class="s2">&quot;_assign&quot;</span>

        <span class="k">return</span> <span class="n">var</span><span class="o">.</span><span class="n">_strided_slice_assign</span><span class="p">(</span>
            <span class="n">begin</span><span class="o">=</span><span class="n">begin</span><span class="p">,</span>
            <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">,</span>
            <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">val</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="n">begin_mask</span><span class="o">=</span><span class="n">begin_mask</span><span class="p">,</span>
            <span class="n">end_mask</span><span class="o">=</span><span class="n">end_mask</span><span class="p">,</span>
            <span class="n">ellipsis_mask</span><span class="o">=</span><span class="n">ellipsis_mask</span><span class="p">,</span>
            <span class="n">new_axis_mask</span><span class="o">=</span><span class="n">new_axis_mask</span><span class="p">,</span>
            <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="n">shrink_axis_mask</span><span class="p">)</span>

    <span class="n">op</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">assign</span>
  <span class="k">return</span> <span class="n">op</span></div>


<span class="k">def</span> <span class="nf">_SliceHelperVar</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">slice_spec</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a slice helper object given a variable.</span>

<span class="sd">  This allows creating a sub-tensor from part of the current contents</span>
<span class="sd">  of a variable. See `tf.Tensor.__getitem__` for detailed examples</span>
<span class="sd">  of slicing.</span>

<span class="sd">  This function in addition also allows assignment to a sliced range.</span>
<span class="sd">  This is similar to `__setitem__` functionality in Python. However,</span>
<span class="sd">  the syntax is different so that the user can capture the assignment</span>
<span class="sd">  operation for grouping or passing to `sess.run()`.</span>
<span class="sd">  For example,</span>

<span class="sd">  ```python</span>
<span class="sd">  import tensorflow as tf</span>
<span class="sd">  A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)</span>
<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">    sess.run(tf.compat.v1.global_variables_initializer())</span>
<span class="sd">    print(sess.run(A[:2, :2]))  # =&gt; [[1,2], [4,5]]</span>

<span class="sd">    op = A[:2,:2].assign(22. * tf.ones((2, 2)))</span>
<span class="sd">    print(sess.run(op))  # =&gt; [[22, 22, 3], [22, 22, 6], [7,8,9]]</span>
<span class="sd">  ```</span>

<span class="sd">  Note that assignments currently do not support NumPy broadcasting</span>
<span class="sd">  semantics.</span>

<span class="sd">  Args:</span>
<span class="sd">    var: An `ops.Variable` object.</span>
<span class="sd">    slice_spec: The arguments to `Tensor.__getitem__`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The appropriate slice of &quot;tensor&quot;, based on &quot;slice_spec&quot;.</span>
<span class="sd">    As an operator. The operator also has a `assign()` method</span>
<span class="sd">    that can be used to generate an assignment operator.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If a slice range is negative size.</span>
<span class="sd">    TypeError: TypeError: If the slice indices aren&#39;t int, slice,</span>
<span class="sd">      ellipsis, tf.newaxis or int32/int64 tensors.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">return</span> <span class="n">_slice_helper</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">value</span><span class="p">(),</span> <span class="n">slice_spec</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>


<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__getitem__&quot;</span><span class="p">,</span> <span class="n">_slice_helper</span><span class="p">)</span>


<div class="viewcode-block" id="parallel_stack"><a class="viewcode-back" href="../../../../index.html#tensorflow.parallel_stack">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;parallel_stack&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">parallel_stack</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;parallel_stack&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor in parallel.</span>

<span class="sd">  Requires that the shape of inputs be known at graph construction time.</span>

<span class="sd">  Packs the list of tensors in `values` into a tensor with rank one higher than</span>
<span class="sd">  each tensor in `values`, by packing them along the first dimension.</span>
<span class="sd">  Given a list of length `N` of tensors of shape `(A, B, C)`; the `output`</span>
<span class="sd">  tensor will have the shape `(N, A, B, C)`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([1, 4])</span>
<span class="sd">  y = tf.constant([2, 5])</span>
<span class="sd">  z = tf.constant([3, 6])</span>
<span class="sd">  tf.parallel_stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]]</span>
<span class="sd">  ```</span>

<span class="sd">  The difference between `stack` and `parallel_stack` is that `stack` requires</span>
<span class="sd">  all the inputs be computed before the operation will begin but doesn&#39;t require</span>
<span class="sd">  that the input shapes be known during graph construction.</span>

<span class="sd">  `parallel_stack` will copy pieces of the input into the output as they become</span>
<span class="sd">  available, in some situations this can provide a performance benefit.</span>

<span class="sd">  Unlike `stack`, `parallel_stack` does NOT support backpropagation.</span>

<span class="sd">  This is the opposite of unstack.  The numpy equivalent is</span>

<span class="sd">      tf.parallel_stack([x, y, z]) = np.asarray([x, y, z])</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of `Tensor` objects with the same shape and type.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    output: A stacked `Tensor` with the same type as `values`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">value_t</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">value_shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value_t</span><span class="p">)</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>

    <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)])</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="n">output_shape</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">value_shape</span><span class="p">)</span>
    <span class="c1"># expand_dims converts concat to stack.</span>
    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">parallel_concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">values</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="stack"><a class="viewcode-back" href="../../../../index.html#tensorflow.stack">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;stack&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;stack&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.concat`, `tf.tile`, `tf.repeat`.</span>

<span class="sd">  Packs the list of tensors in `values` into a tensor with rank one higher than</span>
<span class="sd">  each tensor in `values`, by packing them along the `axis` dimension.</span>
<span class="sd">  Given a list of length `N` of tensors of shape `(A, B, C)`;</span>

<span class="sd">  if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.</span>
<span class="sd">  if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.</span>
<span class="sd">  Etc.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([1, 4])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([2, 5])</span>
<span class="sd">  &gt;&gt;&gt; z = tf.constant([3, 6])</span>
<span class="sd">  &gt;&gt;&gt; tf.stack([x, y, z])</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 4],</span>
<span class="sd">         [2, 5],</span>
<span class="sd">         [3, 6]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.stack([x, y, z], axis=1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 2, 3],</span>
<span class="sd">         [4, 5, 6]], dtype=int32)&gt;</span>

<span class="sd">  This is the opposite of unstack.  The numpy equivalent is `np.stack`</span>

<span class="sd">  &gt;&gt;&gt; np.array_equal(np.stack([x, y, z]), tf.stack([x, y, z]))</span>
<span class="sd">  True</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of `Tensor` objects with the same shape and type.</span>
<span class="sd">    axis: An `int`. The axis to stack along. Defaults to the first dimension.</span>
<span class="sd">      Negative values wrap around, so the valid range is `[-(R+1), R+1)`.</span>
<span class="sd">    name: A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    output: A stacked `Tensor` with the same type as `values`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `axis` is out of the range [-(R+1), R+1).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># If the input is a constant list, it can be converted to a constant op</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
      <span class="k">pass</span>  <span class="c1"># Input list contains non-constant tensors</span>

  <span class="n">value_shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">value_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">expanded_num_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">value_shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">expanded_num_dims</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">expanded_num_dims</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;axis = </span><span class="si">%d</span><span class="s2"> not in [</span><span class="si">%d</span><span class="s2">, </span><span class="si">%d</span><span class="s2">)&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">expanded_num_dims</span><span class="p">,</span> <span class="n">expanded_num_dims</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="c1"># pylint: disable=invalid-name</span>
<span class="k">def</span> <span class="nf">_autopacking_helper</span><span class="p">(</span><span class="n">list_or_tuple</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given list or tuple to a tensor by packing.</span>

<span class="sd">  Args:</span>
<span class="sd">    list_or_tuple: A (possibly nested) list or tuple containing a tensor.</span>
<span class="sd">    dtype: The element type of the returned tensor.</span>
<span class="sd">    name: A name for the returned tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tf.Tensor` with value equivalent to `list_or_tuple`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="c1"># NOTE: Fast path when all the items are tensors, this doesn&#39;t do any type</span>
    <span class="c1"># checking.</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">is_dense_tensor_like</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">list_or_tuple</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">list_or_tuple</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">must_pack</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">converted_elems</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elem</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">list_or_tuple</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_dense_tensor_like</span><span class="p">(</span><span class="n">elem</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot convert a list containing a tensor of dtype &quot;</span>
                          <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> to </span><span class="si">%s</span><span class="s2"> (Tensor is: </span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span>
                          <span class="p">(</span><span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">elem</span><span class="p">))</span>
        <span class="n">converted_elems</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
        <span class="n">must_pack</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">converted_elem</span> <span class="o">=</span> <span class="n">_autopacking_helper</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_dense_tensor_like</span><span class="p">(</span><span class="n">converted_elem</span><span class="p">):</span>
          <span class="n">must_pack</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">converted_elems</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">converted_elem</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">converted_elems</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">must_pack</span><span class="p">:</span>
      <span class="n">elems_as_tensors</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elem</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">converted_elems</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_dense_tensor_like</span><span class="p">(</span><span class="n">elem</span><span class="p">):</span>
          <span class="n">elems_as_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># NOTE(mrry): This is inefficient, but it enables us to</span>
          <span class="c1"># handle the case where the list arguments are other</span>
          <span class="c1"># convertible-to-tensor types, such as numpy arrays.</span>
          <span class="n">elems_as_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
              <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">elems_as_tensors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">converted_elems</span>


<span class="k">def</span> <span class="nf">_get_dtype_from_nested_lists</span><span class="p">(</span><span class="n">list_or_tuple</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the dtype of any tensor-like object in `list_or_tuple`, if found.</span>

<span class="sd">  Args:</span>
<span class="sd">    list_or_tuple: A list or tuple representing an object that can be converted</span>
<span class="sd">      to a `tf.Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The dtype of any tensor-like object in `list_or_tuple`, or `None` if no</span>
<span class="sd">    such object exists.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">list_or_tuple</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_dense_tensor_like</span><span class="p">(</span><span class="n">elem</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="n">maybe_dtype</span> <span class="o">=</span> <span class="n">_get_dtype_from_nested_lists</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">maybe_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">maybe_dtype</span>
  <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_cast_nested_seqs_to_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">_maybe_cast</span><span class="p">(</span><span class="n">elem</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_dense_tensor_like</span><span class="p">(</span><span class="n">elem</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">:</span>
        <span class="n">elem</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">elem</span>

  <span class="k">return</span> <span class="n">_maybe_cast</span>


<span class="n">_NON_AUTOPACKABLE_TYPES</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">numerictypes</span><span class="o">.</span><span class="n">ScalarType</span><span class="p">)</span>
<span class="n">_NON_AUTOPACKABLE_TYPES</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_should_not_autopack</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
  <span class="c1"># The condition we really want is</span>
  <span class="c1">#    ops.is_dense_tensor_like(...)</span>
  <span class="c1"># but it is &gt;5x slower due to abc.ABCMeta.__instancecheck__.</span>
  <span class="c1"># pylint: disable=unidiomatic-typecheck</span>
  <span class="c1"># TODO(slebedev): add nest.all?</span>
  <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_NON_AUTOPACKABLE_TYPES</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
  <span class="c1"># pylint: enable=unidiomatic-typecheck</span>


<span class="k">def</span> <span class="nf">_autopacking_conversion_function</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Tensor conversion function that automatically packs arguments.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">as_ref</span> <span class="ow">or</span> <span class="n">_should_not_autopack</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">NotImplemented</span>
  <span class="n">inferred_dtype</span> <span class="o">=</span> <span class="n">_get_dtype_from_nested_lists</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">inferred_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># We did not find any tensor-like objects in the nested lists, so defer to</span>
    <span class="c1"># other conversion functions.</span>
    <span class="k">return</span> <span class="bp">NotImplemented</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">inferred_dtype</span>
  <span class="k">elif</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">inferred_dtype</span><span class="p">:</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_cast_nested_seqs_to_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">v</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_autopacking_helper</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span> <span class="ow">or</span> <span class="s2">&quot;packed&quot;</span><span class="p">)</span>


<span class="c1"># pylint: enable=invalid-name</span>

<span class="c1"># NOTE: Register this conversion function to run *before* one that</span>
<span class="c1"># assumes every element is a value.</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">((</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span>
                                        <span class="n">_autopacking_conversion_function</span><span class="p">,</span> <span class="mi">99</span><span class="p">)</span>


<div class="viewcode-block" id="unstack"><a class="viewcode-back" href="../../../../index.html#tensorflow.unstack">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;unstack&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">unstack</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;unstack&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Unpacks the given dimension of a rank-`R` tensor into rank-`(R-1)` tensors.</span>

<span class="sd">  Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.</span>
<span class="sd">  If `num` is not specified (the default), it is inferred from `value`&#39;s shape.</span>
<span class="sd">  If `value.shape[axis]` is not known, `ValueError` is raised.</span>

<span class="sd">  For example, given a tensor of shape `(A, B, C, D)`;</span>

<span class="sd">  If `axis == 0` then the i&#39;th tensor in `output` is the slice</span>
<span class="sd">    `value[i, :, :, :]` and each tensor in `output` will have shape `(B, C, D)`.</span>
<span class="sd">    (Note that the dimension unpacked along is gone, unlike `split`).</span>

<span class="sd">  If `axis == 1` then the i&#39;th tensor in `output` is the slice</span>
<span class="sd">    `value[:, i, :, :]` and each tensor in `output` will have shape `(A, C, D)`.</span>
<span class="sd">  Etc.</span>

<span class="sd">  This is the opposite of stack.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A rank `R &gt; 0` `Tensor` to be unstacked.</span>
<span class="sd">    num: An `int`. The length of the dimension `axis`. Automatically inferred if</span>
<span class="sd">      `None` (the default).</span>
<span class="sd">    axis: An `int`. The axis to unstack along. Defaults to the first dimension.</span>
<span class="sd">      Negative values wrap around, so the valid range is `[-R, R)`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The list of `Tensor` objects unstacked from `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `num` is unspecified and cannot be inferred.</span>
<span class="sd">    ValueError: If `axis` is out of the range [-R, R).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">num</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">value_shape</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">value_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">value_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">value_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;axis = </span><span class="si">%d</span><span class="s2"> not in [</span><span class="si">%d</span><span class="s2">, </span><span class="si">%d</span><span class="s2">)&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">value_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">value_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">))</span>
      <span class="n">num</span> <span class="o">=</span> <span class="n">value_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
  <span class="k">if</span> <span class="n">num</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot infer num from shape </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">value_shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="concat"><a class="viewcode-back" href="../../../../index.html#tensorflow.concat">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;concat&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;concat&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates tensors along one dimension.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.tile`, `tf.stack`, `tf.repeat`.</span>

<span class="sd">  Concatenates the list of tensors `values` along dimension `axis`.  If</span>
<span class="sd">  `values[i].shape = [D0, D1, ... Daxis(i), ...Dn]`, the concatenated</span>
<span class="sd">  result has shape</span>

<span class="sd">      [D0, D1, ... Raxis, ...Dn]</span>

<span class="sd">  where</span>

<span class="sd">      Raxis = sum(Daxis(i))</span>

<span class="sd">  That is, the data from the input tensors is joined along the `axis`</span>
<span class="sd">  dimension.</span>

<span class="sd">  The number of dimensions of the input tensors must match, and all dimensions</span>
<span class="sd">  except `axis` must be equal.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; t1 = [[1, 2, 3], [4, 5, 6]]</span>
<span class="sd">  &gt;&gt;&gt; t2 = [[7, 8, 9], [10, 11, 12]]</span>
<span class="sd">  &gt;&gt;&gt; concat([t1, t2], 0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=</span>
<span class="sd">  array([[ 1,  2,  3],</span>
<span class="sd">         [ 4,  5,  6],</span>
<span class="sd">         [ 7,  8,  9],</span>
<span class="sd">         [10, 11, 12]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; concat([t1, t2], 1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=</span>
<span class="sd">  array([[ 1,  2,  3,  7,  8,  9],</span>
<span class="sd">         [ 4,  5,  6, 10, 11, 12]], dtype=int32)&gt;</span>

<span class="sd">  As in Python, the `axis` could also be negative numbers. Negative `axis`</span>
<span class="sd">  are interpreted as counting from the end of the rank, i.e.,</span>
<span class="sd">   `axis + rank(values)`-th dimension.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]</span>
<span class="sd">  &gt;&gt;&gt; t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]</span>
<span class="sd">  &gt;&gt;&gt; tf.concat([t1, t2], -1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy=</span>
<span class="sd">    array([[[ 1,  2,  7,  4],</span>
<span class="sd">            [ 2,  3,  8,  4]],</span>
<span class="sd">           [[ 4,  4,  2, 10],</span>
<span class="sd">            [ 5,  3, 15, 11]]], dtype=int32)&gt;</span>

<span class="sd">  Note: If you are concatenating along a new axis consider using stack.</span>
<span class="sd">  E.g.</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.concat([tf.expand_dims(t, axis) for t in tensors], axis)</span>
<span class="sd">  ```</span>

<span class="sd">  can be rewritten as</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.stack(tensors, axis=axis)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of `Tensor` objects or a single `Tensor`.</span>
<span class="sd">    axis: 0-D `int32` `Tensor`.  Dimension along which to concatenate. Must be</span>
<span class="sd">      in the range `[-rank(values), rank(values))`. As in Python, indexing for</span>
<span class="sd">      axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers</span>
<span class="sd">      to `axis`-th dimension. And negative axis refers to `axis +</span>
<span class="sd">      rank(values)`-th dimension.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` resulting from concatenation of the input tensors.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="n">values</span><span class="p">]</span>
  <span class="c1"># TODO(mrry): Change to return values?</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Degenerate case of one tensor.</span>
    <span class="c1"># Make a throwaway call to convert_to_tensor to make sure</span>
    <span class="c1"># that axis is of the correct type, and make sure that</span>
    <span class="c1"># the returned tensor is a scalar.</span>
    <span class="c1"># TODO(keveman): Implement a standalone type and shape checker.</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;concat_dim&quot;</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">assert_has_rank</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">identity</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">concat_v2</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="boolean_mask"><a class="viewcode-back" href="../../../../index.html#tensorflow.boolean_mask">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;boolean_mask&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">boolean_mask</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;boolean_mask&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply boolean mask to tensor.</span>

<span class="sd">  Numpy equivalent is `tensor[mask]`.</span>

<span class="sd">  ```python</span>
<span class="sd">  # 1-D example</span>
<span class="sd">  tensor = [0, 1, 2, 3]</span>
<span class="sd">  mask = np.array([True, False, True, False])</span>
<span class="sd">  boolean_mask(tensor, mask)  # [0, 2]</span>
<span class="sd">  ```</span>

<span class="sd">  In general, `0 &lt; dim(mask) = K &lt;= dim(tensor)`, and `mask`&#39;s shape must match</span>
<span class="sd">  the first K dimensions of `tensor`&#39;s shape.  We then have:</span>
<span class="sd">    `boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]`</span>
<span class="sd">  where `(i1,...,iK)` is the ith `True` entry of `mask` (row-major order).</span>
<span class="sd">  The `axis` could be used with `mask` to indicate the axis to mask from.</span>
<span class="sd">  In that case, `axis + dim(mask) &lt;= dim(tensor)` and `mask`&#39;s shape must match</span>
<span class="sd">  the first `axis + dim(mask)` dimensions of `tensor`&#39;s shape.</span>

<span class="sd">  See also: `tf.ragged.boolean_mask`, which can be applied to both dense and</span>
<span class="sd">  ragged tensors, and can be used if you need to preserve the masked dimensions</span>
<span class="sd">  of `tensor` (rather than flattening them, as `tf.boolean_mask` does).</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor:  N-D tensor.</span>
<span class="sd">    mask:  K-D boolean tensor, K &lt;= N and K must be known statically.</span>
<span class="sd">    name:  A name for this operation (optional).</span>
<span class="sd">    axis:  A 0-D int Tensor representing the axis in `tensor` to mask from. By</span>
<span class="sd">      default, axis is 0 which will mask from the first dimension. Otherwise K +</span>
<span class="sd">      axis &lt;= N.</span>

<span class="sd">  Returns:</span>
<span class="sd">    (N-K+1)-dimensional tensor populated by entries in `tensor` corresponding</span>
<span class="sd">    to `True` values in `mask`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError:  If shapes do not conform.</span>

<span class="sd">  Examples:</span>

<span class="sd">  ```python</span>
<span class="sd">  # 2-D example</span>
<span class="sd">  tensor = [[1, 2], [3, 4], [5, 6]]</span>
<span class="sd">  mask = np.array([True, False, True])</span>
<span class="sd">  boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_apply_mask_1d</span><span class="p">(</span><span class="n">reshaped_tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Mask tensor along dimension 0 with a 1-D mask.&quot;&quot;&quot;</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">where_v2</span><span class="p">(</span><span class="n">mask</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">gather</span><span class="p">(</span><span class="n">reshaped_tensor</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">]):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mask&quot;</span><span class="p">)</span>

    <span class="n">shape_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="n">ndims_mask</span> <span class="o">=</span> <span class="n">shape_mask</span><span class="o">.</span><span class="n">ndims</span>
    <span class="n">shape_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ndims_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mask cannot be scalar.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndims_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Number of mask dimensions must be specified, even if some dimensions&quot;</span>
          <span class="s2">&quot; are None.  E.g. shape=[None] is ok, but shape=None is not.&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis</span>
    <span class="n">shape_tensor</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span> <span class="o">+</span> <span class="n">ndims_mask</span><span class="p">]</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">(</span><span class="n">shape_mask</span><span class="p">)</span>

    <span class="n">leading_size</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span> <span class="o">+</span> <span class="n">ndims_mask</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span>
        <span class="n">tensor</span><span class="p">,</span>
        <span class="n">concat</span><span class="p">([</span>
            <span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)[:</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="n">leading_size</span><span class="p">],</span>
            <span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)[</span><span class="n">axis</span> <span class="o">+</span> <span class="n">ndims_mask</span><span class="p">:]</span>
        <span class="p">],</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">first_dim</span> <span class="o">=</span> <span class="n">shape_tensor</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span> <span class="o">+</span> <span class="n">ndims_mask</span><span class="p">]</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span>
        <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape_tensor</span><span class="p">[:</span><span class="n">axis</span><span class="p">])</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span><span class="n">first_dim</span><span class="p">])</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">shape_tensor</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="n">ndims_mask</span><span class="p">:]))</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">_apply_mask_1d</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;boolean_mask&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">boolean_mask_v2</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;boolean_mask&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply boolean mask to tensor.</span>

<span class="sd">  Numpy equivalent is `tensor[mask]`.</span>

<span class="sd">  ```python</span>
<span class="sd">  # 1-D example</span>
<span class="sd">  tensor = [0, 1, 2, 3]</span>
<span class="sd">  mask = np.array([True, False, True, False])</span>
<span class="sd">  boolean_mask(tensor, mask)  # [0, 2]</span>
<span class="sd">  ```</span>

<span class="sd">  In general, `0 &lt; dim(mask) = K &lt;= dim(tensor)`, and `mask`&#39;s shape must match</span>
<span class="sd">  the first K dimensions of `tensor`&#39;s shape.  We then have:</span>
<span class="sd">    `boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]`</span>
<span class="sd">  where `(i1,...,iK)` is the ith `True` entry of `mask` (row-major order).</span>
<span class="sd">  The `axis` could be used with `mask` to indicate the axis to mask from.</span>
<span class="sd">  In that case, `axis + dim(mask) &lt;= dim(tensor)` and `mask`&#39;s shape must match</span>
<span class="sd">  the first `axis + dim(mask)` dimensions of `tensor`&#39;s shape.</span>

<span class="sd">  See also: `tf.ragged.boolean_mask`, which can be applied to both dense and</span>
<span class="sd">  ragged tensors, and can be used if you need to preserve the masked dimensions</span>
<span class="sd">  of `tensor` (rather than flattening them, as `tf.boolean_mask` does).</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor:  N-D tensor.</span>
<span class="sd">    mask:  K-D boolean tensor, K &lt;= N and K must be known statically.</span>
<span class="sd">    axis:  A 0-D int Tensor representing the axis in `tensor` to mask from. By</span>
<span class="sd">      default, axis is 0 which will mask from the first dimension. Otherwise K +</span>
<span class="sd">      axis &lt;= N.</span>
<span class="sd">    name:  A name for this operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    (N-K+1)-dimensional tensor populated by entries in `tensor` corresponding</span>
<span class="sd">    to `True` values in `mask`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError:  If shapes do not conform.</span>

<span class="sd">  Examples:</span>

<span class="sd">  ```python</span>
<span class="sd">  # 2-D example</span>
<span class="sd">  tensor = [[1, 2], [3, 4], [5, 6]]</span>
<span class="sd">  mask = np.array([True, False, True])</span>
<span class="sd">  boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">boolean_mask</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sparse.mask&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.mask&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_mask&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_mask&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_mask</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mask_indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Masks elements of `IndexedSlices`.</span>

<span class="sd">  Given an `IndexedSlices` instance `a`, returns another `IndexedSlices` that</span>
<span class="sd">  contains a subset of the slices of `a`. Only the slices at indices not</span>
<span class="sd">  specified in `mask_indices` are returned.</span>

<span class="sd">  This is useful when you need to extract a subset of slices in an</span>
<span class="sd">  `IndexedSlices` object.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  # `a` contains slices at indices [12, 26, 37, 45] from a large tensor</span>
<span class="sd">  # with shape [1000, 10]</span>
<span class="sd">  a.indices  # [12, 26, 37, 45]</span>
<span class="sd">  tf.shape(a.values)  # [4, 10]</span>

<span class="sd">  # `b` will be the subset of `a` slices at its second and third indices, so</span>
<span class="sd">  # we want to mask its first and last indices (which are at absolute</span>
<span class="sd">  # indices 12, 45)</span>
<span class="sd">  b = tf.sparse.mask(a, [12, 45])</span>

<span class="sd">  b.indices  # [26, 37]</span>
<span class="sd">  tf.shape(b.values)  # [2, 10]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    a: An `IndexedSlices` instance.</span>
<span class="sd">    mask_indices: Indices of elements to mask.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The masked `IndexedSlices` instance.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;sparse_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">mask_indices</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">indices</span>
    <span class="n">out_indices</span><span class="p">,</span> <span class="n">to_gather</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">list_diff</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">mask_indices</span><span class="p">)</span>
    <span class="n">out_values</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">to_gather</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">out_values</span><span class="p">,</span> <span class="n">out_indices</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>


<div class="viewcode-block" id="unique"><a class="viewcode-back" href="../../../../index.html#tensorflow.unique">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;unique&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out_idx</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Finds unique elements in a 1-D tensor.</span>

<span class="sd">  This operation returns a tensor `y` containing all of the unique elements</span>
<span class="sd">  of `x` sorted in the same order that they occur in `x`. This operation</span>
<span class="sd">  also returns a tensor `idx` the same size as `x` that contains the index</span>
<span class="sd">  of each value of `x` in the unique output `y`. In other words:</span>


<span class="sd">    y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([1, 1, 2, 4, 4, 4, 7, 8, 8])</span>
<span class="sd">  &gt;&gt;&gt; y, idx = unique(x)</span>
<span class="sd">  &gt;&gt;&gt; y</span>
<span class="sd">  &lt;tf.Tensor: id=5, shape=(5,), dtype=int32,</span>
<span class="sd">  numpy=array([1, 2, 4, 7, 8], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; idx</span>
<span class="sd">  &lt;tf.Tensor: id=6, shape=(9,), dtype=int32,</span>
<span class="sd">  numpy=array([0, 0, 1, 2, 2, 2, 3, 4, 4], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor. 1-D.</span>
<span class="sd">    out_idx: An optional tf.DType from: tf.int32, tf.int64. Defaults to</span>
<span class="sd">      tf.int32.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of Tensor objects (y, idx).</span>
<span class="sd">      y: A Tensor. Has the same type as x.</span>
<span class="sd">      idx: A Tensor of type out_idx.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(yongtang): switch to v2 once API deprecation</span>
  <span class="c1"># period (3 weeks) pass.</span>
  <span class="c1"># TODO(yongtang): The documentation should also</span>
  <span class="c1"># be updated when switch  to v2.</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out_idx</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="n">unique</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">unique</span><span class="o">.</span><span class="vm">__doc__</span>


<div class="viewcode-block" id="unique_with_counts"><a class="viewcode-back" href="../../../../index.html#tensorflow.unique_with_counts">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;unique_with_counts&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">unique_with_counts</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out_idx</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Finds unique elements in a 1-D tensor.</span>

<span class="sd">  This operation returns a tensor `y` containing all of the unique elements</span>
<span class="sd">  of `x` sorted in the same order that they occur in `x`. This operation</span>
<span class="sd">  also returns a tensor `idx` the same size as `x` that contains the index</span>
<span class="sd">  of each value of `x` in the unique output `y`. Finally, it returns a</span>
<span class="sd">  third tensor `count` that contains the count of each element of `y`</span>
<span class="sd">  in `x`. In other words:</span>

<span class="sd">    y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([1, 1, 2, 4, 4, 4, 7, 8, 8])</span>
<span class="sd">  &gt;&gt;&gt; y, idx, count = unique_with_counts(x)</span>
<span class="sd">  &gt;&gt;&gt; y</span>
<span class="sd">  &lt;tf.Tensor: id=8, shape=(5,), dtype=int32,</span>
<span class="sd">  numpy=array([1, 2, 4, 7, 8], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; idx</span>
<span class="sd">  &lt;tf.Tensor: id=9, shape=(9,), dtype=int32,</span>
<span class="sd">  numpy=array([0, 0, 1, 2, 2, 2, 3, 4, 4], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; count</span>
<span class="sd">  &lt;tf.Tensor: id=10, shape=(5,), dtype=int32,</span>
<span class="sd">  numpy=array([2, 1, 3, 1, 2], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor. 1-D.</span>
<span class="sd">    out_idx: An optional tf.DType from: tf.int32, tf.int64. Defaults to</span>
<span class="sd">      tf.int32.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of Tensor objects (y, idx, count).</span>
<span class="sd">      y: A Tensor. Has the same type as x.</span>
<span class="sd">      idx: A Tensor of type out_idx.</span>
<span class="sd">      count: A Tensor of type out_idx.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(yongtang): switch to v2 once API deprecation</span>
  <span class="c1"># period (3 weeks) pass.</span>
  <span class="c1"># TODO(yongtang): The documentation should also</span>
  <span class="c1"># be updated when switch  to v2.</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">unique_with_counts</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out_idx</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="n">unique_with_counts</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">unique_with_counts</span><span class="o">.</span><span class="vm">__doc__</span>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../../../index.html#tensorflow.split">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;split&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;split&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Splits a tensor `value` into a list of sub tensors.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.unstack`.</span>

<span class="sd">  If `num_or_size_splits` is an integer, then `value` is split along the</span>
<span class="sd">  dimension `axis` into `num_split` smaller tensors. This requires that</span>
<span class="sd">  `value.shape[axis]` is divisible by `num_split`.</span>

<span class="sd">  If `num_or_size_splits` is a 1-D Tensor (or list), we call it `size_splits`</span>
<span class="sd">  and `value` is split into `len(size_splits)` elements. The shape of the `i`-th</span>
<span class="sd">  element has the same size as the `value` except along dimension `axis` where</span>
<span class="sd">  the size is `size_splits[i]`.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.Variable(tf.random.uniform([5, 30], -1, 1))</span>

<span class="sd">  Split `x` into 3 tensors along dimension 1</span>
<span class="sd">  &gt;&gt;&gt; s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)</span>
<span class="sd">  &gt;&gt;&gt; tf.shape(s0).numpy()</span>
<span class="sd">  array([ 5, 10], dtype=int32)</span>

<span class="sd">  Split `x` into 3 tensors with sizes [4, 15, 11] along dimension 1</span>
<span class="sd">  &gt;&gt;&gt; split0, split1, split2 = tf.split(x, [4, 15, 11], 1)</span>
<span class="sd">  &gt;&gt;&gt; tf.shape(split0).numpy()</span>
<span class="sd">  array([5, 4], dtype=int32)</span>
<span class="sd">  &gt;&gt;&gt; tf.shape(split1).numpy()</span>
<span class="sd">  array([ 5, 15], dtype=int32)</span>
<span class="sd">  &gt;&gt;&gt; tf.shape(split2).numpy()</span>
<span class="sd">  array([ 5, 11], dtype=int32)</span>

<span class="sd">  Args:</span>
<span class="sd">    value: The `Tensor` to split.</span>
<span class="sd">    num_or_size_splits: Either an integer indicating the number of splits along</span>
<span class="sd">      `axis` or a 1-D integer `Tensor` or Python list containing the sizes of</span>
<span class="sd">      each output tensor along `axis`. If a scalar, then it must evenly divide</span>
<span class="sd">      `value.shape[axis]`; otherwise the sum of sizes along the split axis</span>
<span class="sd">      must match that of the `value`.</span>
<span class="sd">    axis: An integer or scalar `int32` `Tensor`. The dimension along which to</span>
<span class="sd">      split. Must be in the range `[-rank(value), rank(value))`. Defaults to 0.</span>
<span class="sd">    num: Optional, used to specify the number of outputs when it cannot be</span>
<span class="sd">      inferred from the shape of `size_splits`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    if `num_or_size_splits` is a scalar returns a list of `num_or_size_splits`</span>
<span class="sd">    `Tensor` objects; if `num_or_size_splits` is a 1-D Tensor returns</span>
<span class="sd">    `num_or_size_splits.get_shape[0]` `Tensor` objects resulting from splitting</span>
<span class="sd">    `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `num` is unspecified and cannot be inferred.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">size_splits</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">num_or_size_splits</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_or_size_splits</span><span class="p">,</span>
                <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)):</span>
    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">num_split</span><span class="o">=</span><span class="n">num_or_size_splits</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">size_splits</span><span class="o">.</span><span class="n">_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Rank-0 tensors are not supported as the num_or_size_splits argument &quot;</span>
        <span class="s2">&quot;to split. Argument provided: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_or_size_splits</span><span class="p">,))</span>

  <span class="k">if</span> <span class="n">num</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">size_splits_shape</span> <span class="o">=</span> <span class="n">size_splits</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">size_splits_shape</span><span class="p">:</span>
      <span class="n">num</span> <span class="o">=</span> <span class="n">size_splits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">num</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot infer num from shape </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">num_or_size_splits</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">split_v</span><span class="p">(</span>
      <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">size_splits</span><span class="o">=</span><span class="n">size_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">num_split</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;transpose&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">transpose_v2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;transpose&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transposes `a`, where `a` is a Tensor.</span>

<span class="sd">  Permutes the dimensions according to the value of `perm`.</span>

<span class="sd">  The returned tensor&#39;s dimension `i` will correspond to the input dimension</span>
<span class="sd">  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank</span>
<span class="sd">  of the input tensor. Hence by default, this operation performs a regular</span>
<span class="sd">  matrix transpose on 2-D input Tensors.</span>

<span class="sd">  If conjugate is `True` and `a.dtype` is either `complex64` or `complex128`</span>
<span class="sd">  then the values of `a` are conjugated and transposed.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  In `numpy` transposes are memory-efficient constant time operations as they</span>
<span class="sd">  simply return a new view of the same data with adjusted `strides`.</span>

<span class="sd">  TensorFlow does not support strides, so `transpose` returns a new tensor with</span>
<span class="sd">  the items permuted.</span>
<span class="sd">  @end_compatibility</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">  &gt;&gt;&gt; tf.transpose(x)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 4],</span>
<span class="sd">         [2, 5],</span>
<span class="sd">         [3, 6]], dtype=int32)&gt;</span>

<span class="sd">  Equivalently, you could call `tf.transpose(x, perm=[1, 0])`.</span>

<span class="sd">  If `x` is complex, setting conjugate=True gives the conjugate transpose:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],</span>
<span class="sd">  ...                  [4 + 4j, 5 + 5j, 6 + 6j]])</span>
<span class="sd">  &gt;&gt;&gt; tf.transpose(x, conjugate=True)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=</span>
<span class="sd">  array([[1.-1.j, 4.-4.j],</span>
<span class="sd">         [2.-2.j, 5.-5.j],</span>
<span class="sd">         [3.-3.j, 6.-6.j]])&gt;</span>

<span class="sd">  &#39;perm&#39; is more useful for n-dimensional tensors where n &gt; 2:</span>

<span class="sd">  &gt;&gt;&gt; x = tf.constant([[[ 1,  2,  3],</span>
<span class="sd">  ...                   [ 4,  5,  6]],</span>
<span class="sd">  ...                  [[ 7,  8,  9],</span>
<span class="sd">  ...                   [10, 11, 12]]])</span>

<span class="sd">  As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.</span>

<span class="sd">  To take the transpose of the matrices in dimension-0 (such as when you are</span>
<span class="sd">  transposing matrices where 0 is the batch dimesnion), you would set</span>
<span class="sd">  `perm=[0,2,1]`.</span>

<span class="sd">  &gt;&gt;&gt; tf.transpose(x, perm=[0, 2, 1])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[[ 1,  4],</span>
<span class="sd">          [ 2,  5],</span>
<span class="sd">          [ 3,  6]],</span>
<span class="sd">          [[ 7, 10],</span>
<span class="sd">          [ 8, 11],</span>
<span class="sd">          [ 9, 12]]], dtype=int32)&gt;</span>

<span class="sd">  Note: This has a shorthand `linalg.matrix_transpose`):</span>

<span class="sd">  Args:</span>
<span class="sd">    a: A `Tensor`.</span>
<span class="sd">    perm: A permutation of the dimensions of `a`.  This should be a vector.</span>
<span class="sd">    conjugate: Optional bool. Setting it to `True` is mathematically equivalent</span>
<span class="sd">      to tf.math.conj(tf.transpose(input)).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A transposed `Tensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">perm</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="n">conjugate</span><span class="p">)</span>


<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../../../index.html#tensorflow.transpose">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;transpose&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;transpose&quot;</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transposes `a`.</span>

<span class="sd">  Permutes the dimensions according to `perm`.</span>

<span class="sd">  The returned tensor&#39;s dimension i will correspond to the input dimension</span>
<span class="sd">  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is</span>
<span class="sd">  the rank of the input tensor. Hence by default, this operation performs a</span>
<span class="sd">  regular matrix transpose on 2-D input Tensors. If conjugate is True and</span>
<span class="sd">  `a.dtype` is either `complex64` or `complex128` then the values of `a`</span>
<span class="sd">  are conjugated and transposed.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  In `numpy` transposes are memory-efficient constant time operations as they</span>
<span class="sd">  simply return a new view of the same data with adjusted `strides`.</span>

<span class="sd">  TensorFlow does not support strides, so `transpose` returns a new tensor with</span>
<span class="sd">  the items permuted.</span>
<span class="sd">  @end_compatibility</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">  tf.transpose(x)  # [[1, 4]</span>
<span class="sd">                   #  [2, 5]</span>
<span class="sd">                   #  [3, 6]]</span>

<span class="sd">  # Equivalently</span>
<span class="sd">  tf.transpose(x, perm=[1, 0])  # [[1, 4]</span>
<span class="sd">                                #  [2, 5]</span>
<span class="sd">                                #  [3, 6]]</span>

<span class="sd">  # If x is complex, setting conjugate=True gives the conjugate transpose</span>
<span class="sd">  x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],</span>
<span class="sd">                   [4 + 4j, 5 + 5j, 6 + 6j]])</span>
<span class="sd">  tf.transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],</span>
<span class="sd">                                   #  [2 - 2j, 5 - 5j],</span>
<span class="sd">                                   #  [3 - 3j, 6 - 6j]]</span>

<span class="sd">  # &#39;perm&#39; is more useful for n-dimensional tensors, for n &gt; 2</span>
<span class="sd">  x = tf.constant([[[ 1,  2,  3],</span>
<span class="sd">                    [ 4,  5,  6]],</span>
<span class="sd">                   [[ 7,  8,  9],</span>
<span class="sd">                    [10, 11, 12]]])</span>

<span class="sd">  # Take the transpose of the matrices in dimension-0</span>
<span class="sd">  # (this common operation has a shorthand `linalg.matrix_transpose`)</span>
<span class="sd">  tf.transpose(x, perm=[0, 2, 1])  # [[[1,  4],</span>
<span class="sd">                                   #   [2,  5],</span>
<span class="sd">                                   #   [3,  6]],</span>
<span class="sd">                                   #  [[7, 10],</span>
<span class="sd">                                   #   [8, 11],</span>
<span class="sd">                                   #   [9, 12]]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    a: A `Tensor`.</span>
<span class="sd">    perm: A permutation of the dimensions of `a`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    conjugate: Optional bool. Setting it to `True` is mathematically equivalent</span>
<span class="sd">      to tf.math.conj(tf.transpose(input)).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A transposed `Tensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;transpose&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
      <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">conjugate</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="n">transpose_fn</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">conjugate_transpose</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">transpose_fn</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">transpose</span>

    <span class="k">if</span> <span class="n">perm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">transpose_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">perm</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">gen_array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">transpose_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="c1"># pylint: disable=invalid-name</span>
<span class="nd">@tf_export</span><span class="p">(</span>
    <span class="s2">&quot;linalg.matrix_transpose&quot;</span><span class="p">,</span>
    <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;linalg.transpose&quot;</span><span class="p">,</span> <span class="s2">&quot;linalg.matrix_transpose&quot;</span><span class="p">,</span> <span class="s2">&quot;matrix_transpose&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;matrix_transpose&quot;</span><span class="p">,</span> <span class="s2">&quot;linalg.transpose&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matrix_transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;matrix_transpose&quot;</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transposes last two dimensions of tensor `a`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">  tf.linalg.matrix_transpose(x)  # [[1, 4],</span>
<span class="sd">                                 #  [2, 5],</span>
<span class="sd">                                 #  [3, 6]]</span>

<span class="sd">  x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],</span>
<span class="sd">                   [4 + 4j, 5 + 5j, 6 + 6j]])</span>
<span class="sd">  tf.linalg.matrix_transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],</span>
<span class="sd">                                                 #  [2 - 2j, 5 - 5j],</span>
<span class="sd">                                                 #  [3 - 3j, 6 - 6j]]</span>

<span class="sd">  # Matrix with two batch dimensions.</span>
<span class="sd">  # x.shape is [1, 2, 3, 4]</span>
<span class="sd">  # tf.linalg.matrix_transpose(x) is shape [1, 2, 4, 3]</span>
<span class="sd">  ```</span>

<span class="sd">  Note that `tf.matmul` provides kwargs allowing for transpose of arguments.</span>
<span class="sd">  This is done with minimal cost, and is preferable to using this function. E.g.</span>

<span class="sd">  ```python</span>
<span class="sd">  # Good!  Transpose is taken at minimal additional cost.</span>
<span class="sd">  tf.matmul(matrix, b, transpose_b=True)</span>

<span class="sd">  # Inefficient!</span>
<span class="sd">  tf.matmul(matrix, tf.linalg.matrix_transpose(b))</span>
<span class="sd">  ```</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  In `numpy` transposes are memory-efficient constant time operations as they</span>
<span class="sd">  simply return a new view of the same data with adjusted `strides`.</span>

<span class="sd">  TensorFlow does not support strides, `linalg.matrix_transpose` returns a new</span>
<span class="sd">  tensor with the items permuted.</span>
<span class="sd">  @end_compatibility</span>

<span class="sd">  Args:</span>
<span class="sd">    a: A `Tensor` with `rank &gt;= 2`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    conjugate: Optional bool. Setting it to `True` is mathematically equivalent</span>
<span class="sd">      to tf.math.conj(tf.linalg.matrix_transpose(input)).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A transposed batch matrix `Tensor`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError:  If `a` is determined statically to have `rank &lt; 2`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">]):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>

    <span class="c1"># If we know the number of dimensions (statically), we can do two things:</span>
    <span class="c1"># 1. Check that `a` is a (batch) matrix.</span>
    <span class="c1"># 2. Use a python list for perm.  This preserves static shape information</span>
    <span class="c1">#    and avoids extra computations.</span>
    <span class="n">a_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span>
    <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">ndims</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Argument &#39;a&#39; should be a (batch) matrix, with rank &gt;= 2.  Found: &quot;</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">a_shape</span><span class="p">)</span>
      <span class="n">perm</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ndims</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="n">ndims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ndims</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">a_rank</span> <span class="o">=</span> <span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="n">perm</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(</span>
          <span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">a_rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">a_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">a_rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]),</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">perm</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="n">conjugate</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.diag&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;linalg.diag&quot;</span><span class="p">,</span> <span class="s2">&quot;matrix_diag&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;matrix_diag&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matrix_diag</span><span class="p">(</span><span class="n">diagonal</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;diag&quot;</span><span class="p">,</span>
                <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">num_rows</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">num_cols</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a batched diagonal tensor with given batched diagonal values.</span>

<span class="sd">  Returns a tensor with the contents in `diagonal` as `k[0]`-th to `k[1]`-th</span>
<span class="sd">  diagonals of a matrix, with everything else padded with `padding`. `num_rows`</span>
<span class="sd">  and `num_cols` specify the dimension of the innermost matrix of the output. If</span>
<span class="sd">  both are not specified, the op assumes the innermost matrix is square and</span>
<span class="sd">  infers its size from `k` and the innermost dimension of `diagonal`. If only</span>
<span class="sd">  one of them is specified, the op assumes the unspecified value is the smallest</span>
<span class="sd">  possible based on other criteria.</span>

<span class="sd">  Let `diagonal` have `r` dimensions `[I, J, ..., L, M, N]`. The output tensor</span>
<span class="sd">  has rank `r+1` with shape `[I, J, ..., L, M, num_rows, num_cols]` when only</span>
<span class="sd">  one diagonal is given (`k` is an integer or `k[0] == k[1]`). Otherwise, it has</span>
<span class="sd">  rank `r` with shape `[I, J, ..., L, num_rows, num_cols]`.</span>

<span class="sd">  The second innermost dimension of `diagonal` has double meaning. When `k` is</span>
<span class="sd">  scalar or `k[0] == k[1]`, `M` is part of the batch size [I, J, ..., M], and</span>
<span class="sd">  the output tensor is:</span>

<span class="sd">  ```</span>
<span class="sd">  output[i, j, ..., l, m, n]</span>
<span class="sd">    = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper</span>
<span class="sd">      padding_value                             ; otherwise</span>
<span class="sd">  ```</span>

<span class="sd">  Otherwise, `M` is treated as the number of diagonals for the matrix in the</span>
<span class="sd">  same batch (`M = k[1]-k[0]+1`), and the output tensor is:</span>

<span class="sd">  ```</span>
<span class="sd">  output[i, j, ..., l, m, n]</span>
<span class="sd">    = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] &lt;= d &lt;= k[1]</span>
<span class="sd">      padding_value                                     ; otherwise</span>
<span class="sd">  ```</span>
<span class="sd">  where `d = n - m`, `diag_index = k[1] - d`, and</span>
<span class="sd">  `index_in_diag = n - max(d, 0) + offset`.</span>

<span class="sd">  `offset` is zero except when the alignment of the diagonal is to the right.</span>
<span class="sd">  ```</span>
<span class="sd">  offset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}</span>
<span class="sd">                                             and `d &gt;= 0`) or</span>
<span class="sd">                                           (`align` in {LEFT_RIGHT, RIGHT_RIGHT}</span>
<span class="sd">                                             and `d &lt;= 0`)</span>
<span class="sd">           0                          ; otherwise</span>
<span class="sd">  ```</span>
<span class="sd">  where `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```</span>
<span class="sd">  # The main diagonal.</span>
<span class="sd">  diagonal = np.array([[1, 2, 3, 4],            # Input shape: (2, 4)</span>
<span class="sd">                       [5, 6, 7, 8]])</span>
<span class="sd">  tf.matrix_diag(diagonal) ==&gt; [[[1, 0, 0, 0],  # Output shape: (2, 4, 4)</span>
<span class="sd">                                 [0, 2, 0, 0],</span>
<span class="sd">                                 [0, 0, 3, 0],</span>
<span class="sd">                                 [0, 0, 0, 4]],</span>
<span class="sd">                                [[5, 0, 0, 0],</span>
<span class="sd">                                 [0, 6, 0, 0],</span>
<span class="sd">                                 [0, 0, 7, 0],</span>
<span class="sd">                                 [0, 0, 0, 8]]]</span>

<span class="sd">  # A superdiagonal (per batch).</span>
<span class="sd">  diagonal = np.array([[1, 2, 3],  # Input shape: (2, 3)</span>
<span class="sd">                       [4, 5, 6]])</span>
<span class="sd">  tf.matrix_diag(diagonal, k = 1)</span>
<span class="sd">    ==&gt; [[[0, 1, 0, 0],  # Output shape: (2, 4, 4)</span>
<span class="sd">          [0, 0, 2, 0],</span>
<span class="sd">          [0, 0, 0, 3],</span>
<span class="sd">          [0, 0, 0, 0]],</span>
<span class="sd">         [[0, 4, 0, 0],</span>
<span class="sd">          [0, 0, 5, 0],</span>
<span class="sd">          [0, 0, 0, 6],</span>
<span class="sd">          [0, 0, 0, 0]]]</span>

<span class="sd">  # A tridiagonal band (per batch).</span>
<span class="sd">  diagonals = np.array([[[8, 9, 0],  # Input shape: (2, 2, 3)</span>
<span class="sd">                         [1, 2, 3],</span>
<span class="sd">                         [0, 4, 5]],</span>
<span class="sd">                        [[2, 3, 0],</span>
<span class="sd">                         [6, 7, 9],</span>
<span class="sd">                         [0, 9, 1]]])</span>
<span class="sd">  tf.matrix_diag(diagonals, k = (-1, 1))</span>
<span class="sd">    ==&gt; [[[1, 8, 0],  # Output shape: (2, 3, 3)</span>
<span class="sd">          [4, 2, 9],</span>
<span class="sd">          [0, 5, 3]],</span>
<span class="sd">         [[6, 2, 0],</span>
<span class="sd">          [9, 7, 3],</span>
<span class="sd">          [0, 1, 9]]]</span>

<span class="sd">  # RIGHT_LEFT alignment.</span>
<span class="sd">  diagonals = np.array([[[0, 8, 9],  # Input shape: (2, 2, 3)</span>
<span class="sd">                         [1, 2, 3],</span>
<span class="sd">                         [4, 5, 0]],</span>
<span class="sd">                        [[0, 2, 3],</span>
<span class="sd">                         [6, 7, 9],</span>
<span class="sd">                         [9, 1, 0]]])</span>
<span class="sd">  tf.matrix_diag(diagonals, k = (-1, 1), align=&quot;RIGHT_LEFT&quot;)</span>
<span class="sd">    ==&gt; [[[1, 8, 0],  # Output shape: (2, 3, 3)</span>
<span class="sd">          [4, 2, 9],</span>
<span class="sd">          [0, 5, 3]],</span>
<span class="sd">         [[6, 2, 0],</span>
<span class="sd">          [9, 7, 3],</span>
<span class="sd">          [0, 1, 9]]]</span>

<span class="sd">  # Rectangular matrix.</span>
<span class="sd">  diagonal = np.array([1, 2])  # Input shape: (2)</span>
<span class="sd">  tf.matrix_diag(diagonal, k = -1, num_rows = 3, num_cols = 4)</span>
<span class="sd">    ==&gt; [[0, 0, 0, 0],  # Output shape: (3, 4)</span>
<span class="sd">         [1, 0, 0, 0],</span>
<span class="sd">         [0, 2, 0, 0]]</span>

<span class="sd">  # Rectangular matrix with inferred num_cols and padding_value = 9.</span>
<span class="sd">  tf.matrix_diag(diagonal, k = -1, num_rows = 3, padding_value = 9)</span>
<span class="sd">    ==&gt; [[9, 9],  # Output shape: (3, 2)</span>
<span class="sd">         [1, 9],</span>
<span class="sd">         [9, 2]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    diagonal: A `Tensor` with `rank k &gt;= 1`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    k: Diagonal offset(s). Positive value means superdiagonal, 0 refers to the</span>
<span class="sd">      main diagonal, and negative value means subdiagonals. `k` can be a single</span>
<span class="sd">      integer (for a single diagonal) or a pair of integers specifying the low</span>
<span class="sd">      and high ends of a matrix band. `k[0]` must not be larger than `k[1]`.</span>
<span class="sd">    num_rows: The number of rows of the output matrix. If it is not provided,</span>
<span class="sd">      the op assumes the output matrix is a square matrix and infers the matrix</span>
<span class="sd">      size from `d_lower`, `d_upper`, and the innermost dimension of `diagonal`.</span>
<span class="sd">    num_cols: The number of columns of the output matrix. If it is not provided,</span>
<span class="sd">      the op assumes the output matrix is a square matrix and infers the matrix</span>
<span class="sd">      size from `d_lower`, `d_upper`, and the innermost dimension of `diagonal`.</span>
<span class="sd">    padding_value: The value to fill the area outside the specified diagonal</span>
<span class="sd">      band with. Default is 0.</span>
<span class="sd">    align: Some diagonals are shorter than `max_diag_len` and need to be padded.</span>
<span class="sd">      `align` is a string specifying how superdiagonals and subdiagonals should</span>
<span class="sd">      be aligned, respectively. There are four possible alignments: &quot;RIGHT_LEFT&quot;</span>
<span class="sd">      (default), &quot;LEFT_RIGHT&quot;, &quot;LEFT_LEFT&quot;, and &quot;RIGHT_RIGHT&quot;. &quot;RIGHT_LEFT&quot;</span>
<span class="sd">      aligns superdiagonals to the right (left-pads the row) and subdiagonals to</span>
<span class="sd">      the left (right-pads the row). It is the packing format LAPACK uses.</span>
<span class="sd">      cuSPARSE uses &quot;LEFT_RIGHT&quot;, which is the opposite alignment.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor. Has the same type as `diagonal`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Special case to sidestep the tf.constant conversion error:</span>
  <span class="c1"># TypeError: Expected bool, got 0 of type &#39;int&#39; instead.</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">diagonal</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">diagonal</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;bool&quot;</span><span class="p">:</span>
    <span class="n">padding_value</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">padding_value</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">matrix_diag_v3</span><span class="p">(</span>
      <span class="n">diagonal</span><span class="o">=</span><span class="n">diagonal</span><span class="p">,</span>
      <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
      <span class="n">num_rows</span><span class="o">=</span><span class="n">num_rows</span><span class="p">,</span>
      <span class="n">num_cols</span><span class="o">=</span><span class="n">num_cols</span><span class="p">,</span>
      <span class="n">padding_value</span><span class="o">=</span><span class="n">padding_value</span><span class="p">,</span>
      <span class="n">align</span><span class="o">=</span><span class="n">align</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.diag_part&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;linalg.diag_part&quot;</span><span class="p">,</span> <span class="s2">&quot;matrix_diag_part&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;matrix_diag_part&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">matrix_diag_part</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint:disable=redefined-builtin</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;diag_part&quot;</span><span class="p">,</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the batched diagonal part of a batched tensor.</span>

<span class="sd">  Returns a tensor with the `k[0]`-th to `k[1]`-th diagonals of the batched</span>
<span class="sd">  `input`.</span>

<span class="sd">  Assume `input` has `r` dimensions `[I, J, ..., L, M, N]`.</span>
<span class="sd">  Let `max_diag_len` be the maximum length among all diagonals to be extracted,</span>
<span class="sd">  `max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`</span>
<span class="sd">  Let `num_diags` be the number of diagonals to extract,</span>
<span class="sd">  `num_diags = k[1] - k[0] + 1`.</span>

<span class="sd">  If `num_diags == 1`, the output tensor is of rank `r - 1` with shape</span>
<span class="sd">  `[I, J, ..., L, max_diag_len]` and values:</span>

<span class="sd">  ```</span>
<span class="sd">  diagonal[i, j, ..., l, n]</span>
<span class="sd">    = input[i, j, ..., l, n+y, n+x] ; if 0 &lt;= n+y &lt; M and 0 &lt;= n+x &lt; N,</span>
<span class="sd">      padding_value                 ; otherwise.</span>
<span class="sd">  ```</span>
<span class="sd">  where `y = max(-k[1], 0)`, `x = max(k[1], 0)`.</span>

<span class="sd">  Otherwise, the output tensor has rank `r` with dimensions</span>
<span class="sd">  `[I, J, ..., L, num_diags, max_diag_len]` with values:</span>

<span class="sd">  ```</span>
<span class="sd">  diagonal[i, j, ..., l, m, n]</span>
<span class="sd">    = input[i, j, ..., l, n+y, n+x] ; if 0 &lt;= n+y &lt; M and 0 &lt;= n+x &lt; N,</span>
<span class="sd">      padding_value                 ; otherwise.</span>
<span class="sd">  ```</span>
<span class="sd">  where `d = k[1] - m`, `y = max(-d, 0) - offset`, and `x = max(d, 0) - offset`.</span>

<span class="sd">  `offset` is zero except when the alignment of the diagonal is to the right.</span>
<span class="sd">  ```</span>
<span class="sd">  offset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}</span>
<span class="sd">                                             and `d &gt;= 0`) or</span>
<span class="sd">                                           (`align` in {LEFT_RIGHT, RIGHT_RIGHT}</span>
<span class="sd">                                             and `d &lt;= 0`)</span>
<span class="sd">           0                          ; otherwise</span>
<span class="sd">  ```</span>
<span class="sd">  where `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.</span>

<span class="sd">  The input must be at least a matrix.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```</span>
<span class="sd">  input = np.array([[[1, 2, 3, 4],  # Input shape: (2, 3, 4)</span>
<span class="sd">                     [5, 6, 7, 8],</span>
<span class="sd">                     [9, 8, 7, 6]],</span>
<span class="sd">                    [[5, 4, 3, 2],</span>
<span class="sd">                     [1, 2, 3, 4],</span>
<span class="sd">                     [5, 6, 7, 8]]])</span>

<span class="sd">  # A main diagonal from each batch.</span>
<span class="sd">  tf.linalg.diag_part(input) ==&gt; [[1, 6, 7],  # Output shape: (2, 3)</span>
<span class="sd">                                  [5, 2, 7]]</span>

<span class="sd">  # A superdiagonal from each batch.</span>
<span class="sd">  tf.linalg.diag_part(input, k = 1)</span>
<span class="sd">    ==&gt; [[2, 7, 6],  # Output shape: (2, 3)</span>
<span class="sd">         [4, 3, 8]]</span>

<span class="sd">  # A band from each batch.</span>
<span class="sd">  tf.linalg.diag_part(input, k = (-1, 2))</span>
<span class="sd">    ==&gt; [[[3, 8, 0],  # Output shape: (2, 4, 3)</span>
<span class="sd">          [2, 7, 6],</span>
<span class="sd">          [1, 6, 7],</span>
<span class="sd">          [0, 5, 8]],</span>
<span class="sd">         [[3, 4, 0],</span>
<span class="sd">          [4, 3, 8],</span>
<span class="sd">          [5, 2, 7],</span>
<span class="sd">          [0, 1, 6]]]</span>

<span class="sd">  # RIGHT_LEFT alignment.</span>
<span class="sd">  tf.linalg.diag_part(input, k = (-1, 2), align=&quot;RIGHT_LEFT&quot;)</span>
<span class="sd">    ==&gt; [[[0, 3, 8],  # Output shape: (2, 4, 3)</span>
<span class="sd">          [2, 7, 6],</span>
<span class="sd">          [1, 6, 7],</span>
<span class="sd">          [5, 8, 0]],</span>
<span class="sd">         [[0, 3, 4],</span>
<span class="sd">          [4, 3, 8],</span>
<span class="sd">          [5, 2, 7],</span>
<span class="sd">          [1, 6, 0]]]</span>

<span class="sd">  # max_diag_len can be shorter than the main diagonal.</span>
<span class="sd">  tf.linalg.diag_part(input, k = (-2, -1))</span>
<span class="sd">    ==&gt; [[[5, 8],</span>
<span class="sd">          [0, 9]],</span>
<span class="sd">         [[1, 6],</span>
<span class="sd">          [0, 5]]]</span>

<span class="sd">  # padding_value = 9</span>
<span class="sd">  tf.linalg.diag_part(input, k = (1, 3), padding_value = 9)</span>
<span class="sd">    ==&gt; [[[4, 9, 9],  # Output shape: (2, 3, 3)</span>
<span class="sd">          [3, 8, 9],</span>
<span class="sd">          [2, 7, 6]],</span>
<span class="sd">         [[2, 9, 9],</span>
<span class="sd">          [3, 4, 9],</span>
<span class="sd">          [4, 3, 8]]]</span>

<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` with `rank k &gt;= 2`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    k: Diagonal offset(s). Positive value means superdiagonal, 0 refers to the</span>
<span class="sd">      main diagonal, and negative value means subdiagonals. `k` can be a single</span>
<span class="sd">      integer (for a single diagonal) or a pair of integers specifying the low</span>
<span class="sd">      and high ends of a matrix band. `k[0]` must not be larger than `k[1]`.</span>
<span class="sd">    padding_value: The value to fill the area outside the specified diagonal</span>
<span class="sd">      band with. Default is 0.</span>
<span class="sd">    align: Some diagonals are shorter than `max_diag_len` and need to be padded.</span>
<span class="sd">      `align` is a string specifying how superdiagonals and subdiagonals should</span>
<span class="sd">      be aligned, respectively. There are four possible alignments: &quot;RIGHT_LEFT&quot;</span>
<span class="sd">      (default), &quot;LEFT_RIGHT&quot;, &quot;LEFT_LEFT&quot;, and &quot;RIGHT_RIGHT&quot;. &quot;RIGHT_LEFT&quot;</span>
<span class="sd">      aligns superdiagonals to the right (left-pads the row) and subdiagonals to</span>
<span class="sd">      the left (right-pads the row). It is the packing format LAPACK uses.</span>
<span class="sd">      cuSPARSE uses &quot;LEFT_RIGHT&quot;, which is the opposite alignment.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor containing diagonals of `input`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Special case to sidestep the tf.constant conversion error:</span>
  <span class="c1"># TypeError: Expected bool, got 0 of type &#39;int&#39; instead.</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;bool&quot;</span><span class="p">:</span>
    <span class="n">padding_value</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">padding_value</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">matrix_diag_part_v3</span><span class="p">(</span>
      <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="n">padding_value</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="n">align</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.set_diag&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;linalg.set_diag&quot;</span><span class="p">,</span> <span class="s2">&quot;matrix_set_diag&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;matrix_set_diag&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matrix_set_diag</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint:disable=redefined-builtin</span>
    <span class="n">diagonal</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;set_diag&quot;</span><span class="p">,</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a batched matrix tensor with new batched diagonal values.</span>

<span class="sd">  Given `input` and `diagonal`, this operation returns a tensor with the</span>
<span class="sd">  same shape and values as `input`, except for the specified diagonals of the</span>
<span class="sd">  innermost matrices. These will be overwritten by the values in `diagonal`.</span>

<span class="sd">  `input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or</span>
<span class="sd">  `k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.</span>
<span class="sd">  Otherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.</span>
<span class="sd">  `num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.</span>
<span class="sd">  `max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,</span>
<span class="sd">  `max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`</span>

<span class="sd">  The output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.</span>
<span class="sd">  If `k` is scalar or `k[0] == k[1]`:</span>

<span class="sd">  ```</span>
<span class="sd">  output[i, j, ..., l, m, n]</span>
<span class="sd">    = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]</span>
<span class="sd">      input[i, j, ..., l, m, n]              ; otherwise</span>
<span class="sd">  ```</span>

<span class="sd">  Otherwise,</span>

<span class="sd">  ```</span>
<span class="sd">  output[i, j, ..., l, m, n]</span>
<span class="sd">    = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] &lt;= d &lt;= k[1]</span>
<span class="sd">      input[i, j, ..., l, m, n]                         ; otherwise</span>
<span class="sd">  ```</span>
<span class="sd">  where `d = n - m`, `diag_index = k[1] - d`, and</span>
<span class="sd">  `index_in_diag = n - max(d, 0) + offset`.</span>

<span class="sd">  `offset` is zero except when the alignment of the diagonal is to the right.</span>
<span class="sd">  ```</span>
<span class="sd">  offset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}</span>
<span class="sd">                                             and `d &gt;= 0`) or</span>
<span class="sd">                                           (`align` in {LEFT_RIGHT, RIGHT_RIGHT}</span>
<span class="sd">                                             and `d &lt;= 0`)</span>
<span class="sd">           0                          ; otherwise</span>
<span class="sd">  ```</span>
<span class="sd">  where `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```</span>
<span class="sd">  # The main diagonal.</span>
<span class="sd">  input = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)</span>
<span class="sd">                     [7, 7, 7, 7],</span>
<span class="sd">                     [7, 7, 7, 7]],</span>
<span class="sd">                    [[7, 7, 7, 7],</span>
<span class="sd">                     [7, 7, 7, 7],</span>
<span class="sd">                     [7, 7, 7, 7]]])</span>
<span class="sd">  diagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)</span>
<span class="sd">                       [4, 5, 6]])</span>
<span class="sd">  tf.matrix_set_diag(input, diagonal)</span>
<span class="sd">    ==&gt; [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)</span>
<span class="sd">          [7, 2, 7, 7],</span>
<span class="sd">          [7, 7, 3, 7]],</span>
<span class="sd">         [[4, 7, 7, 7],</span>
<span class="sd">          [7, 5, 7, 7],</span>
<span class="sd">          [7, 7, 6, 7]]]</span>

<span class="sd">  # A superdiagonal (per batch).</span>
<span class="sd">  tf.matrix_set_diag(input, diagonal, k = 1)</span>
<span class="sd">    ==&gt; [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)</span>
<span class="sd">          [7, 7, 2, 7],</span>
<span class="sd">          [7, 7, 7, 3]],</span>
<span class="sd">         [[7, 4, 7, 7],</span>
<span class="sd">          [7, 7, 5, 7],</span>
<span class="sd">          [7, 7, 7, 6]]]</span>

<span class="sd">  # A band of diagonals.</span>
<span class="sd">  diagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)</span>
<span class="sd">                         [6, 5, 8],</span>
<span class="sd">                         [1, 2, 3],</span>
<span class="sd">                         [0, 4, 5]],</span>
<span class="sd">                        [[1, 2, 0],</span>
<span class="sd">                         [5, 6, 4],</span>
<span class="sd">                         [6, 1, 2],</span>
<span class="sd">                         [0, 3, 4]]])</span>
<span class="sd">  tf.matrix_set_diag(input, diagonals, k = (-1, 2))</span>
<span class="sd">    ==&gt; [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)</span>
<span class="sd">          [4, 2, 5, 1],</span>
<span class="sd">          [7, 5, 3, 8]],</span>
<span class="sd">         [[6, 5, 1, 7],</span>
<span class="sd">          [3, 1, 6, 2],</span>
<span class="sd">          [7, 4, 2, 4]]]</span>

<span class="sd">  # RIGHT_LEFT alignment.</span>
<span class="sd">  diagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)</span>
<span class="sd">                         [6, 5, 8],</span>
<span class="sd">                         [1, 2, 3],</span>
<span class="sd">                         [4, 5, 0]],</span>
<span class="sd">                        [[0, 1, 2],</span>
<span class="sd">                         [5, 6, 4],</span>
<span class="sd">                         [6, 1, 2],</span>
<span class="sd">                         [3, 4, 0]]])</span>
<span class="sd">  tf.matrix_set_diag(input, diagonals, k = (-1, 2), align=&quot;RIGHT_LEFT&quot;)</span>
<span class="sd">    ==&gt; [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)</span>
<span class="sd">          [4, 2, 5, 1],</span>
<span class="sd">          [7, 5, 3, 8]],</span>
<span class="sd">         [[6, 5, 1, 7],</span>
<span class="sd">          [3, 1, 6, 2],</span>
<span class="sd">          [7, 4, 2, 4]]]</span>

<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` with rank `k + 1`, where `k &gt;= 1`.</span>
<span class="sd">    diagonal:  A `Tensor` with rank `k`, when `d_lower == d_upper`, or `k + 1`,</span>
<span class="sd">      otherwise. `k &gt;= 1`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    k: Diagonal offset(s). Positive value means superdiagonal, 0 refers to the</span>
<span class="sd">      main diagonal, and negative value means subdiagonals. `k` can be a single</span>
<span class="sd">      integer (for a single diagonal) or a pair of integers specifying the low</span>
<span class="sd">      and high ends of a matrix band. `k[0]` must not be larger than `k[1]`.</span>
<span class="sd">    align: Some diagonals are shorter than `max_diag_len` and need to be padded.</span>
<span class="sd">      `align` is a string specifying how superdiagonals and subdiagonals should</span>
<span class="sd">      be aligned, respectively. There are four possible alignments: &quot;RIGHT_LEFT&quot;</span>
<span class="sd">      (default), &quot;LEFT_RIGHT&quot;, &quot;LEFT_LEFT&quot;, and &quot;RIGHT_RIGHT&quot;. &quot;RIGHT_LEFT&quot;</span>
<span class="sd">      aligns superdiagonals to the right (left-pads the row) and subdiagonals to</span>
<span class="sd">      the left (right-pads the row). It is the packing format LAPACK uses.</span>
<span class="sd">      cuSPARSE uses &quot;LEFT_RIGHT&quot;, which is the opposite alignment.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">matrix_set_diag_v3</span><span class="p">(</span>
      <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="n">diagonal</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="n">align</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: enable=invalid-name</span>


<span class="k">def</span> <span class="nf">_constant_if_small</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="c1"># Happens when shape is a Tensor, list with Tensor elements, etc.</span>
    <span class="k">pass</span>
  <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_tag_zeros_tensor</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Tags the result of function by setting _is_zeros_tensor attribute.</span>

<span class="sd">  This is useful to compute Hessians of fused ops such as cross_entropy.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">_is_zeros_tensor</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">tensor</span>

  <span class="k">return</span> <span class="n">tf_decorator</span><span class="o">.</span><span class="n">make_decorator</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">wrapped</span><span class="p">)</span>


<div class="viewcode-block" id="zeros"><a class="viewcode-back" href="../../../../index.html#tensorflow.zeros">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;zeros&quot;</span><span class="p">)</span>
<span class="nd">@_tag_zeros_tensor</span>
<span class="k">def</span> <span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a tensor with all elements set to zero.</span>

<span class="sd">  This operation returns a tensor of type `dtype` with shape `shape` and</span>
<span class="sd">  all elements set to zero.</span>

<span class="sd">  &gt;&gt;&gt; tf.zeros([3, 4], tf.int32)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=</span>
<span class="sd">  array([[0, 0, 0, 0],</span>
<span class="sd">         [0, 0, 0, 0],</span>
<span class="sd">         [0, 0, 0, 0]], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: A `list` of integers, a `tuple` of integers, or</span>
<span class="sd">      a 1-D `Tensor` of type `int32`.</span>
<span class="sd">    dtype: The DType of an element in the resulting `Tensor`.</span>
<span class="sd">    name: Optional string. A name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with all elements set to zero.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">shape</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">string</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="c1"># Create a constant if it won&#39;t be very big. Otherwise create a fill</span>
          <span class="c1"># op to prevent serialized GraphDefs from becoming too large.</span>
          <span class="n">output</span> <span class="o">=</span> <span class="n">_constant_if_small</span><span class="p">(</span><span class="n">zero</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>

        <span class="c1"># Go through tensor shapes to get int64-if-needed semantics</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">_tensor_shape_tensor_conversion_function</span><span class="p">(</span>
            <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="c1"># Happens when shape is a list with tensor elements</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">():</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Ensure it&#39;s a vector</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fill</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">constant</span><span class="p">(</span><span class="n">zero</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">==</span> <span class="n">dtype</span>
  <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="zeros_like"><a class="viewcode-back" href="../../../../index.html#tensorflow.zeros_like">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;zeros_like&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a tensor with all elements set to zero.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.zeros`.</span>

<span class="sd">  Given a single tensor (`tensor`), this operation returns a tensor of the</span>
<span class="sd">  same type and shape as `tensor` with all elements set to zero. Optionally,</span>
<span class="sd">  you can use `dtype` to specify a new type for the returned tensor.</span>

<span class="sd">  Examples:</span>

<span class="sd">    &gt;&gt;&gt; tensor = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">    &gt;&gt;&gt; tf.zeros_like(tensor)</span>
<span class="sd">    &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="sd">    array([[0, 0, 0],</span>
<span class="sd">           [0, 0, 0]], dtype=int32)&gt;</span>

<span class="sd">    &gt;&gt;&gt; tf.zeros_like(tensor, dtype=tf.float32)</span>
<span class="sd">    &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=</span>
<span class="sd">    array([[0., 0., 0.],</span>
<span class="sd">           [0., 0., 0.]], dtype=float32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A `Tensor`.</span>
<span class="sd">    dtype: A type for the returned `Tensor`. Must be `float16`, `float32`,</span>
<span class="sd">      `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`,</span>
<span class="sd">      `complex64`, `complex128`, `bool` or `string`. (optional)</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    optimize: if `True`, attempt to statically determine the shape of `tensor`</span>
<span class="sd">      and encode it as a constant. (optional, defaults to `True`)</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with all elements set to zero.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">zeros_like_impl</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;zeros_like&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">zeros_like_v2</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a tensor with all elements set to zero.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.zeros`.</span>

<span class="sd">  Given a single tensor or array-like object (`input`), this operation returns</span>
<span class="sd">  a tensor of the same type and shape as `input` with all elements set to zero.</span>
<span class="sd">  Optionally, you can use `dtype` to specify a new type for the returned tensor.</span>

<span class="sd">  Examples:</span>

<span class="sd">    &gt;&gt;&gt; tensor = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">    &gt;&gt;&gt; tf.zeros_like(tensor)</span>
<span class="sd">    &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="sd">    array([[0, 0, 0],</span>
<span class="sd">           [0, 0, 0]], dtype=int32)&gt;</span>

<span class="sd">    &gt;&gt;&gt; tf.zeros_like(tensor, dtype=tf.float32)</span>
<span class="sd">    &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=</span>
<span class="sd">    array([[0., 0., 0.],</span>
<span class="sd">           [0., 0., 0.]], dtype=float32)&gt;</span>

<span class="sd">    &gt;&gt;&gt; tf.zeros_like([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">    &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="sd">    array([[0, 0, 0],</span>
<span class="sd">           [0, 0, 0]], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` or array-like object.</span>
<span class="sd">    dtype: A type for the returned `Tensor`. Must be `float16`, `float32`,</span>
<span class="sd">      `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`,</span>
<span class="sd">      `complex64`, `complex128`, `bool` or `string` (optional).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with all elements set to zero.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">zeros_like_impl</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="nd">@_tag_zeros_tensor</span>
<span class="k">def</span> <span class="nf">zeros_like_impl</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Internal implementation for the v1/v2 zeros_like API calls.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;zeros_like&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">tensor</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
      <span class="n">tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="n">tensor_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">tensor_dtype</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">tensor_dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">zeros</span><span class="p">(</span>
            <span class="n">shape_internal</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># For now, variant types must be created via zeros_like; as we need to</span>
    <span class="c1"># pass the input variant object to the proper zeros callback.</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">optimize</span> <span class="ow">and</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span> <span class="ow">and</span>
        <span class="n">tensor_dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">):</span>
      <span class="c1"># We can produce a zeros tensor independent of the value of &#39;tensor&#39;,</span>
      <span class="c1"># since the shape is known statically.</span>
      <span class="k">return</span> <span class="n">zeros</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">tensor_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">tensor_dtype</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">zeros</span><span class="p">(</span>
          <span class="n">shape_internal</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="ones_like"><a class="viewcode-back" href="../../../../index.html#tensorflow.ones_like">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ones_like&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">ones_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a tensor with all elements set to 1.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.ones`.</span>

<span class="sd">  Given a single tensor (`tensor`), this operation returns a tensor of the same</span>
<span class="sd">  type and shape as `tensor` with all elements set to 1. Optionally, you can</span>
<span class="sd">  specify a new type (`dtype`) for the returned tensor.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  tensor = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">  tf.ones_like(tensor)  # [[1, 1, 1], [1, 1, 1]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A `Tensor`.</span>
<span class="sd">    dtype: A type for the returned `Tensor`. Must be `float32`, `float64`,</span>
<span class="sd">      `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `complex64`,</span>
<span class="sd">      `complex128` or `bool`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    optimize: if true, attempt to statically determine the shape of &#39;tensor&#39; and</span>
<span class="sd">      encode it as a constant.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with all elements set to 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">ones_like_impl</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;ones_like&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">ones_like_v2</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a tensor of all ones that has the same shape as the input.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.ones`.</span>

<span class="sd">  Given a single tensor (`tensor`), this operation returns a tensor of the</span>
<span class="sd">  same type and shape as `tensor` with all elements set to 1. Optionally,</span>
<span class="sd">  you can use `dtype` to specify a new type for the returned tensor.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; tensor = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">  &gt;&gt;&gt; tf.ones_like(tensor)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="sd">    array([[1, 1, 1],</span>
<span class="sd">           [1, 1, 1]], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`.</span>
<span class="sd">    dtype: A type for the returned `Tensor`. Must be `float16`, `float32`,</span>
<span class="sd">      `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`,</span>
<span class="sd">      `complex64`, `complex128`, `bool` or `string`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with all elements set to one.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">ones_like_impl</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ones_like_impl</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Internal implementation for the v1/v2 ones_like API calls.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;ones_like&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">tensor</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="n">ones_shape</span> <span class="o">=</span> <span class="n">shape_internal</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">ones_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="ones"><a class="viewcode-back" href="../../../../index.html#tensorflow.ones">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;ones&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a tensor with all elements set to one (1).</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.ones_like`.</span>

<span class="sd">  This operation returns a tensor of type `dtype` with shape `shape` and</span>
<span class="sd">  all elements set to one.</span>

<span class="sd">  &gt;&gt;&gt; tf.ones([3, 4], tf.int32)</span>
<span class="sd">  &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 1, 1, 1],</span>
<span class="sd">         [1, 1, 1, 1],</span>
<span class="sd">         [1, 1, 1, 1]], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: A `list` of integers, a `tuple` of integers, or</span>
<span class="sd">      a 1-D `Tensor` of type `int32`.</span>
<span class="sd">    dtype: Optional DType of an element in the resulting `Tensor`. Default is</span>
<span class="sd">      `tf.float32`.</span>
<span class="sd">    name: Optional string. A name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with all elements set to one (1).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;ones&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">shape</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">one</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="c1"># Create a constant if it won&#39;t be very big. Otherwise create a fill</span>
          <span class="c1"># op to prevent serialized GraphDefs from becoming too large.</span>
          <span class="n">output</span> <span class="o">=</span> <span class="n">_constant_if_small</span><span class="p">(</span><span class="n">one</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>

        <span class="c1"># Go through tensor shapes to get int64-if-needed semantics</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">_tensor_shape_tensor_conversion_function</span><span class="p">(</span>
            <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="c1"># Happens when shape is a list with tensor elements</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">():</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Ensure it&#39;s a vector</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fill</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">constant</span><span class="p">(</span><span class="n">one</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">==</span> <span class="n">dtype</span>
  <span class="k">return</span> <span class="n">output</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;placeholder&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Inserts a placeholder for a tensor that will be always fed.</span>

<span class="sd">  **Important**: This tensor will produce an error if evaluated. Its value must</span>
<span class="sd">  be fed using the `feed_dict` optional argument to `Session.run()`,</span>
<span class="sd">  `Tensor.eval()`, or `Operation.run()`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.compat.v1.placeholder(tf.float32, shape=(1024, 1024))</span>
<span class="sd">  y = tf.matmul(x, x)</span>

<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">    print(sess.run(y))  # ERROR: will fail because x was not fed.</span>

<span class="sd">    rand_array = np.random.rand(1024, 1024)</span>
<span class="sd">    print(sess.run(y, feed_dict={x: rand_array}))  # Will succeed.</span>
<span class="sd">  ```</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Placeholders are not compatible with eager execution.</span>
<span class="sd">  @end_compatibility</span>

<span class="sd">  Args:</span>
<span class="sd">    dtype: The type of elements in the tensor to be fed.</span>
<span class="sd">    shape: The shape of the tensor to be fed (optional). If the shape is not</span>
<span class="sd">      specified, you can feed a tensor of any shape.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` that may be used as a handle for feeding a value, but not</span>
<span class="sd">    evaluated directly.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: if eager execution is enabled</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;tf.placeholder() is not compatible with &quot;</span>
                       <span class="s2">&quot;eager execution.&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;placeholder_with_default&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">placeholder_with_default</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;A placeholder op that passes through `input` when its output is not fed.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. The default value to produce when output is not fed.</span>
<span class="sd">    shape: A `tf.TensorShape` or list of `int`s. The (possibly partial) shape of</span>
<span class="sd">      the tensor.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">placeholder_with_default</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.placeholder&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_placeholder&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_placeholder&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Inserts a placeholder for a sparse tensor that will be always fed.</span>

<span class="sd">  **Important**: This sparse tensor will produce an error if evaluated.</span>
<span class="sd">  Its value must be fed using the `feed_dict` optional argument to</span>
<span class="sd">  `Session.run()`, `Tensor.eval()`, or `Operation.run()`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.compat.v1.sparse.placeholder(tf.float32)</span>
<span class="sd">  y = tf.sparse.reduce_sum(x)</span>

<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">    print(sess.run(y))  # ERROR: will fail because x was not fed.</span>

<span class="sd">    indices = np.array([[3, 2, 0], [4, 5, 1]], dtype=np.int64)</span>
<span class="sd">    values = np.array([1.0, 2.0], dtype=np.float32)</span>
<span class="sd">    shape = np.array([7, 9, 2], dtype=np.int64)</span>
<span class="sd">    print(sess.run(y, feed_dict={</span>
<span class="sd">      x: tf.compat.v1.SparseTensorValue(indices, values, shape)}))  # Will</span>
<span class="sd">      succeed.</span>
<span class="sd">    print(sess.run(y, feed_dict={</span>
<span class="sd">      x: (indices, values, shape)}))  # Will succeed.</span>

<span class="sd">    sp = tf.SparseTensor(indices=indices, values=values, dense_shape=shape)</span>
<span class="sd">    sp_value = sp.eval(session=sess)</span>
<span class="sd">    print(sess.run(y, feed_dict={x: sp_value}))  # Will succeed.</span>
<span class="sd">  ```</span>

<span class="sd">  @compatibility{eager} Placeholders are not compatible with eager execution.</span>

<span class="sd">  Args:</span>
<span class="sd">    dtype: The type of `values` elements in the tensor to be fed.</span>
<span class="sd">    shape: The shape of the tensor to be fed (optional). If the shape is not</span>
<span class="sd">      specified, you can feed a sparse tensor of any shape.</span>
<span class="sd">    name: A name for prefixing the operations (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `SparseTensor` that may be used as a handle for feeding a value, but not</span>
<span class="sd">    evaluated directly.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: if eager execution is enabled</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;`sparse_placeholder` is not compatible with &quot;</span>
                       <span class="s2">&quot;eager execution.&quot;</span><span class="p">)</span>

  <span class="n">shape_name</span> <span class="o">=</span> <span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/shape&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
  <span class="n">default_shape_name</span> <span class="o">=</span> <span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/shape_default&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dense_shape</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">shape_name</span><span class="p">)</span>
    <span class="n">dense_shape_default</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value_as_shape</span><span class="p">(</span><span class="n">dense_shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">rank</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">dense_shape_default</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value_as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
      <span class="c1"># determine the shape, to override the `.shape` property of the</span>
      <span class="c1"># `SparseTensor`</span>
      <span class="n">dense_shape_default</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span>
          <span class="nb">tuple</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">dim</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">))</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dim</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">)</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">default_shape_name</span><span class="p">)</span>

    <span class="c1"># `dense_shape` needs to be feedable (for users that treat this as an</span>
    <span class="c1"># actual placeholder). `constant_value_as_shape` sets constants to</span>
    <span class="c1"># not-feedable. placeholder_with_default works, but blocks `SparseTensor`</span>
    <span class="c1"># from reading the default value back out.</span>
    <span class="n">dense_shape</span> <span class="o">=</span> <span class="n">placeholder_with_default</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">shape_name</span><span class="p">)</span>

  <span class="n">result</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span>
      <span class="n">values</span><span class="o">=</span><span class="n">placeholder</span><span class="p">(</span>
          <span class="n">dtype</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span>
          <span class="n">name</span><span class="o">=</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/values&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">),</span>
      <span class="n">indices</span><span class="o">=</span><span class="n">placeholder</span><span class="p">(</span>
          <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">rank</span><span class="p">],</span>
          <span class="n">name</span><span class="o">=</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/indices&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">),</span>
      <span class="n">dense_shape</span><span class="o">=</span><span class="n">dense_shape</span><span class="p">)</span>

  <span class="c1"># Now the SparseTensor.shape is a list of `None`s, since it couldn&#39;t read the</span>
  <span class="c1"># default shape out of the placeholder. Override that</span>
  <span class="c1"># shape to be the value determined here, so partial shapes can be</span>
  <span class="c1"># propagated.</span>
  <span class="n">result</span><span class="o">.</span><span class="n">_dense_shape_default</span> <span class="o">=</span> <span class="n">dense_shape_default</span>
  <span class="k">return</span> <span class="n">result</span>

<span class="c1"># pylint: enable=redefined-outer-name</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">pad_v2</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;CONSTANT&quot;</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pads a tensor.</span>

<span class="sd">  This operation pads a `tensor` according to the `paddings` you specify.</span>
<span class="sd">  `paddings` is an integer tensor with shape `[n, 2]`, where n is the rank of</span>
<span class="sd">  `tensor`. For each dimension D of `input`, `paddings[D, 0]` indicates how</span>
<span class="sd">  many values to add before the contents of `tensor` in that dimension, and</span>
<span class="sd">  `paddings[D, 1]` indicates how many values to add after the contents of</span>
<span class="sd">  `tensor` in that dimension. If `mode` is &quot;REFLECT&quot; then both `paddings[D, 0]`</span>
<span class="sd">  and `paddings[D, 1]` must be no greater than `tensor.dim_size(D) - 1`. If</span>
<span class="sd">  `mode` is &quot;SYMMETRIC&quot; then both `paddings[D, 0]` and `paddings[D, 1]` must be</span>
<span class="sd">  no greater than `tensor.dim_size(D)`.</span>

<span class="sd">  The padded size of each dimension D of the output is:</span>

<span class="sd">  `paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]`</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  t = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">  paddings = tf.constant([[1, 1,], [2, 2]])</span>
<span class="sd">  # &#39;constant_values&#39; is 0.</span>
<span class="sd">  # rank of &#39;t&#39; is 2.</span>
<span class="sd">  tf.pad(t, paddings, &quot;CONSTANT&quot;)  # [[0, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">                                   #  [0, 0, 1, 2, 3, 0, 0],</span>
<span class="sd">                                   #  [0, 0, 4, 5, 6, 0, 0],</span>
<span class="sd">                                   #  [0, 0, 0, 0, 0, 0, 0]]</span>

<span class="sd">  tf.pad(t, paddings, &quot;REFLECT&quot;)  # [[6, 5, 4, 5, 6, 5, 4],</span>
<span class="sd">                                  #  [3, 2, 1, 2, 3, 2, 1],</span>
<span class="sd">                                  #  [6, 5, 4, 5, 6, 5, 4],</span>
<span class="sd">                                  #  [3, 2, 1, 2, 3, 2, 1]]</span>

<span class="sd">  tf.pad(t, paddings, &quot;SYMMETRIC&quot;)  # [[2, 1, 1, 2, 3, 3, 2],</span>
<span class="sd">                                    #  [2, 1, 1, 2, 3, 3, 2],</span>
<span class="sd">                                    #  [5, 4, 4, 5, 6, 6, 5],</span>
<span class="sd">                                    #  [5, 4, 4, 5, 6, 6, 5]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A `Tensor`.</span>
<span class="sd">    paddings: A `Tensor` of type `int32`.</span>
<span class="sd">    mode: One of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot; (case-insensitive)</span>
<span class="sd">    constant_values: In &quot;CONSTANT&quot; mode, the scalar pad value to use. Must be</span>
<span class="sd">      same type as `tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `tensor`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: When mode is not one of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">pad</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">constant_values</span><span class="p">)</span>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../../index.html#tensorflow.pad">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pad&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;CONSTANT&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;Pads a tensor.</span>

<span class="sd">  This operation pads a `tensor` according to the `paddings` you specify.</span>
<span class="sd">  `paddings` is an integer tensor with shape `[n, 2]`, where n is the rank of</span>
<span class="sd">  `tensor`. For each dimension D of `input`, `paddings[D, 0]` indicates how</span>
<span class="sd">  many values to add before the contents of `tensor` in that dimension, and</span>
<span class="sd">  `paddings[D, 1]` indicates how many values to add after the contents of</span>
<span class="sd">  `tensor` in that dimension. If `mode` is &quot;REFLECT&quot; then both `paddings[D, 0]`</span>
<span class="sd">  and `paddings[D, 1]` must be no greater than `tensor.dim_size(D) - 1`. If</span>
<span class="sd">  `mode` is &quot;SYMMETRIC&quot; then both `paddings[D, 0]` and `paddings[D, 1]` must be</span>
<span class="sd">  no greater than `tensor.dim_size(D)`.</span>

<span class="sd">  The padded size of each dimension D of the output is:</span>

<span class="sd">  `paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]`</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  t = tf.constant([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">  paddings = tf.constant([[1, 1,], [2, 2]])</span>
<span class="sd">  # &#39;constant_values&#39; is 0.</span>
<span class="sd">  # rank of &#39;t&#39; is 2.</span>
<span class="sd">  tf.pad(t, paddings, &quot;CONSTANT&quot;)  # [[0, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">                                   #  [0, 0, 1, 2, 3, 0, 0],</span>
<span class="sd">                                   #  [0, 0, 4, 5, 6, 0, 0],</span>
<span class="sd">                                   #  [0, 0, 0, 0, 0, 0, 0]]</span>

<span class="sd">  tf.pad(t, paddings, &quot;REFLECT&quot;)  # [[6, 5, 4, 5, 6, 5, 4],</span>
<span class="sd">                                  #  [3, 2, 1, 2, 3, 2, 1],</span>
<span class="sd">                                  #  [6, 5, 4, 5, 6, 5, 4],</span>
<span class="sd">                                  #  [3, 2, 1, 2, 3, 2, 1]]</span>

<span class="sd">  tf.pad(t, paddings, &quot;SYMMETRIC&quot;)  # [[2, 1, 1, 2, 3, 3, 2],</span>
<span class="sd">                                    #  [2, 1, 1, 2, 3, 3, 2],</span>
<span class="sd">                                    #  [5, 4, 4, 5, 6, 6, 5],</span>
<span class="sd">                                    #  [5, 4, 4, 5, 6, 6, 5]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A `Tensor`.</span>
<span class="sd">    paddings: A `Tensor` of type `int32`.</span>
<span class="sd">    mode: One of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot; (case-insensitive)</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    constant_values: In &quot;CONSTANT&quot; mode, the scalar pad value to use. Must be</span>
<span class="sd">      same type as `tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `tensor`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: When mode is not one of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot;.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Convert lower/mixed case to upper for NumPy compatibility</span>
  <span class="c1"># NumPy uses all lower-case modes.</span>
  <span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;CONSTANT&quot;</span><span class="p">:</span>
    <span class="c1"># TODO(rjryan): Once the forward compatibility period (3 weeks) have passed</span>
    <span class="c1"># remove the &quot;Pad&quot; fallback here.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">constant_values</span><span class="p">)</span> <span class="ow">and</span> <span class="n">constant_values</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">pad_v2</span><span class="p">(</span>
          <span class="n">tensor</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">constant_values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;REFLECT&quot;</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">mirror_pad</span><span class="p">(</span>
        <span class="n">tensor</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;REFLECT&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;SYMMETRIC&quot;</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">mirror_pad</span><span class="p">(</span>
        <span class="n">tensor</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;SYMMETRIC&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown padding mode: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">mode</span><span class="p">)</span>

  <span class="c1"># Restore shape information where possible.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">paddings_constant</span> <span class="o">=</span> <span class="n">_get_paddings_constant</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">result</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">input_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span> <span class="ow">and</span> <span class="n">paddings_constant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
      <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">paddings_constant</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">any</span><span class="p">((</span><span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">)):</span>
          <span class="n">new_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">new_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">+</span> <span class="n">dim</span><span class="p">)</span>
      <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">result</span></div>


<span class="k">def</span> <span class="nf">_get_paddings_constant</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper to get the constant values of the paddings arg to pad().</span>

<span class="sd">  Used under V1 graph mode to facilitate computation of the shape of the output</span>
<span class="sd">  tensor of `pad()`.</span>

<span class="sd">  Args:</span>
<span class="sd">    paddings: The same paddings arg as passed to pad(). Can be a Tensor, or</span>
<span class="sd">      a nested list or tuple of Tensor and/or numbers.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A nested list or numbers or `None`, in which `None` indicates unknown</span>
<span class="sd">    padding size.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">partial</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_get_paddings_constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">paddings</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">paddings</span>


<div class="viewcode-block" id="meshgrid"><a class="viewcode-back" href="../../../../index.html#tensorflow.meshgrid">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;meshgrid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Broadcasts parameters for evaluation on an N-D grid.</span>

<span class="sd">  Given N one-dimensional coordinate arrays `*args`, returns a list `outputs`</span>
<span class="sd">  of N-D coordinate arrays for evaluating expressions on an N-D grid.</span>

<span class="sd">  Notes:</span>

<span class="sd">  `meshgrid` supports cartesian (&#39;xy&#39;) and matrix (&#39;ij&#39;) indexing conventions.</span>
<span class="sd">  When the `indexing` argument is set to &#39;xy&#39; (the default), the broadcasting</span>
<span class="sd">  instructions for the first two dimensions are swapped.</span>

<span class="sd">  Examples:</span>

<span class="sd">  Calling `X, Y = meshgrid(x, y)` with the tensors</span>

<span class="sd">  ```python</span>
<span class="sd">  x = [1, 2, 3]</span>
<span class="sd">  y = [4, 5, 6]</span>
<span class="sd">  X, Y = tf.meshgrid(x, y)</span>
<span class="sd">  # X = [[1, 2, 3],</span>
<span class="sd">  #      [1, 2, 3],</span>
<span class="sd">  #      [1, 2, 3]]</span>
<span class="sd">  # Y = [[4, 4, 4],</span>
<span class="sd">  #      [5, 5, 5],</span>
<span class="sd">  #      [6, 6, 6]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    *args: `Tensor`s with rank 1.</span>
<span class="sd">    **kwargs:</span>
<span class="sd">      - indexing: Either &#39;xy&#39; or &#39;ij&#39; (optional, default: &#39;xy&#39;).</span>
<span class="sd">      - name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    outputs: A list of N `Tensor`s with rank N.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: When no keyword arguments (kwargs) are passed.</span>
<span class="sd">    ValueError: When indexing keyword argument is not one of `xy` or `ij`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">indexing</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;indexing&quot;</span><span class="p">,</span> <span class="s2">&quot;xy&quot;</span><span class="p">)</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;meshgrid&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
    <span class="n">key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is an invalid keyword argument &quot;</span>
                    <span class="s2">&quot;for this function&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">indexing</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;xy&quot;</span><span class="p">,</span> <span class="s2">&quot;ij&quot;</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;indexing parameter must be either &#39;xy&#39; or &#39;ij&#39;&quot;</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;meshgrid&quot;</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">s0</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="n">ndim</span>

    <span class="c1"># Prepare reshape by inserting dimensions with size 1 where needed</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">stack</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">s0</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">s0</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">::])))</span>
    <span class="c1"># Create parameters for broadcasting each tensor to the full size</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>

    <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>

    <span class="k">if</span> <span class="n">indexing</span> <span class="o">==</span> <span class="s2">&quot;xy&quot;</span> <span class="ow">and</span> <span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>
      <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>
      <span class="n">shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># TODO(nolivia): improve performance with a broadcast</span>
    <span class="n">mult_fact</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="n">output_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span> <span class="o">*</span> <span class="n">mult_fact</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span></div>


<span class="n">NEW_AXIS</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">SHRINK_AXIS</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>


<span class="c1"># PEP-8 naming</span>
<span class="c1"># pylint: disable=invalid-name,redefined-outer-name</span>
<span class="k">def</span> <span class="nf">_compute_size_of_strided_dim</span><span class="p">(</span><span class="n">shrink</span><span class="p">,</span> <span class="n">spec</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the size of a single strided slice dimension.&quot;&quot;&quot;</span>

  <span class="n">unknown</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Document what None means here.</span>
  <span class="n">use_full_range</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Document other use of None.</span>
  <span class="c1"># if this is a shrink axis (i.e. a non-range index)</span>
  <span class="c1"># it either will produce an error or return 1</span>
  <span class="k">if</span> <span class="n">shrink</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="n">unknown</span> <span class="ow">or</span> <span class="n">size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="n">unknown</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">unknown</span>
  <span class="n">size</span> <span class="o">=</span> <span class="n">size</span><span class="o">.</span><span class="n">value</span>
  <span class="n">stride</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">step</span>
  <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">unknown</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">unknown</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">step</span>
    <span class="n">valid_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="p">]</span> <span class="k">if</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># PEP-8 naming</span>
    <span class="c1"># pylint: disable=invalid-name</span>
    <span class="k">def</span> <span class="nf">canonical</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="n">use_full_range</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">valid_range</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">if</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">valid_range</span><span class="p">[(</span><span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mi">1</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">x_fwd</span> <span class="o">=</span> <span class="n">size</span> <span class="o">+</span> <span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">x</span>  <span class="c1"># make negative indices positive</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">valid_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">min</span><span class="p">(</span><span class="n">valid_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_fwd</span><span class="p">))</span>

    <span class="n">begin</span> <span class="o">=</span> <span class="n">canonical</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">canonical</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">interval_length</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">begin</span>
    <span class="k">if</span> <span class="n">interval_length</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">((</span><span class="n">interval_length</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="n">stride</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)):</span>
      <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">remainder</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">interval_length</span> <span class="o">%</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
      <span class="k">return</span> <span class="n">interval_length</span> <span class="o">//</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">remainder</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">unknown</span>  <span class="c1"># unknown because stride is unknown</span>


<span class="k">def</span> <span class="nf">_TileGradShape</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Shape function for the TileGrad op.&quot;&quot;&quot;</span>
  <span class="n">multiples_shape</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">input_shape</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="n">multiples_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="c1"># NOTE(mrry): Represent `multiples` as a `TensorShape` because (i)</span>
  <span class="c1"># it is a vector of non-negative integers, and (ii) doing so allows</span>
  <span class="c1"># us to handle partially-known multiples.</span>
  <span class="n">multiples</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value_as_shape</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span>
      <span class="n">input_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">multiples</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">()]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">output_dims</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">multiple</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">,</span> <span class="n">multiples</span><span class="o">.</span><span class="n">dims</span><span class="p">):</span>
      <span class="n">output_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span> <span class="o">//</span> <span class="n">multiple</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">output_dims</span><span class="p">)]</span>


<div class="viewcode-block" id="edit_distance"><a class="viewcode-back" href="../../../../index.html#tensorflow.edit_distance">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;edit_distance&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">edit_distance</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">,</span> <span class="n">truth</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;edit_distance&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the Levenshtein distance between sequences.</span>

<span class="sd">  This operation takes variable-length sequences (`hypothesis` and `truth`),</span>
<span class="sd">  each provided as a `SparseTensor`, and computes the Levenshtein distance.</span>
<span class="sd">  You can normalize the edit distance by length of `truth` by setting</span>
<span class="sd">  `normalize` to true.</span>

<span class="sd">  For example, given the following input:</span>

<span class="sd">  ```python</span>
<span class="sd">  # &#39;hypothesis&#39; is a tensor of shape `[2, 1]` with variable-length values:</span>
<span class="sd">  #   (0,0) = [&quot;a&quot;]</span>
<span class="sd">  #   (1,0) = [&quot;b&quot;]</span>
<span class="sd">  hypothesis = tf.SparseTensor(</span>
<span class="sd">      [[0, 0, 0],</span>
<span class="sd">       [1, 0, 0]],</span>
<span class="sd">      [&quot;a&quot;, &quot;b&quot;],</span>
<span class="sd">      (2, 1, 1))</span>

<span class="sd">  # &#39;truth&#39; is a tensor of shape `[2, 2]` with variable-length values:</span>
<span class="sd">  #   (0,0) = []</span>
<span class="sd">  #   (0,1) = [&quot;a&quot;]</span>
<span class="sd">  #   (1,0) = [&quot;b&quot;, &quot;c&quot;]</span>
<span class="sd">  #   (1,1) = [&quot;a&quot;]</span>
<span class="sd">  truth = tf.SparseTensor(</span>
<span class="sd">      [[0, 1, 0],</span>
<span class="sd">       [1, 0, 0],</span>
<span class="sd">       [1, 0, 1],</span>
<span class="sd">       [1, 1, 0]],</span>
<span class="sd">      [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;],</span>
<span class="sd">      (2, 2, 2))</span>

<span class="sd">  normalize = True</span>
<span class="sd">  ```</span>

<span class="sd">  This operation would return the following:</span>

<span class="sd">  ```python</span>
<span class="sd">  # &#39;output&#39; is a tensor of shape `[2, 2]` with edit distances normalized</span>
<span class="sd">  # by &#39;truth&#39; lengths.</span>
<span class="sd">  output ==&gt; [[inf, 1.0],  # (0,0): no truth, (0,1): no hypothesis</span>
<span class="sd">             [0.5, 1.0]]  # (1,0): addition, (1,1): no hypothesis</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    hypothesis: A `SparseTensor` containing hypothesis sequences.</span>
<span class="sd">    truth: A `SparseTensor` containing truth sequences.</span>
<span class="sd">    normalize: A `bool`. If `True`, normalizes the Levenshtein distance by</span>
<span class="sd">      length of `truth.`</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A dense `Tensor` with rank `R - 1`, where R is the rank of the</span>
<span class="sd">    `SparseTensor` inputs `hypothesis` and `truth`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If either `hypothesis` or `truth` are not a `SparseTensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
      <span class="n">hypothesis</span><span class="p">,</span>
      <span class="p">(</span><span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensorValue</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Hypothesis must be a SparseTensor.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
      <span class="n">truth</span><span class="p">,</span> <span class="p">(</span><span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensorValue</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Truth must be a SparseTensor.&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">edit_distance</span><span class="p">(</span>
      <span class="n">hypothesis</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
      <span class="n">hypothesis</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
      <span class="n">hypothesis</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span>
      <span class="n">truth</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
      <span class="n">truth</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
      <span class="n">truth</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span>
      <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;FakeQuantWithMinMaxArgs&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_FakeQuantWithMinMaxArgsGradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gradient for FakeQuantWithMinMaxArgs op.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">fake_quant_with_min_max_args_gradient</span><span class="p">(</span>
      <span class="n">grad</span><span class="p">,</span>
      <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
      <span class="nb">min</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;min&quot;</span><span class="p">),</span>
      <span class="nb">max</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;max&quot;</span><span class="p">),</span>
      <span class="n">num_bits</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;num_bits&quot;</span><span class="p">),</span>
      <span class="n">narrow_range</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;narrow_range&quot;</span><span class="p">))</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;FakeQuantWithMinMaxVars&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_FakeQuantWithMinMaxVarsGradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gradient for FakeQuantWithMinMaxVars op.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">fake_quant_with_min_max_vars_gradient</span><span class="p">(</span>
      <span class="n">grad</span><span class="p">,</span>
      <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
      <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
      <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
      <span class="n">num_bits</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;num_bits&quot;</span><span class="p">),</span>
      <span class="n">narrow_range</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;narrow_range&quot;</span><span class="p">))</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;FakeQuantWithMinMaxVarsPerChannel&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_FakeQuantWithMinMaxVarsPerChannelGradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gradient for FakeQuantWithMinMaxVarsPerChannel op.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">fake_quant_with_min_max_vars_per_channel_gradient</span><span class="p">(</span>
      <span class="n">grad</span><span class="p">,</span>
      <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
      <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
      <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
      <span class="n">num_bits</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;num_bits&quot;</span><span class="p">),</span>
      <span class="n">narrow_range</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;narrow_range&quot;</span><span class="p">))</span>


<div class="viewcode-block" id="required_space_to_batch_paddings"><a class="viewcode-back" href="../../../../index.html#tensorflow.required_space_to_batch_paddings">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;required_space_to_batch_paddings&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">required_space_to_batch_paddings</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span>
                                     <span class="n">block_shape</span><span class="p">,</span>
                                     <span class="n">base_paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculate padding required to make block_shape divide input_shape.</span>

<span class="sd">  This function can be used to calculate a suitable paddings argument for use</span>
<span class="sd">  with space_to_batch_nd and batch_to_space_nd.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_shape: int32 Tensor of shape [N].</span>
<span class="sd">    block_shape: int32 Tensor of shape [N].</span>
<span class="sd">    base_paddings: Optional int32 Tensor of shape [N, 2].  Specifies the minimum</span>
<span class="sd">      amount of padding to use.  All elements must be &gt;= 0.  If not specified,</span>
<span class="sd">      defaults to 0.</span>
<span class="sd">    name: string.  Optional name prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    (paddings, crops), where:</span>

<span class="sd">    `paddings` and `crops` are int32 Tensors of rank 2 and shape [N, 2]</span>
<span class="sd">    satisfying:</span>

<span class="sd">        paddings[i, 0] = base_paddings[i, 0].</span>
<span class="sd">        0 &lt;= paddings[i, 1] - base_paddings[i, 1] &lt; block_shape[i]</span>
<span class="sd">        (input_shape[i] + paddings[i, 0] + paddings[i, 1]) % block_shape[i] == 0</span>

<span class="sd">        crops[i, 0] = 0</span>
<span class="sd">        crops[i, 1] = paddings[i, 1] - base_paddings[i, 1]</span>

<span class="sd">  Raises: ValueError if called with incompatible shapes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;required_space_to_batch_paddings&quot;</span><span class="p">,</span>
                      <span class="p">[</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">]):</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input_shape&quot;</span><span class="p">)</span>
    <span class="n">block_shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="n">block_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;block_shape&quot;</span><span class="p">)</span>

    <span class="n">block_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">assert_is_fully_defined</span><span class="p">()</span>
    <span class="n">block_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">assert_has_rank</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">num_block_dims</span> <span class="o">=</span> <span class="n">block_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
    <span class="k">if</span> <span class="n">num_block_dims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">input_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">([</span><span class="n">num_block_dims</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">base_paddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">base_paddings</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">base_paddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;base_paddings&quot;</span><span class="p">)</span>
      <span class="n">base_paddings</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">([</span><span class="n">num_block_dims</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">base_paddings</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">([</span><span class="n">num_block_dims</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">const_block_shape</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">block_shape</span><span class="p">)</span>
    <span class="n">const_input_shape</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="n">const_base_paddings</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">base_paddings</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">const_block_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">const_input_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="n">const_base_paddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
      <span class="n">block_shape</span> <span class="o">=</span> <span class="n">const_block_shape</span>
      <span class="n">input_shape</span> <span class="o">=</span> <span class="n">const_input_shape</span>
      <span class="n">base_paddings</span> <span class="o">=</span> <span class="n">const_base_paddings</span>

    <span class="c1"># Use same expression for both constant and non-constant case.</span>
    <span class="n">pad_start</span> <span class="o">=</span> <span class="n">base_paddings</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">orig_pad_end</span> <span class="o">=</span> <span class="n">base_paddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">full_input_shape</span> <span class="o">=</span> <span class="n">input_shape</span> <span class="o">+</span> <span class="n">pad_start</span> <span class="o">+</span> <span class="n">orig_pad_end</span>
    <span class="n">pad_end_extra</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_shape</span> <span class="o">-</span> <span class="n">full_input_shape</span> <span class="o">%</span> <span class="n">block_shape</span><span class="p">)</span> <span class="o">%</span> <span class="n">block_shape</span>
    <span class="n">pad_end</span> <span class="o">=</span> <span class="n">orig_pad_end</span> <span class="o">+</span> <span class="n">pad_end_extra</span>

    <span class="n">result_paddings</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span>
        <span class="p">[[</span><span class="n">pad_start</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">pad_end</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_block_dims</span><span class="p">)],</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;paddings&quot;</span><span class="p">)</span>
    <span class="n">result_crops</span> <span class="o">=</span> <span class="n">stack</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_end_extra</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_block_dims</span><span class="p">)],</span>
                         <span class="n">name</span><span class="o">=</span><span class="s2">&quot;crops&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result_paddings</span><span class="p">,</span> <span class="n">result_crops</span></div>


<div class="viewcode-block" id="space_to_batch"><a class="viewcode-back" href="../../../../index.html#tensorflow.space_to_batch">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;nn.space_to_batch&quot;</span><span class="p">,</span> <span class="s2">&quot;space_to_batch&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;space_to_batch&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">space_to_batch</span><span class="p">(</span>  <span class="c1"># pylint: disable=missing-docstring</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">paddings</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">block_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="n">block_size</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;block_shape&quot;</span><span class="p">,</span>
                                                      <span class="n">block_shape</span><span class="p">,</span> <span class="s2">&quot;block_size&quot;</span><span class="p">,</span>
                                                      <span class="n">block_size</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">space_to_batch_nd</span><span class="p">(</span>
      <span class="nb">input</span><span class="p">,</span>
      <span class="n">paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span>
      <span class="n">block_shape</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">result</span></div>


<span class="n">space_to_batch</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">space_to_batch</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;space_to_batch&quot;</span><span class="p">,</span> <span class="s2">&quot;nn.space_to_batch&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">space_to_batch_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="k">return</span> <span class="n">space_to_batch_nd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">space_to_batch_v2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">space_to_batch_nd</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;nn.space_to_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;space_to_depth&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;space_to_depth&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">space_to_depth</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NHWC&quot;</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">space_to_depth</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">space_to_depth</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">space_to_depth</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;nn.space_to_depth&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">space_to_depth_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NHWC&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">space_to_depth</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">space_to_depth_v2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">space_to_depth</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;nn.depth_to_space&quot;</span><span class="p">,</span> <span class="s2">&quot;depth_to_space&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;depth_to_space&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">depth_to_space</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NHWC&quot;</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">depth_to_space</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">depth_to_space</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">depth_to_space</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;nn.depth_to_space&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">depth_to_space_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NHWC&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">depth_to_space</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">depth_to_space_v2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">depth_to_space</span><span class="o">.</span><span class="vm">__doc__</span>


<div class="viewcode-block" id="batch_to_space"><a class="viewcode-back" href="../../../../index.html#tensorflow.batch_to_space">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;batch_to_space&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">batch_to_space</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">crops</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">block_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin,missing-docstring</span>
  <span class="n">block_size</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;block_shape&quot;</span><span class="p">,</span>
                                                      <span class="n">block_shape</span><span class="p">,</span> <span class="s2">&quot;block_size&quot;</span><span class="p">,</span>
                                                      <span class="n">block_size</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">batch_to_space_nd</span><span class="p">(</span>
      <span class="nb">input</span><span class="p">,</span>
      <span class="n">crops</span><span class="o">=</span><span class="n">crops</span><span class="p">,</span>
      <span class="n">block_shape</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">result</span></div>


<span class="n">batch_to_space</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">batch_to_space</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;batch_to_space&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">batch_to_space_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;BatchToSpace for N-D tensors of type T.</span>

<span class="sd">  This operation reshapes the &quot;batch&quot; dimension 0 into `M + 1` dimensions of</span>
<span class="sd">  shape `block_shape + [batch]`, interleaves these blocks back into the grid</span>
<span class="sd">  defined by the spatial dimensions `[1, ..., M]`, to obtain a result with the</span>
<span class="sd">  same rank as the input.  The spatial dimensions of this intermediate result</span>
<span class="sd">  are then optionally cropped according to `crops` to produce the output.  This</span>
<span class="sd">  is the reverse of SpaceToBatch (see `tf.space_to_batch`).</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A N-D `Tensor` with shape `input_shape = [batch] + spatial_shape +</span>
<span class="sd">      remaining_shape`, where `spatial_shape` has M dimensions.</span>
<span class="sd">    block_shape: A 1-D `Tensor` with shape [M]. Must be one of the following</span>
<span class="sd">      types: `int32`, `int64`. All values must be &gt;= 1. For backwards</span>
<span class="sd">      compatibility with TF 1.0, this parameter may be an int, in which case it</span>
<span class="sd">      is converted to</span>
<span class="sd">      `numpy.array([block_shape, block_shape],</span>
<span class="sd">      dtype=numpy.int64)`.</span>
<span class="sd">    crops: A  2-D `Tensor` with shape `[M, 2]`. Must be one of the</span>
<span class="sd">      following types: `int32`, `int64`. All values must be &gt;= 0.</span>
<span class="sd">      `crops[i] = [crop_start, crop_end]` specifies the amount to crop from</span>
<span class="sd">      input dimension `i + 1`, which corresponds to spatial dimension `i`.</span>
<span class="sd">      It is required that</span>
<span class="sd">      `crop_start[i] + crop_end[i] &lt;= block_shape[i] * input_shape[i + 1]`.</span>
<span class="sd">      This operation is equivalent to the following steps:</span>
<span class="sd">      1. Reshape `input` to `reshaped` of shape: [block_shape[0], ...,</span>
<span class="sd">        block_shape[M-1], batch / prod(block_shape), input_shape[1], ...,</span>
<span class="sd">        input_shape[N-1]]</span>
<span class="sd">      2. Permute dimensions of `reshaped` to produce `permuted` of shape</span>
<span class="sd">         [batch / prod(block_shape),  input_shape[1], block_shape[0], ...,</span>
<span class="sd">         input_shape[M], block_shape[M-1], input_shape[M+1],</span>
<span class="sd">        ..., input_shape[N-1]]</span>
<span class="sd">      3. Reshape `permuted` to produce `reshaped_permuted` of shape</span>
<span class="sd">         [batch / prod(block_shape), input_shape[1] * block_shape[0], ...,</span>
<span class="sd">         input_shape[M] * block_shape[M-1], input_shape[M+1], ...,</span>
<span class="sd">         input_shape[N-1]]</span>
<span class="sd">      4. Crop the start and end of dimensions `[1, ..., M]` of</span>
<span class="sd">         `reshaped_permuted` according to `crops` to produce the output</span>
<span class="sd">         of shape:</span>
<span class="sd">         [batch / prod(block_shape),  input_shape[1] *</span>
<span class="sd">           block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] *</span>
<span class="sd">           block_shape[M-1] - crops[M-1,0] - crops[M-1,1],  input_shape[M+1],</span>
<span class="sd">           ..., input_shape[N-1]]</span>
<span class="sd">      Some Examples:</span>
<span class="sd">      (1) For the following input of shape `[4, 1, 1, 1]`,</span>
<span class="sd">         `block_shape = [2, 2]`, and `crops = [[0, 0], [0, 0]]`:</span>
<span class="sd">         ```python</span>
<span class="sd">         [[[[1]]],</span>
<span class="sd">          [[[2]]],</span>
<span class="sd">          [[[3]]],</span>
<span class="sd">          [[[4]]]]</span>
<span class="sd">         ```</span>
<span class="sd">         The output tensor has shape `[1, 2, 2, 1]` and value:</span>
<span class="sd">         ``` x = [[[[1], [2]],</span>
<span class="sd">                   [[3], [4]]]] ```</span>
<span class="sd">      (2) For the following input of shape `[4, 1, 1, 3]`,</span>
<span class="sd">         `block_shape = [2, 2]`, and `crops = [[0, 0], [0, 0]]`:</span>
<span class="sd">         ```python</span>
<span class="sd">         [[[1,  2,   3]],</span>
<span class="sd">          [[4,  5,   6]],</span>
<span class="sd">          [[7,  8,   9]],</span>
<span class="sd">          [[10, 11, 12]]]</span>
<span class="sd">         ```</span>
<span class="sd">         The output tensor has shape `[1, 2, 2, 3]` and value:</span>
<span class="sd">         ```python</span>
<span class="sd">         x = [[[[1, 2, 3], [4,  5,  6 ]],</span>
<span class="sd">               [[7, 8, 9], [10, 11, 12]]]]</span>
<span class="sd">         ```</span>
<span class="sd">      (3) For the following</span>
<span class="sd">         input of shape `[4, 2, 2, 1]`,</span>
<span class="sd">         `block_shape = [2, 2]`, and `crops = [[0, 0], [0, 0]]`:</span>
<span class="sd">         ```python</span>
<span class="sd">         x = [[[[1], [3]], [[ 9], [11]]],</span>
<span class="sd">              [[[2], [4]], [[10], [12]]],</span>
<span class="sd">              [[[5], [7]], [[13], [15]]],</span>
<span class="sd">              [[[6], [8]], [[14], [16]]]]</span>
<span class="sd">         ```</span>
<span class="sd">         The output tensor has shape `[1, 4, 4, 1]` and value:</span>
<span class="sd">         ```python</span>
<span class="sd">         x = [[[1],  [2],  [ 3], [ 4]],</span>
<span class="sd">              [[5],  [6],  [ 7], [ 8]],</span>
<span class="sd">              [[9],  [10], [11], [12]],</span>
<span class="sd">              [[13], [14], [15], [16]]]</span>
<span class="sd">         ```</span>
<span class="sd">       (4) For the following input of shape</span>
<span class="sd">          `[8, 1, 3, 1]`,</span>
<span class="sd">          `block_shape = [2, 2]`, and `crops = [[0, 0], [2, 0]]`:</span>
<span class="sd">          ```python</span>
<span class="sd">          x = [[[[0], [ 1], [ 3]]],</span>
<span class="sd">               [[[0], [ 9], [11]]],</span>
<span class="sd">               [[[0], [ 2], [ 4]]],</span>
<span class="sd">               [[[0], [10], [12]]],</span>
<span class="sd">               [[[0], [ 5], [ 7]]],</span>
<span class="sd">               [[[0], [13], [15]]],</span>
<span class="sd">               [[[0], [ 6], [ 8]]],</span>
<span class="sd">               [[[0], [14], [16]]]]</span>
<span class="sd">          ```</span>
<span class="sd">          The output tensor has shape `[2, 2, 4, 1]` and value:</span>
<span class="sd">          ```python</span>
<span class="sd">          x = [[[[ 1], [ 2], [ 3], [ 4]],</span>
<span class="sd">                [[ 5], [ 6], [ 7], [ 8]]],</span>
<span class="sd">               [[[ 9], [10], [11], [12]],</span>
<span class="sd">                [[13], [14], [15], [16]]]] ```</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">block_shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">block_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">batch_to_space_nd</span><span class="p">(</span>
      <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">block_shape</span><span class="o">=</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="o">=</span><span class="n">crops</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../../../index.html#tensorflow.one_hot">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;one_hot&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">depth</span><span class="p">,</span>
            <span class="n">on_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">off_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a one-hot tensor.</span>

<span class="sd">  The locations represented by indices in `indices` take value `on_value`,</span>
<span class="sd">  while all other locations take value `off_value`.</span>

<span class="sd">  `on_value` and `off_value` must have matching data types. If `dtype` is also</span>
<span class="sd">  provided, they must be the same data type as specified by `dtype`.</span>

<span class="sd">  If `on_value` is not provided, it will default to the value `1` with type</span>
<span class="sd">  `dtype`</span>

<span class="sd">  If `off_value` is not provided, it will default to the value `0` with type</span>
<span class="sd">  `dtype`</span>

<span class="sd">  If the input `indices` is rank `N`, the output will have rank `N+1`. The</span>
<span class="sd">  new axis is created at dimension `axis` (default: the new axis is appended</span>
<span class="sd">  at the end).</span>

<span class="sd">  If `indices` is a scalar the output shape will be a vector of length `depth`</span>

<span class="sd">  If `indices` is a vector of length `features`, the output shape will be:</span>

<span class="sd">  ```</span>
<span class="sd">    features x depth if axis == -1</span>
<span class="sd">    depth x features if axis == 0</span>
<span class="sd">  ```</span>

<span class="sd">  If `indices` is a matrix (batch) with shape `[batch, features]`, the output</span>
<span class="sd">  shape will be:</span>

<span class="sd">  ```</span>
<span class="sd">    batch x features x depth if axis == -1</span>
<span class="sd">    batch x depth x features if axis == 1</span>
<span class="sd">    depth x batch x features if axis == 0</span>
<span class="sd">  ```</span>

<span class="sd">  If `indices` is a RaggedTensor, the &#39;axis&#39; argument must be positive and refer</span>
<span class="sd">  to a non-ragged axis. The output will be equivalent to applying &#39;one_hot&#39; on</span>
<span class="sd">  the values of the RaggedTensor, and creating a new RaggedTensor from the</span>
<span class="sd">  result.</span>

<span class="sd">  If `dtype` is not provided, it will attempt to assume the data type of</span>
<span class="sd">  `on_value` or `off_value`, if one or both are passed in. If none of</span>
<span class="sd">  `on_value`, `off_value`, or `dtype` are provided, `dtype` will default to the</span>
<span class="sd">  value `tf.float32`.</span>

<span class="sd">  Note: If a non-numeric data type output is desired (`tf.string`, `tf.bool`,</span>
<span class="sd">  etc.), both `on_value` and `off_value` _must_ be provided to `one_hot`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  indices = [0, 1, 2]</span>
<span class="sd">  depth = 3</span>
<span class="sd">  tf.one_hot(indices, depth)  # output: [3 x 3]</span>
<span class="sd">  # [[1., 0., 0.],</span>
<span class="sd">  #  [0., 1., 0.],</span>
<span class="sd">  #  [0., 0., 1.]]</span>

<span class="sd">  indices = [0, 2, -1, 1]</span>
<span class="sd">  depth = 3</span>
<span class="sd">  tf.one_hot(indices, depth,</span>
<span class="sd">             on_value=5.0, off_value=0.0,</span>
<span class="sd">             axis=-1)  # output: [4 x 3]</span>
<span class="sd">  # [[5.0, 0.0, 0.0],  # one_hot(0)</span>
<span class="sd">  #  [0.0, 0.0, 5.0],  # one_hot(2)</span>
<span class="sd">  #  [0.0, 0.0, 0.0],  # one_hot(-1)</span>
<span class="sd">  #  [0.0, 5.0, 0.0]]  # one_hot(1)</span>

<span class="sd">  indices = [[0, 2], [1, -1]]</span>
<span class="sd">  depth = 3</span>
<span class="sd">  tf.one_hot(indices, depth,</span>
<span class="sd">             on_value=1.0, off_value=0.0,</span>
<span class="sd">             axis=-1)  # output: [2 x 2 x 3]</span>
<span class="sd">  # [[[1.0, 0.0, 0.0],   # one_hot(0)</span>
<span class="sd">  #   [0.0, 0.0, 1.0]],  # one_hot(2)</span>
<span class="sd">  #  [[0.0, 1.0, 0.0],   # one_hot(1)</span>
<span class="sd">  #   [0.0, 0.0, 0.0]]]  # one_hot(-1)</span>

<span class="sd">  indices = tf.ragged.constant([[0, 1], [2]])</span>
<span class="sd">  depth = 3</span>
<span class="sd">  tf.one_hot(indices, depth)  # output: [2 x None x 3]</span>
<span class="sd">  # [[[1., 0., 0.],</span>
<span class="sd">  #   [0., 1., 0.]],</span>
<span class="sd">  #  [[0., 0., 1.]]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    indices: A `Tensor` of indices.</span>
<span class="sd">    depth: A scalar defining the depth of the one hot dimension.</span>
<span class="sd">    on_value: A scalar defining the value to fill in output when `indices[j]</span>
<span class="sd">      = i`. (default: 1)</span>
<span class="sd">    off_value: A scalar defining the value to fill in output when `indices[j]</span>
<span class="sd">      != i`. (default: 0)</span>
<span class="sd">    axis: The axis to fill (default: -1, a new inner-most axis).</span>
<span class="sd">    dtype: The data type of the output tensor.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    output: The one-hot tensor.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If dtype of either `on_value` or `off_value` don&#39;t match `dtype`</span>
<span class="sd">    TypeError: If dtype of `on_value` and `off_value` don&#39;t match one another</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
      <span class="n">name</span><span class="p">,</span> <span class="s2">&quot;one_hot&quot;</span><span class="p">,</span>
      <span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">on_exists</span> <span class="o">=</span> <span class="n">on_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">off_exists</span> <span class="o">=</span> <span class="n">off_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">on_exists</span><span class="p">:</span>
      <span class="n">on_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">on_value</span><span class="p">,</span> <span class="n">dtype_hint</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">off_exists</span><span class="p">:</span>
      <span class="n">off_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">off_value</span><span class="p">,</span> <span class="n">dtype_hint</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">on_dtype</span> <span class="o">=</span> <span class="n">on_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="k">if</span> <span class="n">on_exists</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">off_dtype</span> <span class="o">=</span> <span class="n">off_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="k">if</span> <span class="n">off_exists</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">on_exists</span> <span class="ow">or</span> <span class="n">off_exists</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Ensure provided on_value and/or off_value match dtype</span>
        <span class="k">if</span> <span class="n">on_exists</span> <span class="ow">and</span> <span class="n">on_dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;dtype </span><span class="si">{0}</span><span class="s2"> of on_value does not match &quot;</span>
                          <span class="s2">&quot;dtype parameter </span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">on_dtype</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">off_exists</span> <span class="ow">and</span> <span class="n">off_dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;dtype </span><span class="si">{0}</span><span class="s2"> of off_value does not match &quot;</span>
                          <span class="s2">&quot;dtype parameter </span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">off_dtype</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># dtype not provided: automatically assign it</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">on_dtype</span> <span class="k">if</span> <span class="n">on_exists</span> <span class="k">else</span> <span class="n">off_dtype</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># None of on_value, off_value, or dtype provided. Default dtype to float32</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">on_exists</span><span class="p">:</span>
      <span class="c1"># on_value not provided: assign to value 1 of type dtype</span>
      <span class="n">on_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;on_value&quot;</span><span class="p">)</span>
      <span class="n">on_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">off_exists</span><span class="p">:</span>
      <span class="c1"># off_value not provided: assign to value 0 of type dtype</span>
      <span class="n">off_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;off_value&quot;</span><span class="p">)</span>
      <span class="n">off_dtype</span> <span class="o">=</span> <span class="n">dtype</span>

    <span class="k">if</span> <span class="n">on_dtype</span> <span class="o">!=</span> <span class="n">off_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;dtype </span><span class="si">{0}</span><span class="s2"> of on_value does not match &quot;</span>
                      <span class="s2">&quot;dtype </span><span class="si">{1}</span><span class="s2"> of off_value&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">on_dtype</span><span class="p">,</span> <span class="n">off_dtype</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
                                 <span class="n">name</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_all_dimensions</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a 1D-tensor listing all dimensions in x.&quot;&quot;&quot;</span>
  <span class="c1"># Fast path: avoid creating Rank and Range ops if ndims is known.</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span> <span class="ow">and</span>
      <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>  <span class="c1"># sparse.dense_shape is 1-D.</span>
    <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">r</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="c1"># Otherwise, we rely on `range` and `rank` to do the right thing at runtime.</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>


<div class="viewcode-block" id="sequence_mask"><a class="viewcode-back" href="../../../../index.html#tensorflow.sequence_mask">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sequence_mask&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sequence_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a mask tensor representing the first N positions of each cell.</span>

<span class="sd">  If `lengths` has shape `[d_1, d_2, ..., d_n]` the resulting tensor `mask` has</span>
<span class="sd">  dtype `dtype` and shape `[d_1, d_2, ..., d_n, maxlen]`, with</span>

<span class="sd">  ```</span>
<span class="sd">  mask[i_1, i_2, ..., i_n, j] = (j &lt; lengths[i_1, i_2, ..., i_n])</span>
<span class="sd">  ```</span>

<span class="sd">  Examples:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],</span>
<span class="sd">                                  #  [True, True, True, False, False],</span>
<span class="sd">                                  #  [True, True, False, False, False]]</span>

<span class="sd">  tf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],</span>
<span class="sd">                                    #   [True, True, True]],</span>
<span class="sd">                                    #  [[True, True, False],</span>
<span class="sd">                                    #   [False, False, False]]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    lengths: integer tensor, all its values &lt;= maxlen.</span>
<span class="sd">    maxlen: scalar integer tensor, size of last dimension of returned tensor.</span>
<span class="sd">      Default is the maximum value in `lengths`.</span>
<span class="sd">    dtype: output type of the resulting tensor.</span>
<span class="sd">    name: name of the op.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A mask tensor of shape `lengths.shape + (maxlen,)`, cast to specified dtype.</span>
<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if `maxlen` is not a scalar.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;SequenceMask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">lengths</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">]):</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">maxlen</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">maxlen</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_max</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">_all_dimensions</span><span class="p">(</span><span class="n">lengths</span><span class="p">))</span>
      <span class="n">maxlen</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">maxlen</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">maxlen</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">maxlen</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">maxlen</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;maxlen must be scalar for sequence_mask&quot;</span><span class="p">)</span>

    <span class="c1"># The basic idea is to compare a range row vector of size maxlen:</span>
    <span class="c1"># [0, 1, 2, 3, 4]</span>
    <span class="c1"># to length as a matrix with 1 column: [[1], [3], [2]].</span>
    <span class="c1"># Because of broadcasting on both arguments this comparison results</span>
    <span class="c1"># in a matrix of size (len(lengths), maxlen)</span>
    <span class="n">row_vector</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span>
        <span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">,</span> <span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="c1"># Since maxlen &gt;= max(lengths), it is safe to use maxlen as a cast</span>
    <span class="c1"># authoritative type. Whenever maxlen fits into tf.int32, so do the lengths.</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">maxlen</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">row_vector</span> <span class="o">&lt;</span> <span class="n">matrix</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">result</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">==</span> <span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="squeeze"><a class="viewcode-back" href="../../../../index.html#tensorflow.squeeze">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;squeeze&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use the `axis` argument instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;squeeze_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">squeeze_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Removes dimensions of size 1 from the shape of a tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of the same type with</span>
<span class="sd">  all dimensions of size 1 removed. If you don&#39;t want to remove all size 1</span>
<span class="sd">  dimensions, you can remove specific size 1 dimensions by specifying</span>
<span class="sd">  `axis`.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; # &#39;t&#39; is a tensor of shape [1, 2, 1, 3, 1, 1]</span>
<span class="sd">  &gt;&gt;&gt; t = tf.ones([1, 2, 1, 3, 1, 1])</span>
<span class="sd">  &gt;&gt;&gt; print(tf.shape(tf.squeeze(t)).numpy())</span>
<span class="sd">  [2 3]</span>

<span class="sd">  Or, to remove specific size 1 dimensions:</span>

<span class="sd">  &gt;&gt;&gt; # &#39;t&#39; is a tensor of shape [1, 2, 1, 3, 1, 1]</span>
<span class="sd">  &gt;&gt;&gt; t = tf.ones([1, 2, 1, 3, 1, 1])</span>
<span class="sd">  &gt;&gt;&gt; print(tf.shape(tf.squeeze(t, [2, 4])).numpy())</span>
<span class="sd">  [1 2 3 1]</span>

<span class="sd">  Note: if `input` is a `tf.RaggedTensor`, then this operation takes `O(N)`</span>
<span class="sd">  time, where `N` is the number of elements in the squeezed dimensions.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. The `input` to squeeze.</span>
<span class="sd">    axis: An optional list of `ints`. Defaults to `[]`. If specified, only</span>
<span class="sd">      squeezes the dimensions listed. The dimension index starts at 0. It is an</span>
<span class="sd">      error to squeeze a dimension that is not 1. Must be in the range</span>
<span class="sd">      `[-rank(input), rank(input))`. Must be specified if `input` is a</span>
<span class="sd">      `RaggedTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    squeeze_dims: Deprecated keyword argument that is now axis.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    Contains the same data as `input`, but has one or more dimensions of</span>
<span class="sd">    size 1 removed.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: When both `squeeze_dims` and `axis` are specified.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;squeeze_dims&quot;</span><span class="p">,</span>
                                                <span class="n">squeeze_dims</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;squeeze&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">squeeze_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes dimensions of size 1 from the shape of a tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of the same type with</span>
<span class="sd">  all dimensions of size 1 removed. If you don&#39;t want to remove all size 1</span>
<span class="sd">  dimensions, you can remove specific size 1 dimensions by specifying</span>
<span class="sd">  `axis`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  # &#39;t&#39; is a tensor of shape [1, 2, 1, 3, 1, 1]</span>
<span class="sd">  tf.shape(tf.squeeze(t))  # [2, 3]</span>
<span class="sd">  ```</span>

<span class="sd">  Or, to remove specific size 1 dimensions:</span>

<span class="sd">  ```python</span>
<span class="sd">  # &#39;t&#39; is a tensor of shape [1, 2, 1, 3, 1, 1]</span>
<span class="sd">  tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]</span>
<span class="sd">  ```</span>

<span class="sd">  Unlike the older op `tf.compat.v1.squeeze`, this op does not accept a</span>
<span class="sd">  deprecated `squeeze_dims` argument.</span>

<span class="sd">  Note: if `input` is a `tf.RaggedTensor`, then this operation takes `O(N)`</span>
<span class="sd">  time, where `N` is the number of elements in the squeezed dimensions.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. The `input` to squeeze.</span>
<span class="sd">    axis: An optional list of `ints`. Defaults to `[]`. If specified, only</span>
<span class="sd">      squeezes the dimensions listed. The dimension index starts at 0. It is an</span>
<span class="sd">      error to squeeze a dimension that is not 1. Must be in the range</span>
<span class="sd">      `[-rank(input), rank(input))`. Must be specified if `input` is a</span>
<span class="sd">      `RaggedTensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    Contains the same data as `input`, but has one or more dimensions of</span>
<span class="sd">    size 1 removed.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: The input cannot be converted to a tensor, or the specified</span>
<span class="sd">      axis cannot be squeezed.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="k">return</span> <span class="n">squeeze</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="where"><a class="viewcode-back" href="../../../../index.html#tensorflow.where">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;where&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">where</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return the elements, either from `x` or `y`, depending on the `condition`.</span>

<span class="sd">  If both `x` and `y` are None, then this operation returns the coordinates of</span>
<span class="sd">  true elements of `condition`.  The coordinates are returned in a 2-D tensor</span>
<span class="sd">  where the first dimension (rows) represents the number of true elements, and</span>
<span class="sd">  the second dimension (columns) represents the coordinates of the true</span>
<span class="sd">  elements. Keep in mind, the shape of the output tensor can vary depending on</span>
<span class="sd">  how many true values there are in input. Indices are output in row-major</span>
<span class="sd">  order.</span>

<span class="sd">  If both non-None, `x` and `y` must have the same shape.</span>
<span class="sd">  The `condition` tensor must be a scalar if `x` and `y` are scalar.</span>
<span class="sd">  If `x` and `y` are tensors of higher rank, then `condition` must be either a</span>
<span class="sd">  vector with size matching the first dimension of `x`, or must have the same</span>
<span class="sd">  shape as `x`.</span>

<span class="sd">  The `condition` tensor acts as a mask that chooses, based on the value at each</span>
<span class="sd">  element, whether the corresponding element / row in the output should be taken</span>
<span class="sd">  from `x` (if true) or `y` (if false).</span>

<span class="sd">  If `condition` is a vector and `x` and `y` are higher rank matrices, then it</span>
<span class="sd">  chooses which row (outer dimension) to copy from `x` and `y`. If `condition`</span>
<span class="sd">  has the same shape as `x` and `y`, then it chooses which element to copy from</span>
<span class="sd">  `x` and `y`.</span>

<span class="sd">  Args:</span>
<span class="sd">    condition: A `Tensor` of type `bool`</span>
<span class="sd">    x: A Tensor which may have the same shape as `condition`. If `condition` is</span>
<span class="sd">      rank 1, `x` may have higher rank, but its first dimension must match the</span>
<span class="sd">      size of `condition`.</span>
<span class="sd">    y: A `tensor` with the same shape and type as `x`.</span>
<span class="sd">    name: A name of the operation (optional)</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with the same type and shape as `x`, `y` if they are non-None.</span>
<span class="sd">    Otherwise, a `Tensor` with shape `(num_true, rank(condition))`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: When exactly one of `x` or `y` is non-None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Where&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">condition</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">condition</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">condition</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;condition&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="n">condition</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x and y must both be non-None or both be None.&quot;</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;where&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;where_v2&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">where_v2</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return the elements where `condition` is `True` (multiplexing `x` and `y`).</span>

<span class="sd">  This operator has two modes: in one mode both `x` and `y` are provided, in</span>
<span class="sd">  another mode neither are provided. `condition` is always expected to be a</span>
<span class="sd">  `tf.Tensor` of type `bool`.</span>

<span class="sd">  #### Retrieving indices of `True` elements</span>

<span class="sd">  If `x` and `y` are not provided (both are None):</span>

<span class="sd">  `tf.where` will return the indices of `condition` that are `True`, in</span>
<span class="sd">  the form of a 2-D tensor with shape (n, d).</span>
<span class="sd">  (Where n is the number of matching indices in `condition`,</span>
<span class="sd">  and d is the number of dimensions in `condition`).</span>

<span class="sd">  Indices are output in row-major order.</span>

<span class="sd">  &gt;&gt;&gt; tf.where([True, False, False, True])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy=</span>
<span class="sd">  array([[0],</span>
<span class="sd">         [3]])&gt;</span>

<span class="sd">  &gt;&gt;&gt; tf.where([[True, False], [False, True]])</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy=</span>
<span class="sd">  array([[0, 0],</span>
<span class="sd">         [1, 1]])&gt;</span>

<span class="sd">  &gt;&gt;&gt; tf.where([[[True, False], [False, True], [True, True]]])</span>
<span class="sd">  &lt;tf.Tensor: shape=(4, 3), dtype=int64, numpy=</span>
<span class="sd">  array([[0, 0, 0],</span>
<span class="sd">         [0, 1, 1],</span>
<span class="sd">         [0, 2, 0],</span>
<span class="sd">         [0, 2, 1]])&gt;</span>

<span class="sd">  #### Multiplexing between `x` and `y`</span>

<span class="sd">  If `x` and `y` are provided (both have non-None values):</span>

<span class="sd">  `tf.where` will choose an output shape from the shapes of `condition`, `x`,</span>
<span class="sd">  and `y` that all three shapes are</span>
<span class="sd">  [broadcastable](https://docs.scipy.org/doc/numpy/reference/ufuncs.html) to.</span>

<span class="sd">  The `condition` tensor acts as a mask that chooses whether the corresponding</span>
<span class="sd">  element / row in the output should be taken from `x`</span>
<span class="sd">  (if the elemment in `condition is True) or `y` (if it is false).</span>

<span class="sd">  &gt;&gt;&gt; tf.where([True, False, False, True], [1,2,3,4], [100,200,300,400])</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 200, 300,   4],</span>
<span class="sd">  dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.where([True, False, False, True], [1,2,3,4], [100])</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   4],</span>
<span class="sd">  dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.where([True, False, False, True], [1,2,3,4], 100)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   4],</span>
<span class="sd">  dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.where([True, False, False, True], 1, 100)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   1],</span>
<span class="sd">  dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; tf.where(True, [1,2,3,4], 100)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4],</span>
<span class="sd">  dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.where(False, [1,2,3,4], 100)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([100, 100, 100, 100],</span>
<span class="sd">  dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    condition: A `tf.Tensor` of type `bool`</span>
<span class="sd">    x: If provided, a Tensor which is of the same type as `y`, and has a shape</span>
<span class="sd">      broadcastable with `condition` and `y`.</span>
<span class="sd">    y: If provided, a Tensor which is of the same type as `y`, and has a shape</span>
<span class="sd">      broadcastable with `condition` and `x`.</span>
<span class="sd">    name: A name of the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    If `x` and `y` are provided:</span>
<span class="sd">      A `Tensor` with the same type as `x` and `y`, and shape that</span>
<span class="sd">      is broadcast from `condition`, `x`, and `y`.</span>
<span class="sd">    Otherwise, a `Tensor` with shape `(num_true, dim_size(condition))`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: When exactly one of `x` or `y` is non-None, or the shapes</span>
<span class="sd">      are not all broadcastable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Where&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">condition</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">condition</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">condition</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;condition&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="n">condition</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">select_v2</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="n">condition</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x and y must both be non-None or both be None.&quot;</span><span class="p">)</span>


<span class="c1"># pylint: disable=redefined-builtin</span>
<div class="viewcode-block" id="reverse_sequence"><a class="viewcode-back" href="../../../../index.html#tensorflow.reverse_sequence">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;reverse_sequence&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;seq_dim is deprecated, use seq_axis instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;seq_dim&quot;</span><span class="p">)</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;batch_dim is deprecated, use batch_axis instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;batch_dim&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reverse_sequence</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
                     <span class="n">seq_lengths</span><span class="p">,</span>
                     <span class="n">seq_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">batch_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">seq_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">batch_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Reverses variable length slices.</span>

<span class="sd">  This op first slices `input` along the dimension `batch_axis`, and for</span>
<span class="sd">  each slice `i`, reverses the first `seq_lengths[i]` elements along the</span>
<span class="sd">  dimension `seq_axis`.</span>

<span class="sd">  The elements of `seq_lengths` must obey `seq_lengths[i] &lt;=</span>
<span class="sd">  input.dims[seq_dim]`, and `seq_lengths` must be a vector of length</span>
<span class="sd">  `input.dims[batch_dim]`.</span>

<span class="sd">  The output slice `i` along dimension `batch_axis` is then given by</span>
<span class="sd">  input slice `i`, with the first `seq_lengths[i]` slices along</span>
<span class="sd">  dimension `seq_axis` reversed.</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; seq_lengths = [7, 2, 3, 5]</span>
<span class="sd">  &gt;&gt;&gt; input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">  ...          [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]</span>
<span class="sd">  &gt;&gt;&gt; output = tf.reverse_sequence(input, seq_lengths, seq_axis=1, batch_axis=0)</span>
<span class="sd">  &gt;&gt;&gt; output</span>
<span class="sd">  &lt;tf.Tensor: shape=(4, 8), dtype=int32, numpy=</span>
<span class="sd">  array([[0, 0, 5, 4, 3, 2, 1, 0],</span>
<span class="sd">         [2, 1, 0, 0, 0, 0, 0, 0],</span>
<span class="sd">         [3, 2, 1, 4, 0, 0, 0, 0],</span>
<span class="sd">         [5, 4, 3, 2, 1, 6, 7, 8]], dtype=int32)&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    `input`: A `Tensor`. The input to reverse.</span>
<span class="sd">    `seq_lengths`: A `Tensor`. Must be one of the following types: `int32`,</span>
<span class="sd">      `int64`. 1-D with length `input.dims(batch_dim)` and `max(seq_lengths) &lt;=</span>
<span class="sd">      input.dims(seq_dim)`</span>
<span class="sd">    `seq_axis`: An `int`. The dimension which is partially reversed.</span>
<span class="sd">    `batch_axis`: An optional `int`. Defaults to `0`. The dimension along which</span>
<span class="sd">      reversal is performed.</span>
<span class="sd">    `name`: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor. Has the same type as input.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">seq_axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;seq_axis&quot;</span><span class="p">,</span> <span class="n">seq_axis</span><span class="p">,</span>
                                                    <span class="s2">&quot;seq_dim&quot;</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">)</span>
  <span class="n">batch_axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;batch_axis&quot;</span><span class="p">,</span> <span class="n">batch_axis</span><span class="p">,</span>
                                                      <span class="s2">&quot;batch_dim&quot;</span><span class="p">,</span> <span class="n">batch_dim</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
      <span class="n">seq_lengths</span><span class="o">=</span><span class="n">seq_lengths</span><span class="p">,</span>
      <span class="n">seq_dim</span><span class="o">=</span><span class="n">seq_axis</span><span class="p">,</span>
      <span class="n">batch_dim</span><span class="o">=</span><span class="n">batch_axis</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;reverse_sequence&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">reverse_sequence_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
                        <span class="n">seq_lengths</span><span class="p">,</span>
                        <span class="n">seq_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">batch_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
      <span class="n">seq_lengths</span><span class="o">=</span><span class="n">seq_lengths</span><span class="p">,</span>
      <span class="n">seq_dim</span><span class="o">=</span><span class="n">seq_axis</span><span class="p">,</span>
      <span class="n">batch_dim</span><span class="o">=</span><span class="n">batch_axis</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<span class="n">reverse_sequence_v2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">reverse_sequence</span><span class="o">.</span><span class="vm">__doc__</span>
<span class="c1"># pylint: enable=redefined-builtin</span>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../../index.html#tensorflow.gather">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gather&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">params</span><span class="p">,</span>
           <span class="n">indices</span><span class="p">,</span>
           <span class="n">validate_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>  <span class="c1"># pylint: disable=g-doc-args</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gather slices from params axis `axis` according to indices.</span>

<span class="sd">  Gather slices from params axis `axis` according to `indices`.  `indices` must</span>
<span class="sd">  be an integer tensor of any dimension (usually 0-D or 1-D).</span>

<span class="sd">  For 0-D (scalar) `indices`:</span>

<span class="sd">  $$\begin{align*}</span>
<span class="sd">  output[p_0, ..., p_{axis-1}, &amp;&amp;          &amp;&amp;&amp; p_{axis + 1}, ..., p_{N-1}] = \\</span>
<span class="sd">  params[p_0, ..., p_{axis-1}, &amp;&amp; indices, &amp;&amp;&amp; p_{axis + 1}, ..., p_{N-1}]</span>
<span class="sd">  \end{align*}$$</span>

<span class="sd">  Where *N* = `ndims(params)`.</span>

<span class="sd">  For 1-D (vector) `indices` with `batch_dims=0`:</span>

<span class="sd">  $$\begin{align*}</span>
<span class="sd">  output[p_0, ..., p_{axis-1}, &amp;&amp;         &amp;i,  &amp;&amp;p_{axis + 1}, ..., p_{N-1}] =\\</span>
<span class="sd">  params[p_0, ..., p_{axis-1}, &amp;&amp; indices[&amp;i], &amp;&amp;p_{axis + 1}, ..., p_{N-1}]</span>
<span class="sd">  \end{align*}$$</span>

<span class="sd">  In the general case, produces an output tensor where:</span>

<span class="sd">  $$\begin{align*}</span>
<span class="sd">  output[p_0,             &amp;..., p_{axis-1},                       &amp;</span>
<span class="sd">         &amp;i_{B},           ..., i_{M-1},                          &amp;</span>
<span class="sd">         p_{axis + 1},    &amp;..., p_{N-1}]                          = \\</span>
<span class="sd">  params[p_0,             &amp;..., p_{axis-1},                       &amp;</span>
<span class="sd">         indices[p_0, ..., p_{B-1}, &amp;i_{B}, ..., i_{M-1}],        &amp;</span>
<span class="sd">         p_{axis + 1},    &amp;..., p_{N-1}]</span>
<span class="sd">  \end{align*}$$</span>

<span class="sd">  Where *N* = `ndims(params)`, *M* = `ndims(indices)`, and *B* = `batch_dims`.</span>
<span class="sd">  Note that `params.shape[:batch_dims]` must be identical to</span>
<span class="sd">  `indices.shape[:batch_dims]`.</span>

<span class="sd">  The shape of the output tensor is:</span>

<span class="sd">  &gt; `output.shape = params.shape[:axis] + indices.shape[batch_dims:] +</span>
<span class="sd">  &gt; params.shape[axis + 1:]`.</span>

<span class="sd">  Note that on CPU, if an out of bound index is found, an error is returned.</span>
<span class="sd">  On GPU, if an out of bound index is found, a 0 is stored in the corresponding</span>
<span class="sd">  output value.</span>

<span class="sd">  See also `tf.gather_nd`.</span>

<span class="sd">  &lt;div style=&quot;width:70%; margin:auto; margin-bottom:10px; margin-top:20px;&quot;&gt;</span>
<span class="sd">  &lt;img style=&quot;width:100%&quot; src=&quot;https://www.tensorflow.org/images/Gather.png&quot;</span>
<span class="sd">  alt&gt;</span>
<span class="sd">  &lt;/div&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    params: The `Tensor` from which to gather values. Must be at least rank</span>
<span class="sd">      `axis + 1`.</span>
<span class="sd">    indices: The index `Tensor`.  Must be one of the following types: `int32`,</span>
<span class="sd">      `int64`. Must be in range `[0, params.shape[axis])`.</span>
<span class="sd">    validate_indices: Deprecated, does nothing.</span>
<span class="sd">    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`. The</span>
<span class="sd">      `axis` in `params` to gather `indices` from. Must be greater than or equal</span>
<span class="sd">      to `batch_dims`.  Defaults to the first non-batch dimension. Supports</span>
<span class="sd">      negative indexes.</span>
<span class="sd">    batch_dims: An `integer`.  The number of batch dimensions.  Must be less</span>
<span class="sd">      than or equal to `rank(indices)`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `params`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">del</span> <span class="n">validate_indices</span>

  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">batch_dims</span>
  <span class="k">if</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">gather_v2</span><span class="p">(</span>
        <span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="c1"># TODO(apassos) find a less bad way of detecting resource variables</span>
    <span class="c1"># without introducing a circular dependency.</span>
    <span class="k">return</span> <span class="n">params</span><span class="o">.</span><span class="n">sparse_read</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">gather_v2</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;gather&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">gather_v2</span><span class="p">(</span><span class="n">params</span><span class="p">,</span>
              <span class="n">indices</span><span class="p">,</span>
              <span class="n">validate_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gather</span><span class="p">(</span>
      <span class="n">params</span><span class="p">,</span>
      <span class="n">indices</span><span class="p">,</span>
      <span class="n">validate_indices</span><span class="o">=</span><span class="n">validate_indices</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
      <span class="n">batch_dims</span><span class="o">=</span><span class="n">batch_dims</span><span class="p">)</span>


<span class="n">gather_v2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gather</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;batch_gather&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2017-10-25&quot;</span><span class="p">,</span> <span class="s2">&quot;`tf.batch_gather` is deprecated, please use `tf.gather` &quot;</span>
    <span class="s2">&quot;with `batch_dims=-1` instead.&quot;</span><span class="p">)</span>  <span class="c1"># pylint: disable=missing-docstring</span>
<span class="k">def</span> <span class="nf">batch_gather</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gather slices from params according to indices with leading batch dims.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;BatchGather&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">]):</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;indices&quot;</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;params&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;batch_gather does not allow indices with unknown shape.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_batch_gather</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_batch_gather</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gather slices from params according to indices with leading batch dims.</span>

<span class="sd">  This operation assumes that the leading `batch_dims` dimensions of `indices`</span>
<span class="sd">  and `params` are batch dimensions; and performs a `tf.gather` operation within</span>
<span class="sd">  each batch. (If `batch_dims` is not specified, then it defaults to</span>
<span class="sd">  `rank(indices)-1`.)  In the case in which `batch_dims==0`, this operation</span>
<span class="sd">  is equivalent to `tf.gather`.</span>

<span class="sd">  Args:</span>
<span class="sd">    params: A Tensor. The tensor from which to gather values.</span>
<span class="sd">    indices: A Tensor. Must be one of the following types: int32, int64. Index</span>
<span class="sd">      tensor. Must be in range `[0, params.shape[batch_dims]]`.</span>
<span class="sd">    batch_dims: An integer or none.  The number of batch dimensions.  Must be</span>
<span class="sd">      less than `rank(indices)`.  Defaults to `rank(indices) - 1` if None.</span>
<span class="sd">    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`. The</span>
<span class="sd">      `axis` in `params` to gather `indices` from. Must be greater than or equal</span>
<span class="sd">      to `batch_dims`.  Defaults to the first non-batch dimension. Supports</span>
<span class="sd">      negative indexes.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor. Has the same type as `params`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if `indices` has an unknown shape.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">batch_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;batch_dims must be an int; got </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch_dims</span><span class="p">,))</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;indices&quot;</span><span class="p">)</span>
  <span class="n">params</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;params&quot;</span><span class="p">)</span>

  <span class="n">indices_ndims</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
  <span class="k">if</span> <span class="n">indices_ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tf.gather does not allow indices with unknown &quot;</span>
                     <span class="s2">&quot;rank when batch_dims is specified.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">batch_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">batch_dims</span> <span class="o">=</span> <span class="n">indices_ndims</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">batch_dims</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">batch_dims</span> <span class="o">+=</span> <span class="n">indices_ndims</span>
  <span class="k">if</span> <span class="n">batch_dims</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">batch_dims</span> <span class="o">&gt;=</span> <span class="n">indices_ndims</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;batch_dims = </span><span class="si">%d</span><span class="s2"> must be less than rank(indices) = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">indices_ndims</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">batch_dims</span> <span class="o">&gt;=</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;batch_dims = </span><span class="si">%d</span><span class="s2"> must be less than rank(params) = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">))</span>

  <span class="c1"># Handle axis by transposing the axis dimension to be the first non-batch</span>
  <span class="c1"># dimension, recursively calling batch_gather with axis=0, and then</span>
  <span class="c1"># transposing the result to put the pre-axis dimensions before the indices</span>
  <span class="c1"># dimensions.</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">!=</span> <span class="n">batch_dims</span><span class="p">:</span>
    <span class="c1"># Adjust axis to be positive.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">params</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;axis (</span><span class="si">%d</span><span class="s2">) out of range [</span><span class="si">%d</span><span class="s2">, </span><span class="si">%d</span><span class="s2">)&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">+=</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
      <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="n">batch_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;batch_dims = </span><span class="si">%d</span><span class="s2"> must be less than or equal to &quot;</span>
                         <span class="s2">&quot;axis = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>

    <span class="c1"># Move params[axis] up to params[batch_dims].</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)),</span> <span class="p">[</span><span class="n">axis</span><span class="p">],</span>
        <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rank</span><span class="p">(</span><span class="n">params</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">concat</span><span class="p">(</span><span class="n">perm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">_batch_gather</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="n">batch_dims</span><span class="p">)</span>

    <span class="c1"># Move the result dimensions corresponding to params[batch_dims:axis]</span>
    <span class="c1"># to just before the dimensions corresponding to indices[batch_dims:].</span>
    <span class="n">params_start</span> <span class="o">=</span> <span class="n">indices_ndims</span> <span class="o">+</span> <span class="n">axis</span> <span class="o">-</span> <span class="n">batch_dims</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)),</span>
        <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">indices_ndims</span><span class="p">,</span> <span class="n">params_start</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">indices_ndims</span><span class="p">)),</span>
        <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">params_start</span><span class="p">,</span> <span class="n">rank</span><span class="p">(</span><span class="n">result</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">concat</span><span class="p">(</span><span class="n">perm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

  <span class="n">indices_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
  <span class="n">params_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
  <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span>
  <span class="n">indices_dtype</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
  <span class="n">accum_dim_value</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">indices_dtype</span><span class="p">)</span>
  <span class="c1"># Use correct type for offset index computation</span>
  <span class="n">casted_params_shape</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">params_shape</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">dim_value</span> <span class="o">=</span> <span class="n">casted_params_shape</span><span class="p">[</span><span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">accum_dim_value</span> <span class="o">*=</span> <span class="n">casted_params_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">indices_dtype</span><span class="p">)</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">indices_dtype</span><span class="p">)</span>
    <span class="n">dim_indices</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dim_value</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">dim_indices</span> <span class="o">*=</span> <span class="n">accum_dim_value</span>
    <span class="n">dim_shape</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">dim_value</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">indices_ndims</span> <span class="o">-</span> <span class="n">dim</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">batch_indices</span> <span class="o">+=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">dim_indices</span><span class="p">,</span> <span class="n">dim_shape</span><span class="p">)</span>

  <span class="n">flat_indices</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">batch_indices</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">outer_shape</span> <span class="o">=</span> <span class="n">params_shape</span><span class="p">[</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
  <span class="n">flat_inner_shape</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">params_shape</span><span class="p">[:</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                       <span class="kc">False</span><span class="p">)</span>

  <span class="n">flat_params</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">concat</span><span class="p">([[</span><span class="n">flat_inner_shape</span><span class="p">],</span> <span class="n">outer_shape</span><span class="p">],</span>
                                       <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="n">flat_result</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span><span class="n">flat_params</span><span class="p">,</span> <span class="n">flat_indices</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">flat_result</span><span class="p">,</span> <span class="n">concat</span><span class="p">([</span><span class="n">indices_shape</span><span class="p">,</span> <span class="n">outer_shape</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="n">final_shape</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[:</span><span class="n">batch_dims</span><span class="p">]</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span>
      <span class="n">params</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[:</span><span class="n">batch_dims</span><span class="p">])</span>
  <span class="n">final_shape</span> <span class="o">=</span> <span class="n">final_shape</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="n">batch_dims</span><span class="p">:])</span>
  <span class="n">final_shape</span> <span class="o">=</span> <span class="n">final_shape</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:])</span>
  <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">final_shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span>


<div class="viewcode-block" id="gather_nd"><a class="viewcode-back" href="../../../../index.html#tensorflow.gather_nd">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gather_nd&quot;</span><span class="p">,</span> <span class="s2">&quot;manip.gather_nd&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;manip.gather_nd&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gather slices from `params` into a Tensor with shape specified by `indices`.</span>

<span class="sd">  `indices` is an K-dimensional integer tensor, best thought of as a</span>
<span class="sd">  (K-1)-dimensional tensor of indices into `params`, where each element defines</span>
<span class="sd">  a slice of `params`:</span>

<span class="sd">      output[\\(i_0, ..., i_{K-2}\\)] = params[indices[\\(i_0, ..., i_{K-2}\\)]]</span>

<span class="sd">  Whereas in `tf.gather` `indices` defines slices into the first</span>
<span class="sd">  dimension of `params`, in `tf.gather_nd`, `indices` defines slices into the</span>
<span class="sd">  first `N` dimensions of `params`, where `N = indices.shape[-1]`.</span>

<span class="sd">  The last dimension of `indices` can be at most the rank of</span>
<span class="sd">  `params`:</span>

<span class="sd">      indices.shape[-1] &lt;= params.rank</span>

<span class="sd">  The last dimension of `indices` corresponds to elements</span>
<span class="sd">  (if `indices.shape[-1] == params.rank`) or slices</span>
<span class="sd">  (if `indices.shape[-1] &lt; params.rank`) along dimension `indices.shape[-1]`</span>
<span class="sd">  of `params`.  The output tensor has shape</span>

<span class="sd">      indices.shape[:-1] + params.shape[indices.shape[-1]:]</span>

<span class="sd">  Additionally both &#39;params&#39; and &#39;indices&#39; can have M leading batch</span>
<span class="sd">  dimensions that exactly match. In this case &#39;batch_dims&#39; must be M.</span>

<span class="sd">  Note that on CPU, if an out of bound index is found, an error is returned.</span>
<span class="sd">  On GPU, if an out of bound index is found, a 0 is stored in the</span>
<span class="sd">  corresponding output value.</span>

<span class="sd">  Some examples below.</span>

<span class="sd">  Simple indexing into a matrix:</span>

<span class="sd">  ```python</span>
<span class="sd">      indices = [[0, 0], [1, 1]]</span>
<span class="sd">      params = [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;]]</span>
<span class="sd">      output = [&#39;a&#39;, &#39;d&#39;]</span>
<span class="sd">  ```</span>

<span class="sd">  Slice indexing into a matrix:</span>

<span class="sd">  ```python</span>
<span class="sd">      indices = [[1], [0]]</span>
<span class="sd">      params = [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;]]</span>
<span class="sd">      output = [[&#39;c&#39;, &#39;d&#39;], [&#39;a&#39;, &#39;b&#39;]]</span>
<span class="sd">  ```</span>

<span class="sd">  Indexing into a 3-tensor:</span>

<span class="sd">  ```python</span>
<span class="sd">      indices = [[1]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>


<span class="sd">      indices = [[0, 1], [1, 0]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[&#39;c0&#39;, &#39;d0&#39;], [&#39;a1&#39;, &#39;b1&#39;]]</span>


<span class="sd">      indices = [[0, 0, 1], [1, 0, 1]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [&#39;b0&#39;, &#39;b1&#39;]</span>
<span class="sd">  ```</span>

<span class="sd">  The examples below are for the case when only indices have leading extra</span>
<span class="sd">  dimensions. If both &#39;params&#39; and &#39;indices&#39; have leading batch dimensions, use</span>
<span class="sd">  the &#39;batch_dims&#39; parameter to run gather_nd in batch mode.</span>

<span class="sd">  Batched indexing into a matrix:</span>

<span class="sd">  ```python</span>
<span class="sd">      indices = [[[0, 0]], [[0, 1]]]</span>
<span class="sd">      params = [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;]]</span>
<span class="sd">      output = [[&#39;a&#39;], [&#39;b&#39;]]</span>
<span class="sd">  ```</span>

<span class="sd">  Batched slice indexing into a matrix:</span>

<span class="sd">  ```python</span>
<span class="sd">      indices = [[[1]], [[0]]]</span>
<span class="sd">      params = [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;]]</span>
<span class="sd">      output = [[[&#39;c&#39;, &#39;d&#39;]], [[&#39;a&#39;, &#39;b&#39;]]]</span>
<span class="sd">  ```</span>

<span class="sd">  Batched indexing into a 3-tensor:</span>

<span class="sd">  ```python</span>
<span class="sd">      indices = [[[1]], [[0]]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[[[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]],</span>
<span class="sd">                [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]]]]</span>

<span class="sd">      indices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[[&#39;c0&#39;, &#39;d0&#39;], [&#39;a1&#39;, &#39;b1&#39;]],</span>
<span class="sd">                [[&#39;a0&#39;, &#39;b0&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>


<span class="sd">      indices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[&#39;b0&#39;, &#39;b1&#39;], [&#39;d0&#39;, &#39;c1&#39;]]</span>
<span class="sd">  ```</span>

<span class="sd">  Examples with batched &#39;params&#39; and &#39;indices&#39;:</span>

<span class="sd">  ```python</span>
<span class="sd">      batch_dims = 1</span>
<span class="sd">      indices = [[1], [0]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[&#39;c0&#39;, &#39;d0&#39;], [&#39;a1&#39;, &#39;b1&#39;]]</span>

<span class="sd">      batch_dims = 1</span>
<span class="sd">      indices = [[[1]], [[0]]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[[&#39;c0&#39;, &#39;d0&#39;]], [[&#39;a1&#39;, &#39;b1&#39;]]]</span>

<span class="sd">      batch_dims = 1</span>
<span class="sd">      indices = [[[1, 0]], [[0, 1]]]</span>
<span class="sd">      params = [[[&#39;a0&#39;, &#39;b0&#39;], [&#39;c0&#39;, &#39;d0&#39;]],</span>
<span class="sd">                [[&#39;a1&#39;, &#39;b1&#39;], [&#39;c1&#39;, &#39;d1&#39;]]]</span>
<span class="sd">      output = [[&#39;c0&#39;], [&#39;b1&#39;]]</span>
<span class="sd">  ```</span>

<span class="sd">  See also `tf.gather`.</span>

<span class="sd">  Args:</span>
<span class="sd">    params: A `Tensor`. The tensor from which to gather values.</span>
<span class="sd">    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.</span>
<span class="sd">      Index tensor.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    batch_dims: An integer or a scalar &#39;Tensor&#39;. The number of batch dimensions.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `params`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">batch_dims_</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">batch_dims_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">batch_dims</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">batch_dims_</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">batch_dims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># TODO(apassos) find a less bad way of detecting resource variables</span>
      <span class="c1"># without introducing a circular dependency.</span>
      <span class="k">return</span> <span class="n">params</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">batch_gather_nd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;gather_nd&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">gather_nd_v2</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gather_nd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="n">batch_dims</span><span class="p">)</span>


<span class="n">gather_nd_v2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gather_nd</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="k">def</span> <span class="nf">batch_gather_nd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;gather_nd implementation with batch support.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;BatchGatherND&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">]):</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;indices&quot;</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;params&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;batch_dims must be an int; got </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch_dims</span><span class="p">,))</span>
    <span class="k">if</span> <span class="n">batch_dims</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tf.gather_nd does not allow negative batch_dims.&quot;</span><span class="p">)</span>
    <span class="n">params_ndims</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
    <span class="n">indices_ndims</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
    <span class="k">if</span> <span class="n">indices_ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">batch_dims</span> <span class="o">&gt;=</span> <span class="n">indices_ndims</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;batch_dims = </span><span class="si">%d</span><span class="s2"> must be less than rank(indices) = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">indices_ndims</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">params_ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">batch_dims</span> <span class="o">&gt;=</span> <span class="n">params_ndims</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;batch_dims = </span><span class="si">%d</span><span class="s2"> must be less than rank(params) = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">params_ndims</span><span class="p">))</span>

    <span class="n">expand</span> <span class="o">=</span> <span class="n">batch_dims</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">expand</span><span class="p">:</span>
      <span class="c1"># Normally gather_nd will be called when batch_dims == 0.</span>
      <span class="c1"># But if this function is called with batch_dims = 0, e.g. for testing</span>
      <span class="c1"># purposes, this adds a dummy batch dimension to make batch_dims = 1.</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">indices</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">batch_dims</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">params_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">indices_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">params_shape</span><span class="p">[:</span><span class="n">batch_dims</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">index_internal_ndims</span> <span class="o">=</span> <span class="n">rank</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">-</span> <span class="n">batch_dims</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">indices_internal_shape</span> <span class="o">=</span> <span class="n">indices_shape</span><span class="p">[</span><span class="n">batch_dims</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Assuming a &#39;params&#39; with shape [b1, ..., bM, g1, ..., gN] and an &#39;indices&#39;</span>
    <span class="c1"># with shape [b1, ..., bM, i1, ..., iK, C], where C &lt;= N, we need to modify</span>
    <span class="c1"># &#39;indices&#39; s.t. it has shape [i1, ..., iK, D], where D &lt;= M + N and slices</span>
    <span class="c1"># to the entire &#39;params&#39; tensor.</span>
    <span class="c1"># Assuming we have a batch of shape [B1, B2], we use meshgrid to create a</span>
    <span class="c1"># grid of size B1 x B2.</span>
    <span class="n">batch_dim_list</span> <span class="o">=</span> <span class="n">unstack</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dim_ranges</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch_dim_list</span>
    <span class="p">]</span>
    <span class="n">mesh_list</span> <span class="o">=</span> <span class="n">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">dim_ranges</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim_ranges</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="c1"># Then we flatten and stack the tensors to form a (B1.B2) by 2 matrix.</span>
    <span class="n">flat_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mesh_list</span><span class="p">]</span>
    <span class="n">index_grid</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">stack</span><span class="p">(</span><span class="n">flat_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="c1"># We need to concatenate these batch coordinates with the internal indices.</span>
    <span class="c1"># concat -&gt; index_grid [B1.B2, 2] with indices [i1, ..., iK, C]</span>
    <span class="c1"># So we reshape them both to [(B1.B2), i1, ..., iK, *]</span>
    <span class="n">index_grid_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">index_grid</span><span class="p">)</span>
    <span class="n">index_grid</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span>
        <span class="n">index_grid</span><span class="p">,</span>
        <span class="n">concat</span><span class="p">([</span>
            <span class="n">index_grid_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">ones</span><span class="p">(</span><span class="n">index_internal_ndims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">index_grid_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="p">],</span>
               <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">tile_shape</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(((</span><span class="mi">1</span><span class="p">,),</span> <span class="n">indices_internal_shape</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">index_grid</span> <span class="o">=</span> <span class="n">tile</span><span class="p">(</span><span class="n">index_grid</span><span class="p">,</span> <span class="n">multiples</span><span class="o">=</span><span class="n">tile_shape</span><span class="p">)</span>
    <span class="c1"># index_grid now has shape [(B1.B2), i1, ..., iK, 2]</span>
    <span class="n">flat_shape</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(([</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">indices_shape</span><span class="p">[</span><span class="n">batch_dims</span><span class="p">:]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">flat_indices</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">flat_shape</span><span class="p">)</span>
    <span class="c1"># flat_indices now has shape [(B1.B2), i1, ..., iK, C]</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">concat</span><span class="p">((</span><span class="n">index_grid</span><span class="p">,</span> <span class="n">flat_indices</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># indices has shape [(B1.B2), i1, ..., iK, 2+C]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="c1"># out has shape [(B1.B2), i1, ..., iK, N-C]. Now we reshape batch to</span>
    <span class="c1"># its original form.</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">concat</span><span class="p">((</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">expand</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">out</span>


<span class="c1"># Define quantize_v2 here in order to make name the second-to-last attribute,</span>
<span class="c1"># because round_mode was added later.</span>
<span class="c1"># (And also now because of &#39;axis&#39; processing).</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;quantize_v2&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2017-10-25&quot;</span><span class="p">,</span>
    <span class="s2">&quot;`tf.quantize_v2` is deprecated, please use `tf.quantization.quantize` &quot;</span>
    <span class="s2">&quot;instead.&quot;</span><span class="p">)</span>  <span class="c1"># pylint: disable=missing-docstring</span>
<span class="k">def</span> <span class="nf">quantize_v2</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">min_range</span><span class="p">,</span>
    <span class="n">max_range</span><span class="p">,</span>
    <span class="n">T</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;MIN_COMBINED&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">round_mode</span><span class="o">=</span><span class="s2">&quot;HALF_AWAY_FROM_ZERO&quot;</span><span class="p">,</span>
    <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ensure_minimum_range</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
  <span class="k">elif</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input should have known rank to use negative axis.&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">%=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>

  <span class="k">if</span> <span class="n">ensure_minimum_range</span> <span class="o">!=</span> <span class="mf">0.01</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">quantize_v2</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">min_range</span><span class="p">,</span>
        <span class="n">max_range</span><span class="p">,</span>
        <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">round_mode</span><span class="o">=</span><span class="n">round_mode</span><span class="p">,</span>
        <span class="n">narrow_range</span><span class="o">=</span><span class="n">narrow_range</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
        <span class="n">ensure_minimum_range</span><span class="o">=</span><span class="n">ensure_minimum_range</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">quantize_v2</span><span class="p">(</span>
      <span class="nb">input</span><span class="p">,</span>
      <span class="n">min_range</span><span class="p">,</span>
      <span class="n">max_range</span><span class="p">,</span>
      <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span>
      <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">round_mode</span><span class="o">=</span><span class="n">round_mode</span><span class="p">,</span>
      <span class="n">narrow_range</span><span class="o">=</span><span class="n">narrow_range</span><span class="p">,</span>
      <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>


<span class="n">quantize_v2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Please use `tf.quantization.quantize` instead.&quot;&quot;&quot;</span>


<span class="c1"># We want to expose tf.quantization.quantize instead of</span>
<span class="c1"># tf.quantization.quantize; we can deprecate tf.quantization.quantize in next</span>
<span class="c1"># version of TensorFlow.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;quantization.quantize&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;quantization.quantize&quot;</span><span class="p">,</span> <span class="s2">&quot;quantize&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;quantize&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">min_range</span><span class="p">,</span>
    <span class="n">max_range</span><span class="p">,</span>
    <span class="n">T</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;MIN_COMBINED&quot;</span><span class="p">,</span>
    <span class="n">round_mode</span><span class="o">=</span><span class="s2">&quot;HALF_AWAY_FROM_ZERO&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ensure_minimum_range</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Quantize the input tensor.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">ensure_minimum_range</span> <span class="o">!=</span> <span class="mf">0.01</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">quantize_v2</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">min_range</span><span class="p">,</span>
        <span class="n">max_range</span><span class="p">,</span>
        <span class="n">T</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="n">round_mode</span><span class="o">=</span><span class="n">round_mode</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">narrow_range</span><span class="o">=</span><span class="n">narrow_range</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
        <span class="n">ensure_minimum_range</span><span class="o">=</span><span class="n">ensure_minimum_range</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">quantize_v2</span><span class="p">(</span>
      <span class="nb">input</span><span class="p">,</span>
      <span class="n">min_range</span><span class="p">,</span>
      <span class="n">max_range</span><span class="p">,</span>
      <span class="n">T</span><span class="p">,</span>
      <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
      <span class="n">round_mode</span><span class="o">=</span><span class="n">round_mode</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">narrow_range</span><span class="o">=</span><span class="n">narrow_range</span><span class="p">,</span>
      <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;quantization.dequantize&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;quantization.dequantize&quot;</span><span class="p">,</span>
                                          <span class="s2">&quot;dequantize&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;dequantize&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dequantize</span><span class="p">(</span>  <span class="c1"># pylint: disable=missing-docstring</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">min_range</span><span class="p">,</span>
    <span class="n">max_range</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;MIN_COMBINED&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
  <span class="k">elif</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input should have known rank to use negative axis.&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">%=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>

  <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">narrow_range</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">min_range</span><span class="p">,</span>
        <span class="n">max_range</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">narrow_range</span><span class="o">=</span><span class="n">narrow_range</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span>
      <span class="nb">input</span><span class="p">,</span> <span class="n">min_range</span><span class="p">,</span> <span class="n">max_range</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="n">dequantize</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">dequantize</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;quantization.quantize_and_dequantize&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">quantize_and_dequantize</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="n">input_min</span><span class="p">,</span>
    <span class="n">input_max</span><span class="p">,</span>
    <span class="n">signed_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">range_given</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">round_mode</span><span class="o">=</span><span class="s2">&quot;HALF_TO_EVEN&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Quantizes then dequantizes a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor` to quantize and dequantize.</span>
<span class="sd">    input_min: If range_given=True, the minimum input value, that needs to be</span>
<span class="sd">      represented in the quantized representation. If axis is specified, this</span>
<span class="sd">      should be a vector of minimum values for each slice along axis.</span>
<span class="sd">    input_max: If range_given=True, the maximum input value that needs to be</span>
<span class="sd">      represented in the quantized representation. If axis is specified, this</span>
<span class="sd">      should be a vector of maximum values for each slice along axis.</span>
<span class="sd">    signed_input: True if the quantization is signed or unsigned.</span>
<span class="sd">    num_bits: The bitwidth of the quantization.</span>
<span class="sd">    range_given: If true use `input_min` and `input_max` for the range of the</span>
<span class="sd">      input, otherwise determine min and max from the input `Tensor`.</span>
<span class="sd">    round_mode: Rounding mode when rounding from float values to quantized ones.</span>
<span class="sd">      one of [&#39;HALF_TO_EVEN&#39;, &#39;HALF_UP&#39;]</span>
<span class="sd">    name: Optional name for the operation.</span>
<span class="sd">    narrow_range: If true, then the absolute value of the quantized minimum</span>
<span class="sd">      value is the same as the quantized maximum value, instead of 1 greater.</span>
<span class="sd">      i.e. for 8 bit quantization, the minimum value is -127 instead of -128.</span>
<span class="sd">    axis: Integer. If specified, refers to a dimension of the input tensor, such</span>
<span class="sd">      that quantization will be per slice along that dimension.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Each element is the result of quantizing and dequantizing the</span>
<span class="sd">    corresponding element of `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
  <span class="k">elif</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input should have known rank to use negative axis.&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">%=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>

  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">quantize_and_dequantize_v2</span><span class="p">(</span>
      <span class="nb">input</span><span class="p">,</span>
      <span class="n">input_min</span><span class="o">=</span><span class="n">input_min</span><span class="p">,</span>
      <span class="n">input_max</span><span class="o">=</span><span class="n">input_max</span><span class="p">,</span>
      <span class="n">signed_input</span><span class="o">=</span><span class="n">signed_input</span><span class="p">,</span>
      <span class="n">num_bits</span><span class="o">=</span><span class="n">num_bits</span><span class="p">,</span>
      <span class="n">range_given</span><span class="o">=</span><span class="n">range_given</span><span class="p">,</span>
      <span class="n">round_mode</span><span class="o">=</span><span class="n">round_mode</span><span class="p">,</span>
      <span class="n">narrow_range</span><span class="o">=</span><span class="n">narrow_range</span><span class="p">,</span>
      <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="searchsorted"><a class="viewcode-back" href="../../../../index.html#tensorflow.searchsorted">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;searchsorted&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">searchsorted</span><span class="p">(</span><span class="n">sorted_sequence</span><span class="p">,</span>
                 <span class="n">values</span><span class="p">,</span>
                 <span class="n">side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span>
                 <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Searches input tensor for values on the innermost dimension.</span>

<span class="sd">  A 2-D example:</span>

<span class="sd">  ```</span>
<span class="sd">    sorted_sequence = [[0, 3, 9, 9, 10],</span>
<span class="sd">                       [1, 2, 3, 4, 5]]</span>
<span class="sd">    values = [[2, 4, 9],</span>
<span class="sd">              [0, 2, 6]]</span>

<span class="sd">    result = searchsorted(sorted_sequence, values, side=&quot;left&quot;)</span>

<span class="sd">    result == [[1, 2, 2],</span>
<span class="sd">               [0, 1, 5]]</span>

<span class="sd">    result = searchsorted(sorted_sequence, values, side=&quot;right&quot;)</span>

<span class="sd">    result == [[1, 2, 4],</span>
<span class="sd">               [0, 2, 5]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    sorted_sequence: N-D `Tensor` containing a sorted sequence.</span>
<span class="sd">    values: N-D `Tensor` containing the search values.</span>
<span class="sd">    side: &#39;left&#39; or &#39;right&#39;; &#39;left&#39; corresponds to lower_bound and &#39;right&#39; to</span>
<span class="sd">      upper_bound.</span>
<span class="sd">    out_type: The output type (`int32` or `int64`).  Default is `tf.int32`.</span>
<span class="sd">    name: Optional name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An N-D `Tensor` the size of values containing the result of applying either</span>
<span class="sd">    lower_bound or upper_bound (depending on side) to each value.  The result</span>
<span class="sd">    is not a global index to the entire `Tensor`, but the index in the last</span>
<span class="sd">    dimension.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the last dimension of `sorted_sequence &gt;= 2^31-1` elements.</span>
<span class="sd">                If the total size of values exceeds `2^31 - 1` elements.</span>
<span class="sd">                If the first `N-1` dimensions of the two tensors don&#39;t match.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">sequence_size</span> <span class="o">=</span> <span class="n">shape_internal</span><span class="p">(</span><span class="n">sorted_sequence</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">values_size</span> <span class="o">=</span> <span class="n">shape_internal</span><span class="p">(</span><span class="n">values</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">sorted_sequence_2d</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">sorted_sequence</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_size</span><span class="p">])</span>
  <span class="n">values_2d</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">values_size</span><span class="p">])</span>
  <span class="k">if</span> <span class="n">side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">upper_bound</span><span class="p">(</span><span class="n">sorted_sequence_2d</span><span class="p">,</span> <span class="n">values_2d</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span>
                                       <span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">lower_bound</span><span class="p">(</span><span class="n">sorted_sequence_2d</span><span class="p">,</span> <span class="n">values_2d</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span>
                                       <span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;side must be either &#39;right&#39; or &#39;left&#39;.  Saw: </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">side</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">shape_internal</span><span class="p">(</span><span class="n">values</span><span class="p">))</span></div>


<span class="n">quantize</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">quantize_v2</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;image.extract_patches&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">extract_image_patches_v2</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Extract `patches` from `images`.</span>

<span class="sd">  This op collects patches from the input image, as if applying a</span>
<span class="sd">  convolution. All extracted patches are stacked in the depth (last) dimension</span>
<span class="sd">  of the output.</span>

<span class="sd">  Specifically, the op extracts patches of shape `sizes` which are `strides`</span>
<span class="sd">  apart in the input image. The output is subsampled using the `rates` argument,</span>
<span class="sd">  in the same manner as &quot;atrous&quot; or &quot;dilated&quot; convolutions.</span>

<span class="sd">  The result is a 4D tensor which is indexed by batch, row, and column.</span>
<span class="sd">  `output[i, x, y]` contains a flattened patch of size `sizes[1], sizes[2]`</span>
<span class="sd">  which is taken from the input starting at</span>
<span class="sd">  `images[i, x*strides[1], y*strides[2]]`.</span>

<span class="sd">  Each output patch can be reshaped to `sizes[1], sizes[2], depth`, where</span>
<span class="sd">  `depth` is `images.shape[3]`.</span>

<span class="sd">  The output elements are taken from the input at intervals given by the `rate`</span>
<span class="sd">  argument, as in dilated convolutions.</span>

<span class="sd">  The `padding` argument has no effect on the size of each patch, it determines</span>
<span class="sd">  how many patches are extracted. If `VALID`, only patches which are fully</span>
<span class="sd">  contained in the input image are included. If `SAME`, all patches whose</span>
<span class="sd">  starting point is inside the input are included, and areas outside the input</span>
<span class="sd">  default to zero.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```</span>
<span class="sd">    n = 10</span>
<span class="sd">    # images is a 1 x 10 x 10 x 1 array that contains the numbers 1 through 100</span>
<span class="sd">    images = [[[[x * n + y + 1] for y in range(n)] for x in range(n)]]</span>

<span class="sd">    # We generate two outputs as follows:</span>
<span class="sd">    # 1. 3x3 patches with stride length 5</span>
<span class="sd">    # 2. Same as above, but the rate is increased to 2</span>
<span class="sd">    tf.extract_image_patches(images=images,</span>
<span class="sd">                             ksizes=[1, 3, 3, 1],</span>
<span class="sd">                             strides=[1, 5, 5, 1],</span>
<span class="sd">                             rates=[1, 1, 1, 1],</span>
<span class="sd">                             padding=&#39;VALID&#39;)</span>

<span class="sd">    # Yields:</span>
<span class="sd">    [[[[ 1  2  3 11 12 13 21 22 23]</span>
<span class="sd">       [ 6  7  8 16 17 18 26 27 28]]</span>
<span class="sd">      [[51 52 53 61 62 63 71 72 73]</span>
<span class="sd">       [56 57 58 66 67 68 76 77 78]]]]</span>
<span class="sd">  ```</span>

<span class="sd">  If we mark the pixels in the input image which are taken for the output with</span>
<span class="sd">  `*`, we see the pattern:</span>

<span class="sd">  ```</span>
<span class="sd">     *  *  *  4  5  *  *  *  9 10</span>
<span class="sd">     *  *  * 14 15  *  *  * 19 20</span>
<span class="sd">     *  *  * 24 25  *  *  * 29 30</span>
<span class="sd">    31 32 33 34 35 36 37 38 39 40</span>
<span class="sd">    41 42 43 44 45 46 47 48 49 50</span>
<span class="sd">     *  *  * 54 55  *  *  * 59 60</span>
<span class="sd">     *  *  * 64 65  *  *  * 69 70</span>
<span class="sd">     *  *  * 74 75  *  *  * 79 80</span>
<span class="sd">    81 82 83 84 85 86 87 88 89 90</span>
<span class="sd">    91 92 93 94 95 96 97 98 99 100</span>
<span class="sd">  ```</span>

<span class="sd">  ```</span>
<span class="sd">    tf.extract_image_patches(images=images,</span>
<span class="sd">                             sizes=[1, 3, 3, 1],</span>
<span class="sd">                             strides=[1, 5, 5, 1],</span>
<span class="sd">                             rates=[1, 2, 2, 1],</span>
<span class="sd">                             padding=&#39;VALID&#39;)</span>

<span class="sd">    # Yields:</span>
<span class="sd">    [[[[  1   3   5  21  23  25  41  43  45]</span>
<span class="sd">       [  6   8  10  26  28  30  46  48  50]]</span>

<span class="sd">      [[ 51  53  55  71  73  75  91  93  95]</span>
<span class="sd">       [ 56  58  60  76  78  80  96  98 100]]]]</span>
<span class="sd">  ```</span>

<span class="sd">  We can again draw the effect, this time using the symbols `*`, `x`, `+` and</span>
<span class="sd">  `o` to distinguish the patches:</span>

<span class="sd">  ```</span>
<span class="sd">     *  2  *  4  *  x  7  x  9  x</span>
<span class="sd">    11 12 13 14 15 16 17 18 19 20</span>
<span class="sd">     * 22  * 24  *  x 27  x 29  x</span>
<span class="sd">    31 32 33 34 35 36 37 38 39 40</span>
<span class="sd">     * 42  * 44  *  x 47  x 49  x</span>
<span class="sd">     + 52  + 54  +  o 57  o 59  o</span>
<span class="sd">    61 62 63 64 65 66 67 68 69 70</span>
<span class="sd">     + 72  + 74  +  o 77  o 79  o</span>
<span class="sd">    81 82 83 84 85 86 87 88 89 90</span>
<span class="sd">     + 92  + 94  +  o 97  o 99  o</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    images: A 4-D Tensor with shape `[batch, in_rows, in_cols, depth]</span>
<span class="sd">    sizes: The size of the extracted patches. Must be [1, size_rows, size_cols,</span>
<span class="sd">      1].</span>
<span class="sd">    strides: A 1-D Tensor of length 4. How far the centers of two consecutive</span>
<span class="sd">      patches are in the images. Must be: `[1, stride_rows, stride_cols, 1]`.</span>
<span class="sd">    rates: A 1-D Tensor of length 4. Must be: `[1, rate_rows, rate_cols, 1]`.</span>
<span class="sd">      This is the input stride, specifying how far two consecutive patch samples</span>
<span class="sd">      are in the input. Equivalent to extracting patches with `patch_sizes_eff =</span>
<span class="sd">      patch_sizes + (patch_sizes - 1) * (rates - 1)`, followed by subsampling</span>
<span class="sd">      them spatially by a factor of `rates`. This is equivalent to `rate` in</span>
<span class="sd">      dilated (a.k.a. Atrous) convolutions.</span>
<span class="sd">    padding: The type of padding algorithm to use.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A 4-D Tensor of the same type as the input.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">extract_image_patches</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span>
                                             <span class="n">padding</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;image.extract_image_patches&quot;</span><span class="p">,</span> <span class="s2">&quot;extract_image_patches&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;ksizes is deprecated, use sizes instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ksizes&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">extract_image_patches</span><span class="p">(</span>  <span class="c1"># pylint: disable=missing-docstring</span>
    <span class="n">images</span><span class="p">,</span>
    <span class="n">ksizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">rates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Extract patches from images and put them in the &quot;depth&quot; output dimension.</span>

<span class="sd">  Args:</span>
<span class="sd">    `images`: A `Tensor`. Must be one of the following types: `float32`,</span>
<span class="sd">      `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`,</span>
<span class="sd">      `uint16`, `half`, `uint32`, `uint64`. 4-D Tensor with shape</span>
<span class="sd">    `[batch, in_rows, in_cols, depth]`. `ksizes`: A list of `ints` that has</span>
<span class="sd">      length `&gt;= 4`. The size of the sliding window for each</span>
<span class="sd">    dimension of `images`. `strides`: A list of `ints` that has length `&gt;= 4`.</span>
<span class="sd">      1-D of length 4. How far the centers of two consecutive</span>
<span class="sd">    patches are in the images. Must be:</span>
<span class="sd">    `[1, stride_rows, stride_cols, 1]`. `rates`: A list of `ints`</span>
<span class="sd">    that has length `&gt;= 4`. 1-D of length 4. Must be: `[1, rate_rows, rate_cols,</span>
<span class="sd">      1]`. This is the input stride, specifying how far two consecutive patch</span>
<span class="sd">      samples are in the input. Equivalent to extracting patches with</span>
<span class="sd">      `patch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1)`,</span>
<span class="sd">      followed by subsampling them spatially by a factor of `rates`. This is</span>
<span class="sd">      equivalent to `rate` in dilated (a.k.a. Atrous) convolutions.</span>
<span class="sd">    `padding`: A `string` from: &quot;SAME&quot;, &quot;VALID&quot;. The type of padding algorithm</span>
<span class="sd">      to use.</span>
<span class="sd">    We specify the size-related attributes as:  ``` ksizes = [1, ksize_rows,</span>
<span class="sd">      ksize_cols, 1] strides = [1, strides_rows, strides_cols, 1] rates = [1,</span>
<span class="sd">      rates_rows, rates_cols, 1]</span>
<span class="sd">    name: A name for the operation (optional). ```</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor. Has the same type as images.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ksizes</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;sizes&quot;</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="s2">&quot;ksizes&quot;</span><span class="p">,</span>
                                                  <span class="n">ksizes</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">extract_image_patches</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span>
                                             <span class="n">padding</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">extract_image_patches</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">extract_image_patches</span><span class="o">.</span><span class="vm">__doc__</span>


<div class="viewcode-block" id="fingerprint"><a class="viewcode-back" href="../../../../index.html#tensorflow.fingerprint">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;fingerprint&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fingerprint</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;farmhash64&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Generates fingerprint values.</span>

<span class="sd">  Generates fingerprint values of `data`.</span>

<span class="sd">  Fingerprint op considers the first dimension of `data` as the batch dimension,</span>
<span class="sd">  and `output[i]` contains the fingerprint value generated from contents in</span>
<span class="sd">  `data[i, ...]` for all `i`.</span>

<span class="sd">  Fingerprint op writes fingerprint values as byte arrays. For example, the</span>
<span class="sd">  default method `farmhash64` generates a 64-bit fingerprint value at a time.</span>
<span class="sd">  This 8-byte value is written out as an `tf.uint8` array of size 8, in</span>
<span class="sd">  little-endian order.</span>

<span class="sd">  For example, suppose that `data` has data type `tf.int32` and shape (2, 3, 4),</span>
<span class="sd">  and that the fingerprint method is `farmhash64`. In this case, the output</span>
<span class="sd">  shape is (2, 8), where 2 is the batch dimension size of `data`, and 8 is the</span>
<span class="sd">  size of each fingerprint value in bytes. `output[0, :]` is generated from</span>
<span class="sd">  12 integers in `data[0, :, :]` and similarly `output[1, :]` is generated from</span>
<span class="sd">  other 12 integers in `data[1, :, :]`.</span>

<span class="sd">  Note that this op fingerprints the raw underlying buffer, and it does not</span>
<span class="sd">  fingerprint Tensor&#39;s metadata such as data type and/or shape. For example, the</span>
<span class="sd">  fingerprint values are invariant under reshapes and bitcasts as long as the</span>
<span class="sd">  batch dimension remain the same:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.fingerprint(data) == tf.fingerprint(tf.reshape(data, ...))</span>
<span class="sd">  tf.fingerprint(data) == tf.fingerprint(tf.bitcast(data, ...))</span>
<span class="sd">  ```</span>

<span class="sd">  For string data, one should expect `tf.fingerprint(data) !=</span>
<span class="sd">  tf.fingerprint(tf.string.reduce_join(data))` in general.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor`. Must have rank 1 or higher.</span>
<span class="sd">    method: A `Tensor` of type `tf.string`. Fingerprint method used by this op.</span>
<span class="sd">      Currently available method is `farmhash64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A two-dimensional `Tensor` of type `tf.uint8`. The first dimension equals to</span>
<span class="sd">    `data`&#39;s first dimension, and the second dimension size depends on the</span>
<span class="sd">    fingerprint algorithm.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">fingerprint</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">convert_to_int_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given value to an integer Tensor.&quot;&quot;&quot;</span>
  <span class="n">tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> must be an integer tensor; dtype=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                    <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tensor</span>


<span class="k">def</span> <span class="nf">get_positive_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndims</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">ndims_name</span><span class="o">=</span><span class="s2">&quot;ndims&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Validate an `axis` parameter, and normalize it to be positive.</span>

<span class="sd">  If `ndims` is known (i.e., not `None`), then check that `axis` is in the</span>
<span class="sd">  range `-ndims &lt;= axis &lt; ndims`, and return `axis` (if `axis &gt;= 0`) or</span>
<span class="sd">  `axis + ndims` (otherwise).</span>
<span class="sd">  If `ndims` is not known, and `axis` is positive, then return it as-is.</span>
<span class="sd">  If `ndims` is not known, and `axis` is negative, then report an error.</span>

<span class="sd">  Args:</span>
<span class="sd">    axis: An integer constant</span>
<span class="sd">    ndims: An integer constant, or `None`</span>
<span class="sd">    axis_name: The name of `axis` (for error messages).</span>
<span class="sd">    ndims_name: The name of `ndims` (for error messages).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The normalized `axis` value.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `axis` is out-of-bounds, or if `axis` is negative and</span>
<span class="sd">      `ndims is None`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> must be an int; got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                    <span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="n">ndims</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">axis</span>
    <span class="k">elif</span> <span class="o">-</span><span class="n">ndims</span> <span class="o">&lt;=</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndims</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">=</span><span class="si">%s</span><span class="s2"> out of bounds: expected </span><span class="si">%s</span><span class="s2">&lt;=</span><span class="si">%s</span><span class="s2">&lt;</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">ndims</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">ndims</span><span class="p">))</span>
  <span class="k">elif</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> may only be negative if </span><span class="si">%s</span><span class="s2"> is statically known.&quot;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">ndims_name</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">axis</span>


<span class="c1"># This op is intended to exactly match the semantics of numpy.repeat, with</span>
<span class="c1"># one exception: numpy.repeat has special (and somewhat non-intuitive) behavior</span>
<span class="c1"># when axis is not specified.  Rather than implement that special behavior, we</span>
<span class="c1"># simply make `axis` be a required argument.</span>
<span class="c1">#</span>
<span class="c1"># External (OSS) `tf.repeat` feature request:</span>
<span class="c1"># https://github.com/tensorflow/tensorflow/issues/8246</span>
<span class="k">def</span> <span class="nf">repeat_with_axis</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Repeats elements of `data`.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: An `N`-dimensional tensor.</span>
<span class="sd">    repeats: A 1-D integer tensor specifying how many times each element in</span>
<span class="sd">      `axis` should be repeated.  `len(repeats)` must equal `data.shape[axis]`.</span>
<span class="sd">      Supports broadcasting from a scalar value.</span>
<span class="sd">    axis: `int`.  The axis along which to repeat values.  Must be less than</span>
<span class="sd">      `max(N, 1)`.</span>
<span class="sd">    name: A name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor with `max(N, 1)` dimensions.  Has the same shape as `data`,</span>
<span class="sd">    except that dimension `axis` has size `sum(repeats)`.</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; repeat([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], repeats=[3, 0, 2], axis=0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5,), dtype=string,</span>
<span class="sd">  numpy=array([b&#39;a&#39;, b&#39;a&#39;, b&#39;a&#39;, b&#39;c&#39;, b&#39;c&#39;], dtype=object)&gt;</span>
<span class="sd">  &gt;&gt;&gt; repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 2],</span>
<span class="sd">         [1, 2],</span>
<span class="sd">         [3, 4],</span>
<span class="sd">         [3, 4],</span>
<span class="sd">         [3, 4]], dtype=int32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 5), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 1, 2, 2, 2],</span>
<span class="sd">         [3, 3, 4, 4, 4]], dtype=int32)&gt;</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;axis must be an int; got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Repeat&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">repeats</span><span class="p">]):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
    <span class="n">repeats</span> <span class="o">=</span> <span class="n">convert_to_int_tensor</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;repeats&quot;</span><span class="p">)</span>
    <span class="n">repeats</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">with_rank_at_most</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># If `data` is a scalar, then upgrade it to a vector.</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">_with_nonzero_rank</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># If `axis` is negative, then convert it to a positive value.</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">get_positive_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">ndims_name</span><span class="o">=</span><span class="s2">&quot;rank(data)&quot;</span><span class="p">)</span>

    <span class="c1"># Check data Tensor shapes.</span>
    <span class="k">if</span> <span class="n">repeats</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">(</span><span class="n">repeats</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># If we know that `repeats` is a scalar, then we can just tile &amp; reshape.</span>
    <span class="k">if</span> <span class="n">repeats</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">expanded</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">tiled</span> <span class="o">=</span> <span class="n">tile_one_dimension</span><span class="p">(</span><span class="n">expanded</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>
      <span class="n">result_shape</span> <span class="o">=</span> <span class="n">concat</span><span class="p">([</span><span class="n">data_shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">reshape</span><span class="p">(</span><span class="n">tiled</span><span class="p">,</span> <span class="n">result_shape</span><span class="p">)</span>

    <span class="c1"># Broadcast the `repeats` tensor so rank(repeats) == axis + 1.</span>
    <span class="k">if</span> <span class="n">repeats</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">repeats_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span>
      <span class="n">repeats_ndims</span> <span class="o">=</span> <span class="n">rank</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span>
      <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="n">data_shape</span><span class="p">[:</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">repeats_ndims</span><span class="p">],</span> <span class="n">repeats_shape</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">repeats</span> <span class="o">=</span> <span class="n">broadcast_to</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
      <span class="n">repeats</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Create a &quot;sequence mask&quot; based on `repeats`, where slices across `axis`</span>
    <span class="c1"># contain one `True` value for each repetition.  E.g., if</span>
    <span class="c1"># `repeats = [3, 1, 2]`, then `mask = [[1, 1, 1], [1, 0, 0], [1, 1, 0]]`.</span>
    <span class="n">max_repeat</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
        <span class="mi">0</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_max</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="n">_all_dimensions</span><span class="p">(</span><span class="n">repeats</span><span class="p">)))</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">sequence_mask</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="n">max_repeat</span><span class="p">)</span>

    <span class="c1"># Add a new dimension around each value that needs to be repeated, and</span>
    <span class="c1"># then tile that new dimension to match the maximum number of repetitions.</span>
    <span class="n">expanded</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">tiled</span> <span class="o">=</span> <span class="n">tile_one_dimension</span><span class="p">(</span><span class="n">expanded</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_repeat</span><span class="p">)</span>

    <span class="c1"># Use `boolean_mask` to discard the extra repeated values.  This also</span>
    <span class="c1"># flattens all dimensions up through `axis`.</span>
    <span class="n">masked</span> <span class="o">=</span> <span class="n">boolean_mask</span><span class="p">(</span><span class="n">tiled</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

    <span class="c1"># Reshape the output tensor to add the outer dimensions back.</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">masked</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">result_shape</span> <span class="o">=</span> <span class="n">concat</span><span class="p">([</span><span class="n">data_shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">masked</span><span class="p">,</span> <span class="n">result_shape</span><span class="p">)</span>

    <span class="c1"># Preserve shape information.</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">new_axis_size</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">repeats</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
      <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">]</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
          <span class="p">[</span><span class="n">new_axis_size</span><span class="p">])</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]))</span>

    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">tile_one_dimension</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">multiple</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Tiles a single dimension of a tensor.&quot;&quot;&quot;</span>
  <span class="c1"># Assumes axis is a nonnegative int.</span>
  <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">multiples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
    <span class="n">multiples</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">multiple</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ones_value</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">rank</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">multiples</span> <span class="o">=</span> <span class="n">concat</span><span class="p">([</span><span class="n">ones_value</span><span class="p">[:</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="n">multiple</span><span class="p">],</span> <span class="n">ones_value</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]],</span>
                       <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">multiples</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_with_nonzero_rank</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;If `data` is scalar, then add a dimension; otherwise return as-is.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">stack</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">data</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">data_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data_ndims</span> <span class="o">=</span> <span class="n">rank</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reshape</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">concat</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="n">data_shape</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="o">-</span><span class="n">data_ndims</span><span class="p">:])</span>


<div class="viewcode-block" id="repeat"><a class="viewcode-back" href="../../../../index.html#tensorflow.repeat">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;repeat&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Repeat elements of `input`.</span>
<span class="sd">  </span>
<span class="sd">  See also `tf.concat`, `tf.stack`, `tf.tile`.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: An `N`-dimensional Tensor.</span>
<span class="sd">    repeats: An 1-D `int` Tensor. The number of repetitions for each element.</span>
<span class="sd">      repeats is broadcasted to fit the shape of the given axis. `len(repeats)`</span>
<span class="sd">      must equal `input.shape[axis]` if axis is not None.</span>
<span class="sd">    axis: An int. The axis along which to repeat values. By default (axis=None),</span>
<span class="sd">      use the flattened input array, and return a flat output array.</span>
<span class="sd">    name: A name for the operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor which has the same shape as `input`, except along the given axis.</span>
<span class="sd">      If axis is None then the output array is flattened to match the flattened</span>
<span class="sd">      input array.</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; repeat([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], repeats=[3, 0, 2], axis=0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5,), dtype=string,</span>
<span class="sd">  numpy=array([b&#39;a&#39;, b&#39;a&#39;, b&#39;a&#39;, b&#39;c&#39;, b&#39;c&#39;], dtype=object)&gt;</span>

<span class="sd">  &gt;&gt;&gt; repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=0)</span>
<span class="sd">  &lt;tf.Tensor: shape=(5, 2), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 2],</span>
<span class="sd">         [1, 2],</span>
<span class="sd">         [3, 4],</span>
<span class="sd">         [3, 4],</span>
<span class="sd">         [3, 4]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=1)</span>
<span class="sd">  &lt;tf.Tensor: shape=(2, 5), dtype=int32, numpy=</span>
<span class="sd">  array([[1, 1, 2, 2, 2],</span>
<span class="sd">         [3, 3, 4, 4, 4]], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; repeat(3, repeats=4)</span>
<span class="sd">  &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 3, 3, 3], dtype=int32)&gt;</span>

<span class="sd">  &gt;&gt;&gt; repeat([[1,2], [3,4]], repeats=2)</span>
<span class="sd">  &lt;tf.Tensor: shape=(8,), dtype=int32,</span>
<span class="sd">  numpy=array([1, 1, 2, 2, 3, 3, 4, 4], dtype=int32)&gt;</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">repeat_with_axis</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>