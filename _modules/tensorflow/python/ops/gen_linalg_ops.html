

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.gen_linalg_ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.gen_linalg_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.gen_linalg_ops</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Python wrappers around TensorFlow ops.</span>

<span class="sd">This file is MACHINE GENERATED! Do not edit.</span>
<span class="sd">Original C++ source file: linalg_ops.cc</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">collections</span>

<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tfe</span> <span class="k">as</span> <span class="n">pywrap_tfe</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span> <span class="k">as</span> <span class="n">_context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">core</span> <span class="k">as</span> <span class="n">_core</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">execute</span> <span class="k">as</span> <span class="n">_execute</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span> <span class="k">as</span> <span class="n">_dtypes</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">op_def_registry</span> <span class="k">as</span> <span class="n">_op_def_registry</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span> <span class="k">as</span> <span class="n">_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">op_def_library</span> <span class="k">as</span> <span class="n">_op_def_library</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="k">import</span> <span class="n">deprecated_endpoints</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">dispatch</span> <span class="k">as</span> <span class="n">_dispatch</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>


<span class="k">def</span> <span class="nf">batch_cholesky</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchCholesky&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_cholesky_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchCholesky&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchCholesky&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchCholesky</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchCholesky&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_cholesky</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_cholesky_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchCholesky&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchCholesky&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">batch_cholesky_grad</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    l: A `Tensor`. Must be one of the following types: `float32`, `float64`.</span>
<span class="sd">    grad: A `Tensor`. Must have the same type as `l`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `l`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchCholeskyGrad&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_cholesky_grad_eager_fallback</span><span class="p">(</span>
            <span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchCholeskyGrad&quot;</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchCholeskyGrad&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchCholeskyGrad</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchCholeskyGrad&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_cholesky_grad</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_cholesky_grad_eager_fallback</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchCholeskyGrad&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchCholeskyGrad&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">batch_matrix_determinant</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchMatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_matrix_determinant_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixDeterminant&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchMatrixDeterminant</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchMatrixDeterminant&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_matrix_determinant</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_matrix_determinant_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchMatrixDeterminant&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                             <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span>
                             <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">batch_matrix_inverse</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`.</span>
<span class="sd">    adjoint: An optional `bool`. Defaults to `False`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchMatrixInverse&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_matrix_inverse_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixInverse&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixInverse&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchMatrixInverse</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchMatrixInverse&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_matrix_inverse</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_matrix_inverse_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchMatrixInverse&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixInverse&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">batch_matrix_solve</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    matrix: A `Tensor`. Must be one of the following types: `float64`, `float32`.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `matrix`.</span>
<span class="sd">    adjoint: An optional `bool`. Defaults to `False`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `matrix`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchMatrixSolve&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_matrix_solve_eager_fallback</span><span class="p">(</span>
            <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixSolve&quot;</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchMatrixSolve</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchMatrixSolve&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_matrix_solve</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_matrix_solve_eager_fallback</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchMatrixSolve&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">batch_matrix_solve_ls</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    matrix: A `Tensor`. Must be one of the following types: `float64`, `float32`.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `matrix`.</span>
<span class="sd">    l2_regularizer: A `Tensor` of type `float64`.</span>
<span class="sd">    fast: An optional `bool`. Defaults to `True`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `matrix`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchMatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span> <span class="n">fast</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_matrix_solve_ls_eager_fallback</span><span class="p">(</span>
            <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="n">fast</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">fast</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fast</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">fast</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">fast</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span>
                              <span class="n">l2_regularizer</span><span class="o">=</span><span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="n">fast</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;fast&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchMatrixSolveLs</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchMatrixSolveLs&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_matrix_solve_ls</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_matrix_solve_ls_eager_fallback</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">fast</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fast</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">fast</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">fast</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">l2_regularizer</span> <span class="o">=</span> <span class="n">_ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">_dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span> <span class="n">fast</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchMatrixSolveLs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">batch_matrix_triangular_solve</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    matrix: A `Tensor`. Must be one of the following types: `float64`, `float32`.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `matrix`.</span>
<span class="sd">    lower: An optional `bool`. Defaults to `True`.</span>
<span class="sd">    adjoint: An optional `bool`. Defaults to `False`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `matrix`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchMatrixTriangularSolve&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span>
        <span class="n">adjoint</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_matrix_triangular_solve_eager_fallback</span><span class="p">(</span>
            <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">lower</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">lower</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;lower&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span>
                                      <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">),</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchMatrixTriangularSolve</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchMatrixTriangularSolve&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_matrix_triangular_solve</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_matrix_triangular_solve_eager_fallback</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">lower</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">lower</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;lower&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchMatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                             <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span>
                             <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchMatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">batch_self_adjoint_eig</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchSelfAdjointEig&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_self_adjoint_eig_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchSelfAdjointEig&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchSelfAdjointEig&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchSelfAdjointEig</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchSelfAdjointEig&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_self_adjoint_eig</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_self_adjoint_eig_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchSelfAdjointEig&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchSelfAdjointEig&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_BatchSelfAdjointEigV2Output</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;BatchSelfAdjointEigV2&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">batch_self_adjoint_eig_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`.</span>
<span class="sd">    compute_v: An optional `bool`. Defaults to `True`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (e, v).</span>

<span class="sd">    e: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    v: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchSelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_BatchSelfAdjointEigV2Output</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_self_adjoint_eig_v2_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">compute_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_v</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_v</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchSelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchSelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_BatchSelfAdjointEigV2Output</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchSelfAdjointEigV2</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchSelfAdjointEigV2&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_self_adjoint_eig_v2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_self_adjoint_eig_v2_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">compute_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_v</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_v</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchSelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchSelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_BatchSelfAdjointEigV2Output</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_BatchSvdOutput</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;BatchSvd&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;u&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">batch_svd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;TODO: add doc.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `complex64`, `complex128`.</span>
<span class="sd">    compute_uv: An optional `bool`. Defaults to `True`.</span>
<span class="sd">    full_matrices: An optional `bool`. Defaults to `False`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (s, u, v).</span>

<span class="sd">    s: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    u: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    v: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;BatchSvd&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;compute_uv&quot;</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span>
        <span class="n">full_matrices</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_BatchSvdOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_svd_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="n">compute_uv</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">compute_uv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_uv</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_uv</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;compute_uv&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">full_matrices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">full_matrices</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;BatchSvd&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="n">compute_uv</span><span class="p">,</span>
                    <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_uv&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;compute_uv&quot;</span><span class="p">),</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;full_matrices&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchSvd&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_BatchSvdOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">BatchSvd</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.BatchSvd&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">batch_svd</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">batch_svd_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">compute_uv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_uv</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_uv</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;compute_uv&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">full_matrices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">full_matrices</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_uv&quot;</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
  <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;BatchSvd&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;BatchSvd&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_BatchSvdOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="nd">@_dispatch</span><span class="o">.</span><span class="n">add_dispatch_list</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s1">&#39;linalg.cholesky&#39;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;linalg.cholesky&#39;</span><span class="p">,</span> <span class="s1">&#39;cholesky&#39;</span><span class="p">])</span>
<span class="nd">@deprecated_endpoints</span><span class="p">(</span><span class="s1">&#39;cholesky&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the Cholesky decomposition of one or more square matrices.</span>

<span class="sd">  The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices.</span>

<span class="sd">  The input has to be symmetric and positive definite. Only the lower-triangular</span>
<span class="sd">  part of the input will be used for this operation. The upper-triangular part</span>
<span class="sd">  will not be read.</span>

<span class="sd">  The output is a tensor of the same shape as the input</span>
<span class="sd">  containing the Cholesky decompositions for all input submatrices `[..., :, :]`.</span>

<span class="sd">  **Note**: The gradient computation on GPU is faster for large matrices but</span>
<span class="sd">  not for large batch dimensions when the submatrices are small. In this</span>
<span class="sd">  case it might be faster to use the CPU.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;Cholesky&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cholesky_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
              <span class="n">cholesky</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">result</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;Cholesky&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
          <span class="n">cholesky</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Cholesky&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">Cholesky</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.Cholesky&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">cholesky</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">cholesky_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;Cholesky&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Cholesky&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">cholesky_grad</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the reverse mode backpropagated gradient of the Cholesky algorithm.</span>

<span class="sd">  For an explanation see &quot;Differentiation of the Cholesky algorithm&quot; by</span>
<span class="sd">  Iain Murray http://arxiv.org/abs/1602.07527.</span>

<span class="sd">  Args:</span>
<span class="sd">    l: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.</span>
<span class="sd">      Output of batch Cholesky algorithm l = cholesky(A). Shape is `[..., M, M]`.</span>
<span class="sd">      Algorithm depends only on lower triangular part of the innermost matrices of</span>
<span class="sd">      this tensor.</span>
<span class="sd">    grad: A `Tensor`. Must have the same type as `l`.</span>
<span class="sd">      df/dl where f is some scalar function. Shape is `[..., M, M]`.</span>
<span class="sd">      Algorithm depends only on lower triangular part of the innermost matrices of</span>
<span class="sd">      this tensor.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `l`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;CholeskyGrad&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cholesky_grad_eager_fallback</span><span class="p">(</span>
            <span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;CholeskyGrad&quot;</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;CholeskyGrad&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">CholeskyGrad</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.CholeskyGrad&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">cholesky_grad</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">cholesky_grad_eager_fallback</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="p">,</span> <span class="n">grad</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;CholeskyGrad&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;CholeskyGrad&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_EigOutput</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;Eig&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">eig</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the eigen decomposition of one or more square matrices.</span>

<span class="sd">  Computes the eigenvalues and (optionally) right eigenvectors of each inner matrix in</span>
<span class="sd">  `input` such that `input[..., :, :] = v[..., :, :] * diag(e[..., :])`. The eigenvalues</span>
<span class="sd">  are sorted in non-decreasing order.</span>

<span class="sd">  ```python</span>
<span class="sd">  # a is a tensor.</span>
<span class="sd">  # e is a tensor of eigenvalues.</span>
<span class="sd">  # v is a tensor of eigenvectors.</span>
<span class="sd">  e, v = eig(a)</span>
<span class="sd">  e = eig(a, compute_v=False)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `complex64`, `complex128`.</span>
<span class="sd">      `Tensor` input of shape `[N, N]`.</span>
<span class="sd">    Tout: A `tf.DType` from: `tf.complex64, tf.complex128`.</span>
<span class="sd">    compute_v: An optional `bool`. Defaults to `True`.</span>
<span class="sd">      If `True` then eigenvectors will be computed and returned in `v`.</span>
<span class="sd">      Otherwise, only the eigenvalues will be computed.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (e, v).</span>

<span class="sd">    e: A `Tensor` of type `Tout`.</span>
<span class="sd">    v: A `Tensor` of type `Tout`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;Eig&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;Tout&quot;</span><span class="p">,</span> <span class="n">Tout</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_EigOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">eig_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="n">compute_v</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">Tout</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_type</span><span class="p">(</span><span class="n">Tout</span><span class="p">,</span> <span class="s2">&quot;Tout&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">compute_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_v</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_v</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;Eig&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="s2">&quot;Tout&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;Tout&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Eig&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_EigOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">Eig</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.Eig&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">eig</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">eig_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">Tout</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_type</span><span class="p">(</span><span class="n">Tout</span><span class="p">,</span> <span class="s2">&quot;Tout&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">compute_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_v</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_v</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">,</span> <span class="s2">&quot;Tout&quot;</span><span class="p">,</span> <span class="n">Tout</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;Eig&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span>
                             <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Eig&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_EigOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">einsum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">equation</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Tensor contraction according to Einstein summation convention.</span>

<span class="sd">  Implements generalized Tensor contraction and reduction. Each input Tensor must</span>
<span class="sd">  have a corresponding input subscript appearing in the comma-separated left-hand</span>
<span class="sd">  side of the equation. The right-hand side of the equation consists of the</span>
<span class="sd">  output subscript. The input subscripts and the output subscript should consist</span>
<span class="sd">  of zero or more named axis labels and at most one ellipsis (`...`).</span>

<span class="sd">  The named axis labels may be any single character other than those having</span>
<span class="sd">  special meaning, namely `,.-&gt;`. The behavior of this Op is undefined if it</span>
<span class="sd">  receives an ill-formatted equation; since the validation is done at</span>
<span class="sd">  graph-building time, we omit format validation checks at runtime.</span>

<span class="sd">  Note: This Op is *not* intended to be called by the user; instead users should</span>
<span class="sd">  call `tf.einsum` directly. It is a hidden Op used by `tf.einsum`.</span>

<span class="sd">  Operations are applied to the input(s) according to the following rules:</span>

<span class="sd">   (a) Generalized Diagonals: For input dimensions corresponding to axis labels</span>
<span class="sd">       appearing more than once in the same input subscript, we take the</span>
<span class="sd">       generalized (`k`-dimensional) diagonal.</span>
<span class="sd">       For example, in the equation `iii-&gt;i` with input shape `[3, 3, 3]`, the</span>
<span class="sd">       generalized diagonal would consist of `3` elements at indices `(0, 0, 0)`,</span>
<span class="sd">       `(1, 1, 1)` and `(2, 2, 2)` to create a Tensor of shape `[3]`.</span>

<span class="sd">   (b) Reduction: Axes corresponding to labels appearing only in one input</span>
<span class="sd">       subscript but not in the output subscript are summed over prior to Tensor</span>
<span class="sd">       contraction.</span>
<span class="sd">       For example, in the equation `ab,bc-&gt;b`, the axis labels `a` and `c` are</span>
<span class="sd">       the reduction axis labels.</span>

<span class="sd">   (c) Batch Dimensions: Axes corresponding to labels appearing in each of the</span>
<span class="sd">       input subscripts and also in the output subscript make up the batch</span>
<span class="sd">       dimensions in Tensor contraction. Unnamed axis labels corresponding to</span>
<span class="sd">       ellipsis (`...`) also correspond to batch dimensions.</span>
<span class="sd">       For example, for the equation denoting batch matrix multiplication,</span>
<span class="sd">       `bij,bjk-&gt;bik`, the axis label `b` corresponds to a batch dimension.</span>

<span class="sd">   (d) Contraction: In case of binary einsum, axes corresponding to labels</span>
<span class="sd">       appearing in two different inputs (and not in the output) are contracted</span>
<span class="sd">       against each other.</span>
<span class="sd">       Considering the batch matrix multiplication equation again</span>
<span class="sd">       (`bij,bjk-&gt;bik`), the contracted axis label is `j`.</span>

<span class="sd">   (e) Expand Diagonal: If the output subscripts contain repeated (explicit) axis</span>
<span class="sd">       labels, the opposite operation of (a) is applied. For example, in the</span>
<span class="sd">       equation `i-&gt;iii`, and input shape `[3]`, the output of shape `[3, 3, 3]`</span>
<span class="sd">       are all zeros, except for the (generalized) diagonal which is populated</span>
<span class="sd">       with values from the input.</span>
<span class="sd">       Note: This operation is not supported by `np.einsum` or `tf.einsum`; it is</span>
<span class="sd">       provided to enable computing the symbolic gradient of `tf.einsum`.</span>

<span class="sd">  The output subscripts must contain only labels appearing in at least one of the</span>
<span class="sd">  input subscripts. Furthermore, all dimensions mapping to the same axis label</span>
<span class="sd">  must be equal.</span>

<span class="sd">  Any of the input and output subscripts may contain at most a single ellipsis</span>
<span class="sd">  (`...`). These ellipsis are mapped against dimensions not corresponding to any</span>
<span class="sd">  named axis label. If two inputs contain ellipsis, then they are broadcasted</span>
<span class="sd">  according to standard NumPy broadcasting</span>
<span class="sd">  [rules](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).</span>

<span class="sd">  The broadcasted dimensions are placed in the corresponding location of the</span>
<span class="sd">  ellipsis in the output subscript. If the broadcasted dimensions are non-empty</span>
<span class="sd">  and the output subscripts do not contain ellipsis, then an InvalidArgument error</span>
<span class="sd">  is raised.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Similar to [`numpy.einsum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).</span>

<span class="sd">  Comparison with `numpy.einsum`:</span>

<span class="sd">   * This Op only supports unary and binary forms of `numpy.einsum`.</span>
<span class="sd">   * This Op does not support implicit form. (i.e. equations without `-&gt;`).</span>
<span class="sd">   * This Op also supports repeated indices in the output subscript, which is not</span>
<span class="sd">     supported by `numpy.einsum`.</span>
<span class="sd">  @end_compatibility</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A list of at least 1 `Tensor` objects with the same type.</span>
<span class="sd">      List of 1 or 2 Tensors.</span>
<span class="sd">    equation: A `string`.</span>
<span class="sd">      String describing the Einstein Summation operation; in the format of np.einsum.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `inputs`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;Einsum&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;equation&quot;</span><span class="p">,</span> <span class="n">equation</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">einsum_eager_fallback</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">equation</span><span class="o">=</span><span class="n">equation</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s2">&quot;Expected list for &#39;inputs&#39; argument to &quot;</span>
        <span class="s2">&quot;&#39;einsum&#39; Op, not </span><span class="si">%r</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">inputs</span><span class="p">)</span>
  <span class="n">_attr_N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">equation</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_str</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="s2">&quot;equation&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;Einsum&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">equation</span><span class="o">=</span><span class="n">equation</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;equation&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;equation&quot;</span><span class="p">),</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_int</span><span class="p">(</span><span class="s2">&quot;N&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Einsum&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">Einsum</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.Einsum&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">einsum</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">einsum_eager_fallback</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">equation</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s2">&quot;Expected list for &#39;inputs&#39; argument to &quot;</span>
        <span class="s2">&quot;&#39;einsum&#39; Op, not </span><span class="si">%r</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">inputs</span><span class="p">)</span>
  <span class="n">_attr_N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">equation</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_str</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="s2">&quot;equation&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;equation&quot;</span><span class="p">,</span> <span class="n">equation</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="n">_attr_N</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;Einsum&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span>
                             <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Einsum&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_LogMatrixDeterminantOutput</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;LogMatrixDeterminant&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;sign&quot;</span><span class="p">,</span> <span class="s2">&quot;log_abs_determinant&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">log_matrix_determinant</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sign and the log of the absolute value of the determinant of</span>

<span class="sd">  one or more square matrices.</span>

<span class="sd">  The input is a tensor of shape `[N, M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices. The outputs are two tensors containing the signs and</span>
<span class="sd">  absolute values of the log determinants for all N input submatrices</span>
<span class="sd">  `[..., :, :]` such that the determinant = sign*exp(log_abs_determinant).</span>
<span class="sd">  The log_abs_determinant is computed as det(P)*sum(log(diag(LU))) where LU</span>
<span class="sd">  is the LU decomposition of the input and P is the corresponding</span>
<span class="sd">  permutation matrix.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[N, M, M]`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (sign, log_abs_determinant).</span>

<span class="sd">    sign: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    log_abs_determinant: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;LogMatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_LogMatrixDeterminantOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">log_matrix_determinant_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;LogMatrixDeterminant&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;LogMatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_LogMatrixDeterminantOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">LogMatrixDeterminant</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.LogMatrixDeterminant&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">log_matrix_determinant</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">log_matrix_determinant_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;LogMatrixDeterminant&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;LogMatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_LogMatrixDeterminantOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_LuOutput</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;Lu&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;lu&quot;</span><span class="p">,</span> <span class="s2">&quot;p&quot;</span><span class="p">])</span>


<span class="nd">@_dispatch</span><span class="o">.</span><span class="n">add_dispatch_list</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s1">&#39;linalg.lu&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">lu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="o">=</span><span class="n">_dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the LU decomposition of one or more square matrices.</span>

<span class="sd">  The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices.</span>

<span class="sd">  The input has to be invertible.</span>

<span class="sd">  The output consists of two tensors LU and P containing the LU decomposition</span>
<span class="sd">  of all input submatrices `[..., :, :]`. LU encodes the lower triangular and</span>
<span class="sd">  upper triangular factors.</span>

<span class="sd">  For each input submatrix of shape `[M, M]`, L is a lower triangular matrix of</span>
<span class="sd">  shape `[M, M]` with unit diagonal whose entries correspond to the strictly lower</span>
<span class="sd">  triangular part of LU. U is a upper triangular matrix of shape `[M, M]` whose</span>
<span class="sd">  entries correspond to the upper triangular part, including the diagonal, of LU.</span>

<span class="sd">  P represents a permutation matrix encoded as a list of indices each between `0`</span>
<span class="sd">  and `M-1`, inclusive. If P_mat denotes the permutation matrix corresponding to</span>
<span class="sd">  P, then the L, U and P satisfies P_mat * input = L * U.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      A tensor of shape `[..., M, M]` whose inner-most 2 dimensions form matrices of</span>
<span class="sd">      size `[M, M]`.</span>
<span class="sd">    output_idx_type: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (lu, p).</span>

<span class="sd">    lu: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    p: A `Tensor` of type `output_idx_type`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;Lu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;output_idx_type&quot;</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_LuOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">lu_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="o">=</span><span class="n">output_idx_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
              <span class="n">lu</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="o">=</span><span class="n">output_idx_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">result</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">output_idx_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">output_idx_type</span> <span class="o">=</span> <span class="n">_dtypes</span><span class="o">.</span><span class="n">int32</span>
  <span class="n">output_idx_type</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_type</span><span class="p">(</span><span class="n">output_idx_type</span><span class="p">,</span> <span class="s2">&quot;output_idx_type&quot;</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;Lu&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="o">=</span><span class="n">output_idx_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
          <span class="n">lu</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="o">=</span><span class="n">output_idx_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="s2">&quot;output_idx_type&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;output_idx_type&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Lu&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_LuOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">Lu</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.Lu&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">lu</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">lu_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">output_idx_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">output_idx_type</span> <span class="o">=</span> <span class="n">_dtypes</span><span class="o">.</span><span class="n">int32</span>
  <span class="n">output_idx_type</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_type</span><span class="p">(</span><span class="n">output_idx_type</span><span class="p">,</span> <span class="s2">&quot;output_idx_type&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">,</span> <span class="s2">&quot;output_idx_type&quot;</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;Lu&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span>
                             <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Lu&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_LuOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="nd">@_dispatch</span><span class="o">.</span><span class="n">add_dispatch_list</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s1">&#39;linalg.det&#39;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;linalg.det&#39;</span><span class="p">,</span> <span class="s1">&#39;matrix_determinant&#39;</span><span class="p">])</span>
<span class="nd">@deprecated_endpoints</span><span class="p">(</span><span class="s1">&#39;matrix_determinant&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matrix_determinant</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the determinant of one or more square matrices.</span>

<span class="sd">  The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices. The output is a tensor containing the determinants</span>
<span class="sd">  for all input submatrices `[..., :, :]`.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_determinant_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
              <span class="n">matrix_determinant</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">result</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixDeterminant&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
          <span class="n">matrix_determinant</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">MatrixDeterminant</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixDeterminant&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_determinant</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_determinant_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixDeterminant&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixDeterminant&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">matrix_exponential</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Deprecated, use python implementation tf.linalg.matrix_exponential.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixExponential&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_exponential_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixExponential&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixExponential&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">MatrixExponential</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixExponential&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_exponential</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_exponential_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixExponential&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixExponential&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="nd">@_dispatch</span><span class="o">.</span><span class="n">add_dispatch_list</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s1">&#39;linalg.inv&#39;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;linalg.inv&#39;</span><span class="p">,</span> <span class="s1">&#39;matrix_inverse&#39;</span><span class="p">])</span>
<span class="nd">@deprecated_endpoints</span><span class="p">(</span><span class="s1">&#39;matrix_inverse&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matrix_inverse</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the inverse of one or more square invertible matrices or their</span>

<span class="sd">  adjoints (conjugate transposes).</span>

<span class="sd">  The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices. The output is a tensor of the same shape as the input</span>
<span class="sd">  containing the inverse for all input submatrices `[..., :, :]`.</span>

<span class="sd">  The op uses LU decomposition with partial pivoting to compute the inverses.</span>

<span class="sd">  If a matrix is not invertible there is no guarantee what the op does. It</span>
<span class="sd">  may detect the condition and raise an exception or it may simply return a</span>
<span class="sd">  garbage result.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    adjoint: An optional `bool`. Defaults to `False`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixInverse&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_inverse_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
              <span class="n">matrix_inverse</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">result</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixInverse&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
          <span class="n">matrix_inverse</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixInverse&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">MatrixInverse</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixInverse&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_inverse</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_inverse_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixInverse&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixInverse&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">matrix_logarithm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the matrix logarithm of one or more square matrices:</span>

<span class="sd">  </span>
<span class="sd">  \\(log(exp(A)) = A\\)</span>

<span class="sd">  This op is only defined for complex matrices. If A is positive-definite and</span>
<span class="sd">  real, then casting to a complex matrix, taking the logarithm and casting back</span>
<span class="sd">  to a real matrix will give the correct result.</span>

<span class="sd">  This function computes the matrix logarithm using the Schur-Parlett algorithm.</span>
<span class="sd">  Details of the algorithm can be found in Section 11.6.2 of:</span>
<span class="sd">  Nicholas J. Higham, Functions of Matrices: Theory and Computation, SIAM 2008.</span>
<span class="sd">  ISBN 978-0-898716-46-7.</span>

<span class="sd">  The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices. The output is a tensor of the same shape as the input</span>
<span class="sd">  containing the exponential for all input submatrices `[..., :, :]`.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixLogarithm&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_logarithm_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixLogarithm&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixLogarithm&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">MatrixLogarithm</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixLogarithm&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_logarithm</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_logarithm_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixLogarithm&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixLogarithm&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="nd">@_dispatch</span><span class="o">.</span><span class="n">add_dispatch_list</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s1">&#39;linalg.solve&#39;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;linalg.solve&#39;</span><span class="p">,</span> <span class="s1">&#39;matrix_solve&#39;</span><span class="p">])</span>
<span class="nd">@deprecated_endpoints</span><span class="p">(</span><span class="s1">&#39;matrix_solve&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matrix_solve</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Solves systems of linear equations.</span>

<span class="sd">  `Matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices. `Rhs` is a tensor of shape `[..., M, K]`. The `output` is</span>
<span class="sd">  a tensor shape `[..., M, K]`.  If `adjoint` is `False` then each output matrix</span>
<span class="sd">  satisfies `matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.</span>
<span class="sd">  If `adjoint` is `True` then each output matrix satisfies</span>
<span class="sd">  `adjoint(matrix[..., :, :]) * output[..., :, :] = rhs[..., :, :]`.</span>

<span class="sd">  Args:</span>
<span class="sd">    matrix: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `matrix`.</span>
<span class="sd">      Shape is `[..., M, K]`.</span>
<span class="sd">    adjoint: An optional `bool`. Defaults to `False`.</span>
<span class="sd">      Boolean indicating whether to solve with `matrix` or its (block-wise)</span>
<span class="sd">      adjoint.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `matrix`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixSolve&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_solve_eager_fallback</span><span class="p">(</span>
            <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
              <span class="n">matrix_solve</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">result</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSolve&quot;</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
          <span class="n">matrix_solve</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">MatrixSolve</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixSolve&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_solve</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_solve_eager_fallback</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixSolve&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">matrix_solve_ls</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Solves one or more linear least-squares problems.</span>

<span class="sd">  `matrix` is a tensor of shape `[..., M, N]` whose inner-most 2 dimensions</span>
<span class="sd">  form real or complex matrices of size `[M, N]`. `Rhs` is a tensor of the same</span>
<span class="sd">  type as `matrix` and shape `[..., M, K]`.</span>
<span class="sd">  The output is a tensor shape `[..., N, K]` where each output matrix solves</span>
<span class="sd">  each of the equations</span>
<span class="sd">  `matrix[..., :, :]` * `output[..., :, :]` = `rhs[..., :, :]`</span>
<span class="sd">  in the least squares sense.</span>

<span class="sd">  We use the following notation for (complex) matrix and right-hand sides</span>
<span class="sd">  in the batch:</span>

<span class="sd">  `matrix`=\\(A \in \mathbb{C}^{m \times n}\\),</span>
<span class="sd">  `rhs`=\\(B  \in \mathbb{C}^{m \times k}\\),</span>
<span class="sd">  `output`=\\(X  \in \mathbb{C}^{n \times k}\\),</span>
<span class="sd">  `l2_regularizer`=\\(\lambda \in \mathbb{R}\\).</span>

<span class="sd">  If `fast` is `True`, then the solution is computed by solving the normal</span>
<span class="sd">  equations using Cholesky decomposition. Specifically, if \\(m \ge n\\) then</span>
<span class="sd">  \\(X = (A^H A + \lambda I)^{-1} A^H B\\), which solves the least-squares</span>
<span class="sd">  problem \\(X = \mathrm{argmin}_{Z \in \Re^{n \times k} } ||A Z - B||_F^2 + \lambda ||Z||_F^2\\).</span>
<span class="sd">  If \\(m \lt n\\) then `output` is computed as</span>
<span class="sd">  \\(X = A^H (A A^H + \lambda I)^{-1} B\\), which (for \\(\lambda = 0\\)) is the</span>
<span class="sd">  minimum-norm solution to the under-determined linear system, i.e.</span>
<span class="sd">  \\(X = \mathrm{argmin}_{Z \in \mathbb{C}^{n \times k} } ||Z||_F^2 \\),</span>
<span class="sd">  subject to \\(A Z = B\\). Notice that the fast path is only numerically stable</span>
<span class="sd">  when \\(A\\) is numerically full rank and has a condition number</span>
<span class="sd">  \\(\mathrm{cond}(A) \lt \frac{1}{\sqrt{\epsilon_{mach} } }\\) or \\(\lambda\\) is</span>
<span class="sd">  sufficiently large.</span>

<span class="sd">  If `fast` is `False` an algorithm based on the numerically robust complete</span>
<span class="sd">  orthogonal decomposition is used. This computes the minimum-norm</span>
<span class="sd">  least-squares solution, even when \\(A\\) is rank deficient. This path is</span>
<span class="sd">  typically 6-7 times slower than the fast path. If `fast` is `False` then</span>
<span class="sd">  `l2_regularizer` is ignored.</span>

<span class="sd">  Args:</span>
<span class="sd">    matrix: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, N]`.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `matrix`.</span>
<span class="sd">      Shape is `[..., M, K]`.</span>
<span class="sd">    l2_regularizer: A `Tensor` of type `float64`. Scalar tensor.</span>

<span class="sd">      @compatibility(numpy)</span>
<span class="sd">      Equivalent to np.linalg.lstsq</span>
<span class="sd">      @end_compatibility</span>
<span class="sd">    fast: An optional `bool`. Defaults to `True`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `matrix`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span> <span class="n">fast</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_solve_ls_eager_fallback</span><span class="p">(</span>
            <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="n">fast</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">fast</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fast</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">fast</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">fast</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span>
                         <span class="n">l2_regularizer</span><span class="o">=</span><span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="n">fast</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;fast&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">MatrixSolveLs</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixSolveLs&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_solve_ls</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_solve_ls_eager_fallback</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">fast</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">fast</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fast</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">fast</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">fast</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">l2_regularizer</span> <span class="o">=</span> <span class="n">_ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">l2_regularizer</span><span class="p">,</span> <span class="n">_dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">,</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span> <span class="n">fast</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixSolveLs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSolveLs&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<div class="viewcode-block" id="matrix_square_root"><a class="viewcode-back" href="../../../../index.html#tensorflow.matrix_square_root">[docs]</a><span class="nd">@_dispatch</span><span class="o">.</span><span class="n">add_dispatch_list</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s1">&#39;linalg.sqrtm&#39;</span><span class="p">,</span> <span class="s1">&#39;matrix_square_root&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matrix_square_root</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the matrix square root of one or more square matrices:</span>

<span class="sd">  matmul(sqrtm(A), sqrtm(A)) = A</span>

<span class="sd">  The input matrix should be invertible. If the input matrix is real, it should</span>
<span class="sd">  have no eigenvalues which are real and negative (pairs of complex conjugate</span>
<span class="sd">  eigenvalues are allowed).</span>

<span class="sd">  The matrix square root is computed by first reducing the matrix to</span>
<span class="sd">  quasi-triangular form with the real Schur decomposition. The square root</span>
<span class="sd">  of the quasi-triangular matrix is then computed directly. Details of</span>
<span class="sd">  the algorithm can be found in: Nicholas J. Higham, &quot;Computing real</span>
<span class="sd">  square roots of a real matrix&quot;, Linear Algebra Appl., 1987.</span>

<span class="sd">  The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices. The output is a tensor of the same shape as the input</span>
<span class="sd">  containing the matrix square root for all input submatrices `[..., :, :]`.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixSquareRoot&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_square_root_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
              <span class="n">matrix_square_root</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">result</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSquareRoot&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
          <span class="n">matrix_square_root</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSquareRoot&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span></div>

<span class="n">MatrixSquareRoot</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixSquareRoot&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_square_root</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_square_root_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixSquareRoot&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixSquareRoot&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">matrix_triangular_solve</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Solves systems of linear equations with upper or lower triangular matrices by backsubstitution.</span>

<span class="sd">  </span>
<span class="sd">  `matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions form</span>
<span class="sd">  square matrices. If `lower` is `True` then the strictly upper triangular part</span>
<span class="sd">  of each inner-most matrix is assumed to be zero and not accessed.</span>
<span class="sd">  If `lower` is False then the strictly lower triangular part of each inner-most</span>
<span class="sd">  matrix is assumed to be zero and not accessed.</span>
<span class="sd">  `rhs` is a tensor of shape `[..., M, N]`.</span>

<span class="sd">  The output is a tensor of shape `[..., M, N]`. If `adjoint` is</span>
<span class="sd">  `True` then the innermost matrices in `output` satisfy matrix equations</span>
<span class="sd">  `matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.</span>
<span class="sd">  If `adjoint` is `False` then the strictly then the  innermost matrices in</span>
<span class="sd">  `output` satisfy matrix equations</span>
<span class="sd">  `adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]`.</span>

<span class="sd">  Note, the batch shapes for the inputs only need to broadcast.</span>

<span class="sd">  Example:</span>
<span class="sd">  ```python</span>

<span class="sd">  a = tf.constant([[3,  0,  0,  0],</span>
<span class="sd">                   [2,  1,  0,  0],</span>
<span class="sd">                   [1,  0,  1,  0],</span>
<span class="sd">                   [1,  1,  1,  1]], dtype=tf.float32)</span>

<span class="sd">  b = tf.constant([[4],</span>
<span class="sd">                   [2],</span>
<span class="sd">                   [4],</span>
<span class="sd">                   [2]], dtype=tf.float32)</span>

<span class="sd">  x = tf.linalg.triangular_solve(a, b, lower=True)</span>
<span class="sd">  x</span>
<span class="sd">  # &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=</span>
<span class="sd">  # array([[ 1.3333334 ],</span>
<span class="sd">  #        [-0.66666675],</span>
<span class="sd">  #        [ 2.6666665 ],</span>
<span class="sd">  #        [-1.3333331 ]], dtype=float32)&gt;</span>

<span class="sd">  # in python3 one can use `a@x`</span>
<span class="sd">  tf.matmul(a, x)</span>
<span class="sd">  # &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=</span>
<span class="sd">  # array([[4.       ],</span>
<span class="sd">  #        [2.       ],</span>
<span class="sd">  #        [4.       ],</span>
<span class="sd">  #        [1.9999999]], dtype=float32)&gt;</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    matrix: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `matrix`.</span>
<span class="sd">      Shape is `[..., M, K]`.</span>
<span class="sd">    lower: An optional `bool`. Defaults to `True`.</span>
<span class="sd">      Boolean indicating whether the innermost matrices in `matrix` are</span>
<span class="sd">      lower or upper triangular.</span>
<span class="sd">    adjoint: An optional `bool`. Defaults to `False`.</span>
<span class="sd">      Boolean indicating whether to solve with `matrix` or its (block-wise)</span>
<span class="sd">               adjoint.</span>

<span class="sd">      @compatibility(numpy)</span>
<span class="sd">      Equivalent to scipy.linalg.solve_triangular</span>
<span class="sd">      @end_compatibility</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `matrix`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;MatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">matrix_triangular_solve_eager_fallback</span><span class="p">(</span>
            <span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">lower</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">lower</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;lower&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;MatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span>
                                 <span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">),</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">MatrixTriangularSolve</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.MatrixTriangularSolve&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">matrix_triangular_solve</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">matrix_triangular_solve_eager_fallback</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">lower</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">lower</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;lower&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">adjoint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">adjoint</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">adjoint</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;MatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;MatrixTriangularSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_QrOutput</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;Qr&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;q&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">])</span>


<span class="nd">@_dispatch</span><span class="o">.</span><span class="n">add_dispatch_list</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s1">&#39;linalg.qr&#39;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;linalg.qr&#39;</span><span class="p">,</span> <span class="s1">&#39;qr&#39;</span><span class="p">])</span>
<span class="nd">@deprecated_endpoints</span><span class="p">(</span><span class="s1">&#39;qr&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">qr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the QR decompositions of one or more matrices.</span>

<span class="sd">  Computes the QR decomposition of each inner matrix in `tensor` such that</span>
<span class="sd">  `tensor[..., :, :] = q[..., :, :] * r[..., :,:])`</span>

<span class="sd">  ```python</span>
<span class="sd">  # a is a tensor.</span>
<span class="sd">  # q is a tensor of orthonormal matrices.</span>
<span class="sd">  # r is a tensor of upper triangular matrices.</span>
<span class="sd">  q, r = qr(a)</span>
<span class="sd">  q_full, r_full = qr(a, full_matrices=True)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      A tensor of shape `[..., M, N]` whose inner-most 2 dimensions</span>
<span class="sd">      form matrices of size `[M, N]`. Let `P` be the minimum of `M` and `N`.</span>
<span class="sd">    full_matrices: An optional `bool`. Defaults to `False`.</span>
<span class="sd">      If true, compute full-sized `q` and `r`. If false</span>
<span class="sd">      (the default), compute only the leading `P` columns of `q`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (q, r).</span>

<span class="sd">    q: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    r: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;Qr&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_QrOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">qr_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
              <span class="n">qr</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">result</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">full_matrices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">full_matrices</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;Qr&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span>
          <span class="n">qr</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_dispatch</span><span class="o">.</span><span class="n">OpDispatcher</span><span class="o">.</span><span class="n">NOT_SUPPORTED</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;full_matrices&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Qr&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_QrOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">Qr</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.Qr&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">qr</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">qr_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">full_matrices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">full_matrices</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;Qr&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span>
                             <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Qr&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_QrOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">self_adjoint_eig</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the Eigen Decomposition of a batch of square self-adjoint matrices.</span>

<span class="sd">  The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions</span>
<span class="sd">  form square matrices, with the same constraints as the single matrix</span>
<span class="sd">  SelfAdjointEig.</span>

<span class="sd">  The result is a [..., M+1, M] matrix with [..., 0,:] containing the</span>
<span class="sd">  eigenvalues, and subsequent [...,1:, :] containing the eigenvectors. The eigenvalues</span>
<span class="sd">  are sorted in non-decreasing order.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`.</span>
<span class="sd">      Shape is `[..., M, M]`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;SelfAdjointEig&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self_adjoint_eig_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;SelfAdjointEig&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;SelfAdjointEig&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">SelfAdjointEig</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.SelfAdjointEig&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">self_adjoint_eig</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">self_adjoint_eig_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;SelfAdjointEig&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;SelfAdjointEig&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_SelfAdjointEigV2Output</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;SelfAdjointEigV2&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">self_adjoint_eig_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the eigen decomposition of one or more square self-adjoint matrices.</span>

<span class="sd">  Computes the eigenvalues and (optionally) eigenvectors of each inner matrix in</span>
<span class="sd">  `input` such that `input[..., :, :] = v[..., :, :] * diag(e[..., :])`. The eigenvalues</span>
<span class="sd">  are sorted in non-decreasing order.</span>

<span class="sd">  ```python</span>
<span class="sd">  # a is a tensor.</span>
<span class="sd">  # e is a tensor of eigenvalues.</span>
<span class="sd">  # v is a tensor of eigenvectors.</span>
<span class="sd">  e, v = self_adjoint_eig(a)</span>
<span class="sd">  e = self_adjoint_eig(a, compute_v=False)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      `Tensor` input of shape `[N, N]`.</span>
<span class="sd">    compute_v: An optional `bool`. Defaults to `True`.</span>
<span class="sd">      If `True` then eigenvectors will be computed and returned in `v`.</span>
<span class="sd">      Otherwise, only the eigenvalues will be computed.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (e, v).</span>

<span class="sd">    e: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    v: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;SelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_SelfAdjointEigV2Output</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self_adjoint_eig_v2_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">compute_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_v</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_v</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;SelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;SelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_SelfAdjointEigV2Output</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">SelfAdjointEigV2</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.SelfAdjointEigV2&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">self_adjoint_eig_v2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">self_adjoint_eig_v2_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">compute_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_v</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_v</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;compute_v&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;SelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;SelfAdjointEigV2&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_SelfAdjointEigV2Output</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">_SvdOutput</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;Svd&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;u&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">svd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the singular value decompositions of one or more matrices.</span>

<span class="sd">  Computes the SVD of each inner matrix in `input` such that</span>
<span class="sd">  `input[..., :, :] = u[..., :, :] * diag(s[..., :, :]) * transpose(v[..., :, :])`</span>

<span class="sd">  ```python</span>
<span class="sd">  # a is a tensor containing a batch of matrices.</span>
<span class="sd">  # s is a tensor of singular values for each matrix.</span>
<span class="sd">  # u is the tensor containing the left singular vectors for each matrix.</span>
<span class="sd">  # v is the tensor containing the right singular vectors for each matrix.</span>
<span class="sd">  s, u, v = svd(a)</span>
<span class="sd">  s, _, _ = svd(a, compute_uv=False)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.</span>
<span class="sd">      A tensor of shape `[..., M, N]` whose inner-most 2 dimensions</span>
<span class="sd">      form matrices of size `[M, N]`. Let `P` be the minimum of `M` and `N`.</span>
<span class="sd">    compute_uv: An optional `bool`. Defaults to `True`.</span>
<span class="sd">      If true, left and right singular vectors will be</span>
<span class="sd">      computed and returned in `u` and `v`, respectively.</span>
<span class="sd">      If false, `u` and `v` are not set and should never referenced.</span>
<span class="sd">    full_matrices: An optional `bool`. Defaults to `False`.</span>
<span class="sd">      If true, compute full-sized `u` and `v`. If false</span>
<span class="sd">      (the default), compute only the leading `P` singular vectors.</span>
<span class="sd">      Ignored if `compute_uv` is `False`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of `Tensor` objects (s, u, v).</span>

<span class="sd">    s: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    u: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">    v: A `Tensor`. Has the same type as `input`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;Svd&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;compute_uv&quot;</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">)</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">_SvdOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">svd_eager_fallback</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="n">compute_uv</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">compute_uv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_uv</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_uv</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;compute_uv&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">full_matrices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">full_matrices</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;Svd&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="n">compute_uv</span><span class="p">,</span>
               <span class="n">full_matrices</span><span class="o">=</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_uv&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;compute_uv&quot;</span><span class="p">),</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;full_matrices&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Svd&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_SvdOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">Svd</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.Svd&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">svd</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">svd_eager_fallback</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">compute_uv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">compute_uv</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">compute_uv</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;compute_uv&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">full_matrices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">full_matrices</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="nb">input</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compute_uv&quot;</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">,</span> <span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
  <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;Svd&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span>
                             <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;Svd&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_SvdOutput</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span><span class="n">_result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">tridiagonal_mat_mul</span><span class="p">(</span><span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="p">,</span> <span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate product with tridiagonal matrix.</span>

<span class="sd">  Calculates product of two matrices, where left matrix is a tridiagonal matrix.</span>

<span class="sd">  Args:</span>
<span class="sd">    superdiag: A `Tensor`. Must be one of the following types: `float64`, `float32`, `complex64`, `complex128`.</span>
<span class="sd">      Tensor of shape `[..., 1, M]`, representing superdiagonals of</span>
<span class="sd">      tri-diagonal matrices to the left of multiplication. Last element is ignored.</span>
<span class="sd">    maindiag: A `Tensor`. Must have the same type as `superdiag`.</span>
<span class="sd">      Tensor of shape `[..., 1, M]`, representing main diagonals of tri-diagonal</span>
<span class="sd">      matrices to the left of multiplication.</span>
<span class="sd">    subdiag: A `Tensor`. Must have the same type as `superdiag`.</span>
<span class="sd">      Tensor of shape `[..., 1, M]`, representing subdiagonals of tri-diagonal</span>
<span class="sd">      matrices to the left of multiplication. First element is ignored.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `superdiag`.</span>
<span class="sd">      Tensor of shape `[..., M, N]`, representing MxN matrices to the right of</span>
<span class="sd">      multiplication.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `superdiag`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;TridiagonalMatMul&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="p">,</span> <span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tridiagonal_mat_mul_eager_fallback</span><span class="p">(</span>
            <span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="p">,</span> <span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;TridiagonalMatMul&quot;</span><span class="p">,</span> <span class="n">superdiag</span><span class="o">=</span><span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="o">=</span><span class="n">maindiag</span><span class="p">,</span>
                             <span class="n">subdiag</span><span class="o">=</span><span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;TridiagonalMatMul&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">TridiagonalMatMul</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.TridiagonalMatMul&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">tridiagonal_mat_mul</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">tridiagonal_mat_mul_eager_fallback</span><span class="p">(</span><span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="p">,</span> <span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="p">,</span> <span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="p">,</span> <span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">superdiag</span><span class="p">,</span> <span class="n">maindiag</span><span class="p">,</span> <span class="n">subdiag</span><span class="p">,</span> <span class="n">rhs</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;TridiagonalMatMul&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;TridiagonalMatMul&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>


<span class="k">def</span> <span class="nf">tridiagonal_solve</span><span class="p">(</span><span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">partial_pivoting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Solves tridiagonal systems of equations.</span>

<span class="sd">    Solves tridiagonal systems of equations.</span>
<span class="sd">    Supports batch dimensions and multiple right-hand sides per each left-hand</span>
<span class="sd">    side.</span>
<span class="sd">    On CPU, solution is computed via Gaussian elimination with or without partial</span>
<span class="sd">    pivoting, depending on `partial_pivoting` attribute. On GPU, Nvidia&#39;s cuSPARSE</span>
<span class="sd">    library is used: https://docs.nvidia.com/cuda/cusparse/index.html#gtsv</span>

<span class="sd">  Args:</span>
<span class="sd">    diagonals: A `Tensor`. Must be one of the following types: `float64`, `float32`, `complex64`, `complex128`.</span>
<span class="sd">      Tensor of shape `[..., 3, M]` whose innermost 2 dimensions represent the</span>
<span class="sd">      tridiagonal matrices with three rows being the superdiagonal, diagonals, and</span>
<span class="sd">      subdiagonals, in order. The last element of the superdiagonal and the first</span>
<span class="sd">      element of the subdiagonal is ignored.</span>
<span class="sd">    rhs: A `Tensor`. Must have the same type as `diagonals`.</span>
<span class="sd">      Tensor of shape `[..., M, K]`, representing K right-hand sides per each</span>
<span class="sd">      left-hand side.</span>
<span class="sd">    partial_pivoting: An optional `bool`. Defaults to `True`.</span>
<span class="sd">      Whether to apply partial pivoting. Partial pivoting makes the procedure more</span>
<span class="sd">      stable, but slower.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `diagonals`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">or</span> <span class="n">_context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">tld</span> <span class="o">=</span> <span class="n">_ctx</span><span class="o">.</span><span class="n">_thread_local_data</span>
  <span class="k">if</span> <span class="n">tld</span><span class="o">.</span><span class="n">is_eager</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">_result</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_FastPathExecute</span><span class="p">(</span>
        <span class="n">_ctx</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">device_name</span><span class="p">,</span> <span class="s2">&quot;TridiagonalSolve&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">tld</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">,</span> <span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="s2">&quot;partial_pivoting&quot;</span><span class="p">,</span>
        <span class="n">partial_pivoting</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_result</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_FallbackException</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tridiagonal_solve_eager_fallback</span><span class="p">(</span>
            <span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">partial_pivoting</span><span class="o">=</span><span class="n">partial_pivoting</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">=</span><span class="n">_ctx</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_SymbolicException</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Add nodes to the TensorFlow graph.</span>
    <span class="k">except</span> <span class="n">_core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">_ops</span><span class="o">.</span><span class="n">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="c1"># Add nodes to the TensorFlow graph.</span>
  <span class="k">if</span> <span class="n">partial_pivoting</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">partial_pivoting</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">partial_pivoting</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">partial_pivoting</span><span class="p">,</span> <span class="s2">&quot;partial_pivoting&quot;</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_op</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">_op_def_library</span><span class="o">.</span><span class="n">_apply_op_helper</span><span class="p">(</span>
        <span class="s2">&quot;TridiagonalSolve&quot;</span><span class="p">,</span> <span class="n">diagonals</span><span class="o">=</span><span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="o">=</span><span class="n">rhs</span><span class="p">,</span>
                            <span class="n">partial_pivoting</span><span class="o">=</span><span class="n">partial_pivoting</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_outputs</span><span class="p">[:]</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;partial_pivoting&quot;</span><span class="p">,</span> <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_bool</span><span class="p">(</span><span class="s2">&quot;partial_pivoting&quot;</span><span class="p">),</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span>
              <span class="n">_op</span><span class="o">.</span><span class="n">_get_attr_type</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">))</span>
    <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">inputs</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;TridiagonalSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

<span class="n">TridiagonalSolve</span> <span class="o">=</span> <span class="n">tf_export</span><span class="p">(</span><span class="s2">&quot;raw_ops.TridiagonalSolve&quot;</span><span class="p">)(</span><span class="n">_ops</span><span class="o">.</span><span class="n">to_raw_op</span><span class="p">(</span><span class="n">tridiagonal_solve</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">tridiagonal_solve_eager_fallback</span><span class="p">(</span><span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">partial_pivoting</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">partial_pivoting</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">partial_pivoting</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">partial_pivoting</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">make_bool</span><span class="p">(</span><span class="n">partial_pivoting</span><span class="p">,</span> <span class="s2">&quot;partial_pivoting&quot;</span><span class="p">)</span>
  <span class="n">_attr_T</span><span class="p">,</span> <span class="n">_inputs_T</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">args_to_matching_eager</span><span class="p">([</span><span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="p">],</span> <span class="n">ctx</span><span class="p">)</span>
  <span class="p">(</span><span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="o">=</span> <span class="n">_inputs_T</span>
  <span class="n">_inputs_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">diagonals</span><span class="p">,</span> <span class="n">rhs</span><span class="p">]</span>
  <span class="n">_attrs</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;partial_pivoting&quot;</span><span class="p">,</span> <span class="n">partial_pivoting</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">_attr_T</span><span class="p">)</span>
  <span class="n">_result</span> <span class="o">=</span> <span class="n">_execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;TridiagonalSolve&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs_flat</span><span class="p">,</span>
                             <span class="n">attrs</span><span class="o">=</span><span class="n">_attrs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_execute</span><span class="o">.</span><span class="n">must_record_gradient</span><span class="p">():</span>
    <span class="n">_execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span>
        <span class="s2">&quot;TridiagonalSolve&quot;</span><span class="p">,</span> <span class="n">_inputs_flat</span><span class="p">,</span> <span class="n">_attrs</span><span class="p">,</span> <span class="n">_result</span><span class="p">)</span>
  <span class="n">_result</span><span class="p">,</span> <span class="o">=</span> <span class="n">_result</span>
  <span class="k">return</span> <span class="n">_result</span>

</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>