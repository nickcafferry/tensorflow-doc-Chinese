

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.ops.parallel_for.control_flow_ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.ops.parallel_for.control_flow_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.ops.parallel_for.control_flow_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;for_loop and pfor ops.&quot;&quot;&quot;</span>
<span class="c1"># pylint: disable=g-direct-tensorflow-import</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">functools</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">def_function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">indexed_slices</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">sparse_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">tensor_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.parallel_for.pfor</span> <span class="k">import</span> <span class="n">PFor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.parallel_for.pfor</span> <span class="k">import</span> <span class="n">PForConfig</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_decorator</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_inspect</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>


<span class="k">def</span> <span class="nf">for_loop</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">loop_fn_dtypes</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Runs `loop_fn` `iters` times and stacks the outputs.</span>


<span class="sd">  Runs `loop_fn` `iters` times, with input values from 0 to `iters - 1`, and</span>
<span class="sd">  stacks corresponding outputs of the different runs.</span>

<span class="sd">  Args:</span>
<span class="sd">    loop_fn: A function that takes an int32 scalar tf.Tensor object representing</span>
<span class="sd">      the iteration number, and returns a possibly nested structure of tensor</span>
<span class="sd">      objects. The shape of these outputs should not depend on the input.</span>
<span class="sd">    loop_fn_dtypes: dtypes for the outputs of loop_fn.</span>
<span class="sd">    iters: Number of iterations for which to run loop_fn.</span>
<span class="sd">    parallel_iterations: The number of iterations that can be dispatched in</span>
<span class="sd">      parallel. This knob can be used to control the total memory usage.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Returns a nested structure of stacked output tensor objects with the same</span>
<span class="sd">    nested structure as the output of `loop_fn`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">flat_loop_fn_dtypes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_fn_dtypes</span><span class="p">)</span>
  <span class="n">is_none_list</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">while_body</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="o">*</span><span class="n">ta_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Body of while loop.&quot;&quot;&quot;</span>
    <span class="n">fn_output</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">fn_output</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_loop_fn_dtypes</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Number of expected outputs, </span><span class="si">%d</span><span class="s2">, does not match the number of &quot;</span>
          <span class="s2">&quot;actual outputs, </span><span class="si">%d</span><span class="s2">, from loop_fn&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">flat_loop_fn_dtypes</span><span class="p">),</span>
                                                <span class="nb">len</span><span class="p">(</span><span class="n">fn_output</span><span class="p">)))</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">del</span> <span class="n">is_none_list</span><span class="p">[:]</span>
    <span class="n">is_none_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fn_output</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">ta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fn_output</span><span class="p">,</span> <span class="n">ta_list</span><span class="p">):</span>
      <span class="c1"># TODO(agarwal): support returning Operation objects from loop_fn.</span>
      <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># out may be a ref tensor, wrap it in identity to get a non-ref tensor.</span>
        <span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
      <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ta</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">parallel_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">extra_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;parallel_iterations&quot;</span><span class="p">:</span> <span class="n">parallel_iterations</span><span class="p">}</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">extra_args</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">ta_list</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="o">*</span><span class="n">ta</span><span class="p">:</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">iters</span><span class="p">,</span>
      <span class="n">while_body</span><span class="p">,</span>
      <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">tensor_array_ops</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">iters</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">flat_loop_fn_dtypes</span><span class="p">],</span>
      <span class="o">**</span><span class="n">extra_args</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>

  <span class="c1"># TODO(rachelim): enable this for sparse tensors</span>

  <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">if</span> <span class="n">is_none</span> <span class="k">else</span> <span class="n">ta</span><span class="o">.</span><span class="n">concat</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">ta</span><span class="p">,</span> <span class="n">is_none</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ta_list</span><span class="p">,</span> <span class="n">is_none_list</span><span class="p">)]</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_loop_fn_dtypes</span><span class="p">))</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">output</span><span class="p">:</span>
    <span class="c1"># This may happen for the case where iters == 0.</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">loop_fn_dtypes</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_flatten_first_two_dims</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Flattens the first two dimensions of x into a single dimension.&quot;&quot;&quot;</span>
  <span class="n">old_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">new_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">old_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">old_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">old_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]],</span>
                               <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>


<span class="n">PFOR_CONFIG_ARG</span> <span class="o">=</span> <span class="s2">&quot;pfor_config&quot;</span>


<span class="k">def</span> <span class="nf">_is_under_xla_context</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Check if we are currently inside an XLA compile context.&quot;&quot;&quot;</span>
  <span class="n">g</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="k">while</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">control_flow_context</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">while</span> <span class="n">control_flow_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">control_flow_context</span><span class="o">.</span><span class="n">IsXLAContext</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">True</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">control_flow_context</span> <span class="o">=</span> <span class="n">control_flow_context</span><span class="o">.</span><span class="n">outer_context</span>
    <span class="c1"># If g is a FuncGraph, get its outer_graph.</span>
    <span class="n">g</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s2">&quot;outer_graph&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">pfor</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Equivalent to running `loop_fn` `iters` times and stacking the outputs.</span>

<span class="sd">  `pfor` has functionality similar to `for_loop`, i.e. running `loop_fn` `iters`</span>
<span class="sd">  times, with input from 0 to `iters - 1`, and stacking corresponding output of</span>
<span class="sd">  each iteration. However the implementation does not use a tf.while_loop.</span>
<span class="sd">  Instead it adds new operations to the graph that collectively compute the same</span>
<span class="sd">  value as what running `loop_fn` in a loop would compute.</span>


<span class="sd">  This is an experimental feature and currently has a lot of limitations:</span>
<span class="sd">    - There should be no data dependency between the different iterations. For</span>
<span class="sd">      example, a future iteration should not depend on a value or side-effect of</span>
<span class="sd">      a previous iteration.</span>
<span class="sd">    - Stateful kernels may mostly not be supported since these often imply a</span>
<span class="sd">      data dependency or ordering of the iterations. We do support a limited set</span>
<span class="sd">      of such stateful kernels though (like RandomFoo, Variable operations like</span>
<span class="sd">      reads, etc).</span>
<span class="sd">    - Conversion works only on a limited set of kernels for which a converter</span>
<span class="sd">      has been registered.</span>
<span class="sd">    - loop_fn has limited support for control flow operations. tf.cond in</span>
<span class="sd">      particular is not supported.</span>
<span class="sd">    - `loop_fn` should return nested structure of Tensors or Operations. However</span>
<span class="sd">      if an Operation is returned, it should have zero outputs.</span>
<span class="sd">    - The shape and dtype of `loop_fn` outputs should not depend on the input</span>
<span class="sd">      to loop_fn.</span>

<span class="sd">  Args:</span>
<span class="sd">    loop_fn: A function that takes an int32 scalar tf.Tensor object representing</span>
<span class="sd">      the iteration number, and optionally a keyword argument `pfor_config` set</span>
<span class="sd">      to a PForConfig object. It returns a possibly nested structure of Tensor</span>
<span class="sd">      or Operation objects. Note that if setting `parallel_iterations` argument</span>
<span class="sd">      to something other than None, `loop_fn` may be called more than once</span>
<span class="sd">      during graph construction. So it may need to avoid mutating global state.</span>
<span class="sd">    iters: Number of iterations for which to run loop_fn.</span>
<span class="sd">    parallel_iterations: A knob to control how many iterations are vectorized</span>
<span class="sd">      and dispatched in parallel. The default value of None corresponds to</span>
<span class="sd">      vectorizing all the iterations.  If `parallel_iterations` is smaller than</span>
<span class="sd">      `iters`, then chunks of at most that many iterations are dispatched in</span>
<span class="sd">      sequence. This knob can be used to control the total memory usage.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Returns a nested structure of stacked tensor objects with the same nested</span>
<span class="sd">    structure as the output of `loop_fn`.</span>
<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If parallel_iterations is not None and not an integer &gt; 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">_pfor_impl</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">)</span>
  <span class="c1"># Note that we wrap into a tf.function if in eager execution mode or under</span>
  <span class="c1"># XLA compilation. The latter is so that we don&#39;t compile operations like</span>
  <span class="c1"># tf.placeholder that are created by the loop body.</span>
  <span class="n">functions_run_eagerly</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">or</span> <span class="n">_is_under_xla_context</span><span class="p">():</span>
    <span class="n">functions_run_eagerly</span> <span class="o">=</span> <span class="n">def_function</span><span class="o">.</span><span class="n">functions_run_eagerly</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">functions_run_eagerly</span><span class="p">:</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
          <span class="s2">&quot;It looks like tf.function behavior was disabled, perhaps using &quot;</span>
          <span class="s2">&quot;tf.config.experimental_run_functions_eagerly. Vectorization &quot;</span>
          <span class="s2">&quot;primitives (e.g. tf.vectorized_map) require tf.function to work. &quot;</span>
          <span class="s2">&quot;These primitives will override the disable.&quot;</span><span class="p">)</span>
      <span class="n">def_function</span><span class="o">.</span><span class="n">run_functions_eagerly</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">def_function</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">functions_run_eagerly</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">def_function</span><span class="o">.</span><span class="n">run_functions_eagerly</span><span class="p">(</span><span class="n">functions_run_eagerly</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">outputs</span>


<span class="k">def</span> <span class="nf">_loop_fn_has_config</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Test if `loop_fn` has a `pfor_config` argument.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">isfunction</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">):</span>
    <span class="n">argspec</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">PFOR_CONFIG_ARG</span> <span class="ow">in</span> <span class="n">argspec</span><span class="o">.</span><span class="n">args</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">):</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">loop_fn</span><span class="o">.</span><span class="n">func</span>
    <span class="n">argspec</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">PFOR_CONFIG_ARG</span> <span class="ow">in</span> <span class="n">argspec</span><span class="o">.</span><span class="n">args</span> <span class="ow">and</span>
            <span class="n">PFOR_CONFIG_ARG</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">loop_fn</span><span class="o">.</span><span class="n">keywords</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">loop_class</span> <span class="o">=</span> <span class="n">tf_decorator</span><span class="o">.</span><span class="n">unwrap</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">loop_class</span><span class="p">,</span> <span class="s2">&quot;__call__&quot;</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;loop_fn object did not have a __call__ method&quot;</span><span class="p">)</span>
    <span class="n">argspec</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">loop_class</span><span class="o">.</span><span class="fm">__call__</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">PFOR_CONFIG_ARG</span> <span class="ow">in</span> <span class="n">argspec</span><span class="o">.</span><span class="n">args</span>


<span class="k">def</span> <span class="nf">_pfor_impl</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pfor_config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implementation of pfor.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
  <span class="n">loop_fn_has_config</span> <span class="o">=</span> <span class="n">_loop_fn_has_config</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">)</span>
  <span class="n">existing_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operations</span><span class="p">())</span>
  <span class="c1"># Run the loop body</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loop_body&quot;</span><span class="p">):</span>
    <span class="n">loop_var</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">placeholder_with_default</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[])</span>
    <span class="k">if</span> <span class="n">loop_fn_has_config</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">pfor_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pfor_config</span> <span class="o">=</span> <span class="n">PForConfig</span><span class="p">()</span>
        <span class="n">pfor_config</span><span class="o">.</span><span class="n">_set_iters</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">loop_fn_outputs</span> <span class="o">=</span> <span class="n">loop_fn</span><span class="p">(</span><span class="n">loop_var</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="n">PFOR_CONFIG_ARG</span><span class="p">:</span> <span class="n">pfor_config</span><span class="p">})</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">pfor_config</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="n">loop_fn_outputs</span> <span class="o">=</span> <span class="n">loop_fn</span><span class="p">(</span><span class="n">loop_var</span><span class="p">)</span>

  <span class="c1"># Convert outputs to Tensor if needed.</span>
  <span class="n">tmp_loop_fn_outputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">loop_fn_output</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_fn_outputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">loop_fn_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">loop_fn_output</span><span class="p">,</span>
        <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">))):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loop_fn_output</span><span class="p">,</span> <span class="n">indexed_slices</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Converting </span><span class="si">%s</span><span class="s2"> to a dense representation may make it slow.&quot;</span>
                     <span class="s2">&quot; Alternatively, output the indices and values of the&quot;</span>
                     <span class="s2">&quot; IndexedSlices separately, and handle the vectorized&quot;</span>
                     <span class="s2">&quot; outputs directly.&quot;</span> <span class="o">%</span> <span class="n">loop_fn_output</span><span class="p">)</span>
      <span class="n">loop_fn_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">loop_fn_output</span><span class="p">)</span>
    <span class="n">tmp_loop_fn_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loop_fn_output</span><span class="p">)</span>
  <span class="n">loop_fn_outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">loop_fn_outputs</span><span class="p">,</span> <span class="n">tmp_loop_fn_outputs</span><span class="p">)</span>

  <span class="n">new_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operations</span><span class="p">())</span> <span class="o">-</span> <span class="n">existing_ops</span>
  <span class="n">iters</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">parallel_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">parallel_iterations</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;parallel_iterations must be None or a positive integer&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">parallel_iterations</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Found parallel_iterations == 1. Use for_loop instead.&quot;</span><span class="p">)</span>
    <span class="n">iters_value</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iters_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">iters_value</span> <span class="o">&lt;</span> <span class="n">parallel_iterations</span><span class="p">:</span>
      <span class="n">parallel_iterations</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">parallel_iterations</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;pfor&quot;</span><span class="p">):</span>
      <span class="n">converter</span> <span class="o">=</span> <span class="n">PFor</span><span class="p">(</span><span class="n">loop_var</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="n">new_ops</span><span class="p">,</span> <span class="n">pfor_config</span><span class="o">=</span><span class="n">pfor_config</span><span class="p">)</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">loop_fn_output</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_fn_outputs</span><span class="p">):</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">loop_fn_output</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">loop_fn_outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">pfor_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pfor_config</span><span class="o">.</span><span class="n">_has_reductions</span><span class="p">():</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Setting parallel_iterations currently unsupported if&quot;</span>
                       <span class="s2">&quot; reductions across iterations are performed.&quot;</span><span class="p">)</span>
    <span class="n">num_tiled_iterations</span> <span class="o">=</span> <span class="n">iters</span> <span class="o">//</span> <span class="n">parallel_iterations</span>
    <span class="n">num_remaining_iterations</span> <span class="o">=</span> <span class="n">iters</span> <span class="o">%</span> <span class="n">parallel_iterations</span>
    <span class="c1"># TODO(agarwal): Avoid calling loop_fn twice. Generate the loop body inside</span>
    <span class="c1"># a tf.function and extract the graph from there to vectorize it.</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;pfor_untiled&quot;</span><span class="p">):</span>
      <span class="n">converter</span> <span class="o">=</span> <span class="n">PFor</span><span class="p">(</span><span class="n">loop_var</span><span class="p">,</span> <span class="n">num_remaining_iterations</span><span class="p">,</span> <span class="n">new_ops</span><span class="p">,</span>
                       <span class="n">pfor_config</span><span class="o">=</span><span class="n">pfor_config</span><span class="p">)</span>
      <span class="n">remaining_outputs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">flattened_loop_fn_outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_fn_outputs</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">loop_fn_output</span> <span class="ow">in</span> <span class="n">flattened_loop_fn_outputs</span><span class="p">:</span>
        <span class="n">remaining_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">loop_fn_output</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;pfor_tiled&quot;</span><span class="p">):</span>
      <span class="n">loop_fn_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
                        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flattened_loop_fn_outputs</span><span class="p">]</span>

      <span class="k">def</span> <span class="nf">tiled_loop_body</span><span class="p">(</span><span class="n">j</span><span class="p">):</span>
        <span class="n">offset</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">parallel_iterations</span> <span class="o">+</span> <span class="n">num_remaining_iterations</span>

        <span class="k">def</span> <span class="nf">tiled_loop_fn</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pfor_config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">loop_fn_has_config</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">pfor_config</span><span class="o">=</span><span class="n">pfor_config</span><span class="p">))</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">offset</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">_pfor_impl</span><span class="p">(</span>
            <span class="n">tiled_loop_fn</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="p">,</span> <span class="n">pfor_config</span><span class="o">=</span><span class="n">pfor_config</span><span class="p">)</span>

      <span class="n">tiled_outputs</span> <span class="o">=</span> <span class="n">for_loop</span><span class="p">(</span><span class="n">tiled_loop_body</span><span class="p">,</span> <span class="n">loop_fn_dtypes</span><span class="p">,</span>
                               <span class="n">num_tiled_iterations</span><span class="p">,</span> <span class="n">parallel_iterations</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">tiled_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">_flatten_first_two_dims</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tiled_outputs</span><span class="p">]</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;pfor&quot;</span><span class="p">):</span>
      <span class="n">iters_value</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">iters_value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">iters_value</span> <span class="o">%</span> <span class="n">parallel_iterations</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">num_remaining_iterations</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="k">lambda</span><span class="p">:</span> <span class="n">tiled_outputs</span><span class="p">,</span>
            <span class="k">lambda</span><span class="p">:</span> <span class="p">[</span><span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">remaining_outputs</span><span class="p">,</span> <span class="n">tiled_outputs</span><span class="p">)])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tiled_outputs</span>
      <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">loop_fn_outputs</span><span class="p">,</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>


<div class="viewcode-block" id="vectorized_map"><a class="viewcode-back" href="../../../../../index.html#tensorflow.vectorized_map">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;vectorized_map&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">vectorized_map</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">elems</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Parallel map on the list of tensors unpacked from `elems` on dimension 0.</span>


<span class="sd">  This method works similar to tf.map_fn but is optimized to run much faster,</span>
<span class="sd">  possibly with a much larger memory footprint. The speedups are obtained by</span>
<span class="sd">  vectorization (see https://arxiv.org/pdf/1903.04243.pdf). The idea behind</span>
<span class="sd">  vectorization is to semantically launch all the invocations of `fn` in</span>
<span class="sd">  parallel and fuse corresponding operations across all these invocations. This</span>
<span class="sd">  fusion is done statically at graph generation time and the generated code is</span>
<span class="sd">  often similar in performance to a manually fused version.</span>

<span class="sd">  Because `tf.vectorized_map` fully parallelizes the batch, this method will</span>
<span class="sd">  generally be significantly faster than using `tf.map_fn`, especially in eager</span>
<span class="sd">  mode. However this is an experimental feature and currently has a lot of</span>
<span class="sd">  limitations:</span>
<span class="sd">    - There should be no data dependency between the different semantic</span>
<span class="sd">      invocations of `fn`, i.e. it should be safe to map the elements of the</span>
<span class="sd">      inputs in any order.</span>
<span class="sd">    - Stateful kernels may mostly not be supported since these often imply a</span>
<span class="sd">      data dependency. We do support a limited set of such stateful kernels</span>
<span class="sd">      though (like RandomFoo, Variable operations like reads, etc).</span>
<span class="sd">    - `fn` has limited support for control flow operations. `tf.cond` in</span>
<span class="sd">      particular is not supported.</span>
<span class="sd">    - `fn` should return nested structure of Tensors or Operations. However</span>
<span class="sd">      if an Operation is returned, it should have zero outputs.</span>
<span class="sd">    - The shape and dtype of any intermediate or output tensors in the</span>
<span class="sd">      computation of `fn` should not depend on the input to `fn`.</span>

<span class="sd">  Examples:</span>
<span class="sd">  ```python</span>
<span class="sd">  def outer_product(a):</span>
<span class="sd">    return tf.tensordot(a, a, 0)</span>

<span class="sd">  batch_size = 100</span>
<span class="sd">  a = tf.ones((batch_size, 32, 32))</span>
<span class="sd">  c = tf.vectorized_map(outer_product, a)</span>
<span class="sd">  assert c.shape == (batch_size, 32, 32, 32, 32)</span>
<span class="sd">  ```</span>

<span class="sd">  ```python</span>
<span class="sd">  # Computing per-example gradients</span>

<span class="sd">  batch_size = 10</span>
<span class="sd">  num_features = 32</span>
<span class="sd">  layer = tf.keras.layers.Dense(1)</span>

<span class="sd">  def model_fn(arg):</span>
<span class="sd">    with tf.GradientTape() as g:</span>
<span class="sd">      inp, label = arg</span>
<span class="sd">      inp = tf.expand_dims(inp, 0)</span>
<span class="sd">      label = tf.expand_dims(label, 0)</span>
<span class="sd">      prediction = layer(inp)</span>
<span class="sd">      loss = tf.nn.l2_loss(label - prediction)</span>
<span class="sd">    return g.gradient(loss, (layer.kernel, layer.bias))</span>

<span class="sd">  inputs = tf.random.uniform([batch_size, num_features])</span>
<span class="sd">  labels = tf.random.uniform([batch_size, 1])</span>
<span class="sd">  per_example_gradients = tf.vectorized_map(model_fn, (inputs, labels))</span>
<span class="sd">  assert per_example_gradients[0].shape == (batch_size, num_features, 1)</span>
<span class="sd">  assert per_example_gradients[1].shape == (batch_size, 1)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    fn: The callable to be performed. It accepts one argument, which will have</span>
<span class="sd">      the same (possibly nested) structure as `elems`, and returns a possibly</span>
<span class="sd">      nested structure of Tensors and Operations, which may be different than</span>
<span class="sd">      the structure of `elems`.</span>
<span class="sd">    elems: A tensor or (possibly nested) sequence of tensors, each of which will</span>
<span class="sd">      be unpacked along their first dimension. The nested sequence of the</span>
<span class="sd">      resulting slices will be mapped over by `fn`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor or (possibly nested) sequence of tensors. Each tensor packs the</span>
<span class="sd">    results of applying fn to tensors unpacked from elems along the first</span>
<span class="sd">    dimension, from first to last.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">loop_fn</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">gathered_elems</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="n">elems</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">gathered_elems</span><span class="p">)</span>
  <span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">first_elem</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">elems</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">if</span> <span class="n">first_elem</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">first_elem</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">first_elem</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">pfor</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>