

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.eager.backprop &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.eager.backprop</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.eager.backprop</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Code for backpropagation using the tape utilities.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tfe</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">_pywrap_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">backprop_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">execute</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">imperative_grad</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">tape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">check_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">default_gradient</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.unconnected_gradients</span> <span class="k">import</span> <span class="n">UnconnectedGradients</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_contextlib</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_inspect</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.lazy_loader</span> <span class="k">import</span> <span class="n">LazyLoader</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>


<span class="c1"># Note that we need to lazy load the following two modules to avoid creating</span>
<span class="c1"># circular dependencies.</span>
<span class="c1"># TODO(b/119775953): fix the circular dependencies.</span>
<span class="n">pfor_ops</span> <span class="o">=</span> <span class="n">LazyLoader</span><span class="p">(</span>
    <span class="s2">&quot;pfor_ops&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span>
    <span class="s2">&quot;tensorflow.python.ops.parallel_for.control_flow_ops&quot;</span><span class="p">)</span>

<span class="n">function</span> <span class="o">=</span> <span class="n">LazyLoader</span><span class="p">(</span><span class="s2">&quot;function&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span>
                      <span class="s2">&quot;tensorflow.python.eager.function&quot;</span><span class="p">)</span>

<span class="n">_op_attr_type_cache</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">def</span> <span class="nf">op_attr_type</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_op_attr_type_cache</span><span class="p">[(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)]</span>
  <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">_handle</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">attr_type</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_OpNameGetAttrType</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
  <span class="n">_op_attr_type_cache</span><span class="p">[(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)]</span> <span class="o">=</span> <span class="n">attr_type</span>
  <span class="k">return</span> <span class="n">attr_type</span>


<span class="k">def</span> <span class="nf">make_attr</span><span class="p">(</span><span class="n">attr_type</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
  <span class="c1"># pybind11 enums do not return the raw value like SWIG enums do. They are</span>
  <span class="c1"># useful when comparing amongst each other but not direct integers as we are</span>
  <span class="c1"># doing in most tests.</span>
  <span class="c1"># https://pybind11.readthedocs.io/en/stable/classes.html#enumerations-and-internal-types</span>
  <span class="c1"># TODO(amitpatankar): After all SWIG transitions, convert the enum comparisons</span>
  <span class="c1"># from integer value to class.</span>
  <span class="k">if</span> <span class="n">attr_type</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_ATTR_TYPE</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">attr_type</span> <span class="o">==</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_ATTR_TYPE</span><span class="p">)]:</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">attr_type</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_ATTR_SHAPE</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">as_proto</span><span class="p">()</span>
  <span class="k">elif</span> <span class="n">attr_type</span> <span class="o">==</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_ATTR_SHAPE</span><span class="p">)]:</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">as_proto</span><span class="p">()</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="p">]</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">encode</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">value</span>


<span class="k">class</span> <span class="nc">_MockOp</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pretends to be a tf.Operation for the gradient functions.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">typ</span><span class="p">,</span> <span class="n">skip_input_indices</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attrs</span> <span class="o">=</span> <span class="n">attrs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="n">typ</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">skip_input_indices</span> <span class="o">=</span> <span class="n">skip_input_indices</span>

  <span class="k">def</span> <span class="nf">get_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
    <span class="n">typ</span> <span class="o">=</span> <span class="n">op_attr_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attrs</span><span class="p">),</span> <span class="mi">2</span><span class="p">):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">attr</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">make_attr</span><span class="p">(</span><span class="n">typ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attrs</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="n">attr</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;tf.GradientTape.gradients() does not support graph control flow &quot;</span>
        <span class="s2">&quot;operations like tf.cond or tf.while at this time. Use tf.gradients() &quot;</span>
        <span class="s2">&quot;instead. If you need this feature, please file a feature request at &quot;</span>
        <span class="s2">&quot;https://github.com/tensorflow/tensorflow/issues/new&quot;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_gradient_function</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">attr_tuple</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span>
                       <span class="n">out_grads</span><span class="p">,</span> <span class="n">skip_input_indices</span><span class="p">,</span> <span class="n">forward_pass_name_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calls the gradient function of the op.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_name: the name of the op to be differentiated.</span>
<span class="sd">    attr_tuple: the attrs, as a tuple.</span>
<span class="sd">    num_inputs: the number of inputs to the op.</span>
<span class="sd">    inputs: inputs to the original operation.</span>
<span class="sd">    outputs: outputs to the original operation.</span>
<span class="sd">    out_grads: gradients of the operation wrt its outputs.</span>
<span class="sd">    skip_input_indices: a tuple that is passed to the gradient function,</span>
<span class="sd">      indicating which inputs to skip calculating the gradient for</span>
<span class="sd">    forward_pass_name_scope: the namescope of the op in the forward pass.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The gradients with respect to the inputs of the function, as a list.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">mock_op</span> <span class="o">=</span> <span class="n">_MockOp</span><span class="p">(</span><span class="n">attr_tuple</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">skip_input_indices</span><span class="p">)</span>
  <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">_gradient_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">op_name</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">grad_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_inputs</span>

  <span class="c1"># This does not work with v1 TensorArrays.</span>
  <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">(</span>
  <span class="p">)</span> <span class="ow">or</span> <span class="n">control_flow_util</span><span class="o">.</span><span class="n">EnableControlFlowV2</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()):</span>
    <span class="k">if</span> <span class="n">forward_pass_name_scope</span><span class="p">:</span>
      <span class="n">gradient_name_scope</span> <span class="o">=</span> <span class="s2">&quot;gradient_tape/&quot;</span> <span class="o">+</span> <span class="n">forward_pass_name_scope</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">gradient_name_scope</span> <span class="o">=</span> <span class="s2">&quot;gradient_tape/&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">gradient_name_scope</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">mock_op</span><span class="p">,</span> <span class="o">*</span><span class="n">out_grads</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">mock_op</span><span class="p">,</span> <span class="o">*</span><span class="n">out_grads</span><span class="p">)</span>


<span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_RegisterGradientFunction</span><span class="p">(</span><span class="n">_gradient_function</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_must_record_gradient</span><span class="p">():</span>
  <span class="k">return</span> <span class="ow">not</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_TapeSetIsEmpty</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_record_gradient</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_RecordGradient</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span>
                                          <span class="n">ops</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">())</span>


<span class="n">execute</span><span class="o">.</span><span class="n">must_record_gradient</span> <span class="o">=</span> <span class="n">_must_record_gradient</span>
<span class="n">execute</span><span class="o">.</span><span class="n">record_gradient</span> <span class="o">=</span> <span class="n">_record_gradient</span>


<span class="k">def</span> <span class="nf">implicit_val_and_grad</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a function which differentiates f with respect to variables.</span>

<span class="sd">  The wrapped function returns the value and the gradient of f when called with</span>
<span class="sd">  the same arguments. The gradient is with respect to all trainable TFE</span>
<span class="sd">  variables accessed by `f`.</span>

<span class="sd">  This function is useful when the exact set of variables to differentiate with</span>
<span class="sd">  is not known ahead of time.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```python</span>
<span class="sd">  dense_layer = tf.compat.v1.layers.Dense(1)</span>
<span class="sd">  def loss(x, y):</span>
<span class="sd">    return tf.reduce_sum(tf.square(dense_layer(x) - y))</span>

<span class="sd">  # Obtain the gradient function.</span>
<span class="sd">  val_grad_fn = tfe.implicit_value_and_gradients(loss)</span>

<span class="sd">  # Invoke the gradient function with concrete values of x and y.</span>
<span class="sd">  x = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span>
<span class="sd">  y = tf.constant([[10.0], [20.0]])</span>
<span class="sd">  value, grads_and_vars = val_grad_fn(x, y)</span>
<span class="sd">  print(&#39;Value of loss: %s&#39; % value)</span>

<span class="sd">  # Apply the gradients to Variables.</span>
<span class="sd">  optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)</span>
<span class="sd">  optimizer.apply_gradients(grads_and_vars)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    f: function to be differentiated. If `f` returns a scalar, this scalar will</span>
<span class="sd">      be differentiated. If `f` returns a tensor or list of tensors, by default</span>
<span class="sd">      a scalar will be computed by adding all their values to produce a single</span>
<span class="sd">      scalar.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function which, when called, returns a tuple pair.</span>
<span class="sd">    Its first element is the value to which the function evaluates.</span>
<span class="sd">    Its second element is list of (gradient, variable) pairs.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if `f` returns None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(cais): Remove calls to tf.constant() once the gradients functions</span>
  <span class="c1"># accept lists and np.ndarrays.</span>

  <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the gradient of the wrapped function.&quot;&quot;&quot;</span>
    <span class="n">this_tape</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">push_new_tape</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">end_node</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">end_node</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot differentiate a function that returns None; &quot;</span>
                         <span class="s2">&quot;did you forget to return a value from </span><span class="si">{}</span><span class="s2">?&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                             <span class="n">f</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">tape</span><span class="o">.</span><span class="n">pop_tape</span><span class="p">(</span><span class="n">this_tape</span><span class="p">)</span>
    <span class="c1"># Note: variables are returned in construction order. This ensures unique</span>
    <span class="c1"># order across executions.</span>
    <span class="n">variables</span> <span class="o">=</span> <span class="n">this_tape</span><span class="o">.</span><span class="n">watched_variables</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">variables</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No trainable variables were accessed while the &quot;</span>
                       <span class="s2">&quot;function was being computed.&quot;</span><span class="p">)</span>

    <span class="n">sources</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">handle</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">]</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">imperative_grad</span><span class="o">.</span><span class="n">imperative_grad</span><span class="p">(</span><span class="n">this_tape</span><span class="p">,</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">end_node</span><span class="p">),</span>
                                           <span class="n">sources</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">end_node</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">variables</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">grad_fn</span>


<span class="k">def</span> <span class="nf">implicit_grad</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a function which differentiates f with respect to variables.</span>

<span class="sd">  The wrapped function returns the gradient of f when called with the same</span>
<span class="sd">  arguments. The gradient is with respect to all trainable TFE variables</span>
<span class="sd">  accessed by `f`.</span>

<span class="sd">  This function is useful when the exact set of variables to differentiate with</span>
<span class="sd">  is not known ahead of time.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```python</span>
<span class="sd">  dense_layer = tf.compat.v1.layers.Dense(1)</span>
<span class="sd">  def loss(x, y):</span>
<span class="sd">    return tf.reduce_sum(tf.square(dense_layer(x) - y))</span>

<span class="sd">  # Obtain the gradient function.</span>
<span class="sd">  grad_fn = tfe.implicit_gradients(loss)</span>

<span class="sd">  # Invoke the gradient function with concrete values of x and y.</span>
<span class="sd">  x = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span>
<span class="sd">  y = tf.constant([[10.0], [20.0]])</span>
<span class="sd">  grads_and_vars = grad_fn(x, y)</span>

<span class="sd">  # Apply the gradients to Variables.</span>
<span class="sd">  optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)</span>
<span class="sd">  optimizer.apply_gradients(grads_and_vars)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    f: function to be differentiated. If `f` returns a scalar, this scalar will</span>
<span class="sd">      be differentiated. If `f` returns a tensor or list of tensors, by default</span>
<span class="sd">      a scalar will be computed by adding all their values to produce a single</span>
<span class="sd">      scalar.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function which, when called, returns a list of (gradient, variable) pairs.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(cais): Remove calls to tf.constant() once the gradients functions</span>
  <span class="c1"># accept lists and np.ndarrays.</span>

  <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the gradient of the wrapped function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">implicit_val_and_grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">return</span> <span class="n">grad_fn</span>


<span class="k">def</span> <span class="nf">_get_arg_spec</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">param_args</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;The positions of the parameters of f to be differentiated in param_args.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">args</span>
  <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># TypeError can happen when f is a callable object.</span>
    <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param_args</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">params</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">params</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Either callable provided is not a function or could not &quot;</span>
                     <span class="s2">&quot;inspect its arguments by name: </span><span class="si">%s</span><span class="s2">. Original error: </span><span class="si">%s</span><span class="s2">&quot;</span>
                     <span class="o">%</span> <span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param_args</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;self&quot;</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
  <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
  <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">params</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;params must be all strings or all integers; got </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">params</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gradients_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a function which differentiates f with respect to params.</span>

<span class="sd">  Example:</span>
<span class="sd">  ```python</span>
<span class="sd">  # f(x, y) = (x ^ 3) * y - x * (y ^ 2)</span>
<span class="sd">  # Therefore, the 1st order derivatives are:</span>
<span class="sd">  #   df / dx = 3 * (x ^ 2) * y - y ^ 2</span>
<span class="sd">  #   df / dy = x ^ 3 - 2 * x * y</span>
<span class="sd">  # The 2nd order derivatives with respect to x is:</span>
<span class="sd">  #   d^2 f / (dx)^2 = 6 * x * y</span>
<span class="sd">  def f(x, y):</span>
<span class="sd">    return x * x * x * y - x * y * y</span>

<span class="sd">  # Obtain a function that returns 1st order gradients.</span>
<span class="sd">  grad_fn = tfe.gradients_function(f)</span>

<span class="sd">  x = 2.0</span>
<span class="sd">  y = 3.0</span>

<span class="sd">  # Invoke the 1st order gradient function.</span>
<span class="sd">  x_grad, y_grad = grad_fn(x, y)</span>
<span class="sd">  assert x_grad.numpy() == 3 * (2 ** 2) * 3 - 3 ** 2</span>
<span class="sd">  assert y_grad.numpy() == (2 ** 3) - 2 * 2 * 3</span>

<span class="sd">  # Obtain a function that returns the 2nd order gradient with respect to x.</span>
<span class="sd">  gradgrad_fn = tfe.gradients_function(lambda x, y: grad_fn(x, y)[0])</span>

<span class="sd">  # Invoke the 2nd order gradient function.</span>
<span class="sd">  x_gradgrad = gradgrad_fn(x, y)[0]</span>
<span class="sd">  assert x_gradgrad.numpy() == 6 * 2 * 3</span>

<span class="sd">  # To obtain a callable that returns the gradient(s) of `f` with respect to a</span>
<span class="sd">  # subset of its inputs, use the `params` keyword argument with</span>
<span class="sd">  # `gradients_function()`.</span>
<span class="sd">  ygrad_fn = tfe.gradients_function(f, params=[1])</span>

<span class="sd">  (y_grad,) = ygrad_fn(x, y)</span>
<span class="sd">  assert y_grad.numpy() == (2 ** 3) - 2 * 2 * 3</span>
<span class="sd">  ```</span>

<span class="sd">  Note that only tensors with real or complex dtypes are differentiable.</span>

<span class="sd">  Args:</span>
<span class="sd">    f: function to be differentiated. If `f` returns a scalar, this scalar will</span>
<span class="sd">      be differentiated. If `f` returns a tensor or list of tensors, by default</span>
<span class="sd">      a scalar will be computed by adding all their values to produce a single</span>
<span class="sd">      scalar. If desired, the tensors can be elementwise multiplied by the</span>
<span class="sd">      tensors passed as the `dy` keyword argument to the returned gradient</span>
<span class="sd">      function.</span>
<span class="sd">    params: list of parameter names of f or list of integers indexing the</span>
<span class="sd">      parameters with respect to which we&#39;ll differentiate. Passing None</span>
<span class="sd">      differentiates with respect to all parameters.</span>

<span class="sd">  Returns:</span>
<span class="sd">    function which, when called, returns the value of f and the gradient</span>
<span class="sd">    of `f` with respect to all of `params`. The function takes an extra optional</span>
<span class="sd">    keyword argument `dy`. Setting it allows computation of vector jacobian</span>
<span class="sd">    products for vectors other than the vector of ones.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if the params are not all strings or all integers.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">decorated</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the gradient of the decorated function.&quot;&quot;&quot;</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">val_and_grad_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad</span>

  <span class="k">return</span> <span class="n">decorated</span>


<span class="k">def</span> <span class="nf">_ensure_unique_tensor_objects</span><span class="p">(</span><span class="n">parameter_positions</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Make each of the parameter_positions in args a unique ops.Tensor object.</span>

<span class="sd">  Ensure that each parameter is treated independently.</span>
<span class="sd">  For example:</span>

<span class="sd">  def f(x, y): return x * y</span>
<span class="sd">  g = gradients_function(f)</span>
<span class="sd">  one = tf.constant(1.)</span>

<span class="sd">  g(one, one) should return [1., 1.]</span>
<span class="sd">  (even though the two arguments are the same Tensor object).</span>

<span class="sd">  Args:</span>
<span class="sd">    parameter_positions: List of indices into args defining the arguments to</span>
<span class="sd">      differentiate against.</span>
<span class="sd">    args: A list of arguments to the function to be differentiated.</span>

<span class="sd">  Returns:</span>
<span class="sd">    args, possibly edited in-place.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">s</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_positions</span><span class="p">:</span>
      <span class="n">tid</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tensor_id</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">tid</span> <span class="ow">in</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tid</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">args</span>


<span class="k">def</span> <span class="nf">val_and_grad_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a function that computes f and its derivative w.r.t. params.</span>

<span class="sd">  Example:</span>
<span class="sd">  ```python</span>
<span class="sd">  # f(x, y) = (x ^ 3) * y - x * (y ^ 2)</span>
<span class="sd">  # Therefore, the 1st order derivatives are:</span>
<span class="sd">  #   df / dx = 3 * (x ^ 2) * y - y ^ 2</span>
<span class="sd">  #   df / dy = x ^ 3 - 2 * x * y</span>
<span class="sd">  def f(x, y):</span>
<span class="sd">    return x * x * x * y - x * y * y</span>

<span class="sd">  # Obtain a function that returns the function value and the 1st order</span>
<span class="sd">  # gradients.</span>
<span class="sd">  val_grads_fn = tfe.value_and_gradients_function(f)</span>

<span class="sd">  x = 2.0</span>
<span class="sd">  y = 3.0</span>

<span class="sd">  # Invoke the value-and-gradients function.</span>
<span class="sd">  f_val, (x_grad, y_grad) = val_grads_fn(x, y)</span>
<span class="sd">  assert f_val.numpy() == (2 ** 3) * 3 - 2 * (3 ** 2)</span>
<span class="sd">  assert x_grad.numpy() == 3 * (2 ** 2) * 3 - 3 ** 2</span>
<span class="sd">  assert y_grad.numpy() == (2 ** 3) - 2 * 2 * 3</span>

<span class="sd">  # To obtain a callable that returns the value of `f` and the gradient(s) of</span>
<span class="sd">  # `f` with respect to a subset of its inputs, use the `params` keyword</span>
<span class="sd">  # argument with `value_and_gradients_function()`.</span>
<span class="sd">  val_ygrad_fn = tfe.value_and_gradients_function(f, params=[1])</span>

<span class="sd">  f_val, (y_grad,) = val_ygrad_fn(x, y)</span>
<span class="sd">  assert f_val.numpy() == (2 ** 3) * 3 - 2 * (3 ** 2)</span>
<span class="sd">  assert y_grad.numpy() == (2 ** 3) - 2 * 2 * 3</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    f: function to be differentiated. If `f` returns a scalar, this scalar will</span>
<span class="sd">      be differentiated. If `f` returns a tensor or list of tensors, by default</span>
<span class="sd">      a scalar will be computed by adding all their values to produce a single</span>
<span class="sd">      scalar. If desired, the tensors can be elementwise multiplied by the</span>
<span class="sd">      tensors passed as the `dy` keyword argument to the returned gradient</span>
<span class="sd">      function.</span>
<span class="sd">    params: list of parameter names of f or list of integers indexing the</span>
<span class="sd">      parameters with respect to which we&#39;ll differentiate. Passing `None`</span>
<span class="sd">      differentiates with respect to all parameters.</span>

<span class="sd">  Returns:</span>
<span class="sd">    function which, when called, returns the value of f and the gradient</span>
<span class="sd">    of f with respect to all of `params`. The function takes an extra optional</span>
<span class="sd">    keyword argument &quot;dy&quot;. Setting it allows computation of vector jacobian</span>
<span class="sd">    products for vectors other than the vector of ones.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if the params are not all strings or all integers.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">decorated</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value and gradient of the decorated function.&quot;&quot;&quot;</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">kwds</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;dy&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwds</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Functions to be differentiated cannot &quot;</span>
                       <span class="s2">&quot;receive keyword arguments.&quot;</span><span class="p">)</span>
    <span class="n">val</span><span class="p">,</span> <span class="n">vjp</span> <span class="o">=</span> <span class="n">make_vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val</span><span class="p">,</span> <span class="n">vjp</span><span class="p">(</span><span class="n">dy</span><span class="o">=</span><span class="n">dy</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">decorated</span>


<span class="k">def</span> <span class="nf">make_vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a function that computes f and its vjp w.r.t.</span>

<span class="sd">  params.</span>

<span class="sd">  The term &quot;vjp&quot; here is an abbreviation for vector-jacobian product.</span>

<span class="sd">  Args:</span>
<span class="sd">    f: the function to be differentiated.</span>
<span class="sd">    params: the parameters (numbers or names) to differentiate with respect to.</span>
<span class="sd">      A value of None will differentiate with respect to all parameters.</span>
<span class="sd">    persistent: Boolean controlling whether the VJP function can be re-used.</span>
<span class="sd">      Must be True or False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function, which when called, returns a tuple (value, vjp), where:</span>
<span class="sd">    - value is the result of calling f.</span>
<span class="sd">    - vjp is a function, which takes a vector as an argument and</span>
<span class="sd">      returns the product of that vector with the Jacobian of f.</span>
<span class="sd">      Providing no argument to vjp is equivalent to providing a</span>
<span class="sd">      vector of ones.</span>

<span class="sd">    For example,</span>
<span class="sd">    ```python</span>
<span class="sd">    def f(x):</span>
<span class="sd">      return x * x</span>

<span class="sd">    wrapped_fn = tfe.make_vjp(f)</span>
<span class="sd">    result, vjp = wrapped_fn(tf.constant(3.0))</span>
<span class="sd">    # result is 9.0</span>
<span class="sd">    vjp()  # the vjp function rturns 6.0</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if `f` returns None.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">decorated</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value and gradient of the decorated function.&quot;&quot;&quot;</span>
    <span class="n">parameter_positions</span> <span class="o">=</span> <span class="n">_get_arg_spec</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">kwds</span><span class="p">,</span> <span class="s2">&quot;The gradient function can&#39;t take keyword arguments.&quot;</span>
    <span class="n">this_tape</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">push_new_tape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="n">persistent</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">sources</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">args</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_positions</span> <span class="k">else</span> <span class="n">arg</span>
          <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
      <span class="p">]</span>
      <span class="n">args</span> <span class="o">=</span> <span class="n">_ensure_unique_tensor_objects</span><span class="p">(</span><span class="n">parameter_positions</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_positions</span><span class="p">:</span>
        <span class="n">sources</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">this_tape</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot differentiate a function that returns None; &quot;</span>
                         <span class="s2">&quot;did you forget to return a value from </span><span class="si">{}</span><span class="s2">?&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                             <span class="n">f</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
      <span class="n">flat_result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
      <span class="n">flat_result</span> <span class="o">=</span> <span class="p">[</span><span class="n">gen_array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_result</span><span class="p">]</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">flat_result</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">tape</span><span class="o">.</span><span class="n">pop_tape</span><span class="p">(</span><span class="n">this_tape</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">dy</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">dy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">dy</span><span class="p">)]</span>
      <span class="k">return</span> <span class="n">imperative_grad</span><span class="o">.</span><span class="n">imperative_grad</span><span class="p">(</span>
          <span class="n">this_tape</span><span class="p">,</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">result</span><span class="p">),</span> <span class="n">sources</span><span class="p">,</span> <span class="n">output_gradients</span><span class="o">=</span><span class="n">dy</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">vjp</span>

  <span class="k">return</span> <span class="n">decorated</span>


<span class="k">def</span> <span class="nf">flatten_nested_indexed_slices</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">flatten_nested_indexed_slices</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
                                                        <span class="n">g</span><span class="o">.</span><span class="n">indices</span><span class="p">),</span>
                             <span class="n">g</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">aggregate_indexed_slices_gradients</span><span class="p">(</span><span class="n">grads</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Aggregates gradients containing `IndexedSlices`s.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span> <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
    <span class="c1"># If any gradient is a `Tensor`, sum them up and return a dense tensor</span>
    <span class="c1"># object.</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>

    <span class="c1"># The following `_as_indexed_slices_list` casts ids of IndexedSlices into</span>
    <span class="c1"># int64. It is to make sure the inputs of `concat` all have same the data</span>
    <span class="c1"># type.</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">_as_indexed_slices_list</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">flatten_nested_indexed_slices</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">]</span>
    <span class="c1"># Form IndexedSlices out of the concatenated values and indices.</span>
    <span class="n">concat_grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
        <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">values</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">indices</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">concat_grad</span>


<span class="k">def</span> <span class="nf">_aggregate_grads</span><span class="p">(</span><span class="n">gradients</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Aggregate gradients from multiple sources.</span>

<span class="sd">  Args:</span>
<span class="sd">    gradients: A list of &#39;Tensor&#39; or &#39;IndexedSlices&#39; gradients.</span>

<span class="sd">  Returns:</span>
<span class="sd">    If &#39;gradients&#39; only has &#39;Tensor&#39;, returns an aggregated &#39;Tensor&#39;.</span>
<span class="sd">    Otherwise returns an aggregated &#39;IndexedSlices&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">gradients</span><span class="p">,</span> <span class="s2">&quot;No gradients to aggregate&quot;</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">))</span>
               <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">aggregate_indexed_slices_gradients</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_num_elements</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;The number of elements in the `grad` tensor.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">shape_tuple</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
    <span class="n">shape_tuple</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`grad` not a Tensor or IndexedSlices.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">shape_tuple</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="kc">None</span> <span class="ow">in</span> <span class="n">shape_tuple</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">shape_tuple</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_fast_fill</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span>
      <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
      <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper to return (possibly cached) zero tensors in eager mode.&quot;&quot;&quot;</span>
  <span class="c1"># Note: variants will use _zeros_like</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">string</span> <span class="ow">or</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">resource</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

  <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_name</span>

  <span class="k">if</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
    <span class="n">shape_key</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">shape_key</span> <span class="o">=</span> <span class="n">shape</span>
  <span class="n">cache_key</span> <span class="o">=</span> <span class="n">shape_key</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span>
  <span class="n">cached</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">zeros_cache</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_key</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">cached</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">is_bool</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">cached</span> <span class="o">=</span> <span class="n">_fast_fill</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">zeros_cache</span><span class="p">()</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">cache_key</span><span class="p">,</span> <span class="n">cached</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">cached</span>


<span class="k">def</span> <span class="nf">_ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="n">as_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">as_dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">string</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">as_dtype</span><span class="o">.</span><span class="n">is_bool</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="k">if</span> <span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>  <span class="c1"># pylint: disable=g-explicit-bool-comparison</span>
    <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_fast_fill</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="n">_default_vspace</span> <span class="o">=</span> <span class="n">imperative_grad</span><span class="o">.</span><span class="n">VSpace</span><span class="p">(</span>
    <span class="n">num_elements_fn</span><span class="o">=</span><span class="n">_num_elements</span><span class="p">,</span>
    <span class="n">aggregate_fn</span><span class="o">=</span><span class="n">_aggregate_grads</span><span class="p">,</span>
    <span class="n">zeros_fn</span><span class="o">=</span><span class="n">_zeros</span><span class="p">,</span>
    <span class="n">ones_fn</span><span class="o">=</span><span class="n">_ones</span><span class="p">,</span>
    <span class="n">zeros_like_fn</span><span class="o">=</span><span class="n">default_gradient</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">,</span>
    <span class="n">ones_like_fn</span><span class="o">=</span><span class="n">default_gradient</span><span class="o">.</span><span class="n">ones_like</span><span class="p">,</span>
    <span class="n">graph_shape_fn</span><span class="o">=</span><span class="n">gen_array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_RegisterVSpace</span><span class="p">(</span><span class="n">_default_vspace</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_handle_or_self</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;If x is ResourceVariable, return its handle, else x.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">is_resource_variable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">handle</span>
  <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="GradientTape"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;GradientTape&quot;</span><span class="p">,</span> <span class="s2">&quot;autodiff.GradientTape&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;GradientTape&quot;</span><span class="p">])</span>
<span class="k">class</span> <span class="nc">GradientTape</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Record operations for automatic differentiation.</span>

<span class="sd">  Operations are recorded if they are executed within this context manager and</span>
<span class="sd">  at least one of their inputs is being &quot;watched&quot;.</span>

<span class="sd">  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,</span>
<span class="sd">  where `trainable=True` is default in both cases) are automatically watched.</span>
<span class="sd">  Tensors can be manually watched by invoking the `watch` method on this context</span>
<span class="sd">  manager.</span>

<span class="sd">  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can</span>
<span class="sd">  be computed as:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant(3.0)</span>
<span class="sd">  with tf.GradientTape() as g:</span>
<span class="sd">    g.watch(x)</span>
<span class="sd">    y = x * x</span>
<span class="sd">  dy_dx = g.gradient(y, x) # Will compute to 6.0</span>
<span class="sd">  ```</span>

<span class="sd">  GradientTapes can be nested to compute higher-order derivatives. For example,</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant(3.0)</span>
<span class="sd">  with tf.GradientTape() as g:</span>
<span class="sd">    g.watch(x)</span>
<span class="sd">    with tf.GradientTape() as gg:</span>
<span class="sd">      gg.watch(x)</span>
<span class="sd">      y = x * x</span>
<span class="sd">    dy_dx = gg.gradient(y, x)     # Will compute to 6.0</span>
<span class="sd">  d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0</span>
<span class="sd">  ```</span>

<span class="sd">  By default, the resources held by a GradientTape are released as soon as</span>
<span class="sd">  GradientTape.gradient() method is called. To compute multiple gradients over</span>
<span class="sd">  the same computation, create a persistent gradient tape. This allows multiple</span>
<span class="sd">  calls to the gradient() method as resources are released when the tape object</span>
<span class="sd">  is garbage collected. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant(3.0)</span>
<span class="sd">  with tf.GradientTape(persistent=True) as g:</span>
<span class="sd">    g.watch(x)</span>
<span class="sd">    y = x * x</span>
<span class="sd">    z = y * y</span>
<span class="sd">  dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)</span>
<span class="sd">  dy_dx = g.gradient(y, x)  # 6.0</span>
<span class="sd">  del g  # Drop the reference to the tape</span>
<span class="sd">  ```</span>

<span class="sd">  By default GradientTape will automatically watch any trainable variables that</span>
<span class="sd">  are accessed inside the context. If you want fine grained control over which</span>
<span class="sd">  variables are watched you can disable automatic tracking by passing</span>
<span class="sd">  `watch_accessed_variables=False` to the tape constructor:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.GradientTape(watch_accessed_variables=False) as tape:</span>
<span class="sd">    tape.watch(variable_a)</span>
<span class="sd">    y = variable_a ** 2  # Gradients will be available for `variable_a`.</span>
<span class="sd">    z = variable_b ** 3  # No gradients will be available since `variable_b` is</span>
<span class="sd">                         # not being watched.</span>
<span class="sd">  ```</span>

<span class="sd">  Note that when using models you should ensure that your variables exist when</span>
<span class="sd">  using `watch_accessed_variables=False`. Otherwise it&#39;s quite easy to make your</span>
<span class="sd">  first iteration not have any gradients:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.keras.layers.Dense(32)</span>
<span class="sd">  b = tf.keras.layers.Dense(32)</span>

<span class="sd">  with tf.GradientTape(watch_accessed_variables=False) as tape:</span>
<span class="sd">    tape.watch(a.variables)  # Since `a.build` has not been called at this point</span>
<span class="sd">                             # `a.variables` will return an empty list and the</span>
<span class="sd">                             # tape will not be watching anything.</span>
<span class="sd">    result = b(a(inputs))</span>
<span class="sd">    tape.gradient(result, a.variables)  # The result of this computation will be</span>
<span class="sd">                                        # a list of `None`s since a&#39;s variables</span>
<span class="sd">                                        # are not being watched.</span>
<span class="sd">  ```</span>

<span class="sd">  Note that only tensors with real or complex dtypes are differentiable.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">watch_accessed_variables</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new GradientTape.</span>

<span class="sd">    Args:</span>
<span class="sd">      persistent: Boolean controlling whether a persistent gradient tape</span>
<span class="sd">        is created. False by default, which means at most one call can</span>
<span class="sd">        be made to the gradient() method on this object.</span>
<span class="sd">      watch_accessed_variables: Boolean controlling whether the tape will</span>
<span class="sd">        automatically `watch` any (trainable) variables accessed while the tape</span>
<span class="sd">        is active. Defaults to True meaning gradients can be requested from any</span>
<span class="sd">        result computed in the tape derived from reading a trainable `Variable`.</span>
<span class="sd">        If False users must explicitly `watch` any `Variable`s they want to</span>
<span class="sd">        request gradients from.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_persistent</span> <span class="o">=</span> <span class="n">persistent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_watch_accessed_variables</span> <span class="o">=</span> <span class="n">watch_accessed_variables</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_watched_variables</span> <span class="o">=</span> <span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_recording</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_created_eagerly</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_eagerly</span><span class="p">:</span>
      <span class="n">context</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
      <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">start_step</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enters a context inside which operations are recorded on this tape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_push_tape</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">typ</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Exits the recording context, no further operations are traced.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recording</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_push_tape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pushes a new tape onto the tape stack.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recording</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tape is still recording, This can happen if you try to &quot;</span>
                       <span class="s2">&quot;re-enter an already-active tape.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">push_new_tape</span><span class="p">(</span>
          <span class="n">persistent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_persistent</span><span class="p">,</span>
          <span class="n">watch_accessed_variables</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_watch_accessed_variables</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">tape</span><span class="o">.</span><span class="n">push_tape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_recording</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">_pop_tape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recording</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tape is not recording.&quot;</span><span class="p">)</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">pop_tape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_recording</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_eagerly</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">end_step</span><span class="p">()</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">pass</span>
      <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
        <span class="k">pass</span>

<div class="viewcode-block" id="GradientTape.watch"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape.watch">[docs]</a>  <span class="k">def</span> <span class="nf">watch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Ensures that `tensor` is being traced by this tape.</span>

<span class="sd">    Args:</span>
<span class="sd">      tensor: a Tensor or list of Tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if it encounters something that is not a tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">_pywrap_utils</span><span class="o">.</span><span class="n">IsTensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_pywrap_utils</span><span class="o">.</span><span class="n">IsVariable</span><span class="p">(</span><span class="n">t</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Passed in object of type </span><span class="si">{}</span><span class="s2">, not tf.Tensor&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">backprop_util</span><span class="o">.</span><span class="n">IsTrainable</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span> <span class="s2">&quot;The dtype of the watched tensor must be &quot;</span>
            <span class="s2">&quot;floating (e.g. tf.float32), got </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s2">&quot;handle&quot;</span><span class="p">):</span>
        <span class="c1"># There are many variable-like objects, all of them currently have</span>
        <span class="c1"># `handle` attribute that points to a tensor. If this changes, internals</span>
        <span class="c1"># of watch_variable need to change as well.</span>
        <span class="n">tape</span><span class="o">.</span><span class="n">watch_variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tape</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tape</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientTape.stop_recording"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape.stop_recording">[docs]</a>  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">stop_recording</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Temporarily stops recording operations on this tape.</span>

<span class="sd">    Operations executed while this context manager is active will not be</span>
<span class="sd">    recorded on the tape. This is useful for reducing the memory used by tracing</span>
<span class="sd">    all computations.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```</span>
<span class="sd">      with tf.GradientTape(persistent=True) as t:</span>
<span class="sd">        loss = compute_loss(model)</span>
<span class="sd">        with t.stop_recording():</span>
<span class="sd">          # The gradient computation below is not traced, saving memory.</span>
<span class="sd">          grads = t.gradient(loss, model.variables)</span>
<span class="sd">    ```</span>

<span class="sd">    Yields:</span>
<span class="sd">      None</span>
<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: if the tape is not currently recording.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Trying to stop recording a tape which is not recording.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_push_tape</span><span class="p">()</span></div>

<div class="viewcode-block" id="GradientTape.reset"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape.reset">[docs]</a>  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clears all information stored in this tape.</span>

<span class="sd">    Equivalent to exiting and reentering the tape context manager with a new</span>
<span class="sd">    tape. For example, the two following code blocks are equivalent:</span>

<span class="sd">    ```</span>
<span class="sd">    with tf.GradientTape() as t:</span>
<span class="sd">      loss = loss_fn()</span>
<span class="sd">    with tf.GradientTape() as t:</span>
<span class="sd">      loss += other_loss_fn()</span>
<span class="sd">    t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn</span>


<span class="sd">    # The following is equivalent to the above</span>
<span class="sd">    with tf.GradientTape() as t:</span>
<span class="sd">      loss = loss_fn()</span>
<span class="sd">      t.reset()</span>
<span class="sd">      loss += other_loss_fn()</span>
<span class="sd">    t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn</span>
<span class="sd">    ```</span>

<span class="sd">    This is useful if you don&#39;t want to exit the context manager for the tape,</span>
<span class="sd">    or can&#39;t because the desired reset point is inside a control flow construct:</span>

<span class="sd">    ```</span>
<span class="sd">    with tf.GradientTape() as t:</span>
<span class="sd">      loss = ...</span>
<span class="sd">      if loss &gt; k:</span>
<span class="sd">        t.reset()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_push_tape</span><span class="p">()</span></div>

<div class="viewcode-block" id="GradientTape.watched_variables"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape.watched_variables">[docs]</a>  <span class="k">def</span> <span class="nf">watched_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns variables watched by this tape in order of construction.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_watched_variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span><span class="o">.</span><span class="n">watched_variables</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_watched_variables</span></div>

<div class="viewcode-block" id="GradientTape.gradient"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape.gradient">[docs]</a>  <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">target</span><span class="p">,</span>
               <span class="n">sources</span><span class="p">,</span>
               <span class="n">output_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the gradient using operations recorded in context of this tape.</span>

<span class="sd">    Args:</span>
<span class="sd">      target: a list or nested structure of Tensors or Variables to be</span>
<span class="sd">        differentiated.</span>
<span class="sd">      sources: a list or nested structure of Tensors or Variables. `target`</span>
<span class="sd">        will be differentiated against elements in `sources`.</span>
<span class="sd">      output_gradients: a list of gradients, one for each element of</span>
<span class="sd">        target. Defaults to None.</span>
<span class="sd">      unconnected_gradients: a value which can either hold &#39;none&#39; or &#39;zero&#39; and</span>
<span class="sd">        alters the value which will be returned if the target and sources are</span>
<span class="sd">        unconnected. The possible values and effects are detailed in</span>
<span class="sd">        &#39;UnconnectedGradients&#39; and it defaults to &#39;none&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">      a list or nested structure of Tensors (or IndexedSlices, or None),</span>
<span class="sd">      one for each element in `sources`. Returned structure is the same as</span>
<span class="sd">      the structure of `sources`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: if called inside the context of the tape, or if called more</span>
<span class="sd">       than once on a non-persistent tape.</span>
<span class="sd">      ValueError: if the target is a variable or if unconnected gradients is</span>
<span class="sd">       called with an unknown value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;GradientTape.gradient can only be called once on &quot;</span>
                         <span class="s2">&quot;non-persistent tapes.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recording</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span> <span class="s2">&quot;Calling GradientTape.gradient on a persistent &quot;</span>
            <span class="s2">&quot;tape inside its context is significantly less &quot;</span>
            <span class="s2">&quot;efficient than calling it outside the context (it &quot;</span>
            <span class="s2">&quot;causes the gradient ops to be recorded on the &quot;</span>
            <span class="s2">&quot;tape, leading to increased CPU and memory usage). &quot;</span>
            <span class="s2">&quot;Only call GradientTape.gradient inside the &quot;</span>
            <span class="s2">&quot;context if you actually want to trace the &quot;</span>
            <span class="s2">&quot;gradient in order to compute higher order &quot;</span>
            <span class="s2">&quot;derivatives.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">flat_targets</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">backprop_util</span><span class="o">.</span><span class="n">IsTrainable</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span> <span class="s2">&quot;The dtype of the target tensor must be &quot;</span>
            <span class="s2">&quot;floating (e.g. tf.float32) when calling GradientTape.gradient, &quot;</span>
            <span class="s2">&quot;got </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">is_resource_variable</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="p">:</span>
          <span class="n">t</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
      <span class="n">flat_targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="n">flat_sources</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">sources</span><span class="p">)</span>
    <span class="n">flat_sources_raw</span> <span class="o">=</span> <span class="n">flat_sources</span>
    <span class="n">flat_sources</span> <span class="o">=</span> <span class="p">[</span><span class="n">_handle_or_self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_sources</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">flat_sources_raw</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">backprop_util</span><span class="o">.</span><span class="n">IsTrainable</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span> <span class="s2">&quot;The dtype of the source tensor must be &quot;</span>
            <span class="s2">&quot;floating (e.g. tf.float32) when calling GradientTape.gradient, &quot;</span>
            <span class="s2">&quot;got </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_gradients</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">output_gradients</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                          <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">output_gradients</span><span class="p">)]</span>

    <span class="n">flat_grad</span> <span class="o">=</span> <span class="n">imperative_grad</span><span class="o">.</span><span class="n">imperative_grad</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span><span class="p">,</span>
        <span class="n">flat_targets</span><span class="p">,</span>
        <span class="n">flat_sources</span><span class="p">,</span>
        <span class="n">output_gradients</span><span class="o">=</span><span class="n">output_gradients</span><span class="p">,</span>
        <span class="n">sources_raw</span><span class="o">=</span><span class="n">flat_sources_raw</span><span class="p">,</span>
        <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">unconnected_gradients</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent</span><span class="p">:</span>
      <span class="c1"># Keep track of watched variables before setting tape to None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_watched_variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span><span class="o">.</span><span class="n">watched_variables</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tape</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">grad</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">flat_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad</span></div>

<div class="viewcode-block" id="GradientTape.jacobian"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape.jacobian">[docs]</a>  <span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">target</span><span class="p">,</span>
               <span class="n">sources</span><span class="p">,</span>
               <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
               <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">experimental_use_pfor</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the jacobian using operations recorded in context of this tape.</span>

<span class="sd">    See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the</span>
<span class="sd">    definition of a Jacobian.</span>

<span class="sd">    Example usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    with tf.GradientTape() as g:</span>
<span class="sd">      x  = tf.constant([1.0, 2.0])</span>
<span class="sd">      g.watch(x)</span>
<span class="sd">      y = x * x</span>
<span class="sd">    jacobian = g.jacobian(y, x)</span>
<span class="sd">    # jacobian value is [[2., 0.], [0., 4.]]</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      target: Tensor to be differentiated.</span>
<span class="sd">      sources: a list or nested structure of Tensors or Variables. `target`</span>
<span class="sd">        will be differentiated against elements in `sources`.</span>
<span class="sd">      unconnected_gradients: a value which can either hold &#39;none&#39; or &#39;zero&#39; and</span>
<span class="sd">        alters the value which will be returned if the target and sources are</span>
<span class="sd">        unconnected. The possible values and effects are detailed in</span>
<span class="sd">        &#39;UnconnectedGradients&#39; and it defaults to &#39;none&#39;.</span>
<span class="sd">      parallel_iterations: A knob to control how many iterations are dispatched</span>
<span class="sd">        in parallel. This knob can be used to control the total memory usage.</span>
<span class="sd">      experimental_use_pfor: If true, vectorizes the jacobian computation. Else</span>
<span class="sd">        falls back to a sequential while_loop. Vectorization can sometimes fail</span>
<span class="sd">        or lead to excessive memory usage. This option can be used to disable</span>
<span class="sd">        vectorization in such cases.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list or nested structure of Tensors (or None), one for each element in</span>
<span class="sd">      `sources`. Returned structure is the same as the structure of `sources`.</span>
<span class="sd">      Note if any gradient is sparse (IndexedSlices), jacobian function</span>
<span class="sd">      currently makes it dense and returns a Tensor instead. This may change in</span>
<span class="sd">      the future.</span>


<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called on a non-persistent tape with eager execution</span>
<span class="sd">        enabled and without enabling experimental_use_pfor.</span>
<span class="sd">      ValueError: If vectorization of jacobian computation fails.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">flat_sources</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">sources</span><span class="p">)</span>
    <span class="n">target_static_shape</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">target_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="c1"># Note that we push and pop the tape here and below. This is needed since we</span>
    <span class="c1"># need gradients through the enclosed operations.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_push_tape</span><span class="p">()</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">loop_fn</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_push_tape</span><span class="p">()</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">flat_sources</span><span class="p">,</span>
                           <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">unconnected_gradients</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">target_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
      <span class="n">target_size</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">experimental_use_pfor</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">pfor_ops</span><span class="o">.</span><span class="n">pfor</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span>
                               <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="n">six</span><span class="o">.</span><span class="n">reraise</span><span class="p">(</span>
            <span class="ne">ValueError</span><span class="p">,</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Encountered an exception while vectorizing the &quot;</span>
                <span class="s2">&quot;jacobian computation. Vectorization can be disabled by setting&quot;</span>
                <span class="s2">&quot; experimental_use_pfor to False.&quot;</span><span class="p">),</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">()[</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;GradientTape must be created with persistent=True&quot;</span>
            <span class="s2">&quot; to compute the jacobian with eager execution enabled and with &quot;</span>
            <span class="s2">&quot; experimental_use_pfor set to False.&quot;</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">pfor_ops</span><span class="o">.</span><span class="n">for_loop</span><span class="p">(</span>
          <span class="n">loop_fn</span><span class="p">,</span> <span class="p">[</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_sources</span><span class="p">),</span> <span class="n">target_size</span><span class="p">,</span>
          <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">target_shape</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">out</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="n">out</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">target_static_shape</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">flat_sources</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
      <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>

    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientTape.batch_jacobian"><a class="viewcode-back" href="../../../../index.html#tensorflow.GradientTape.batch_jacobian">[docs]</a>  <span class="k">def</span> <span class="nf">batch_jacobian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">target</span><span class="p">,</span>
                     <span class="n">source</span><span class="p">,</span>
                     <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
                     <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">experimental_use_pfor</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes and stacks per-example jacobians.</span>

<span class="sd">    See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the</span>
<span class="sd">    definition of a Jacobian. This function is essentially an efficient</span>
<span class="sd">    implementation of the following:</span>

<span class="sd">    `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.</span>

<span class="sd">    Note that compared to `GradientTape.jacobian` which computes gradient of</span>
<span class="sd">    each output value w.r.t each input value, this function is useful when</span>
<span class="sd">    `target[i,...]` is independent of `source[j,...]` for `j != i`. This</span>
<span class="sd">    assumption allows more efficient computation as compared to</span>
<span class="sd">    `GradientTape.jacobian`. The output, as well as intermediate activations,</span>
<span class="sd">    are lower dimensional and avoid a bunch of redundant zeros which would</span>
<span class="sd">    result in the jacobian computation given the independence assumption.</span>

<span class="sd">    Example usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    with tf.GradientTape() as g:</span>
<span class="sd">      x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)</span>
<span class="sd">      g.watch(x)</span>
<span class="sd">      y = x * x</span>
<span class="sd">    batch_jacobian = g.batch_jacobian(y, x)</span>
<span class="sd">    # batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      target: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].</span>
<span class="sd">        `target[i,...]` should only depend on `source[i,...]`.</span>
<span class="sd">      source: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].</span>
<span class="sd">      unconnected_gradients: a value which can either hold &#39;none&#39; or &#39;zero&#39; and</span>
<span class="sd">        alters the value which will be returned if the target and sources are</span>
<span class="sd">        unconnected. The possible values and effects are detailed in</span>
<span class="sd">        &#39;UnconnectedGradients&#39; and it defaults to &#39;none&#39;.</span>
<span class="sd">      parallel_iterations: A knob to control how many iterations are dispatched</span>
<span class="sd">        in parallel. This knob can be used to control the total memory usage.</span>
<span class="sd">      experimental_use_pfor: If true, uses pfor for computing the Jacobian. Else</span>
<span class="sd">        uses a tf.while_loop.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor `t` with shape [b, y_1, ..., y_n, x1, ..., x_m] where `t[i, ...]`</span>
<span class="sd">      is the jacobian of `target[i, ...]` w.r.t. `source[i, ...]`, i.e. stacked</span>
<span class="sd">      per-example jacobians.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called on a non-persistent tape with eager execution</span>
<span class="sd">        enabled and without enabling experimental_use_pfor.</span>
<span class="sd">      ValueError: If vectorization of jacobian computation fails or if first</span>
<span class="sd">        dimension of `target` and `source` do not match.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">target_shape</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">target_shape</span><span class="o">.</span><span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">dim</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">Dimension</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">dim</span> <span class="o">=</span> <span class="n">target_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target_shape</span><span class="o">.</span><span class="n">with_rank_at_least</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="ow">and</span>
            <span class="n">source</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">with_rank_at_least</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="ow">and</span>
            <span class="n">dim</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">source</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Need first dimension of target shape (</span><span class="si">%s</span><span class="s2">) and &quot;</span>
          <span class="s2">&quot;source shape (</span><span class="si">%s</span><span class="s2">) to match.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">source</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">target_shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="n">target_row_size</span> <span class="o">=</span> <span class="n">target_shape</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">target_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target_row_size</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">source_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="c1"># Flatten target to 2-D.</span>
    <span class="c1"># Note that we push and pop the tape here and below. This is needed since we</span>
    <span class="c1"># need gradients through the enclosed operations.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_push_tape</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span>
        <span class="p">[</span><span class="n">check_ops</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">source_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]):</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">target_row_size</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">loop_fn</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_push_tape</span><span class="p">()</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_pop_tape</span><span class="p">()</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span>
                           <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">unconnected_gradients</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">experimental_use_pfor</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">pfor_ops</span><span class="o">.</span><span class="n">pfor</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">target_row_size</span><span class="p">,</span>
                               <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="n">six</span><span class="o">.</span><span class="n">reraise</span><span class="p">(</span>
            <span class="ne">ValueError</span><span class="p">,</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Encountered an exception while vectorizing the &quot;</span>
                <span class="s2">&quot;batch_jacobian computation. Vectorization can be disabled by &quot;</span>
                <span class="s2">&quot;setting experimental_use_pfor to False.&quot;</span><span class="p">),</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">()[</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;GradientTape must be created with persistent=True&quot;</span>
            <span class="s2">&quot; to compute the batch_jacobian with eager execution enabled and &quot;</span>
            <span class="s2">&quot; with experimental_use_pfor set to False.&quot;</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">pfor_ops</span><span class="o">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">loop_fn</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">target_row_size</span><span class="p">,</span>
                                 <span class="n">parallel_iterations</span><span class="o">=</span><span class="n">parallel_iterations</span><span class="p">)</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">target_shape</span><span class="p">,</span> <span class="n">source_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span>
                                 <span class="p">[</span><span class="n">target_row_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>