

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.eager.context &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.eager.context</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.eager.context</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;State management for eager execution.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">threading</span>

<span class="kn">from</span> <span class="nn">absl</span> <span class="k">import</span> <span class="n">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">function_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="k">import</span> <span class="n">config_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="k">import</span> <span class="n">rewriter_config_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tfe</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">tf2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="k">import</span> <span class="n">pywrap_tf_session</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">executor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">monitoring</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">c_api_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">device</span> <span class="k">as</span> <span class="n">pydev</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">is_in_graph_mode</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_contextlib</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="n">GRAPH_MODE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">EAGER_MODE</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">default_execution_mode</span> <span class="o">=</span> <span class="n">EAGER_MODE</span> <span class="k">if</span> <span class="n">tf2</span><span class="o">.</span><span class="n">enabled</span><span class="p">()</span> <span class="k">else</span> <span class="n">GRAPH_MODE</span>

<span class="c1"># Cache from (old_device_name, partial_new_device_name) -&gt; (new_device_name,</span>
<span class="c1"># new_device_spec).</span>
<span class="c1"># Note that we do not protect this with a lock and instead rely on python&#39;s GIL</span>
<span class="c1"># and the idempotent nature of writes to provide thread safety.</span>
<span class="n">_device_parsing_cache</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_starting_device_spec</span> <span class="o">=</span> <span class="n">pydev</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="o">.</span><span class="n">from_string</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

<span class="n">_MAXINT32</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span>

<span class="n">DEVICE_PLACEMENT_EXPLICIT</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_DEVICE_PLACEMENT_EXPLICIT</span>
<span class="n">DEVICE_PLACEMENT_WARN</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_DEVICE_PLACEMENT_WARN</span>
<span class="n">DEVICE_PLACEMENT_SILENT</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_DEVICE_PLACEMENT_SILENT</span>
<span class="n">DEVICE_PLACEMENT_SILENT_FOR_INT32</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32</span><span class="p">)</span>

<span class="n">SYNC</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">ASYNC</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">MIRRORING_NONE</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_MIRRORING_NONE</span>
<span class="n">MIRRORING_ALL</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_MIRRORING_ALL</span>

<span class="n">_KEEP_ALIVE_SECS</span> <span class="o">=</span> <span class="mi">600</span>

<span class="n">_python_eager_context_create_counter</span> <span class="o">=</span> <span class="n">monitoring</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span>
    <span class="s2">&quot;/tensorflow/api/python/eager_context_create_counter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Counter for number of eager contexts created in Python.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_EagerTensorCache</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Simple cache which evicts items based on length in a FIFO manner.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_items</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">max_tensor_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span> <span class="o">=</span> <span class="n">max_items</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_tensor_size</span> <span class="o">=</span> <span class="n">max_tensor_size</span>

  <span class="k">def</span> <span class="nf">put</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">_num_elements</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_tensor_size</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">popitem</span><span class="p">(</span><span class="n">last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">flush</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">class</span> <span class="nc">FunctionCallOptions</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Options applied at call sites of eager functions.</span>

<span class="sd">  Eager functions are functions decorated with tf.contrib.eager.defun.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">executor_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">config_proto</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      executor_type: (optional) name of the executor to be used to execute the</span>
<span class="sd">        eager function. If None or an empty string, the default Tensorflow</span>
<span class="sd">        executor will be used.</span>
<span class="sd">      config_proto: (optional) a `config_pb2.ConfigProto` proto or</span>
<span class="sd">        a serialized string of that proto.</span>
<span class="sd">        The config used by Grappler when optimizing the function graph.</span>
<span class="sd">        Each concrete function is optimized the first time is called. Changing</span>
<span class="sd">        config_proto after the first call has no effect.</span>
<span class="sd">        If config_proto is None, an empty RewriterConfig will be used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config_proto_serialized</span> <span class="o">=</span> <span class="n">config_proto</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">executor_type</span> <span class="o">=</span> <span class="n">executor_type</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">executor_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_executor_type</span>

  <span class="nd">@executor_type</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">executor_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">executor_type</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_executor_type</span> <span class="o">=</span> <span class="n">executor_type</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">config_proto_serialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config_proto_serialized</span>

  <span class="nd">@config_proto_serialized</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">config_proto_serialized</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_config_proto_serialized</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_config_proto_serialized</span> <span class="o">=</span> <span class="n">config</span>
    <span class="k">elif</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_config_proto_serialized</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">config_pb2</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">()</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the rewriter config must be either a &quot;</span>
                       <span class="s2">&quot;config_pb2.ConfigProto, or a serialized string of that &quot;</span>
                       <span class="s2">&quot;proto or None. got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)))</span>


<span class="c1"># Map from context_id (an int) to _TensorCaches.</span>
<span class="c1"># Dicts are thread safe in CPython.</span>
<span class="c1"># TODO(iga): Remove this once TensorCaches are moved to C++.</span>
<span class="n">_tensor_caches_map</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">class</span> <span class="nc">_TensorCaches</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Thread local tensor caches.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_TensorCaches</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ones_rank_cache</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_zeros_cache</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">ones_rank_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ones_rank_cache</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ones_rank_cache</span> <span class="o">=</span> <span class="n">_EagerTensorCache</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ones_rank_cache</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">zeros_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zeros_cache</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_zeros_cache</span> <span class="o">=</span> <span class="n">_EagerTensorCache</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zeros_cache</span>


<span class="k">class</span> <span class="nc">_ThreadLocalData</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Thread local storage for the eager context.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_ThreadLocalData</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device_spec</span> <span class="o">=</span> <span class="n">_starting_device_spec</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">is_eager</span> <span class="o">=</span> <span class="n">default_execution_mode</span> <span class="o">==</span> <span class="n">EAGER_MODE</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scope_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">executor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">op_callbacks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">invoking_op_callbacks</span> <span class="o">=</span> <span class="kc">False</span>


<span class="n">ContextSwitch</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;ContextSwitch&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;is_building_function&quot;</span><span class="p">,</span> <span class="s2">&quot;enter_context_fn&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;device_stack&quot;</span><span class="p">])</span>


<span class="c1"># `_ContextSwitchStack` is a `threading.local` to match the semantics of</span>
<span class="c1"># ``DefaultGraphStack`, which is also a `threading.local`.</span>
<span class="k">class</span> <span class="nc">_ContextSwitchStack</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A thread-local stack of context switches.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eager</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_ContextSwitchStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">eager</span><span class="p">:</span>
      <span class="c1"># Initialize the stack with a pointer to enter the eager context; this</span>
      <span class="c1"># ensures that the fact that eager execution was enabled is propagated</span>
      <span class="c1"># across threads, since (1) `enable_eager_execution` modifies a</span>
      <span class="c1"># process-level flag (`default_execution_mode`) and (2) `__init__` is</span>
      <span class="c1"># called each time a threading.local object is used in a separate thread.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">is_building_function</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">enter_context_fn</span><span class="o">=</span><span class="n">eager_mode</span><span class="p">,</span>
                <span class="n">device_stack</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_building_function</span><span class="p">,</span> <span class="n">enter_context_fn</span><span class="p">,</span> <span class="n">device_stack</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Push metadata about a context switch onto the stack.</span>

<span class="sd">    A context switch can take any one of the two forms: installing a graph as</span>
<span class="sd">    the default graph, or entering the eager context. For each context switch,</span>
<span class="sd">    we record whether or not the entered context is building a function.</span>

<span class="sd">    Args:</span>
<span class="sd">      is_building_function: (bool.) Whether the context is building a function.</span>
<span class="sd">      enter_context_fn: (function.) A callable that executes the context switch.</span>
<span class="sd">        For example, `graph.as_default` or `eager_mode`.</span>
<span class="sd">      device_stack: If applicable, the device function stack for this</span>
<span class="sd">        graph. When breaking out of graphs in init_scope, the innermost nonempty</span>
<span class="sd">        device stack is used. Eager contexts put `None` here and the value is</span>
<span class="sd">        never used.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ContextSwitch</span><span class="p">(</span><span class="n">is_building_function</span><span class="p">,</span> <span class="n">enter_context_fn</span><span class="p">,</span> <span class="n">device_stack</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pop the stack.&quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;config.LogicalDevice&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LogicalDevice</span><span class="p">(</span>
    <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;LogicalDevice&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;device_type&quot;</span><span class="p">])):</span>
  <span class="sd">&quot;&quot;&quot;Abstraction for a logical device initialized by the runtime.</span>

<span class="sd">  A `tf.config.LogicalDevice` corresponds to an initialized logical device on a</span>
<span class="sd">  `tf.config.PhysicalDevice` or a remote device visible to the cluster. Tensors</span>
<span class="sd">  and operations can be placed on a specific logical device by calling</span>
<span class="sd">  `tf.device` with a specified `tf.config.LogicalDevice`.</span>

<span class="sd">  Fields:</span>
<span class="sd">    name: The fully qualified name of the device. Can be used for Op or function</span>
<span class="sd">      placement.</span>
<span class="sd">    device_type: String declaring the type of device such as &quot;CPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">pass</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;config.LogicalDeviceConfiguration&quot;</span><span class="p">,</span>
           <span class="s2">&quot;config.experimental.VirtualDeviceConfiguration&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LogicalDeviceConfiguration</span><span class="p">(</span>
    <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;LogicalDeviceConfiguration&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;memory_limit&quot;</span><span class="p">])):</span>
  <span class="sd">&quot;&quot;&quot;Configuration class for a logical devices.</span>

<span class="sd">  The class specifies the parameters to configure a `tf.config.PhysicalDevice`</span>
<span class="sd">  as it is initialized to a `tf.config.LogicalDevice` during runtime</span>
<span class="sd">  initialization. Not all fields are valid for all device types.</span>

<span class="sd">  See `tf.config.get_logical_device_configuration` and</span>
<span class="sd">  `tf.config.set_logical_device_configuration` for usage examples.</span>

<span class="sd">  Fields:</span>
<span class="sd">    memory_limit: (optional) Maximum memory (in MB) to allocate on the virtual</span>
<span class="sd">      device. Currently only supported for GPUs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">memory_limit</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">LogicalDeviceConfiguration</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">memory_limit</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;config.PhysicalDevice&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">PhysicalDevice</span><span class="p">(</span>
    <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;PhysicalDevice&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;device_type&quot;</span><span class="p">])):</span>
  <span class="sd">&quot;&quot;&quot;Abstraction for a locally visible physical device.</span>

<span class="sd">  TensorFlow can utilize various devices such as the CPU or multiple GPUs</span>
<span class="sd">  for computation. Before initializing a local device for use, the user can</span>
<span class="sd">  customize certain properties of the device such as it&#39;s visibility or memory</span>
<span class="sd">  configuration.</span>

<span class="sd">  Once a visible `tf.config.PhysicalDevice` is initialized one or more</span>
<span class="sd">  `tf.config.LogicalDevice` objects are created. Use</span>
<span class="sd">  `tf.config.set_visible_devices` to configure the visibility of a physical</span>
<span class="sd">  device and `tf.config.set_logical_device_configuration` to configure multiple</span>
<span class="sd">  `tf.config.LogicalDevice` objects for a `tf.config.PhysicalDevice`. This is</span>
<span class="sd">  useful when separation between models is needed or to simulate a multi-device</span>
<span class="sd">  environment.</span>

<span class="sd">  Fields:</span>
<span class="sd">    name: Unique identifier for device.</span>
<span class="sd">    device_type: String declaring the type of device such as &quot;CPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">pass</span>


<span class="k">class</span> <span class="nc">_AtomicCounter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A simple atomic counter.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">increment_and_get</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_value</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value</span>


<span class="n">_context_id_counter</span> <span class="o">=</span> <span class="n">_AtomicCounter</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_TensorCacheDeleter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Deletes tensor caches for a given context.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_id</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context_id</span> <span class="o">=</span> <span class="n">context_id</span>

  <span class="k">def</span> <span class="nf">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">_tensor_caches_map</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_id</span> <span class="ow">in</span> <span class="n">_tensor_caches_map</span><span class="p">:</span>
      <span class="k">del</span> <span class="n">_tensor_caches_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_id</span><span class="p">]</span>


<span class="c1"># TODO(agarwal): rename to EagerContext / EagerRuntime ?</span>
<span class="c1"># TODO(agarwal): consider keeping the corresponding Graph here.</span>
<span class="k">class</span> <span class="nc">Context</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Environment in which eager operations execute.&quot;&quot;&quot;</span>

  <span class="c1"># TODO(agarwal): create and link in some documentation for `execution_mode`.</span>
  <span class="c1"># pylint: disable=redefined-outer-name</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">device_policy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">execution_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">server_def</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new Context.</span>

<span class="sd">    Args:</span>
<span class="sd">      config: (Optional.) A `ConfigProto` protocol buffer with configuration</span>
<span class="sd">        options for the Context. Note that a lot of these options may be</span>
<span class="sd">        currently unimplemented or irrelevant when eager execution is enabled.</span>
<span class="sd">      device_policy: (Optional.) What policy to use when trying to run an</span>
<span class="sd">        operation on a device with inputs which are not on that device.</span>
<span class="sd">        When set to None, an appropriate value will be picked automatically.</span>
<span class="sd">        The value picked may change between TensorFlow releases.</span>

<span class="sd">        Defaults to DEVICE_PLACEMENT_SILENT.</span>
<span class="sd">        Valid values:</span>
<span class="sd">        - DEVICE_PLACEMENT_EXPLICIT: raises an error if the placement is</span>
<span class="sd">          not correct.</span>
<span class="sd">        - DEVICE_PLACEMENT_WARN: copies the tensors which are not on the</span>
<span class="sd">          right device but raises a warning.</span>
<span class="sd">        - DEVICE_PLACEMENT_SILENT: silently copies the tensors. This might</span>
<span class="sd">          hide performance problems.</span>
<span class="sd">        - DEVICE_PLACEMENT_SILENT_FOR_INT32: silently copies int32 tensors,</span>
<span class="sd">          raising errors on the other ones.</span>
<span class="sd">      execution_mode: (Optional.) Policy controlling how operations dispatched</span>
<span class="sd">        are actually executed. When set to None, an appropriate value will be</span>
<span class="sd">        picked automatically. The value picked may change between TensorFlow</span>
<span class="sd">        releases.</span>
<span class="sd">        Valid values:</span>
<span class="sd">        - SYNC: executes each operation synchronously.</span>
<span class="sd">        - ASYNC: executes each operation asynchronously. These</span>
<span class="sd">          operations may return &quot;non-ready&quot; handles.</span>
<span class="sd">      server_def: (Optional.) A tensorflow::ServerDef proto.</span>
<span class="sd">        Enables execution on remote devices. GrpcServers need to be started by</span>
<span class="sd">        creating an identical server_def to this, and setting the appropriate</span>
<span class="sd">        task_indexes, so that the servers can communicate. It will then be</span>
<span class="sd">        possible to execute operations on remote devices.</span>

<span class="sd">    Raises:</span>
<span class="sd">     ValueError: If execution_mode is not valid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This _id is used only to index the tensor caches.</span>
    <span class="c1"># TODO(iga): Remove this when tensor caches are moved to C++.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_id</span> <span class="o">=</span> <span class="n">_context_id_counter</span><span class="o">.</span><span class="n">increment_and_get</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_cache_deleter</span> <span class="o">=</span> <span class="n">_TensorCacheDeleter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_id</span><span class="p">)</span>
    <span class="n">_tensor_caches_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">_TensorCaches</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_config</span> <span class="o">=</span> <span class="n">config</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span> <span class="o">=</span> <span class="n">_ThreadLocalData</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span> <span class="o">=</span> <span class="n">_ContextSwitchStack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context_devices</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">device_policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">device_policy</span> <span class="o">=</span> <span class="n">DEVICE_PLACEMENT_SILENT</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_policy</span> <span class="o">=</span> <span class="n">device_policy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">execution_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">SYNC</span><span class="p">,</span> <span class="n">ASYNC</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;execution_mode should be None/SYNC/ASYNC. Got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">execution_mode</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">execution_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">execution_mode</span> <span class="o">=</span> <span class="n">SYNC</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_is_async</span> <span class="o">=</span> <span class="n">execution_mode</span> <span class="o">==</span> <span class="n">ASYNC</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_remote_inputs_copy</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_server_def</span> <span class="o">=</span> <span class="n">server_def</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_ops_server_def</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_leader</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_scoped_allocator_enabled_ops</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_use_nccl_communication</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_device_filters</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_device_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_memory_growth_map</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Values set after construction</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_jit</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_intra_op_parallelism_threads</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inter_op_parallelism_threads</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_soft_device_placement</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_log_device_placement</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_enable_mlir_bridge</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_experimental_options</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">_python_eager_context_create_counter</span><span class="o">.</span><span class="n">get_cell</span><span class="p">()</span><span class="o">.</span><span class="n">increase_by</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="c1"># pylint: enable=redefined-outer-name</span>

  <span class="k">def</span> <span class="nf">_set_global_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set a global eager mode seed for random ops.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="n">seed</span>
    <span class="c1"># `random.Random(seed)` needs `seed` to be hashable, while values of type</span>
    <span class="c1"># e.g. `np.int64` or `np.ndarray` are not. We use `int(...)` to convert them</span>
    <span class="c1"># to int.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="nb">hash</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
      <span class="n">seed</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">seed</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">Random</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1"># Also clear the kernel cache, to reset any existing seeds</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextClearCaches</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_internal_operation_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a fake operation seed.</span>

<span class="sd">      In eager mode, user shouldn&#39;t set or depend on operation seed.</span>
<span class="sd">      Here, we generate a random seed based on global seed to make</span>
<span class="sd">      operation&#39;s randomness different and depend on the global seed.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A fake operation seed based on global seed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">_MAXINT32</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_initialize_logical_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Helper to initialize devices.&quot;&quot;&quot;</span>
    <span class="c1"># Store list of devices</span>
    <span class="n">logical_devices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">context_devices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">device_list</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextListDevices</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_num_gpus</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_DeviceListCount</span><span class="p">(</span><span class="n">device_list</span><span class="p">)):</span>
        <span class="n">dev_name</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_DeviceListName</span><span class="p">(</span><span class="n">device_list</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">context_devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pydev</span><span class="o">.</span><span class="n">canonical_name</span><span class="p">(</span><span class="n">dev_name</span><span class="p">))</span>
        <span class="n">spec</span> <span class="o">=</span> <span class="n">pydev</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="o">.</span><span class="n">from_string</span><span class="p">(</span><span class="n">dev_name</span><span class="p">)</span>
        <span class="c1"># If the job is localhost, we assume that the cluster has not yet been</span>
        <span class="c1"># configured and thus clear the job, replica &amp; task.</span>
        <span class="k">if</span> <span class="n">spec</span><span class="o">.</span><span class="n">job</span> <span class="o">==</span> <span class="s2">&quot;localhost&quot;</span><span class="p">:</span>
          <span class="n">spec</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">job</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">replica</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">logical_devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">LogicalDevice</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">spec</span><span class="o">.</span><span class="n">to_string</span><span class="p">(),</span> <span class="n">device_type</span><span class="o">=</span><span class="n">spec</span><span class="o">.</span><span class="n">device_type</span><span class="p">))</span>
        <span class="n">dev_type</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_DeviceListType</span><span class="p">(</span><span class="n">device_list</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dev_type</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_num_gpus</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_logical_devices</span> <span class="o">=</span> <span class="n">logical_devices</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_context_devices</span> <span class="o">=</span> <span class="n">context_devices</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_DeleteDeviceList</span><span class="p">(</span><span class="n">device_list</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">ensure_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize handle and devices if not already done so.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
        <span class="k">return</span>
      <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_devices</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="n">opts</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_NewContextOptions</span><span class="p">()</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">config_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextOptionsSetConfig</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">config_str</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextOptionsSetDevicePlacementPolicy</span><span class="p">(</span>
              <span class="n">opts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_policy</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextOptionsSetMirroringPolicy</span><span class="p">(</span>
              <span class="n">opts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_is_async</span> <span class="o">==</span> <span class="n">ASYNC</span><span class="p">:</span>
          <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextOptionsSetAsync</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_remote_inputs_copy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextOptionsSetLazyRemoteInputsCopy</span><span class="p">(</span>
              <span class="n">opts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_remote_inputs_copy</span><span class="p">)</span>
        <span class="n">context_handle</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_NewContext</span><span class="p">(</span><span class="n">opts</span><span class="p">)</span>
      <span class="k">finally</span><span class="p">:</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_DeleteContextOptions</span><span class="p">(</span><span class="n">opts</span><span class="p">)</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_server_def</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_ops_server_def</span><span class="p">),</span> <span class="p">(</span>
          <span class="s2">&quot;Cannot enable remote execution as well as collective ops at the &quot;</span>
          <span class="s2">&quot;moment. If this is important to you, please file an issue.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server_def</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">server_def_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextSetServerDef</span><span class="p">(</span><span class="n">context_handle</span><span class="p">,</span> <span class="n">_KEEP_ALIVE_SECS</span><span class="p">,</span>
                                           <span class="n">server_def_str</span><span class="p">)</span>
      <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_ops_server_def</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">server_def_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_ops_server_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_EnableCollectiveOps</span><span class="p">(</span><span class="n">context_handle</span><span class="p">,</span> <span class="n">server_def_str</span><span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="o">=</span> <span class="n">context_handle</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_logical_devices</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">_clear_caches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ones_rank_cache</span><span class="p">()</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zeros_cache</span><span class="p">()</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ClearScalarCache</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">get_server_def</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server_def</span>

  <span class="k">def</span> <span class="nf">set_server_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">server_def</span><span class="p">,</span> <span class="n">keep_alive_secs</span><span class="o">=</span><span class="n">_KEEP_ALIVE_SECS</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Allow setting a server_def on the context.</span>

<span class="sd">    When a server def is replaced, it effectively clears a bunch of caches</span>
<span class="sd">    within the context. If you attempt to use a tensor object that was pointing</span>
<span class="sd">    to a tensor on the remote device, it will raise an error.</span>

<span class="sd">    Args:</span>
<span class="sd">      server_def: A tensorflow::ServerDef proto.</span>
<span class="sd">        Enables execution on remote devices.</span>
<span class="sd">      keep_alive_secs: Num. seconds after which the remote end will hang up.</span>
<span class="sd">        As long as the client is still alive, the server state for the context</span>
<span class="sd">        will be kept alive. If the client is killed (or there is some failure),</span>
<span class="sd">        the server will clean up its context keep_alive_secs after the final RPC</span>
<span class="sd">        it receives.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if server_def is None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">server_def</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;server_def is None.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_server_def</span> <span class="o">=</span> <span class="n">server_def</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="n">server_def_str</span> <span class="o">=</span> <span class="n">server_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextSetServerDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">keep_alive_secs</span><span class="p">,</span>
                                         <span class="n">server_def_str</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_logical_devices</span><span class="p">()</span>

    <span class="c1"># Clear all the caches in case there are remote tensors in them.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_clear_caches</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">update_server_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">server_def</span><span class="p">,</span> <span class="n">keep_alive_secs</span><span class="o">=</span><span class="n">_KEEP_ALIVE_SECS</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update a server_def on the context.</span>

<span class="sd">    Args:</span>
<span class="sd">      server_def: A tensorflow::ServerDef proto. Enables execution on remote</span>
<span class="sd">        devices.</span>
<span class="sd">      keep_alive_secs: Num. seconds after which the remote end will hang up. As</span>
<span class="sd">        long as the client is still alive, the server state for the context will</span>
<span class="sd">        be kept alive. If the client is killed (or there is some failure), the</span>
<span class="sd">        server will clean up its context keep_alive_secs after the final RPC it</span>
<span class="sd">        receives.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if server_def is None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">server_def</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;server_def is None.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_server_def</span> <span class="o">=</span> <span class="n">server_def</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="n">server_def_str</span> <span class="o">=</span> <span class="n">server_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
      <span class="c1"># Current executor might have pending nodes that involves updated remote</span>
      <span class="c1"># devices. Wait for them to finish before updating.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">executor</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">executor</span><span class="o">.</span><span class="n">clear_error</span><span class="p">()</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextUpdateServerDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span>
                                            <span class="n">keep_alive_secs</span><span class="p">,</span> <span class="n">server_def_str</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_logical_devices</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_clear_caches</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">check_alive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">worker_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks whether a remote worker is alive or not.</span>

<span class="sd">    Args:</span>
<span class="sd">      worker_name: a string representing the remote worker. It must be a fully</span>
<span class="sd">      specified name like &quot;/job:worker/replica:0/task:0&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">      a boolean indicating whether the remote worker is alive or not.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if context is not initialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO(yuefengz): support checking multiple workers.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextCheckAlive</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">worker_name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context is not initialized.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">sync_executors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sync both local executors and the ones on remote workers.</span>

<span class="sd">    In async execution mode, local function calls can return before the</span>
<span class="sd">    coresponding remote op/function execution requests are completed. Calling</span>
<span class="sd">    this method creates a synchronization barrier for remote executors. It only</span>
<span class="sd">    returns when all remote pending nodes are finished, potentially with errors</span>
<span class="sd">    if any remote executors are in error state.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if context is not initialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextSyncExecutors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context is not initialized.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">clear_executor_errors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clear errors in both local executors and remote workers.</span>

<span class="sd">    After receiving errors from remote workers, additional requests on the fly</span>
<span class="sd">    could further taint the status on the remote workers due to the async nature</span>
<span class="sd">    of remote execution. Calling this method block on waiting for all pending</span>
<span class="sd">    nodes in remote executors to finish and clear their error statuses.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if context is not initialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextClearExecutors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context is not initialized.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">enable_collective_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">server_def</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enable distributed collective ops with an appropriate server_def.</span>

<span class="sd">    Args:</span>
<span class="sd">      server_def: A tensorflow::ServerDef proto. Enables execution on remote</span>
<span class="sd">        devices.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if server_def is None.</span>
<span class="sd">      RuntimeError: if this method is not called at program startup.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">server_def</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;server_def is None.&quot;</span><span class="p">)</span>

    <span class="c1"># TODO(b/129298253): Allow creating datasets/tensors before enabling</span>
    <span class="c1"># collective ops.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Enabling collective ops after program startup may cause &quot;</span>
                      <span class="s2">&quot;error when accessing previously created tensors.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_ops_server_def</span> <span class="o">=</span> <span class="n">server_def</span>

  <span class="k">def</span> <span class="nf">configure_collective_ops</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">collective_leader</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
      <span class="n">scoped_allocator_enabled_ops</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;CollectiveReduce&quot;</span><span class="p">,),</span>
      <span class="n">use_nccl_communication</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">device_filters</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Configure collective ops.</span>

<span class="sd">      Collective group leader is necessary for collective ops to run, other</span>
<span class="sd">      configurations are mainly for the purpose of performance.</span>

<span class="sd">    Args:</span>
<span class="sd">      collective_leader: a device string for collective leader, e.g.</span>
<span class="sd">        &quot;/job:worker/replica:0/task:0&quot;; empty string means local execution of</span>
<span class="sd">          collective ops.</span>
<span class="sd">      scoped_allocator_enabled_ops: a tuple or a list of op names for scoped</span>
<span class="sd">        allocator to run with.</span>
<span class="sd">      use_nccl_communication: whether to use nccl communication for collective</span>
<span class="sd">        ops.</span>
<span class="sd">      device_filters: a tuple or a list of device strings. If set, corresponding</span>
<span class="sd">        task can only see the devices filtered by these device filters.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: if this method is not called at program startup.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_leader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_collective_leader</span> <span class="o">!=</span> <span class="n">collective_leader</span> <span class="ow">or</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_collective_scoped_allocator_enabled_ops</span> <span class="o">!=</span>
          <span class="n">scoped_allocator_enabled_ops</span> <span class="ow">or</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_collective_use_nccl_communication</span> <span class="o">!=</span> <span class="n">use_nccl_communication</span> <span class="ow">or</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_collective_device_filters</span> <span class="o">!=</span> <span class="n">device_filters</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Collective ops are already configured.&quot;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Collective ops must be configured at program startup&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_leader</span> <span class="o">=</span> <span class="n">collective_leader</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_scoped_allocator_enabled_ops</span> <span class="o">=</span> <span class="n">scoped_allocator_enabled_ops</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_use_nccl_communication</span> <span class="o">=</span> <span class="n">use_nccl_communication</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collective_device_filters</span> <span class="o">=</span> <span class="n">device_filters</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Context must be initialized first.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Context must be initialized first.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_devices</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;Eager TensorFlow Context. Devices currently uninitialized.&quot;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">devices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span>
      <span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Eager TensorFlow Context with </span><span class="si">%d</span><span class="s2"> devices&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">))]</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">devices</span><span class="p">):</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;   Device </span><span class="si">%d</span><span class="s2">: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
      <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A context manager to allow setting the mode to EAGER/GRAPH.&quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span>
    <span class="n">old_is_eager</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">is_eager</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">is_eager</span> <span class="o">=</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">EAGER_MODE</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">EAGER_MODE</span><span class="p">:</span>
      <span class="c1"># Entering graph mode does not provide us with sufficient information to</span>
      <span class="c1"># record a context switch; graph-based context switches are only logged</span>
      <span class="c1"># when a graph is registered as the default graph.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">eager_mode</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">ctx</span><span class="o">.</span><span class="n">is_eager</span> <span class="o">=</span> <span class="n">old_is_eager</span>
      <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">EAGER_MODE</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">executing_eagerly</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns True if current thread has eager executing enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">is_eager</span>

  <span class="k">def</span> <span class="nf">ones_rank_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Per-device cache for scalars.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_tensor_caches_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_id</span><span class="p">]</span><span class="o">.</span><span class="n">ones_rank_cache</span>

  <span class="k">def</span> <span class="nf">zeros_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Per-device cache for scalars.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_tensor_caches_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_id</span><span class="p">]</span><span class="o">.</span><span class="n">zeros_cache</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">scope_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns scope name for the current thread.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">scope_name</span>

  <span class="nd">@scope_name</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">scope_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets scope name for the current thread.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">scope_name</span> <span class="o">=</span> <span class="n">s</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the device name for the current thread.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">device_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the device spec for the current thread.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">device_spec</span>

  <span class="k">def</span> <span class="nf">_set_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name</span><span class="p">,</span> <span class="n">device_spec</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">device_name</span> <span class="o">=</span> <span class="n">device_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">device_spec</span> <span class="o">=</span> <span class="n">device_spec</span>

  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Context-manager to force placement of operations and Tensors on a device.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Name of the device or None to get default placement.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Context manager that forces device placement.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If name is not a string or is an invalid device name.</span>
<span class="sd">      RuntimeError: If device scopes are not properly nested.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">LogicalDevice</span><span class="p">):</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">name</span>
    <span class="k">elif</span> <span class="n">pydev</span><span class="o">.</span><span class="n">is_device_spec</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">_EagerDeviceContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">devices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List of the names of devices available to execute operations.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span>

  <span class="k">def</span> <span class="nf">host_address_space</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buffer_</span><span class="p">:</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_HostAddressSpace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">buffer_</span><span class="p">)</span>
      <span class="n">address_space</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buffer_</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">address_space</span>

  <span class="c1"># TODO(fishx): remove this property.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">execution_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets execution mode for current thread.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ASYNC</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_async</span><span class="p">()</span> <span class="k">else</span> <span class="n">SYNC</span>

  <span class="nd">@execution_mode</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">execution_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets execution mode for current thread.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">SYNC</span><span class="p">,</span> <span class="n">ASYNC</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Execution mode should be None/SYNC/ASYNC. Got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">mode</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">mode</span> <span class="o">=</span> <span class="n">SYNC</span>

    <span class="n">enable_async</span> <span class="o">=</span> <span class="p">(</span><span class="n">mode</span> <span class="o">==</span> <span class="n">ASYNC</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_async</span><span class="p">()</span> <span class="o">!=</span> <span class="n">enable_async</span><span class="p">:</span>
      <span class="c1"># Only set the execution mode if the context has already been initialized</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">executor</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="n">executor_new</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">new_executor</span><span class="p">(</span><span class="n">enable_async</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">executor</span> <span class="o">=</span> <span class="n">executor_new</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextSetExecutorForThread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span>
                                                   <span class="n">executor_new</span><span class="o">.</span><span class="n">handle</span><span class="p">())</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_default_is_async</span> <span class="o">=</span> <span class="n">enable_async</span>

  <span class="k">def</span> <span class="nf">is_async</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">executor</span><span class="o">.</span><span class="n">is_async</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_is_async</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">executor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">executor</span><span class="o">.</span><span class="n">Executor</span><span class="p">(</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextGetExecutorForThread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">))</span>

  <span class="nd">@executor</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">executor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">):</span>
    <span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextSetExecutorForThread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">handle</span><span class="p">())</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the ConfigProto with all runtime deltas applied.&quot;&quot;&quot;</span>
    <span class="c1"># Ensure physical devices have been discovered and config has been imported</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="n">config</span> <span class="o">=</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_jit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">optimizer_options</span><span class="o">.</span><span class="n">global_jit_level</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">config_pb2</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="o">.</span><span class="n">ON_1</span>
          <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_jit</span> <span class="k">else</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="o">.</span><span class="n">OFF</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intra_op_parallelism_threads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">intra_op_parallelism_threads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intra_op_parallelism_threads</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inter_op_parallelism_threads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">inter_op_parallelism_threads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inter_op_parallelism_threads</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_soft_device_placement</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">allow_soft_placement</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_soft_device_placement</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">allow_soft_placement</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_device_placement</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">log_device_placement</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_device_placement</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_mlir_bridge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">enable_mlir_bridge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_mlir_bridge</span>

    <span class="k">def</span> <span class="nf">rewriter_toggle</span><span class="p">(</span><span class="n">option</span><span class="p">):</span>
      <span class="n">toggle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_experimental_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">option</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">toggle</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

      <span class="nb">setattr</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span><span class="p">,</span>
              <span class="n">option</span><span class="p">,</span>
              <span class="p">(</span><span class="n">rewriter_config_pb2</span><span class="o">.</span><span class="n">RewriterConfig</span><span class="o">.</span><span class="n">ON</span>
               <span class="k">if</span> <span class="n">toggle</span> <span class="k">else</span> <span class="n">rewriter_config_pb2</span><span class="o">.</span><span class="n">RewriterConfig</span><span class="o">.</span><span class="n">OFF</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">rewriter_bool</span><span class="p">(</span><span class="n">option</span><span class="p">):</span>
      <span class="n">toggle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_experimental_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">option</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">toggle</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

      <span class="nb">setattr</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span><span class="p">,</span>
              <span class="n">option</span><span class="p">,</span>
              <span class="n">toggle</span><span class="p">)</span>

    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;layout_optimizer&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;constant_folding&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;shape_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;remapping&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;arithmetic_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;dependency_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;loop_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;function_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;debug_stripper&quot;</span><span class="p">)</span>
    <span class="n">rewriter_bool</span><span class="p">(</span><span class="s2">&quot;disable_model_pruning&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;scoped_allocator_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;pin_to_host_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;implementation_selector&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;auto_mixed_precision&quot;</span><span class="p">)</span>
    <span class="n">rewriter_bool</span><span class="p">(</span><span class="s2">&quot;disable_meta_optimizer&quot;</span><span class="p">)</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_experimental_options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;min_graph_nodes&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">nodes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span><span class="o">.</span><span class="n">min_graph_nodes</span> <span class="o">=</span> <span class="n">nodes</span>

    <span class="c1"># Compute device counts</span>
    <span class="n">config</span><span class="o">.</span><span class="n">device_count</span><span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">config</span><span class="o">.</span><span class="n">device_count</span><span class="p">[</span><span class="s2">&quot;GPU&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dev</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">dev</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span><span class="p">:</span>
        <span class="k">continue</span>

      <span class="n">virtual_devices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">virtual_devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">device_count</span><span class="p">[</span><span class="n">dev</span><span class="o">.</span><span class="n">device_type</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">device_count</span><span class="p">[</span><span class="n">dev</span><span class="o">.</span><span class="n">device_type</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">virtual_devices</span><span class="p">)</span>

    <span class="c1"># Configure gpu_options</span>
    <span class="n">gpu_options</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_gpu_options</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">gpu_options</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span><span class="n">gpu_options</span><span class="p">)</span>

    <span class="c1"># Configure collective ops</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_leader</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">collective_group_leader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_leader</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_scoped_allocator_enabled_ops</span><span class="p">:</span>
      <span class="n">rewrite_options</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span>
      <span class="n">rewrite_options</span><span class="o">.</span><span class="n">scoped_allocator_optimization</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">rewriter_config_pb2</span><span class="o">.</span><span class="n">RewriterConfig</span><span class="o">.</span><span class="n">ON</span><span class="p">)</span>
      <span class="k">del</span> <span class="n">rewrite_options</span><span class="o">.</span><span class="n">scoped_allocator_opts</span><span class="o">.</span><span class="n">enable_op</span><span class="p">[:]</span>
      <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_scoped_allocator_enabled_ops</span><span class="p">:</span>
        <span class="n">rewrite_options</span><span class="o">.</span><span class="n">scoped_allocator_opts</span><span class="o">.</span><span class="n">enable_op</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_use_nccl_communication</span><span class="p">:</span>
      <span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">collective_nccl</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_device_filters</span><span class="p">:</span>
      <span class="k">del</span> <span class="n">config</span><span class="o">.</span><span class="n">device_filters</span><span class="p">[:]</span>
      <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collective_device_filters</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">device_filters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">config</span>

  <span class="k">def</span> <span class="nf">_compute_gpu_options</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build the GPUOptions proto.&quot;&quot;&quot;</span>
    <span class="n">visible_device_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">virtual_devices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">gpu_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">memory_growths</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">dev</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">):</span>
      <span class="n">gpu_index</span> <span class="o">+=</span> <span class="mi">1</span>

      <span class="k">if</span> <span class="n">dev</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span><span class="p">:</span>
        <span class="k">continue</span>

      <span class="n">growth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_growth_map</span><span class="p">[</span><span class="n">dev</span><span class="p">]</span>
      <span class="n">memory_growths</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">growth</span><span class="p">)</span>
      <span class="n">visible_device_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">gpu_index</span><span class="p">))</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span><span class="p">:</span>
        <span class="n">vdevs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">device_limits</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">virt_dev</span> <span class="ow">in</span> <span class="n">vdevs</span><span class="p">:</span>
          <span class="n">device_limits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">virt_dev</span><span class="o">.</span><span class="n">memory_limit</span><span class="p">)</span>

        <span class="n">virtual_devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">config_pb2</span><span class="o">.</span><span class="n">GPUOptions</span><span class="o">.</span><span class="n">Experimental</span><span class="o">.</span><span class="n">VirtualDevices</span><span class="p">(</span>
                <span class="n">memory_limit_mb</span><span class="o">=</span><span class="n">device_limits</span><span class="p">))</span>

    <span class="c1"># Only compute growth if virtual devices have not been configured and we</span>
    <span class="c1"># have GPUs</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">virtual_devices</span> <span class="ow">and</span> <span class="n">memory_growths</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory_growths</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Memory growth cannot differ between GPU devices&quot;</span><span class="p">)</span>
      <span class="n">allow_growth</span> <span class="o">=</span> <span class="n">memory_growths</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">allow_growth</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">GPUOptions</span><span class="p">(</span>
        <span class="n">allow_growth</span><span class="o">=</span><span class="n">allow_growth</span><span class="p">,</span>
        <span class="n">visible_device_list</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">visible_device_list</span><span class="p">),</span>
        <span class="n">experimental</span><span class="o">=</span><span class="n">config_pb2</span><span class="o">.</span><span class="n">GPUOptions</span><span class="o">.</span><span class="n">Experimental</span><span class="p">(</span>
            <span class="n">virtual_devices</span><span class="o">=</span><span class="n">virtual_devices</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">function_call_options</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns function call options for current thread.</span>

<span class="sd">    Note that the returned object is still referenced by the eager context.</span>

<span class="sd">    Returns: the FunctionCallOptions for current thread.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span>

      <span class="c1"># Default to soft placement for functions unless specified</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_soft_device_placement</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">allow_soft_placement</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="n">FunctionCallOptions</span><span class="p">(</span>
          <span class="n">config_proto</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span>

  <span class="nd">@function_call_options</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">function_call_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns function call options for current thread.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="n">options</span>

  <span class="k">def</span> <span class="nf">num_gpus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The number of GPUs available to execute operations.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_gpus</span>

  <span class="k">def</span> <span class="nf">add_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a function definition to the context.</span>

<span class="sd">    Once added, the function (identified by its name) can be executed like any</span>
<span class="sd">    other operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      fn: A wrapped TF_Function (returned from TF_GraphToFunction_wrapper).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextAddFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">add_function_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fdef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a function definition to the context.</span>

<span class="sd">    Once added, the function (identified by its name) can be executed like any</span>
<span class="sd">    other operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      fdef: A FunctionDef protocol buffer message.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="n">fdef_string</span> <span class="o">=</span> <span class="n">fdef</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextAddFunctionDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">fdef_string</span><span class="p">,</span>
                                         <span class="nb">len</span><span class="p">(</span><span class="n">fdef_string</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">get_function_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a function definition from the context.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: function signature name.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The requested FunctionDef.</span>

<span class="sd">    Raises:</span>
<span class="sd">      tf.errors.NotFoundError: if name is not the name of a registered function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buffer_</span><span class="p">:</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextGetFunctionDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">buffer_</span><span class="p">)</span>
      <span class="n">proto_data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buffer_</span><span class="p">)</span>
    <span class="n">function_def</span> <span class="o">=</span> <span class="n">function_pb2</span><span class="o">.</span><span class="n">FunctionDef</span><span class="p">()</span>
    <span class="n">function_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">proto_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">function_def</span>

  <span class="k">def</span> <span class="nf">remove_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Remove a function from the context.</span>

<span class="sd">    Once removed, the function cannot be executed anymore.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: function signature name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextRemoveFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">has_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Check if a function `name` is registered.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextHasFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">add_op_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a post-op callback to the context.</span>

<span class="sd">    A post-op callback is invoked immediately after an eager operation or</span>
<span class="sd">    function has finished execution or after a op has been added to a graph,</span>
<span class="sd">    providing access to the op&#39;s type, name input and output tensors. Multiple</span>
<span class="sd">    op callbacks can be added, in which case the callbacks will be invoked in</span>
<span class="sd">    the order in which they are added.</span>

<span class="sd">    Args:</span>
<span class="sd">      callback: a callable of the signature</span>
<span class="sd">        `f(op_type, inputs, attrs, outputs, op_name=None, graph=None)`.</span>
<span class="sd">        See doc strings in `op_callbacks.py` for details on the function</span>
<span class="sd">        signature and its semantics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callback</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">op_callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">remove_op_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Remove an already-registered op callback.</span>

<span class="sd">    Args:</span>
<span class="sd">      callback: The op callback to be removed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `callback` is not already registered.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callback</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
          <span class="s2">&quot;The specified op callback has not been registered, &quot;</span>
          <span class="s2">&quot;and hence cannot be removed.&quot;</span><span class="p">)</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">op_callbacks</span><span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">op_callbacks</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">callback</span><span class="p">)]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">op_callbacks</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">invoking_op_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">invoking_op_callbacks</span>

  <span class="nd">@invoking_op_callbacks</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">invoking_op_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">invoking_op_callbacks</span> <span class="o">=</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">_initialize_physical_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get local devices visible to the system.&quot;&quot;&quot;</span>
    <span class="c1"># We lazy initialize self._physical_devices since we do not want to do this</span>
    <span class="c1"># the constructor since the backend may not be initialized yet.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

      <span class="n">devs</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_ListPhysicalDevices</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">PhysicalDevice</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">decode</span><span class="p">(),</span>
                         <span class="n">device_type</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">devs</span><span class="p">]</span>
      <span class="c1"># Construct the visible device list from all physical devices but ignore</span>
      <span class="c1"># XLA devices</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;XLA&quot;</span><span class="p">)</span>
      <span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_memory_growth_map</span> <span class="o">=</span> <span class="p">{</span>
          <span class="n">d</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span>
      <span class="p">}</span>

    <span class="c1"># Import device settings that may have been passed into the constructor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_import_config</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">list_physical_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List local devices visible to the system.</span>

<span class="sd">    This API allows a client to query the devices before they have been</span>
<span class="sd">    initialized by the eager runtime. Additionally a user can filter by device</span>
<span class="sd">    type, to get only CPUs or GPUs.</span>

<span class="sd">    Args:</span>
<span class="sd">      device_type: Optional device type to limit results to</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of PhysicalDevice objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">device_type</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_import_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Import config if passed in during construction.</span>

<span class="sd">    If Context was created with a ConfigProto such as when calling</span>
<span class="sd">    tf.compat.v1.enable_eager_execution(), then we need to pull out the</span>
<span class="sd">    various pieces we might be replacing and import then into our internal</span>
<span class="sd">    class representation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="n">num_cpus</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">device_count</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_cpus</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">cpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">num_cpus</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">([],</span> <span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">num_cpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_logical_device_configuration</span><span class="p">(</span>
            <span class="n">cpus</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">LogicalDeviceConfiguration</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_cpus</span><span class="p">)])</span>

    <span class="c1"># Parse GPU options</span>
    <span class="n">gpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">]</span>

    <span class="c1"># If there are no GPUs detected, simply ignore all the GPU options passed in</span>
    <span class="c1"># rather than doing any validation checks.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">gpus</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="n">gpu_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">device_count</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">visible_gpus</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># TODO(gjn): Handle importing existing virtual GPU configuration</span>
    <span class="n">visible_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">gpu_options</span><span class="o">.</span><span class="n">visible_device_list</span>
    <span class="k">if</span> <span class="n">visible_indices</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">visible_indices</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid visible device index: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">index</span><span class="p">)</span>
        <span class="n">visible_gpus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">)])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">visible_gpus</span> <span class="o">=</span> <span class="n">gpus</span>

    <span class="k">if</span> <span class="n">gpu_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">visible_gpus</span> <span class="o">=</span> <span class="n">visible_gpus</span><span class="p">[:</span><span class="n">gpu_count</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">visible_gpus</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">list_logical_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return logical devices.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_logical_devices</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logical_devices</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">device_type</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">get_visible_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the list of visible devices.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">device_type</span>
    <span class="p">]</span>

  <span class="k">def</span> <span class="nf">set_visible_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set the list of visible devices.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="n">devices</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">d</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unrecognized device: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">!=</span> <span class="n">device_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unrecognized device: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>

    <span class="n">visible_device_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">visible_device_list</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span> <span class="o">!=</span> <span class="n">device_type</span>
      <span class="p">]</span>

    <span class="n">visible_device_list</span> <span class="o">+=</span> <span class="n">devices</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span> <span class="o">==</span> <span class="n">visible_device_list</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Visible devices cannot be modified after being initialized&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_visible_device_list</span> <span class="o">=</span> <span class="n">visible_device_list</span>

  <span class="k">def</span> <span class="nf">get_memory_growth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dev</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get if memory growth is enabled for a PhysicalDevice.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">dev</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unrecognized device: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">dev</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_growth_map</span><span class="p">[</span><span class="n">dev</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">set_memory_growth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">enable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set if memory growth should be enabled for a PhysicalDevice.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">dev</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unrecognized device: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">dev</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">dev</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot set memory growth on device when virtual devices configured&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dev</span><span class="o">.</span><span class="n">device_type</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot set memory growth on non-GPU devices&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_growth_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span> <span class="o">==</span> <span class="n">enable</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Physical devices cannot be modified after being initialized&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_memory_growth_map</span><span class="p">[</span><span class="n">dev</span><span class="p">]</span> <span class="o">=</span> <span class="n">enable</span>

  <span class="k">def</span> <span class="nf">get_logical_device_configuration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dev</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the virtual device configuration for a PhysicalDevice.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">dev</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unrecognized device: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">dev</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">set_logical_device_configuration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">virtual_devices</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set the virtual device configuration for a PhysicalDevice.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_physical_devices</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">dev</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_physical_devices</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unrecognized device: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">dev</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">dev</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">vdev</span> <span class="ow">in</span> <span class="n">virtual_devices</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">vdev</span><span class="o">.</span><span class="n">memory_limit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Setting memory limit on CPU virtual devices is &quot;</span>
                           <span class="s2">&quot;currently not supported&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dev</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">vdev</span> <span class="ow">in</span> <span class="n">virtual_devices</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">vdev</span><span class="o">.</span><span class="n">memory_limit</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="s2">&quot;Setting memory limit is required for GPU virtual devices&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Virtual devices are not supported for </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                       <span class="n">dev</span><span class="o">.</span><span class="n">device_type</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span> <span class="o">==</span> <span class="n">virtual_devices</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Virtual devices cannot be modified after being initialized&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_virtual_device_map</span><span class="p">[</span><span class="n">dev</span><span class="p">]</span> <span class="o">=</span> <span class="n">virtual_devices</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">enable_mlir_bridge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_mlir_bridge</span>

  <span class="nd">@enable_mlir_bridge</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">enable_mlir_bridge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_enable_mlir_bridge</span> <span class="o">=</span> <span class="n">enabled</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">optimizer_jit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">level</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">optimizer_options</span><span class="o">.</span><span class="n">global_jit_level</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">level</span> <span class="o">==</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="o">.</span><span class="n">ON_1</span> <span class="ow">or</span>
            <span class="n">level</span> <span class="o">==</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="o">.</span><span class="n">ON_2</span><span class="p">)</span>

  <span class="nd">@optimizer_jit</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">optimizer_jit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_jit</span> <span class="o">=</span> <span class="n">enabled</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">get_optimizer_experimental_options</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get experimental options for the optimizer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Dictionary of current option values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rewrite_options</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span>
    <span class="n">options</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">rewriter_toggle</span><span class="p">(</span><span class="n">option</span><span class="p">):</span>
      <span class="n">attr</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">rewrite_options</span><span class="p">,</span> <span class="n">option</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">attr</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">options</span><span class="p">[</span><span class="n">option</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">attr</span> <span class="o">==</span> <span class="n">rewriter_config_pb2</span><span class="o">.</span><span class="n">RewriterConfig</span><span class="o">.</span><span class="n">ON</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">rewriter_bool</span><span class="p">(</span><span class="n">option</span><span class="p">):</span>
      <span class="n">options</span><span class="p">[</span><span class="n">option</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">rewrite_options</span><span class="p">,</span> <span class="n">option</span><span class="p">)</span>

    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;layout_optimizer&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;constant_folding&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;shape_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;remapping&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;arithmetic_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;dependency_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;loop_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;function_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;debug_stripper&quot;</span><span class="p">)</span>
    <span class="n">rewriter_bool</span><span class="p">(</span><span class="s2">&quot;disable_model_pruning&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;scoped_allocator_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;pin_to_host_optimization&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;implementation_selector&quot;</span><span class="p">)</span>
    <span class="n">rewriter_toggle</span><span class="p">(</span><span class="s2">&quot;auto_mixed_precision&quot;</span><span class="p">)</span>
    <span class="n">rewriter_bool</span><span class="p">(</span><span class="s2">&quot;disable_meta_optimizer&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rewrite_options</span><span class="o">.</span><span class="n">min_graph_nodes</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">options</span><span class="p">[</span><span class="s2">&quot;min_graph_nodes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rewrite_options</span><span class="o">.</span><span class="n">min_graph_nodes</span>

    <span class="k">return</span> <span class="n">options</span>

  <span class="k">def</span> <span class="nf">set_optimizer_experimental_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set experimental options for the optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">      options: Dictionary of options to modify</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_experimental_options</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">intra_op_parallelism_threads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">intra_op_parallelism_threads</span>

  <span class="nd">@intra_op_parallelism_threads</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">intra_op_parallelism_threads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_intra_op_parallelism_threads</span> <span class="o">==</span> <span class="n">num_threads</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Intra op parallelism cannot be modified after initialization.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_intra_op_parallelism_threads</span> <span class="o">=</span> <span class="n">num_threads</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">inter_op_parallelism_threads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">inter_op_parallelism_threads</span>

  <span class="nd">@inter_op_parallelism_threads</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">inter_op_parallelism_threads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inter_op_parallelism_threads</span> <span class="o">==</span> <span class="n">num_threads</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Inter op parallelism cannot be modified after initialization.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_inter_op_parallelism_threads</span> <span class="o">=</span> <span class="n">num_threads</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">soft_device_placement</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">allow_soft_placement</span>

  <span class="nd">@soft_device_placement</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">soft_device_placement</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_soft_device_placement</span> <span class="o">=</span> <span class="n">enabled</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">log_device_placement</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_device_placement</span>

  <span class="nd">@log_device_placement</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">log_device_placement</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_device_placement</span> <span class="o">==</span> <span class="n">enabled</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Device placement logging must be set at program startup&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_log_device_placement</span> <span class="o">=</span> <span class="n">enabled</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Only get the policy from the context if it has already been initialized</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextGetDevicePlacementPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_policy</span>

  <span class="nd">@device_policy</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">device_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">policy</span> <span class="o">=</span> <span class="n">DEVICE_PLACEMENT_SILENT</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_policy</span> <span class="o">!=</span> <span class="n">policy</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_device_policy</span> <span class="o">=</span> <span class="n">policy</span>

      <span class="c1"># Only set the policy if the context has already been initialized</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextSetThreadLocalDevicePlacementPolicy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_policy</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">mirroring_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Only get the policy from the context if it has already been initialized</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextGetMirroringPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span>

  <span class="nd">@mirroring_policy</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">mirroring_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">policy</span> <span class="o">=</span> <span class="n">MIRRORING_NONE</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span> <span class="o">!=</span> <span class="n">policy</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span> <span class="o">=</span> <span class="n">policy</span>

      <span class="c1"># Only set the policy if the context has already been initialized</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextSetThreadLocalMirroringPolicy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mirroring_policy</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">lazy_remote_inputs_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_remote_inputs_copy</span>

  <span class="nd">@lazy_remote_inputs_copy</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">lazy_remote_inputs_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lazy_copy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets whether to copy remote inputs lazily for functions.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lazy_copy</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expecting a boolean but got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">lazy_copy</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_remote_inputs_copy</span> <span class="o">!=</span> <span class="n">lazy_copy</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;lazy_remote_inputs_copy should be set before being initialized.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_remote_inputs_copy</span> <span class="o">=</span> <span class="n">lazy_copy</span>

  <span class="k">def</span> <span class="nf">enable_run_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enables tracing of op execution via RunMetadata.</span>

<span class="sd">    To retrieve the accumulated metadata call context.export_run_metadata()</span>
<span class="sd">    and to stop tracing call context.disable_run_metadata().</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextEnableRunMetadata</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">disable_run_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Disables tracing of op execution via RunMetadata.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextDisableRunMetadata</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">enable_graph_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enables graph collection of executed functions.</span>

<span class="sd">    To retrieve the accumulated graphs call context.export_run_metadata()</span>
<span class="sd">    and to stop collecting graphs call context.disable_graph_collection().</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextEnableGraphCollection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">disable_graph_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Disables graph collection of executed functions.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextDisableGraphCollection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">export_run_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a RunMetadata proto with accumulated information.</span>

<span class="sd">    The returned protocol buffer contains information since the most recent call</span>
<span class="sd">    to either enable_run_metadata or export_run_metadata.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A RunMetadata protocol buffer. Or None if not enabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buffer_</span><span class="p">:</span>
      <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextExportRunMetadata</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="p">,</span> <span class="n">buffer_</span><span class="p">)</span>
      <span class="n">proto_data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buffer_</span><span class="p">)</span>
    <span class="n">run_metadata</span> <span class="o">=</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">RunMetadata</span><span class="p">()</span>
    <span class="n">run_metadata</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">proto_data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">run_metadata</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">context_switches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of context switches.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span>

  <span class="k">def</span> <span class="nf">start_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextStartStep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">end_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextEndStep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_EagerDeviceContext</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context-manager forcing placement of ops and Tensors on a device.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">device_name</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_name</span> <span class="o">=</span> <span class="n">device_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span> <span class="o">=</span> <span class="n">ctx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stack</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span>
    <span class="n">old_device_name</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_name</span>
    <span class="n">old_device_spec</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_spec</span>
    <span class="n">new_device_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_name</span>
    <span class="n">cache_key</span> <span class="o">=</span> <span class="p">(</span><span class="n">old_device_name</span><span class="p">,</span> <span class="n">new_device_name</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">new_device_name</span><span class="p">,</span> <span class="n">new_device_spec</span> <span class="o">=</span> <span class="n">_device_parsing_cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
      <span class="c1"># Error while trying to compute the cache key.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expecting a string device name. Got </span><span class="si">%s</span><span class="s2">(</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">new_device_name</span><span class="p">),</span> <span class="n">new_device_name</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="c1"># Handle a cache miss.</span>
      <span class="k">if</span> <span class="n">new_device_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_device_name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expecting a string device name. Got </span><span class="si">%s</span><span class="s2">(</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">new_device_name</span><span class="p">),</span> <span class="n">new_device_name</span><span class="p">))</span>
        <span class="n">device_spec</span> <span class="o">=</span> <span class="n">pydev</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="o">.</span><span class="n">from_string</span><span class="p">(</span><span class="n">new_device_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">old_device_name</span><span class="p">:</span>
          <span class="n">new_device_spec</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">old_device_spec</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">ctx</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
          <span class="n">new_device_spec</span> <span class="o">=</span> <span class="n">pydev</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="o">.</span><span class="n">from_string</span><span class="p">(</span>
              <span class="n">ctx</span><span class="o">.</span><span class="n">_context_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">new_device_spec</span> <span class="o">=</span> <span class="n">new_device_spec</span><span class="o">.</span><span class="n">make_merged_spec</span><span class="p">(</span><span class="n">device_spec</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">new_device_spec</span> <span class="o">=</span> <span class="n">pydev</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="o">.</span><span class="n">from_string</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
      <span class="n">new_device_name</span> <span class="o">=</span> <span class="n">new_device_spec</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span>
      <span class="n">_device_parsing_cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_device_name</span><span class="p">,</span> <span class="n">new_device_spec</span><span class="p">)</span>

    <span class="n">ctx</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">new_device_name</span><span class="p">,</span> <span class="n">new_device_spec</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">old_device_name</span><span class="p">,</span> <span class="n">old_device_spec</span><span class="p">,</span> <span class="n">new_device_spec</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">ex_info</span><span class="p">):</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span>
    <span class="n">old_device_name</span><span class="p">,</span> <span class="n">old_device_spec</span><span class="p">,</span> <span class="n">new_device_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_device_spec</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Exiting device scope without proper scope nesting&quot;</span><span class="p">)</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">old_device_name</span><span class="p">,</span> <span class="n">old_device_spec</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="c1"># Do not set directly. Use _set_context.</span>
<span class="n">_context</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">_context_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_set_context_locked</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">_context</span>
  <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_SetEagerContext</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
  <span class="n">_context</span> <span class="o">=</span> <span class="n">ctx</span>


<span class="k">def</span> <span class="nf">_set_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">_context_lock</span><span class="p">:</span>
    <span class="n">_set_context_locked</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_context</span><span class="p">():</span>
  <span class="k">with</span> <span class="n">_context_lock</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ctx</span> <span class="o">=</span> <span class="n">Context</span><span class="p">()</span>
      <span class="n">_set_context_locked</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_reset_context</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Clears and re-initializes the singleton context.</span>

<span class="sd">  Should only be used for testing.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">global</span> <span class="n">_context</span>
  <span class="k">with</span> <span class="n">_context_lock</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">_context</span><span class="o">.</span><span class="n">_clear_caches</span><span class="p">()</span>
      <span class="n">_context</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">_create_context</span><span class="p">()</span>
  <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ClearScalarCache</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">context</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns a singleton context object.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">_create_context</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">_context</span>


<span class="k">def</span> <span class="nf">context_safe</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns current context (or None if one hasn&#39;t been initialized).&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_context</span>


<span class="k">def</span> <span class="nf">ensure_initialized</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Initialize the context.&quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">set_global_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sets the eager mode seed.&quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">_set_global_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">global_seed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the eager mode seed.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">_seed</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">internal_operation_seed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the operation seed generated based on global seed.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">_internal_operation_seed</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>


<div class="viewcode-block" id="executing_eagerly"><a class="viewcode-back" href="../../../../index.html#tensorflow.executing_eagerly">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;executing_eagerly&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">executing_eagerly</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Checks whether the current thread has eager execution enabled.</span>

<span class="sd">  Eager execution is enabled by default and this API returns `True`</span>
<span class="sd">  in most of cases. However, this API might return `False` in the following use</span>
<span class="sd">  cases.</span>

<span class="sd">  *  Executing inside `tf.function`, unless under `tf.init_scope` or</span>
<span class="sd">     `tf.config.experimental_run_functions_eagerly(True)` is previously called.</span>
<span class="sd">  *  Executing inside a transformation function for `tf.dataset`.</span>
<span class="sd">  *  `tf.compat.v1.disable_eager_execution()` is called.</span>

<span class="sd">  General case:</span>

<span class="sd">  &gt;&gt;&gt; print(tf.executing_eagerly())</span>
<span class="sd">  True</span>

<span class="sd">  Inside `tf.function`:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def fn():</span>
<span class="sd">  ...   with tf.init_scope():</span>
<span class="sd">  ...     print(tf.executing_eagerly())</span>
<span class="sd">  ...   print(tf.executing_eagerly())</span>
<span class="sd">  &gt;&gt;&gt; fn()</span>
<span class="sd">  True</span>
<span class="sd">  False</span>

<span class="sd">  Inside `tf.function` after</span>

<span class="sd">  `tf.config.experimental_run_functions_eagerly(True)` is called:</span>
<span class="sd">  &gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(True)</span>
<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def fn():</span>
<span class="sd">  ...   with tf.init_scope():</span>
<span class="sd">  ...     print(tf.executing_eagerly())</span>
<span class="sd">  ...   print(tf.executing_eagerly())</span>
<span class="sd">  &gt;&gt;&gt; fn()</span>
<span class="sd">  True</span>
<span class="sd">  True</span>
<span class="sd">  &gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(False)</span>

<span class="sd">  Inside a transformation function for `tf.dataset`:</span>

<span class="sd">  &gt;&gt;&gt; def data_fn(x):</span>
<span class="sd">  ...   print(tf.executing_eagerly())</span>
<span class="sd">  ...   return x</span>
<span class="sd">  &gt;&gt;&gt; dataset = tf.data.Dataset.range(100)</span>
<span class="sd">  &gt;&gt;&gt; dataset = dataset.map(data_fn)</span>
<span class="sd">  False</span>

<span class="sd">  Returns:</span>
<span class="sd">    `True` if the current thread has eager execution enabled.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ctx</span> <span class="o">=</span> <span class="n">context_safe</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">default_execution_mode</span> <span class="o">==</span> <span class="n">EAGER_MODE</span>

  <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;executing_eagerly&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">executing_eagerly_v1</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Checks whether the current thread has eager execution enabled.</span>

<span class="sd">  Eager execution is typically enabled via</span>
<span class="sd">  `tf.compat.v1.enable_eager_execution`, but may also be enabled within the</span>
<span class="sd">  context of a Python function via tf.contrib.eager.py_func.</span>

<span class="sd">  When eager execution is enabled, returns `True` in most cases. However,</span>
<span class="sd">  this API might return `False` in the following use cases.</span>

<span class="sd">  *  Executing inside `tf.function`, unless under `tf.init_scope` or</span>
<span class="sd">     `tf.config.experimental_run_functions_eagerly(True)` is previously called.</span>
<span class="sd">  *  Executing inside a transformation function for `tf.dataset`.</span>
<span class="sd">  *  `tf.compat.v1.disable_eager_execution()` is called.</span>

<span class="sd">  &gt;&gt;&gt; tf.compat.v1.enable_eager_execution()</span>

<span class="sd">  General case:</span>

<span class="sd">  &gt;&gt;&gt; print(tf.executing_eagerly())</span>
<span class="sd">  True</span>

<span class="sd">  Inside `tf.function`:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def fn():</span>
<span class="sd">  ...   with tf.init_scope():</span>
<span class="sd">  ...     print(tf.executing_eagerly())</span>
<span class="sd">  ...   print(tf.executing_eagerly())</span>
<span class="sd">  &gt;&gt;&gt; fn()</span>
<span class="sd">  True</span>
<span class="sd">  False</span>

<span class="sd">  Inside `tf.function`</span>
<span class="sd">  after  `tf.config.experimental_run_functions_eagerly(True)` is called:</span>

<span class="sd">  &gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(True)</span>
<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def fn():</span>
<span class="sd">  ...   with tf.init_scope():</span>
<span class="sd">  ...     print(tf.executing_eagerly())</span>
<span class="sd">  ...   print(tf.executing_eagerly())</span>
<span class="sd">  &gt;&gt;&gt; fn()</span>
<span class="sd">  True</span>
<span class="sd">  True</span>
<span class="sd">  &gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(False)</span>

<span class="sd">  Inside a transformation function for `tf.dataset`:</span>

<span class="sd">  &gt;&gt;&gt; def data_fn(x):</span>
<span class="sd">  ...   print(tf.executing_eagerly())</span>
<span class="sd">  ...   return x</span>
<span class="sd">  &gt;&gt;&gt; dataset = tf.data.Dataset.range(100)</span>
<span class="sd">  &gt;&gt;&gt; dataset = dataset.map(data_fn)</span>
<span class="sd">  False</span>

<span class="sd">  Returns:</span>
<span class="sd">    `True` if the current thread has eager execution enabled.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">executing_eagerly</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">in_eager_mode</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Use executing_eagerly() instead. This function will be removed.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">executing_eagerly</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">shared_name</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the anonymous shared name GUID if no shared name is specified.</span>

<span class="sd">  In eager mode we need to use a unique shared name to avoid spurious sharing</span>
<span class="sd">  issues. The runtime generates a unique name on our behalf when the reserved</span>
<span class="sd">  GUID is used as a shared name.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: Optional shared name</span>

<span class="sd">  Returns:</span>
<span class="sd">    Eager compatible shared name.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">name</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">name</span>

  <span class="c1"># Ensure a unique name when eager execution is enabled to avoid spurious</span>
  <span class="c1"># sharing issues.</span>
  <span class="k">return</span> <span class="s2">&quot;cd2c89b7-88b7-44c8-ad83-06c2a9158347&quot;</span>


<span class="k">def</span> <span class="nf">graph_mode</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Context-manager to disable eager execution for the current thread.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">_mode</span><span class="p">(</span><span class="n">GRAPH_MODE</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">eager_mode</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Context-manager to enable eager execution for the current thread.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">_mode</span><span class="p">(</span><span class="n">EAGER_MODE</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">scope_name</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Name of the current scope.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">scope_name</span>


<span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context-manager to force placement of operations and Tensors on a device.</span>

<span class="sd">  Example:</span>
<span class="sd">  ```python</span>
<span class="sd">  with tf.device(&#39;gpu:0&#39;):</span>
<span class="sd">    with tf.device(&#39;cpu:0&#39;):</span>
<span class="sd">      shape = tf.constant([], dtype=tf.int32)</span>
<span class="sd">    x = tf.random.truncated_normal(shape, tf.float32)</span>
<span class="sd">  ```</span>
<span class="sd">  will ensure that the `shape` Tensor is on CPU but the `truncated_normal`</span>
<span class="sd">  operation runs on GPU 0.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: Name of the device (see context().devices()), or None to</span>
<span class="sd">      perform automatic placement.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Context manager for setting the device.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ensure_initialized</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;debugging.get_log_device_placement&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_log_device_placement</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get if device placements are logged.</span>

<span class="sd">  Returns:</span>
<span class="sd">    If device placements are logged.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">log_device_placement</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;debugging.set_log_device_placement&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_log_device_placement</span><span class="p">(</span><span class="n">enabled</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set if device placements should be logged.</span>

<span class="sd">  Args:</span>
<span class="sd">    enabled: Whether to enabled device placement logging.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">log_device_placement</span> <span class="o">=</span> <span class="n">enabled</span>


<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">device_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context manager for setting device placement policy for current thread.&quot;&quot;&quot;</span>
  <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="p">()</span>
  <span class="n">old_policy</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_policy</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">device_policy</span> <span class="o">=</span> <span class="n">policy</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">device_policy</span> <span class="o">=</span> <span class="n">old_policy</span>


<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">mirroring_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context manager for setting mirroring policy for current thread.&quot;&quot;&quot;</span>
  <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="p">()</span>
  <span class="n">old_policy</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">mirroring_policy</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">mirroring_policy</span> <span class="o">=</span> <span class="n">policy</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">mirroring_policy</span> <span class="o">=</span> <span class="n">old_policy</span>


<span class="k">def</span> <span class="nf">set_execution_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sets execution mode for the current thread.&quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">execution_mode</span> <span class="o">=</span> <span class="n">mode</span>


<span class="c1"># TODO(fishx): remove this method.</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">execution_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context manager for setting execution mode for current thread.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="p">()</span>
    <span class="n">executor_new</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">new_executor</span><span class="p">(</span><span class="n">mode</span> <span class="o">==</span> <span class="n">ASYNC</span><span class="p">)</span>
    <span class="n">executor_old</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executor</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">executor_old</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
      <span class="n">ctx</span><span class="o">.</span><span class="n">executor</span> <span class="o">=</span> <span class="n">executor_new</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">ctx</span><span class="o">.</span><span class="n">executor</span> <span class="o">=</span> <span class="n">executor_old</span>
      <span class="n">executor_new</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">executor_scope</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context manager for changing executor for current thread.</span>

<span class="sd">  Args:</span>
<span class="sd">    e: A Executor to execute eager ops under this scope. Setting it to None will</span>
<span class="sd">      switch back to use the default executor for the context.</span>

<span class="sd">  Yields:</span>
<span class="sd">    Context manager for setting the executor for current thread.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="p">()</span>
  <span class="n">executor_old</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executor</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">executor</span> <span class="o">=</span> <span class="n">e</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">executor</span> <span class="o">=</span> <span class="n">executor_old</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;experimental.function_executor_type&quot;</span><span class="p">)</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">function_executor_type</span><span class="p">(</span><span class="n">executor_type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context manager for setting the executor of eager defined functions.</span>

<span class="sd">  Eager defined functions are functions decorated by tf.contrib.eager.defun.</span>

<span class="sd">  Args:</span>
<span class="sd">    executor_type: a string for the name of the executor to be used to execute</span>
<span class="sd">      functions defined by tf.contrib.eager.defun.</span>

<span class="sd">  Yields:</span>
<span class="sd">    Context manager for setting the executor of eager defined functions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">current_options</span> <span class="o">=</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">function_call_options</span>
  <span class="n">old_options</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">current_options</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">current_options</span><span class="o">.</span><span class="n">executor_type</span> <span class="o">=</span> <span class="n">executor_type</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">function_call_options</span> <span class="o">=</span> <span class="n">old_options</span>


<span class="k">def</span> <span class="nf">is_async</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns true if current thread is in async mode.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">is_async</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">num_gpus</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the number of available GPU devices.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The number of available GPU devices.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">num_gpus</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">enable_run_metadata</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Enables tracing of op execution via RunMetadata.</span>

<span class="sd">  To retrieve the accumulated metadata call context.export_run_metadata()</span>
<span class="sd">  and to stop tracing call context.disable_run_metadata().</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">enable_run_metadata</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">disable_run_metadata</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Disables tracing of op execution via RunMetadata.&quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">disable_run_metadata</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">enable_graph_collection</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Enables graph collection of executed functions.</span>

<span class="sd">  To retrieve the accumulated graphs call context.export_run_metadata()</span>
<span class="sd">  and to stop collecting graphs call context.disable_graph_collection().</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">enable_graph_collection</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">disable_graph_collection</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Disables graph collection of executed functions.&quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">disable_graph_collection</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">export_run_metadata</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns a RunMetadata proto with accumulated information.</span>

<span class="sd">  The returned protocol buffer contains information since the most recent call</span>
<span class="sd">  to either enable_run_metadata or export_run_metadata.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A RunMetadata protocol buffer.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">export_run_metadata</span><span class="p">()</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">collect_graphs</span><span class="p">(</span><span class="n">optimized</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Collects a flat list of pre- or post-optimization graphs.</span>

<span class="sd">  The collected graphs include device placements, which can be useful for</span>
<span class="sd">  testing.</span>

<span class="sd">  Usage:</span>

<span class="sd">  ```</span>
<span class="sd">  @def_function.function</span>
<span class="sd">  def f(x):</span>
<span class="sd">    return x + constant_op.constant(1.)</span>

<span class="sd">  with context.collect_graphs() as graphs:</span>
<span class="sd">    with ops.device(&quot;CPU:0&quot;):</span>
<span class="sd">      f(constant_op.constant(1.))</span>

<span class="sd">  graph, = graphs  # `graph` contains a single GraphDef for inspection</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    optimized: whether to collect optimized graphs or non-optimized graphs</span>
<span class="sd">  Yields:</span>
<span class="sd">    A list of GraphDefs, populated when the context manager exits.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="p">()</span>
  <span class="n">ctx</span><span class="o">.</span><span class="n">enable_graph_collection</span><span class="p">()</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">graphs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">yield</span> <span class="n">graphs</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">export_run_metadata</span><span class="p">()</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">disable_graph_collection</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">graph</span> <span class="ow">in</span> <span class="n">metadata</span><span class="o">.</span><span class="n">function_graphs</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">optimized</span><span class="p">:</span>
      <span class="n">graphs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">post_optimization_graph</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">graphs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">pre_optimization_graph</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_server_def</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">get_server_def</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">set_server_def</span><span class="p">(</span><span class="n">server_def</span><span class="p">):</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">set_server_def</span><span class="p">(</span><span class="n">server_def</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">update_server_def</span><span class="p">(</span><span class="n">server_def</span><span class="p">):</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">update_server_def</span><span class="p">(</span><span class="n">server_def</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_alive</span><span class="p">(</span><span class="n">worker_name</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">check_alive</span><span class="p">(</span><span class="n">worker_name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;experimental.async_scope&quot;</span><span class="p">)</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">async_scope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Context manager for grouping async operations.</span>

<span class="sd">  Ops/function calls inside the scope can return before finishing the actual</span>
<span class="sd">  execution. When exiting the async scope, a synchronization barrier will be</span>
<span class="sd">  automatically added to ensure the completion of all async op and function</span>
<span class="sd">  execution, potentially raising exceptions if async execution results in</span>
<span class="sd">  an error state.</span>

<span class="sd">  Users may write the following code to asynchronuously invoke `train_step_fn`</span>
<span class="sd">  and log the `loss` metric for every `num_steps` steps in a training loop.</span>
<span class="sd">  `train_step_fn` internally consumes data using `iterator.get_next()`, and may</span>
<span class="sd">  throw OutOfRangeError when running out of data. In the case:</span>

<span class="sd">  ```</span>
<span class="sd">  try:</span>
<span class="sd">    with tf.experimental.async_scope():</span>
<span class="sd">      for _ in range(num_steps):</span>
<span class="sd">        # Step function updates the metric `loss` internally</span>
<span class="sd">        train_step_fn()</span>
<span class="sd">  except tf.errors.OutOfRangeError:</span>
<span class="sd">    tf.experimental.async_clear_error()</span>
<span class="sd">  logging.info(&#39;loss =&#39;, loss.numpy())</span>
<span class="sd">  ```</span>

<span class="sd">  Yields:</span>
<span class="sd">    Context manager for grouping async operations.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(haoyuzhang): replace env var once we have a config method to turn on</span>
  <span class="c1"># and off async streaming RPC</span>
  <span class="n">remote_async_env_var</span> <span class="o">=</span> <span class="s2">&quot;TF_ENABLE_EAGER_CLIENT_STREAMING_ENQUEUE&quot;</span>
  <span class="n">old_policy</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">remote_async_env_var</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">remote_async_env_var</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">yield</span>
    <span class="c1"># Note: sync local and remote executors iff the async block does not raise</span>
    <span class="c1"># an exception. Triggering sync after an exception may lead to derived</span>
    <span class="c1"># runtime errors and unexpected exception types.</span>
    <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">sync_executors</span><span class="p">()</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">old_policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">del</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">remote_async_env_var</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">remote_async_env_var</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_policy</span>


<span class="k">def</span> <span class="nf">async_wait</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Sync all async operations and raise any errors during execution.</span>

<span class="sd">  In async execution mode, an op/function call can return before finishing the</span>
<span class="sd">  actual execution. Calling this method creates a synchronization barrier for</span>
<span class="sd">  all async op and function execution. It only returns when all pending nodes</span>
<span class="sd">  are finished, potentially raising exceptions if async execution results in</span>
<span class="sd">  an error state.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">sync_executors</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;experimental.async_clear_error&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">async_clear_error</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Clear pending operations and error statuses in async execution.</span>

<span class="sd">  In async execution mode, an error in op/function execution can lead to errors</span>
<span class="sd">  in subsequent ops/functions that are scheduled but not yet executed. Calling</span>
<span class="sd">  this method clears all pending operations and reset the async execution state.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```</span>
<span class="sd">  while True:</span>
<span class="sd">    try:</span>
<span class="sd">      # Step function updates the metric `loss` internally</span>
<span class="sd">      train_step_fn()</span>
<span class="sd">    except tf.errors.OutOfRangeError:</span>
<span class="sd">      tf.experimental.async_clear_error()</span>
<span class="sd">      break</span>
<span class="sd">  logging.info(&#39;loss =&#39;, loss.numpy())</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">clear_executor_errors</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">add_function</span><span class="p">(</span><span class="n">fdef</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add a function definition to the context.&quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">add_function</span><span class="p">(</span><span class="n">fdef</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">remove_function</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Remove a function from the context.&quot;&quot;&quot;</span>
  <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">remove_function</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_function_def</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">get_function_def</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># Not every user creates a Context via context.context()</span>
<span class="c1"># (for example, enable_eager_execution in python/framework/ops.py),</span>
<span class="c1"># but they do all import this file.  Note that IS_IN_GRAPH_MODE and</span>
<span class="c1"># in_graph_mode are both parameterless functions.</span>
<span class="k">def</span> <span class="nf">_tmp_in_graph_mode</span><span class="p">():</span>
  <span class="k">if</span> <span class="n">context_safe</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Context not yet initialized. Assume graph mode following the</span>
    <span class="c1"># default implementation in `is_in_graph_mode`.</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">return</span> <span class="ow">not</span> <span class="n">executing_eagerly</span><span class="p">()</span>


<span class="n">is_in_graph_mode</span><span class="o">.</span><span class="n">IS_IN_GRAPH_MODE</span> <span class="o">=</span> <span class="n">_tmp_in_graph_mode</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>