

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.eager.def_function &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.eager.def_function</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.eager.def_function</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="c1"># pylint: disable=unidiomatic-typecheck</span>
<span class="sd">&quot;&quot;&quot;Prototype decorator for defining graph functions with eager semantics.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">weakref</span>

<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tfe</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">function</span> <span class="k">as</span> <span class="n">function_lib</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">lift_to_graph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">func_graph</span> <span class="k">as</span> <span class="n">func_graph_module</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">trackable</span>


<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">object_identity</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_decorator</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="n">FREQUENT_TRACING_WARNING_MAX_CALL_HISTORY</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">FREQUENT_TRACING_WARNING_THRESHOLD</span> <span class="o">=</span> <span class="mi">5</span>


<span class="k">class</span> <span class="nc">_CallCounter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Class keeping track of how many recent calls triggered tracing.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_call_history</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_call_history</span> <span class="o">=</span> <span class="n">max_call_history</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">call_count</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">def</span> <span class="nf">called_with_tracing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">call_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_count</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_call_history</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">call_count</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>

  <span class="k">def</span> <span class="nf">called_without_tracing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># We don&#39;t count tracing when users load a concrete function directly or</span>
    <span class="c1"># call get_concrete_function, so the first call can be not a tracing call.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">call_count</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="k">def</span> <span class="nf">get_tracing_count</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_calls_per_tracings</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">UnliftedInitializerVariable</span><span class="p">(</span><span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">UninitializedVariable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Variable which does not lift its initializer out of function context.</span>

<span class="sd">  Instances of this variable, when created, build a graph which runs their</span>
<span class="sd">  initializer inside a tf.cond(is_initialized) block.</span>

<span class="sd">  This can only be created inside a defun called from (eventually) eager</span>
<span class="sd">  mode. That is, non-function-building graphs are not supported.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">add_initializers_to</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">lifted_initializer_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="o">**</span><span class="n">unused_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called.</span>
<span class="sd">        (Note that initializer functions from init_ops.py must first be bound</span>
<span class="sd">         to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, GradientTapes automatically watch uses of this</span>
<span class="sd">        Variable.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type.</span>
<span class="sd">        If None, either the datatype will be kept (if initial_value is</span>
<span class="sd">       a Tensor) or float32 will be used (if it is a Python object convertible</span>
<span class="sd">       to a Tensor).</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      add_initializers_to: if not None and not in legacy graph mode, the</span>
<span class="sd">        initializer tensor will be added to this map in addition to adding the</span>
<span class="sd">        assignment to the function.</span>
<span class="sd">      lifted_initializer_graph: FuncGraph to try to lift initializers to.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>
<span class="sd">      RuntimeError: If called outside of a function definition.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">inside_function</span><span class="p">():</span>
      <span class="c1"># If we&#39;ve been init_scope()d out of the function definition nothing to do</span>
      <span class="c1"># here; we can&#39;t really do the capturing or conditional logic.</span>
      <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">ResourceVariable</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
          <span class="bp">self</span><span class="p">,</span> <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">)</span>
      <span class="k">return</span>
    <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;initial_value must be specified.&quot;</span><span class="p">)</span>
    <span class="n">init_from_fn</span> <span class="o">=</span> <span class="n">callable</span><span class="p">(</span><span class="n">initial_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">constraint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The `constraint` argument must be a callable.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">trackable</span><span class="o">.</span><span class="n">CheckpointInitialValue</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_initialize_trackable</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_update_uid</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">checkpoint_position</span><span class="o">.</span><span class="n">restore_uid</span>
      <span class="n">initial_value</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">wrapped_value</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="p">[]</span>
                        <span class="k">if</span> <span class="n">init_from_fn</span> <span class="k">else</span> <span class="p">[</span><span class="n">initial_value</span><span class="p">])</span> <span class="k">as</span> <span class="n">scope_name</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Initializer&quot;</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">initial_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
            <span class="n">initial_value</span><span class="p">()</span> <span class="k">if</span> <span class="n">init_from_fn</span> <span class="k">else</span> <span class="n">initial_value</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;initial_value&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">assert</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

      <span class="c1"># Don&#39;t use `shape or initial_value.shape` since TensorShape has</span>
      <span class="c1"># overridden `__bool__`.</span>
      <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Use the constructor for UninitializedVariable to start. Outside the name</span>
    <span class="c1"># scope so we don&#39;t double up the prefix.</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">UnliftedInitializerVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="n">extra_handle_data</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
        <span class="o">**</span><span class="n">unused_kwargs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">scope_name</span><span class="p">):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
          <span class="n">outer_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
        <span class="n">func_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
        <span class="n">function_placeholders</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">func_graph</span><span class="o">.</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">internal_captures</span><span class="p">)</span>
        <span class="n">placeholder_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">function_placeholders</span><span class="p">])</span>
        <span class="n">lifted_initializer</span> <span class="o">=</span> <span class="n">lift_to_graph</span><span class="o">.</span><span class="n">lift_to_graph</span><span class="p">(</span>
            <span class="p">[</span><span class="n">initial_value</span><span class="p">],</span> <span class="n">outer_graph</span><span class="p">,</span>
            <span class="n">disallowed_placeholders</span><span class="o">=</span><span class="n">placeholder_ops</span><span class="p">)[</span><span class="n">initial_value</span><span class="p">]</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">lifted_initializer</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;IsInitialized&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_initialized_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">))</span>
          <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Assign&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">n</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">):</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span> <span class="o">=</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">lifted_initializer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="c1"># In this case, both current scope and init scope are eager.</span>
        <span class="c1"># Assign_variable_op will be executed immediately. So we don&#39;t need to</span>
        <span class="c1"># add it to &quot;add_initializers_to&quot; to lift it out.</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Assign&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">n</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">):</span>
          <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Init scope is eager but current scope is graph. We will lift out this</span>
        <span class="c1"># variable by addint it into &quot;add_initializers_to&quot;.</span>
        <span class="k">if</span> <span class="n">add_initializers_to</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">add_initializers_to</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">))</span>

        <span class="k">def</span> <span class="nf">assign_fn</span><span class="p">():</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Assign&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">n</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">):</span>
            <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
                <span class="n">initial_value</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
            <span class="c1"># Returning values to keep tf.cond happy.</span>
          <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">not_assign_fn</span><span class="p">():</span>
          <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Note: this cond is always guaranteed to run because we&#39;re inside a</span>
        <span class="c1"># defun which will insert automatic control dependencies. It will only</span>
        <span class="c1"># execute assign_fn if lifting failed.</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>

        <span class="c1"># Capture the handle ahead of time in order to avoid querying the shape</span>
        <span class="c1"># of the handle which helps async execution performance</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">capture</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">())</span>
        <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
            <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">),</span>
            <span class="n">not_assign_fn</span><span class="p">,</span> <span class="n">assign_fn</span><span class="p">)</span>


<span class="n">RUN_FUNCTIONS_EAGERLY</span> <span class="o">=</span> <span class="kc">False</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;config.experimental_run_functions_eagerly&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">run_functions_eagerly</span><span class="p">(</span><span class="n">run_eagerly</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Enables / disables eager execution of `tf.function`s.</span>

<span class="sd">  Calling `tf.config.experimental_run_functions_eagerly(True)` will make all</span>
<span class="sd">  invocations of `tf.function` run eagerly instead of running as a traced graph</span>
<span class="sd">  function.</span>

<span class="sd">  This can be useful for debugging or profiling. For example, let&#39;s say you</span>
<span class="sd">  implemented a simple iterative sqrt function, and you want to collect the</span>
<span class="sd">  intermediate values and plot the convergence.  Appending the values to a list</span>
<span class="sd">  in `@tf.function` normally wouldn&#39;t work since it will just record the Tensors</span>
<span class="sd">  being traced, not the values.  Instead, you can do the following.</span>

<span class="sd">  &gt;&gt;&gt; ys = []</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def sqrt(x):</span>
<span class="sd">  ...   y = x / 2</span>
<span class="sd">  ...   d = y</span>
<span class="sd">  ...   for _ in range(10):</span>
<span class="sd">  ...     d /= 2</span>
<span class="sd">  ...     if y * y &lt; x:</span>
<span class="sd">  ...       y += d</span>
<span class="sd">  ...     else:</span>
<span class="sd">  ...       y -= d</span>
<span class="sd">  ...     ys.append(y.numpy())</span>
<span class="sd">  ...   return y</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(True)</span>
<span class="sd">  &gt;&gt;&gt; sqrt(tf.constant(2.))</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4150391&gt;</span>
<span class="sd">  &gt;&gt;&gt; ys</span>
<span class="sd">  [1.5, 1.25, 1.375, 1.4375, 1.40625, 1.421875, 1.4140625, 1.4179688, 1.4160156,</span>
<span class="sd">  1.4150391]</span>
<span class="sd">  &gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(False)</span>

<span class="sd">  Calling `tf.config.experimental_run_functions_eagerly(False)` will undo this</span>
<span class="sd">  behavior.</span>

<span class="sd">  Args:</span>
<span class="sd">    run_eagerly: Boolean. Whether to run functions eagerly.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">global</span> <span class="n">RUN_FUNCTIONS_EAGERLY</span>
  <span class="n">RUN_FUNCTIONS_EAGERLY</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">run_eagerly</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;config.experimental_functions_run_eagerly&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">functions_run_eagerly</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the value of the `experimental_run_functions_eagerly` setting.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">RUN_FUNCTIONS_EAGERLY</span>


<span class="k">class</span> <span class="nc">FunctionDeleter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func_graph</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">func_graph</span> <span class="o">=</span> <span class="n">func_graph</span>

  <span class="k">def</span> <span class="nf">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">func_graph_module</span><span class="o">.</span><span class="n">dismantle_func_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func_graph</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>  <span class="c1"># pylint: disable=bare-except</span>
      <span class="c1"># Note: bare except here because this can be noisy at shutdown time.</span>
      <span class="k">pass</span>


<span class="k">class</span> <span class="nc">Function</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper class for the graph functions defined for a Python function.</span>

<span class="sd">  See the documentation for `tf.function` for more information on the semantics</span>
<span class="sd">  of defined functions.</span>

<span class="sd">  `Function` is thread-compatible.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">python_function</span><span class="p">,</span>
               <span class="n">name</span><span class="p">,</span>
               <span class="n">input_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">autograph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">experimental_implements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">experimental_autograph_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">experimental_relax_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">experimental_compile</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes a `Function`.</span>

<span class="sd">    Args:</span>
<span class="sd">      python_function: the function to be wrapped.</span>
<span class="sd">      name: the name given to it.</span>
<span class="sd">      input_signature: a possibly nested sequence of `TensorSpec` objects</span>
<span class="sd">        specifying the input signature of this function. If `None`, a separate</span>
<span class="sd">        function is instantiated for each inferred input signature.</span>
<span class="sd">      autograph: whether `python_function` should be converted to graph mode.</span>
<span class="sd">        See https://www.tensorflow.org/guide/autograph for more information.</span>
<span class="sd">      experimental_implements: If provided, contains a name of a &quot;known&quot;</span>
<span class="sd">        function this implements. For example &quot;mycompany.my_recurrent_cell&quot;.</span>
<span class="sd">        This is stored as an attribute in the serialized representation,</span>
<span class="sd">        which can then be detected and manipulated when processing serialized</span>
<span class="sd">        graph.</span>
<span class="sd">        See</span>
<span class="sd">        https://github.com/tensorflow/community/blob/master/rfcs/20190610-standardizing-composite_ops.md</span>
<span class="sd">        for details.  For an example of utilizing this attribute see:</span>
<span class="sd">        https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc</span>
<span class="sd">        The code above automatically detects and substitutes function that</span>
<span class="sd">        implements &quot;embedded_matmul&quot; and allows TFLite to substitute its own</span>
<span class="sd">        implementations. For instance, a tensorflow user can use this</span>
<span class="sd">         attribute to mark that their function also implements</span>
<span class="sd">        `embedded_matmul``` (perhaps more efficiently!)</span>
<span class="sd">        by specifying it using this flag.</span>

<span class="sd">        ```python</span>
<span class="sd">        @tf.function(</span>
<span class="sd">            experimental_implements=&quot;lingvo.SimpleEmbeddingLayer.EmbMatmul&quot;)</span>
<span class="sd">        def embedding_matmul(a, b):</span>
<span class="sd">           # custom implementation here</span>
<span class="sd">        ```</span>

<span class="sd">      experimental_autograph_options: optional tuple of</span>
<span class="sd">        tensorflow.autograph.Feature values. Allows enabling additional</span>
<span class="sd">        conversion options when autograph is set to True.</span>
<span class="sd">      experimental_relax_shapes: When true, argument shapes may be relaxed to</span>
<span class="sd">        avoid unnecessary retracing.</span>
<span class="sd">      experimental_compile: If `True`, compiles the function using XLA</span>
<span class="sd">        (see https://tensorflow.org/xla). XLA performs compiler optimizations,</span>
<span class="sd">        such as fusion, and attempts to emit more efficient code. This may</span>
<span class="sd">        drastically improve the performance. If set to `True`,</span>
<span class="sd">        the whole function needs to be compilable by XLA, or an</span>
<span class="sd">        `errors.InvalidArgumentError` is thrown.</span>
<span class="sd">        If `None` (default), compiles the function with XLA when running on TPU</span>
<span class="sd">        and goes through the regular function execution path when running on</span>
<span class="sd">        other devices.</span>
<span class="sd">        If `False`, executes the function in a regular way (graph rewrite</span>
<span class="sd">        passes are applied, kernels are dispatched one-by-one by the TensorFlow</span>
<span class="sd">        executor). Set this value to `False` when directly running a</span>
<span class="sd">        multi-device function on TPUs (e.g. two TPU cores, one TPU core and its</span>
<span class="sd">        host CPU).</span>
<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if `input_signature` is not None and the `python_function`&#39;s</span>
<span class="sd">        argspec has keyword arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span> <span class="o">=</span> <span class="n">python_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_function_spec</span> <span class="o">=</span> <span class="n">function_lib</span><span class="o">.</span><span class="n">FunctionSpec</span><span class="o">.</span><span class="n">from_function_and_signature</span><span class="p">(</span>
        <span class="n">python_function</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_implements</span> <span class="o">=</span> <span class="n">experimental_implements</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_autograph</span> <span class="o">=</span> <span class="n">autograph</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_experimental_autograph_options</span> <span class="o">=</span> <span class="n">experimental_autograph_options</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_experimental_relax_shapes</span> <span class="o">=</span> <span class="n">experimental_relax_shapes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span> <span class="o">=</span> <span class="n">experimental_compile</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_descriptor_cache</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">WeakKeyDictionary</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_signature</span> <span class="o">=</span> <span class="n">input_signature</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_call_counter</span> <span class="o">=</span> <span class="n">_CallCounter</span><span class="p">(</span><span class="n">FREQUENT_TRACING_WARNING_MAX_CALL_HISTORY</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_defun_with_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a defun wrapped inside a variable creator scope.&quot;&quot;&quot;</span>

    <span class="n">weak_wrapped_fn</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">def</span> <span class="nf">wrapped_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Wraps `self._python_function` in a variable creator scope.&quot;&quot;&quot;</span>
      <span class="c1"># We register a variable creator with reduced priority. If an outer</span>
      <span class="c1"># variable creator is just modifying keyword arguments to the variable</span>
      <span class="c1"># constructor, this will work harmoniously. Since the `scope` registered</span>
      <span class="c1"># here actually creates the variable, it taking priority would otherwise</span>
      <span class="c1"># ignore the outer creator.</span>
      <span class="c1">#</span>
      <span class="c1"># If an outer variable creator calls the variable constructor manually,</span>
      <span class="c1"># for example creating a MirroredVariable, then they won&#39;t call our</span>
      <span class="c1"># creator. This means we won&#39;t be able to trace the initialization graph,</span>
      <span class="c1"># and so variable initializers can&#39;t depend on function arguments. This is</span>
      <span class="c1"># better than the alternative, tracing the initialization graph but giving</span>
      <span class="c1"># the user a variable type they didn&#39;t want.</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_variable_creator_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="c1"># __wrapped__ allows AutoGraph to swap in a converted function. We give</span>
        <span class="c1"># the function a weak reference to itself to avoid a reference cycle.</span>
        <span class="k">return</span> <span class="n">weak_wrapped_fn</span><span class="p">()</span><span class="o">.</span><span class="n">__wrapped__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
    <span class="n">weak_wrapped_fn</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_defun</span><span class="p">(</span><span class="n">tf_decorator</span><span class="o">.</span><span class="n">make_decorator</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span><span class="p">,</span>
        <span class="n">wrapped_fn</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_defun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a defun generated from the input function.&quot;&quot;&quot;</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implements</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">attributes</span><span class="p">[</span><span class="n">function_lib</span><span class="o">.</span><span class="n">IMPLEMENTS_ATTRIBUTE_NAME</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_implements</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">attributes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_XlaMustCompile</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span><span class="p">))</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span><span class="p">:</span>
        <span class="n">attributes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_noinline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TF_IsXlaEnabled</span><span class="p">():</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Attempting to use experimental_compile, &quot;</span>
                           <span class="s2">&quot;but XLA support is not linked in. &quot;</span>
                           <span class="s2">&quot;Rebuild with --define=with_xla_support=true.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">attributes</span><span class="p">:</span>
      <span class="n">attributes</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">function_lib</span><span class="o">.</span><span class="n">defun_with_attributes</span><span class="p">(</span>
        <span class="n">fn</span><span class="p">,</span>
        <span class="n">input_signature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_signature</span><span class="p">,</span>
        <span class="n">attributes</span><span class="o">=</span><span class="n">attributes</span><span class="p">,</span>
        <span class="n">autograph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_autograph</span><span class="p">,</span>
        <span class="n">experimental_autograph_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_experimental_autograph_options</span><span class="p">,</span>
        <span class="n">experimental_compile</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span><span class="p">,</span>
        <span class="n">experimental_relax_shapes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_experimental_relax_shapes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwds</span><span class="p">,</span> <span class="n">add_initializers_to</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes, on the first call.</span>

<span class="sd">    Creates two `Function`s, one that will allow creation of variables</span>
<span class="sd">    and one that won&#39;t.</span>

<span class="sd">    Additionally runs a trace for the `Function` that allows creation</span>
<span class="sd">    of variables.</span>

<span class="sd">    Args:</span>
<span class="sd">      args: Arguments to the underlying python callable.</span>
<span class="sd">      kwds: Keyword arguments to the python callable.</span>
<span class="sd">      add_initializers_to: Where to collect variable initializers, if not None.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">created_variables</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lifted_initializer_graph</span> <span class="o">=</span> <span class="n">func_graph_module</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">(</span><span class="s2">&quot;initializer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">variable_capturing_scope</span><span class="p">(</span><span class="n">unused_next_creator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Creates UnliftedInitializerVariables and saves references to them.&quot;&quot;&quot;</span>
      <span class="n">v</span> <span class="o">=</span> <span class="n">UnliftedInitializerVariable</span><span class="p">(</span>
          <span class="n">add_initializers_to</span><span class="o">=</span><span class="n">add_initializers_to</span><span class="p">,</span>
          <span class="n">lifted_initializer_graph</span><span class="o">=</span><span class="n">lifted_initializer_graph</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
      <span class="n">created_variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">v</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span> <span class="o">=</span> <span class="n">created_variables</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_defun_with_scope</span><span class="p">(</span><span class="n">variable_capturing_scope</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="c1"># Force the definition of the function for these arguments</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lifted_initializer_graph</span> <span class="o">=</span> <span class="n">lifted_initializer_graph</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_deleter</span> <span class="o">=</span> <span class="n">FunctionDeleter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lifted_initializer_graph</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_concrete_stateful_fn</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="o">.</span><span class="n">_get_concrete_function_internal_garbage_collected</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">invalid_creator_scope</span><span class="p">(</span><span class="o">*</span><span class="n">unused_args</span><span class="p">,</span> <span class="o">**</span><span class="n">unused_kwds</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Disables variable creation.&quot;&quot;&quot;</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;tf.function-decorated function tried to create &quot;</span>
          <span class="s2">&quot;variables on non-first call.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_defun_with_scope</span><span class="p">(</span><span class="n">invalid_creator_scope</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_clone</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">python_function</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Function</span><span class="p">(</span>
        <span class="n">python_function</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span>
                         <span class="k">if</span> <span class="n">python_function</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">python_function</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
        <span class="n">input_signature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_signature</span><span class="p">,</span>
        <span class="n">autograph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_autograph</span><span class="p">,</span>
        <span class="n">experimental_implements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_implements</span><span class="p">,</span>
        <span class="n">experimental_autograph_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_experimental_autograph_options</span><span class="p">,</span>
        <span class="n">experimental_relax_shapes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_experimental_relax_shapes</span><span class="p">,</span>
        <span class="n">experimental_compile</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_decorate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decorator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Allows the captured Python function to be decorated in place.</span>

<span class="sd">    This method is only safe to call when the Function has not been called by a</span>
<span class="sd">    user. It makes sense to use this method to push a decorator into the</span>
<span class="sd">    function rather than wrapping the function in the decorator.</span>

<span class="sd">    We use this in tf.Module to allow user annotated `tf.functions` to remain as</span>
<span class="sd">    `Function` objects but still automatically enter the Module name_scope</span>
<span class="sd">    when they are evaluated like all other methods.</span>

<span class="sd">    Args:</span>
<span class="sd">      decorator: A callable accepting a single argument which is the function</span>
<span class="sd">        to decorate and returning a callable result.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the function has been called a ValueError is raised.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Functions cannot be decorated after they have been traced.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span> <span class="o">=</span> <span class="n">decorator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_function_spec</span> <span class="o">=</span> <span class="n">function_lib</span><span class="o">.</span><span class="n">FunctionSpec</span><span class="o">.</span><span class="n">from_function_and_signature</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_signature</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_tracing_count</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="o">.</span><span class="n">tracing_count</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">result</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="o">.</span><span class="n">tracing_count</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calls the graph function and warn too frequent tracings.&quot;&quot;&quot;</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">RUN_FUNCTIONS_EAGERLY</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>

    <span class="n">tracing_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_tracing_count</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span> <span class="ow">and</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="n">control_flow_util</span><span class="o">.</span><span class="n">GraphOrParentsInXlaContext</span><span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())):</span>
      <span class="c1"># V2 control flow relies on XLAControlFlowContext to generate a</span>
      <span class="c1"># XLA-compatible function graph. If the function is already called inside</span>
      <span class="c1"># an XLA context, we don&#39;t create nested XLA context.</span>
      <span class="n">xla_context</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">XLAControlFlowContext</span><span class="p">()</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">xla_context</span><span class="o">.</span><span class="n">Enter</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
      <span class="k">finally</span><span class="p">:</span>
        <span class="n">xla_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">tracing_count</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_tracing_count</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_call_counter</span><span class="o">.</span><span class="n">called_without_tracing</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">result</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_call_counter</span><span class="o">.</span><span class="n">called_with_tracing</span><span class="p">()</span>
    <span class="n">recent_tracing_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_counter</span><span class="o">.</span><span class="n">get_tracing_count</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">recent_tracing_count</span> <span class="o">&gt;=</span> <span class="n">FREQUENT_TRACING_WARNING_THRESHOLD</span><span class="p">:</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> out of the last </span><span class="si">{}</span><span class="s2"> calls to </span><span class="si">{}</span><span class="s2"> triggered tf.function retracing. &quot;</span>
          <span class="s2">&quot;Tracing is expensive and the excessive number of tracings is likely &quot;</span>
          <span class="s2">&quot;due to passing python objects instead of tensors. Also, tf.function &quot;</span>
          <span class="s2">&quot;has experimental_relax_shapes=True option that relaxes argument &quot;</span>
          <span class="s2">&quot;shapes that can avoid unnecessary retracing. Please refer to &quot;</span>
          <span class="s2">&quot;https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args&quot;</span>
          <span class="s2">&quot; and https://www.tensorflow.org/api_docs/python/tf/function for more &quot;</span>
          <span class="s2">&quot;details.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">recent_tracing_count</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_counter</span><span class="o">.</span><span class="n">call_count</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calls the graph function.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span><span class="p">:</span>
      <span class="c1"># Release the lock early so that multiple threads can perform the call</span>
      <span class="c1"># in parallel.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
      <span class="c1"># In this case we have created variables on the first call, so we run the</span>
      <span class="c1"># defunned version which is guaranteed to never create variables.</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>  <span class="c1"># pylint: disable=not-callable</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Release the lock early so that multiple threads can perform the call</span>
      <span class="c1"># in parallel.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
      <span class="c1"># In this case we have not created variables on the first call. So we can</span>
      <span class="c1"># run the first trace but we should fail if variables are created.</span>
      <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Creating variables on a non-first call to a function&quot;</span>
                         <span class="s2">&quot; decorated with tf.function.&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">results</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># This is the first call of __call__, so we have to initialize.</span>
      <span class="n">initializers</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwds</span><span class="p">,</span> <span class="n">add_initializers_to</span><span class="o">=</span><span class="n">initializers</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># At this point we know that the initialization is complete (or less</span>
      <span class="c1"># interestingly an exception was raised) so we no longer need a lock.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Attempt to initialize variables eagerly and without conds by lifting</span>
        <span class="c1"># out initialization graphs. This is the only initialization strategy</span>
        <span class="c1"># compatible with XLA at the moment.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_uninitialized_variables</span><span class="p">(</span><span class="n">initializers</span><span class="p">)</span>
      <span class="k">except</span> <span class="n">lift_to_graph</span><span class="o">.</span><span class="n">UnliftableError</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># Fall through to cond-based initialization.</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Lifting succeeded, so variables are initialized and we can run the</span>
        <span class="c1"># stateless function.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">canon_args</span><span class="p">,</span> <span class="n">canon_kwds</span> <span class="o">=</span> \
          <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="o">.</span><span class="n">_function_spec</span><span class="o">.</span><span class="n">canonicalize_function_inputs</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
              <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
      <span class="c1"># If we did not create any variables the trace we have is good enough.</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concrete_stateful_fn</span><span class="o">.</span><span class="n">_filtered_call</span><span class="p">(</span><span class="n">canon_args</span><span class="p">,</span> <span class="n">canon_kwds</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="k">def</span> <span class="nf">fn_with_cond</span><span class="p">(</span><span class="o">*</span><span class="n">inner_args</span><span class="p">,</span> <span class="o">**</span><span class="n">inner_kwds</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Conditionally runs initialization if it&#39;s needed.&quot;&quot;&quot;</span>
      <span class="n">condition</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">for</span> <span class="n">wr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span><span class="p">:</span>
        <span class="n">variable</span> <span class="o">=</span> <span class="n">wr</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">variable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="s2">&quot;A tf.Variable created inside your tf.function has been&quot;</span>
              <span class="s2">&quot; garbage-collected. Your code needs to keep Python references&quot;</span>
              <span class="s2">&quot; to variables created inside `tf.function`s.</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;A common way to raise this error is to create and return a&quot;</span>
              <span class="s2">&quot; variable only referenced inside your function:</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;@tf.function</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;def f():</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;  v = tf.Variable(1.0)</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;  return v</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;v = f()  # Crashes with this error message!</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;The reason this crashes is that @tf.function annotated&quot;</span>
              <span class="s2">&quot; function returns a **`tf.Tensor`** with the **value** of the&quot;</span>
              <span class="s2">&quot; variable when the function is called rather than the&quot;</span>
              <span class="s2">&quot; variable instance itself. As such there is no code holding a&quot;</span>
              <span class="s2">&quot; reference to the `v` created inside the function and Python&quot;</span>
              <span class="s2">&quot; garbage collects it.</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;The simplest way to fix this issue is to create variables&quot;</span>
              <span class="s2">&quot; outside the function and capture them:</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;v = tf.Variable(1.0)</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;@tf.function</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;def f():</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;  return v</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;f()  # &lt;tf.Tensor: numpy=1.&gt;</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;v.assign_add(1.)</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="s2">&quot;f()  # &lt;tf.Tensor: numpy=2.&gt;&quot;</span><span class="p">)</span>
        <span class="n">condition</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
            <span class="n">condition</span><span class="p">,</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span>
                <span class="n">variable</span><span class="o">.</span><span class="n">handle</span><span class="p">))</span>
      <span class="c1"># We want to call stateless_fn if possible because it avoids recomputing</span>
      <span class="c1"># potentially expensive initializers.</span>
      <span class="k">return</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
          <span class="n">condition</span><span class="p">,</span>
          <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="p">(</span><span class="o">*</span><span class="n">inner_args</span><span class="p">,</span> <span class="o">**</span><span class="n">inner_kwds</span><span class="p">),</span>
          <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_concrete_stateful_fn</span><span class="o">.</span><span class="n">_filtered_call</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
                            <span class="n">inner_args</span><span class="p">,</span> <span class="n">inner_kwds</span><span class="p">))</span>

    <span class="c1"># We&#39;ve created variables and are unable to lift the initialization graphs,</span>
    <span class="c1"># so we fall back to initializing with conds while running the function.</span>
    <span class="n">canon_args</span><span class="p">,</span> <span class="n">canon_kwds</span> <span class="o">=</span> \
        <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="o">.</span><span class="n">_function_spec</span><span class="o">.</span><span class="n">canonicalize_function_inputs</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">function_lib</span><span class="o">.</span><span class="n">defun</span><span class="p">(</span><span class="n">fn_with_cond</span><span class="p">)(</span><span class="o">*</span><span class="n">canon_args</span><span class="p">,</span> <span class="o">**</span><span class="n">canon_kwds</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">python_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The python function wrapped in this tf.function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_python_function</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_spec</span><span class="o">.</span><span class="n">input_signature</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">function_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_spec</span>

  <span class="k">def</span> <span class="nf">_initialize_uninitialized_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initializers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make and call a `ConcreteFunction` which initializes variables.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">initializers</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="c1"># Note: using defun here avoids an infinite recursion.</span>
    <span class="c1"># Most of the code in this function runs eagerly with init_scope, where</span>
    <span class="c1"># autograph is not necessary.</span>
    <span class="nd">@function_lib</span><span class="o">.</span><span class="n">defun</span><span class="p">(</span><span class="n">autograph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">initialize_variables</span><span class="p">():</span>
      <span class="n">op_map</span> <span class="o">=</span> <span class="n">object_identity</span><span class="o">.</span><span class="n">ObjectIdentityDictionary</span><span class="p">()</span>
      <span class="c1"># Stack all the var_is_initialized values into one tensor and interpret the</span>
      <span class="c1"># numpy value. This will reduce the number of RPCs between client and</span>
      <span class="c1"># worker in the remote case.</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
        <span class="n">var_is_initialized</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">initializers</span><span class="p">:</span>
          <span class="n">var_is_initialized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
              <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">handle</span><span class="p">))</span>
        <span class="n">var_is_initialized</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">var_is_initialized</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

      <span class="n">inits</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">init</span><span class="p">),</span> <span class="n">is_initialized</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">initializers</span><span class="p">,</span> <span class="n">var_is_initialized</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
          <span class="k">if</span> <span class="n">is_initialized</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">inits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">inits</span><span class="p">:</span>
        <span class="n">op_map</span> <span class="o">=</span> <span class="n">lift_to_graph</span><span class="o">.</span><span class="n">lift_to_graph</span><span class="p">(</span>
            <span class="n">inits</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="n">op_map</span><span class="o">=</span><span class="n">op_map</span><span class="p">)</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">init</span><span class="p">),</span> <span class="n">is_initialized</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">initializers</span><span class="p">,</span> <span class="n">var_is_initialized</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
          <span class="k">if</span> <span class="n">is_initialized</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">v</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">op_map</span><span class="p">[</span><span class="n">init</span><span class="p">],</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">initialize_variables</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()()</span>

  <span class="k">def</span> <span class="nf">get_initialization_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a `ConcreteFunction` which initializes this function&#39;s variables.</span>

<span class="sd">    Requires that this function hasn&#39;t been accessed yet through either calling</span>
<span class="sd">    it or calling get_concrete_function. Fails if we cannot build an initializer</span>
<span class="sd">    function which does not depend on the concrete values of the inputs to this</span>
<span class="sd">    function.</span>

<span class="sd">    Note that running this function will overwrite any values currently assigned</span>
<span class="sd">    to variables, for example restores from a checkpoint.</span>

<span class="sd">    Args:</span>
<span class="sd">      *args: arguments to the underlying python callable.</span>
<span class="sd">      **kwargs: keyword arguments to the python callable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `ConcreteFunction` object which initializes the variables of this</span>
<span class="sd">      function.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: if called after the variables have been initialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;get_initialization_function cannot be called after the function &quot;</span>
            <span class="s2">&quot;has been used&quot;</span><span class="p">)</span>
      <span class="c1"># Here we trace the function, collect the initializers, and attempt to</span>
      <span class="c1"># extract them and run them eagerly. Fail only if we cannot do so.</span>
      <span class="n">initializers</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">add_initializers_to</span><span class="o">=</span><span class="n">initializers</span><span class="p">)</span>

    <span class="c1"># Note: using defun here avoids an infinite recursion.</span>
    <span class="nd">@function_lib</span><span class="o">.</span><span class="n">defun</span>
    <span class="k">def</span> <span class="nf">initialize_variables</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">init</span> <span class="ow">in</span> <span class="n">initializers</span><span class="p">:</span>
        <span class="n">v</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
            <span class="n">lift_to_graph</span><span class="o">.</span><span class="n">lift_to_graph</span><span class="p">([</span><span class="n">init</span><span class="p">],</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())[</span><span class="n">init</span><span class="p">],</span>
            <span class="n">read_value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">initialize_variables</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_list_all_concrete_functions_for_serialization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns all concrete functions for serialization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of instances of `ConcreteFunction`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()</span>
    <span class="n">concrete_functions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="p">:</span>
      <span class="n">concrete_functions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="o">.</span><span class="n">_function_cache</span><span class="o">.</span><span class="n">all_values</span><span class="p">())</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="p">:</span>
      <span class="n">concrete_functions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="o">.</span><span class="n">_function_cache</span><span class="o">.</span><span class="n">all_values</span><span class="p">())</span>
    <span class="c1"># pylint: enable=protected-access</span>
    <span class="n">seen_signatures</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">concrete_function</span> <span class="ow">in</span> <span class="n">concrete_functions</span><span class="p">:</span>
      <span class="n">signature</span> <span class="o">=</span> <span class="n">concrete_function</span><span class="o">.</span><span class="n">structured_input_signature</span>
      <span class="n">flattened</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">signature</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
          <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">func_graph_module</span><span class="o">.</span><span class="n">UnknownArgument</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">flattened</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Unsupported signature for serialization: </span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="p">)</span>
        <span class="k">continue</span>
      <span class="n">equal_to_signature</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
          <span class="n">function_lib</span><span class="o">.</span><span class="n">is_same_structure</span><span class="p">,</span> <span class="n">signature</span><span class="p">,</span> <span class="n">check_values</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">equal_to_signature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">seen_signatures</span><span class="p">):</span>
        <span class="n">seen_signatures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">signature</span><span class="p">)</span>

    <span class="c1"># Re-create concrete functions for these signatures. Re-creating ensures</span>
    <span class="c1"># that if the cache key has changed, the function will be traced again.</span>
    <span class="n">concrete_functions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="n">seen_signatures</span><span class="p">:</span>
      <span class="n">concrete_functions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">concrete_functions</span>

  <span class="k">def</span> <span class="nf">_get_concrete_function_garbage_collected</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a `ConcreteFunction` specialized to inputs and execution context.</span>

<span class="sd">    Unlike `get_concrete_function(...)`, the graph will be deleted when the</span>
<span class="sd">    returned function is deleted.  It&#39;s useful to avoid creating a reference</span>
<span class="sd">    cycle when you know for sure that the graph will be no longer used without</span>
<span class="sd">    the returned function.</span>

<span class="sd">    Args:</span>
<span class="sd">      *args: inputs to specialize on.</span>
<span class="sd">      **kwargs: inputs to specialize on.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A TensorFlow function which takes exactly one `tf.Tensor` per argument.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if this object has not yet been called on concrete values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">initializers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">add_initializers_to</span><span class="o">=</span><span class="n">initializers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_uninitialized_variables</span><span class="p">(</span><span class="n">initializers</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span><span class="p">:</span>
      <span class="c1"># In this case we have created variables on the first call, so we run the</span>
      <span class="c1"># defunned version which is guaranteed to never create variables.</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="o">.</span><span class="n">_get_concrete_function_garbage_collected</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># In this case we have not created variables on the first call. So we can</span>
      <span class="c1"># run the first trace but we should fail if variables are created.</span>
      <span class="n">concrete</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span><span class="o">.</span><span class="n">_get_concrete_function_garbage_collected</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_created_variables</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Creating variables on a non-first call to a function&quot;</span>
                         <span class="s2">&quot; decorated with tf.function.&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">concrete</span>

  <span class="k">def</span> <span class="nf">get_concrete_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a `ConcreteFunction` specialized to inputs and execution context.</span>

<span class="sd">    If this `Function` was created with an `input_signature`, `args` and</span>
<span class="sd">    `kwargs` may be omitted. With an input signature there is only one</span>
<span class="sd">    concrete function associated with this `Function`.</span>

<span class="sd">    If there is no fixed `input_signature` associated with this</span>
<span class="sd">    `Function`, positional and keyword arguments to `get_concrete_function`</span>
<span class="sd">    follow the same rules as input signature specification, with `tf.TensorSpec`</span>
<span class="sd">    objects describing `tf.Tensor`s which will be passed to the concrete</span>
<span class="sd">    function.</span>

<span class="sd">    Each `tf.Tensor` argument to the concrete function must have a unique name,</span>
<span class="sd">    either because it is the only one associated with a named argument of the</span>
<span class="sd">    Python function or because an explicit `name=` was passed to its</span>
<span class="sd">    `tf.TensorSpec` object. These names become the argument names for the</span>
<span class="sd">    concrete function.</span>

<span class="sd">    Arguments to the concrete function may always be specified as keyword</span>
<span class="sd">    arguments, naming the Tensor input. Positional arguments may be used instead</span>
<span class="sd">    when each preceding argument to the Python function is a Tensor.</span>

<span class="sd">    ```python</span>
<span class="sd">    @tf.function</span>
<span class="sd">    def f(x):</span>
<span class="sd">      return x</span>

<span class="sd">    f_concrete = f.get_concrete_function(tf.TensorSpec([], tf.float64))</span>
<span class="sd">    f_concrete(tf.constant(1.))</span>
<span class="sd">    f_concrete(x=tf.constant(1.))</span>
<span class="sd">    ```</span>

<span class="sd">    Nested structures containing Tensors may be specified when retrieving</span>
<span class="sd">    concrete functions. Structures with multiple Tensors are expanded into</span>
<span class="sd">    multiple arguments of the concrete function. Since multiple concrete</span>
<span class="sd">    function arguments are associated with one argument to the original</span>
<span class="sd">    function, these Tensors must be named explicitly. Tensors in nested</span>
<span class="sd">    structures may not be passed using positional arguments when calling the</span>
<span class="sd">    concrete function.</span>

<span class="sd">    ```python</span>
<span class="sd">    f_concrete2 = f.get_concrete_function(</span>
<span class="sd">        (tf.TensorSpec(None, tf.float64, name=&quot;first&quot;),</span>
<span class="sd">         tf.TensorSpec([], tf.float32, name=&quot;second&quot;)))</span>
<span class="sd">    # Keyword arguments are required when identifying Tensors in nested</span>
<span class="sd">    # structures.</span>
<span class="sd">    f_concrete2(first=tf.constant([1.]), second=tf.constant(0.))</span>
<span class="sd">    ```</span>

<span class="sd">    Functions with fixed input signatures have only one concrete function</span>
<span class="sd">    associated with them, which can be retrieved without specifying any</span>
<span class="sd">    arguments. As before Tensors must have unique names, either inferred from</span>
<span class="sd">    the argument names in the original Python function or specified</span>
<span class="sd">    explicitly.</span>

<span class="sd">    ```python</span>
<span class="sd">    @tf.function(input_signature=(tf.TensorSpec(None, tf.float32)))</span>
<span class="sd">    def f_sig(y):</span>
<span class="sd">      return y</span>

<span class="sd">    f_sig_concrete = f.get_concrete_function()</span>
<span class="sd">    f_sig_concrete(tf.constant(1.))</span>
<span class="sd">    f_sig_concrete(y=tf.constant(1.))</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      *args: inputs to specialize on.</span>
<span class="sd">      **kwargs: inputs to specialize on.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A TensorFlow function which takes exactly one `tf.Tensor` per argument.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if this object has not yet been called on concrete values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concrete</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_concrete_function_garbage_collected</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">concrete</span><span class="o">.</span><span class="n">_garbage_collector</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">concrete</span>

  <span class="k">def</span> <span class="nf">__get__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">instance</span><span class="p">,</span> <span class="n">owner</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Makes it possible to defun instance methods.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">owner</span>
    <span class="c1"># `instance` here is the instance that this `Function` was accessed through</span>
    <span class="c1"># e.g., for</span>
    <span class="c1">#</span>
    <span class="c1">#   class Foo(object):</span>
    <span class="c1">#</span>
    <span class="c1">#     @function.defun</span>
    <span class="c1">#     def bar(self):</span>
    <span class="c1">#       ...</span>
    <span class="c1">#</span>
    <span class="c1">#   foo = Foo()</span>
    <span class="c1">#   foo.bar()  # `foo.bar` is a `Function` instance</span>
    <span class="c1">#</span>
    <span class="c1"># then `instance` will be `foo` (and `owner` will be `Foo`).  We create a</span>
    <span class="c1"># new instance of `Function` here to allow different instances each</span>
    <span class="c1"># to create variables once, thereby allowing methods to be decorated with</span>
    <span class="c1"># tf.function. Keeps a cache to avoid retracing the function every time the</span>
    <span class="c1"># descriptor is accessed.</span>
    <span class="k">if</span> <span class="n">instance</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_descriptor_cache</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_descriptor_cache</span><span class="p">[</span><span class="n">instance</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">function_lib</span><span class="o">.</span><span class="n">class_method_to_instance_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">instance</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_descriptor_cache</span><span class="p">[</span><span class="n">instance</span><span class="p">]</span>


<div class="viewcode-block" id="function"><a class="viewcode-back" href="../../../../index.html#tensorflow.function">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;function&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">function</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">input_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">autograph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">experimental_implements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">experimental_autograph_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">experimental_relax_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">experimental_compile</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compiles a function into a callable TensorFlow graph.</span>

<span class="sd">  `tf.function` constructs a callable that executes a TensorFlow graph</span>
<span class="sd">  (`tf.Graph`) created by trace-compiling the TensorFlow operations in `func`,</span>
<span class="sd">  effectively executing `func` as a TensorFlow graph.</span>

<span class="sd">  Example usage:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x, y):</span>
<span class="sd">  ...   return x ** 2 + y</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([2, 3])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.constant([3, -2])</span>
<span class="sd">  &gt;&gt;&gt; f(x, y)</span>
<span class="sd">  &lt;tf.Tensor: ... numpy=array([7, 7], ...)&gt;</span>

<span class="sd">  _Features_</span>

<span class="sd">  `func` may use data-dependent control flow, including `if`, `for`, `while`</span>
<span class="sd">  `break`, `continue` and `return` statements:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   if tf.reduce_sum(x) &gt; 0:</span>
<span class="sd">  ...     return x * x</span>
<span class="sd">  ...   else:</span>
<span class="sd">  ...     return -x // 2</span>
<span class="sd">  &gt;&gt;&gt; f(tf.constant(-2))</span>
<span class="sd">  &lt;tf.Tensor: ... numpy=1&gt;</span>

<span class="sd">  `func`&#39;s closure may include `tf.Tensor` and `tf.Variable` objects:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f():</span>
<span class="sd">  ...   return x ** 2 + y</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([-2, -3])</span>
<span class="sd">  &gt;&gt;&gt; y = tf.Variable([3, -2])</span>
<span class="sd">  &gt;&gt;&gt; f()</span>
<span class="sd">  &lt;tf.Tensor: ... numpy=array([7, 7], ...)&gt;</span>

<span class="sd">  `func` may also use ops with side effects, such as `tf.print`, `tf.Variable`</span>
<span class="sd">  and others:</span>

<span class="sd">  &gt;&gt;&gt; v = tf.Variable(1)</span>
<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   for i in tf.range(x):</span>
<span class="sd">  ...     v.assign_add(i)</span>
<span class="sd">  &gt;&gt;&gt; f(3)</span>
<span class="sd">  &gt;&gt;&gt; v</span>
<span class="sd">  &lt;tf.Variable ... numpy=4&gt;</span>

<span class="sd">  Important: Any Python side-effects (appending to a list, printing with</span>
<span class="sd">  `print`, etc) will only happen once, when `func` is traced. To have</span>
<span class="sd">  side-effects executed into your `tf.function` they need to be written</span>
<span class="sd">  as TF ops:</span>

<span class="sd">  &gt;&gt;&gt; l = []</span>
<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   for i in x:</span>
<span class="sd">  ...     l.append(i + 1)    # Caution! Will only happen once when tracing</span>
<span class="sd">  &gt;&gt;&gt; f(tf.constant([1, 2, 3]))</span>
<span class="sd">  &gt;&gt;&gt; l</span>
<span class="sd">  [&lt;tf.Tensor ...&gt;]</span>

<span class="sd">  Instead, use TensorFlow collections like `tf.TensorArray`:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)</span>
<span class="sd">  ...   for i in range(len(x)):</span>
<span class="sd">  ...     ta = ta.write(i, x[i] + 1)</span>
<span class="sd">  ...   return ta.stack()</span>
<span class="sd">  &gt;&gt;&gt; f(tf.constant([1, 2, 3]))</span>
<span class="sd">  &lt;tf.Tensor: ..., numpy=array([2, 3, 4], ...)&gt;</span>

<span class="sd">  _`tf.function` is polymorphic_</span>

<span class="sd">  Internally, `tf.function` can build more than one graph, to support arguments</span>
<span class="sd">  with different data types or shapes, since TensorFlow can build more</span>
<span class="sd">  efficient graphs that are specialized on shapes and dtypes. `tf.function`</span>
<span class="sd">  also treats any pure Python value as opaque objects, and builds a separate</span>
<span class="sd">  graph for each set of Python arguments that it encounters.</span>

<span class="sd">  To obtain an individual graph, use the `get_concrete_function` method of</span>
<span class="sd">  the callable created by `tf.function`. It can be called with the same</span>
<span class="sd">  arguments as `func` and returns a special `tf.Graph` object:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   return x + 1</span>
<span class="sd">  &gt;&gt;&gt; isinstance(f.get_concrete_function(1).graph, tf.Graph)</span>
<span class="sd">  True</span>

<span class="sd">  Caution: Passing python scalars or lists as arguments to `tf.function` will</span>
<span class="sd">  always build a new graph. To avoid this, pass numeric arguments as Tensors</span>
<span class="sd">  whenever possible:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   return tf.abs(x)</span>
<span class="sd">  &gt;&gt;&gt; f1 = f.get_concrete_function(1)</span>
<span class="sd">  &gt;&gt;&gt; f2 = f.get_concrete_function(2)  # Slow - builds new graph</span>
<span class="sd">  &gt;&gt;&gt; f1 is f2</span>
<span class="sd">  False</span>
<span class="sd">  &gt;&gt;&gt; f1 = f.get_concrete_function(tf.constant(1))</span>
<span class="sd">  &gt;&gt;&gt; f2 = f.get_concrete_function(tf.constant(2))  # Fast - reuses f1</span>
<span class="sd">  &gt;&gt;&gt; f1 is f2</span>
<span class="sd">  True</span>

<span class="sd">  Python numerical arguments should only be used when they take few distinct</span>
<span class="sd">  values, such as hyperparameters like the number of layers in a neural network.</span>

<span class="sd">  _Input signatures_</span>

<span class="sd">  For Tensor arguments, `tf.function` instantiates a separate graph for every</span>
<span class="sd">  unique set of input shapes and datatypes. The example below creates two</span>
<span class="sd">  separate graphs, each specialized to a different shape:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   return x + 1</span>
<span class="sd">  &gt;&gt;&gt; vector = tf.constant([1.0, 1.0])</span>
<span class="sd">  &gt;&gt;&gt; matrix = tf.constant([[3.0]])</span>
<span class="sd">  &gt;&gt;&gt; f.get_concrete_function(vector) is f.get_concrete_function(matrix)</span>
<span class="sd">  False</span>

<span class="sd">  An &quot;input signature&quot; can be optionally provided to `tf.function` to control</span>
<span class="sd">  the graphs traced. The input signature specifies the shape and type of each</span>
<span class="sd">  Tensor argument to the function using a `tf.TensorSpec` object. More general</span>
<span class="sd">  shapes can be used. This is useful to avoid creating multiple graphs when</span>
<span class="sd">  Tensors have dynamic shapes. It also restricts the shape and datatype of</span>
<span class="sd">  Tensors that can be used:</span>

<span class="sd">  &gt;&gt;&gt; @tf.function(</span>
<span class="sd">  ...     input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   return x + 1</span>
<span class="sd">  &gt;&gt;&gt; vector = tf.constant([1.0, 1.0])</span>
<span class="sd">  &gt;&gt;&gt; matrix = tf.constant([[3.0]])</span>
<span class="sd">  &gt;&gt;&gt; f.get_concrete_function(vector) is f.get_concrete_function(matrix)</span>
<span class="sd">  True</span>

<span class="sd">  _Variables may only be created once_</span>

<span class="sd">  `tf.function` only allows creating new `tf.Variable` objects when it is called</span>
<span class="sd">  for the first time:</span>

<span class="sd">  &gt;&gt;&gt; class MyModule(tf.Module):</span>
<span class="sd">  ...   def __init__(self):</span>
<span class="sd">  ...     self.v = None</span>
<span class="sd">  ...</span>
<span class="sd">  ...   @tf.function</span>
<span class="sd">  ...   def call(self, x):</span>
<span class="sd">  ...     if self.v is None:</span>
<span class="sd">  ...       self.v = tf.Variable(tf.ones_like(x))</span>
<span class="sd">  ...     return self.v * x</span>

<span class="sd">  In general, it is recommended to create stateful objects like `tf.Variable`</span>
<span class="sd">  outside of `tf.function` and passing them as arguments.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the function to be compiled. If `func` is None, `tf.function` returns</span>
<span class="sd">      a decorator that can be invoked with a single argument - `func`. In other</span>
<span class="sd">      words, `tf.function(input_signature=...)(func)` is equivalent to</span>
<span class="sd">      `tf.function(func, input_signature=...)`. The former can be used as</span>
<span class="sd">      decorator.</span>
<span class="sd">    input_signature: A possibly nested sequence of `tf.TensorSpec` objects</span>
<span class="sd">      specifying the shapes and dtypes of the Tensors that will be supplied to</span>
<span class="sd">      this function. If `None`, a separate function is instantiated for each</span>
<span class="sd">      inferred input signature.  If input_signature is specified, every input to</span>
<span class="sd">      `func` must be a `Tensor`, and `func` cannot accept `**kwargs`.</span>
<span class="sd">    autograph: Whether autograph should be applied on `func` before tracing a</span>
<span class="sd">      graph. Data-dependent control flow requires `autograph=True`. For more</span>
<span class="sd">      information, see the [tf.function and AutoGraph guide](</span>
<span class="sd">      https://www.tensorflow.org/guide/function).</span>
<span class="sd">    experimental_implements: If provided, contains a name of a &quot;known&quot; function</span>
<span class="sd">      this implements. For example &quot;mycompany.my_recurrent_cell&quot;.</span>
<span class="sd">      This is stored as an attribute in inference function,</span>
<span class="sd">      which can then be detected when processing serialized function.</span>
<span class="sd">      See [standardizing composite ops](https://github.com/tensorflow/community/blob/master/rfcs/20190610-standardizing-composite_ops.md)  # pylint: disable=line-too-long</span>
<span class="sd">      for details.  For an example of utilizing this attribute see this</span>
<span class="sd">      [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc)</span>
<span class="sd">      The code above automatically detects and substitutes function that</span>
<span class="sd">      implements &quot;embedded_matmul&quot; and allows TFLite to substitute its own</span>
<span class="sd">      implementations. For instance, a tensorflow user can use this</span>
<span class="sd">       attribute to mark that their function also implements</span>
<span class="sd">      `embedded_matmul` (perhaps more efficiently!)</span>
<span class="sd">      by specifying it using this parameter:</span>
<span class="sd">      `@tf.function(experimental_implements=&quot;embedded_matmul&quot;)`</span>
<span class="sd">    experimental_autograph_options: Optional tuple of</span>
<span class="sd">      `tf.autograph.experimental.Feature` values.</span>
<span class="sd">    experimental_relax_shapes: When True, `tf.function` may generate fewer,</span>
<span class="sd">      graphs that are less specialized on input shapes.</span>
<span class="sd">    experimental_compile: If True, the function is always compiled by</span>
<span class="sd">      [XLA](https://www.tensorflow.org/xla). XLA may be more efficient in some</span>
<span class="sd">      cases (e.g. TPU, XLA_GPU, dense tensor computations).</span>

<span class="sd">  Returns:</span>
<span class="sd">     If `func` is not None, returns a callable that will execute the compiled</span>
<span class="sd">     function (and return zero or more `tf.Tensor` objects).</span>
<span class="sd">     If `func` is None, returns a decorator that, when invoked with a single</span>
<span class="sd">     `func` argument, returns a callable equivalent to the case above.</span>

<span class="sd">  Raises:</span>
<span class="sd">     ValueError when attempting to use experimental_compile, but XLA support is</span>
<span class="sd">     not enabled.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">input_signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">function_lib</span><span class="o">.</span><span class="n">validate_signature</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">decorated</span><span class="p">(</span><span class="n">inner_function</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">inner_function</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;function&quot;</span>
    <span class="k">return</span> <span class="n">tf_decorator</span><span class="o">.</span><span class="n">make_decorator</span><span class="p">(</span>
        <span class="n">inner_function</span><span class="p">,</span>
        <span class="n">decorator_name</span><span class="o">=</span><span class="s2">&quot;tf.function&quot;</span><span class="p">,</span>
        <span class="n">decorator_func</span><span class="o">=</span><span class="n">Function</span><span class="p">(</span>
            <span class="n">inner_function</span><span class="p">,</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="n">input_signature</span><span class="o">=</span><span class="n">input_signature</span><span class="p">,</span>
            <span class="n">autograph</span><span class="o">=</span><span class="n">autograph</span><span class="p">,</span>
            <span class="n">experimental_autograph_options</span><span class="o">=</span><span class="n">experimental_autograph_options</span><span class="p">,</span>
            <span class="n">experimental_relax_shapes</span><span class="o">=</span><span class="n">experimental_relax_shapes</span><span class="p">,</span>
            <span class="n">experimental_compile</span><span class="o">=</span><span class="n">experimental_compile</span><span class="p">,</span>
            <span class="n">experimental_implements</span><span class="o">=</span><span class="n">experimental_implements</span><span class="p">))</span>

  <span class="c1"># This code path is for the `foo = tf.function(foo, ...)` use case</span>
  <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">decorated</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

  <span class="c1"># This code path is for the</span>
  <span class="c1">#</span>
  <span class="c1"># @tf.function(...)</span>
  <span class="c1"># def foo(...):</span>
  <span class="c1">#    ...</span>
  <span class="c1">#</span>
  <span class="c1"># use case, which is equivalent to `foo = tf.function(...)(foo)`</span>
  <span class="k">return</span> <span class="n">decorated</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>