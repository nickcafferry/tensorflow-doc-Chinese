

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.framework.ops &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="../../../../_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.framework.ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.framework.ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Classes and functions used to construct graphs.&quot;&quot;&quot;</span>
<span class="c1"># pylint: disable=g-bad-name</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">types</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">map</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">attr_value_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">function_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">graph_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">node_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">op_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">versions_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="k">import</span> <span class="n">config_pb2</span>
<span class="c1"># pywrap_tensorflow must be imported first to avoid profobuf issues.</span>
<span class="c1"># (b/143110113)</span>
<span class="c1"># pylint: disable=invalid-import-order,g-bad-import-order,unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tensorflow</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tfe</span>
<span class="c1"># pylint: enable=invalid-import-order,g-bad-import-order,unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">tf2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="k">import</span> <span class="n">pywrap_tf_session</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">monitoring</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">tape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">c_api_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">composite_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">device</span> <span class="k">as</span> <span class="n">pydev</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">errors</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">indexed_slices</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">registry</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_conversion_registry</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_like</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">traceable_stack</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">versions</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">control_flow_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">app</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">decorator_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">function_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">lock_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">memory</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">object_identity</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_contextlib</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_stack</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.compat</span> <span class="k">import</span> <span class="n">collections_abc</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="k">import</span> <span class="n">deprecated_args</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.lazy_loader</span> <span class="k">import</span> <span class="n">LazyLoader</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">kwarg_only</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="n">ag_ctx</span> <span class="o">=</span> <span class="n">LazyLoader</span><span class="p">(</span>
    <span class="s2">&quot;ag_ctx&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span>
    <span class="s2">&quot;tensorflow.python.autograph.core.ag_ctx&quot;</span><span class="p">)</span>


<span class="c1"># Temporary global switches determining if we should enable the work-in-progress</span>
<span class="c1"># calls to the C API. These will be removed once all functionality is supported.</span>
<span class="n">_USE_C_API</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_USE_C_SHAPES</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">_api_usage_gauge</span> <span class="o">=</span> <span class="n">monitoring</span><span class="o">.</span><span class="n">BoolGauge</span><span class="p">(</span>
    <span class="s2">&quot;/tensorflow/api/ops_eager_execution&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Whether ops.enable_eager_execution() is called.&quot;</span><span class="p">)</span>


<span class="c1"># pylint: disable=protected-access</span>
<span class="n">_TensorLike</span> <span class="o">=</span> <span class="n">tensor_like</span><span class="o">.</span><span class="n">_TensorLike</span>
<span class="n">_DTYPES_INTERN_TABLE</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">_INTERN_TABLE</span>
<span class="c1"># pylint: enable=protected-access</span>


<span class="k">def</span> <span class="nf">tensor_id</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a unique identifier for this Tensor.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">_id</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">class</span> <span class="nc">_UserDeviceSpec</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Store user-specified device and provide computation of merged device.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name_or_function</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span> <span class="o">=</span> <span class="n">device_name_or_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">display_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">=</span> <span class="n">device_name_or_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">raw_string</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">,</span> <span class="n">pydev</span><span class="o">.</span><span class="n">MergeDevice</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">is_null_merge</span> <span class="o">=</span> <span class="n">device_name_or_function</span><span class="o">.</span><span class="n">is_null_merge</span>

    <span class="k">elif</span> <span class="n">callable</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">is_null_merge</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="n">dev_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_name_or_function</span>
      <span class="n">func_name</span> <span class="o">=</span> <span class="n">function_utils</span><span class="o">.</span><span class="n">get_func_name</span><span class="p">(</span><span class="n">dev_func</span><span class="p">)</span>
      <span class="n">func_code</span> <span class="o">=</span> <span class="n">function_utils</span><span class="o">.</span><span class="n">get_func_code</span><span class="p">(</span><span class="n">dev_func</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">func_code</span><span class="p">:</span>
        <span class="n">fname</span> <span class="o">=</span> <span class="n">func_code</span><span class="o">.</span><span class="n">co_filename</span>
        <span class="n">lineno</span> <span class="o">=</span> <span class="n">func_code</span><span class="o">.</span><span class="n">co_firstlineno</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;unknown&quot;</span>
        <span class="n">lineno</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">display_name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&lt;</span><span class="si">%s</span><span class="s2">, </span><span class="si">%d</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">lineno</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">device_name_or_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># NOTE(taylorrobie): This MUST be False. None signals a break in the</span>
      <span class="c1">#   device stack, so `is_null_merge` must be False for such a case to</span>
      <span class="c1">#   allow callers to safely skip over null merges without missing a None.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">is_null_merge</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">raw_string</span> <span class="o">=</span> <span class="n">device_name_or_function</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">=</span> <span class="n">pydev</span><span class="o">.</span><span class="n">merge_device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">is_null_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">is_null_merge</span>

    <span class="c1"># We perform this check in __init__ because it is of non-trivial cost,</span>
    <span class="c1"># and self.string_merge is typically called many times.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fast_string_merge</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">,</span> <span class="n">pydev</span><span class="o">.</span><span class="n">MergeDevice</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">string_merge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_def</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fast_string_merge</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">shortcut_string_merge</span><span class="p">(</span><span class="n">node_def</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">_device_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">node_def</span><span class="p">)))</span>


<span class="k">class</span> <span class="nc">NullContextmanager</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span>  <span class="c1"># False values do not suppress exceptions</span>


<span class="k">def</span> <span class="nf">_override_helper</span><span class="p">(</span><span class="n">clazz_object</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Overrides (string) operator on Tensors to call func.</span>

<span class="sd">  Args:</span>
<span class="sd">    clazz_object: the class to override for; either Tensor or SparseTensor.</span>
<span class="sd">    operator: the string name of the operator to override.</span>
<span class="sd">    func: the function that replaces the overridden operator.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If operator has already been overwritten,</span>
<span class="sd">      or if operator is not allowed to be overwritten.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">existing</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">clazz_object</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">existing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Check to see if this is a default method-wrapper or slot wrapper which</span>
    <span class="c1"># will be true for the comparison operators.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="nb">object</span><span class="o">.</span><span class="fm">__lt__</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;operator </span><span class="si">%s</span><span class="s2"> cannot be overwritten again on class </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">operator</span><span class="p">,</span> <span class="n">clazz_object</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">operator</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">OVERLOADABLE_OPERATORS</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Overriding </span><span class="si">%s</span><span class="s2"> is disallowed&quot;</span> <span class="o">%</span> <span class="n">operator</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">clazz_object</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_as_graph_element</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert `obj` to a graph element if possible, otherwise return `None`.</span>

<span class="sd">  Args:</span>
<span class="sd">    obj: Object to convert.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The result of `obj._as_graph_element()` if that method is available;</span>
<span class="sd">        otherwise `None`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">conv_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="s2">&quot;_as_graph_element&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">conv_fn</span> <span class="ow">and</span> <span class="n">callable</span><span class="p">(</span><span class="n">conv_fn</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conv_fn</span><span class="p">()</span>
  <span class="k">return</span> <span class="kc">None</span>


<span class="n">_TENSOR_LIKE_TYPES</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">is_dense_tensor_like</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: Returns true if `t` implements the tensor interface.</span>

<span class="sd">  See `register_dense_tensor_like_type()` for the current definition of a</span>
<span class="sd">  &quot;tensor-like type&quot;.</span>

<span class="sd">  Args:</span>
<span class="sd">    t: An object.</span>

<span class="sd">  Returns:</span>
<span class="sd">    True iff `t` is an instance of one of the registered &quot;tensor-like&quot; types.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_TENSOR_LIKE_TYPES</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: Registers `tensor_type` as implementing the tensor interface.</span>

<span class="sd">  A &quot;tensor-like type&quot; can represent a single dense tensor, and implements</span>
<span class="sd">  the `name`, `dtype` and `shape` properties.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor_type: A type implementing the tensor interface.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `tensor_type` does not implement the tensor interface.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span> <span class="ow">and</span>
          <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_type</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">property</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> does not define a `name` property&quot;</span> <span class="o">%</span>
                    <span class="n">tensor_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">)</span> <span class="ow">and</span>
          <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_type</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">property</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> does not define a `dtype` property&quot;</span> <span class="o">%</span>
                    <span class="n">tensor_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">,</span> <span class="s2">&quot;shape&quot;</span><span class="p">)</span> <span class="ow">and</span>
          <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_type</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">property</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">%s</span><span class="s2"> does not define a `shape` property&quot;</span> <span class="o">%</span>
                    <span class="n">tensor_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
  <span class="c1"># We expect this list to be small, so choose quadratic complexity</span>
  <span class="c1"># for registration, so that we have a tuple that can be used for</span>
  <span class="c1"># more efficient `isinstance` checks later.</span>
  <span class="k">global</span> <span class="n">_TENSOR_LIKE_TYPES</span>
  <span class="n">_TENSOR_LIKE_TYPES</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">_TENSOR_LIKE_TYPES</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">tensor_type</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">uid</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;A unique (within this program execution) integer.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_UID</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">numpy_text</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">is_repr</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Human readable representation of a tensor&#39;s numpy value.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_numpy_compatible</span><span class="p">:</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">text</span> <span class="o">=</span> <span class="nb">repr</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">_numpy</span><span class="p">())</span> <span class="k">if</span> <span class="n">is_repr</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">_numpy</span><span class="p">())</span>
    <span class="c1"># pylint: enable=protected-access</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&lt;unprintable&gt;&quot;</span>
  <span class="k">if</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">text</span>
  <span class="k">return</span> <span class="n">text</span>

<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;enable_tensor_equality&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">enable_tensor_equality</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Compare Tensors with element-wise comparison and thus be unhashable.</span>

<span class="sd">  Comparing tensors with element-wise allows comparisons such as</span>
<span class="sd">  tf.Variable(1.0) == 1.0. Element-wise equality implies that tensors are</span>
<span class="sd">  unhashable. Thus tensors can no longer be directly used in sets or as a key in</span>
<span class="sd">  a dictionary.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># pylint: disable=protected-access</span>

<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;disable_tensor_equality&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">disable_tensor_equality</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Compare Tensors by their id and be hashable.</span>

<span class="sd">  This is a legacy behaviour of TensorFlow and is highly discouraged.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># pylint: disable=protected-access</span>


<div class="viewcode-block" id="Tensor"><a class="viewcode-back" href="../../../../index.html#tensorflow.Tensor">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">_TensorLike</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A tensor represents a rectangular array of data.</span>

<span class="sd">  When writing a TensorFlow program, the main object you manipulate and pass</span>
<span class="sd">  around is the `tf.Tensor`. A `tf.Tensor` object represents a rectangular array</span>
<span class="sd">  of arbitrary dimension, filled with data of a specific data type.</span>

<span class="sd">  A `tf.Tensor` has the following properties:</span>

<span class="sd">  * a data type (float32, int32, or string, for example)</span>
<span class="sd">  * a shape</span>

<span class="sd">  Each element in the Tensor has the same data type, and the data type is always</span>
<span class="sd">  known.</span>

<span class="sd">  In eager execution, which is the default mode in TensorFlow, results are</span>
<span class="sd">  calculated immediately.</span>

<span class="sd">  &gt;&gt;&gt; # Compute some values using a Tensor</span>
<span class="sd">  &gt;&gt;&gt; c = tf.constant([[1.0, 2.0], [3.0, 4.0]])</span>
<span class="sd">  &gt;&gt;&gt; d = tf.constant([[1.0, 1.0], [0.0, 1.0]])</span>
<span class="sd">  &gt;&gt;&gt; e = tf.matmul(c, d)</span>
<span class="sd">  &gt;&gt;&gt; print(e)</span>
<span class="sd">  tf.Tensor(</span>
<span class="sd">  [[1. 3.]</span>
<span class="sd">   [3. 7.]], shape=(2, 2), dtype=float32)</span>


<span class="sd">  Note that during eager execution, you may discover your `Tensors` are actually</span>
<span class="sd">  of type `EagerTensor`.  This is an internal detail, but it does give you</span>
<span class="sd">  access to a useful function, `numpy`:</span>

<span class="sd">  &gt;&gt;&gt; type(e)</span>
<span class="sd">  &lt;class &#39;...ops.EagerTensor&#39;&gt;</span>
<span class="sd">  &gt;&gt;&gt; print(e.numpy())</span>
<span class="sd">    [[1. 3.]</span>
<span class="sd">     [3. 7.]]</span>

<span class="sd">  TensorFlow can define computations without immediately executing them, most</span>
<span class="sd">  commonly inside `tf.function`s, as well as in (legacy) Graph mode. In those</span>
<span class="sd">  cases, the shape (that is, the rank of the Tensor and the size of</span>
<span class="sd">  each dimension) might be only partially known.</span>

<span class="sd">  Most operations produce tensors of fully-known shapes if the shapes of their</span>
<span class="sd">  inputs are also fully known, but in some cases it&#39;s only possible to find the</span>
<span class="sd">  shape of a tensor at execution time.</span>

<span class="sd">  There are specialized tensors; for these, see `tf.Variable`, `tf.constant`,</span>
<span class="sd">  `tf.placeholder`, `tf.SparseTensor`, and `tf.RaggedTensor`.</span>

<span class="sd">  For more on Tensors, see the [guide](https://tensorflow.org/guide/tensor`).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># List of Python operators that we allow to override.</span>
  <span class="n">OVERLOADABLE_OPERATORS</span> <span class="o">=</span> <span class="p">{</span>
      <span class="c1"># Binary.</span>
      <span class="s2">&quot;__add__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__radd__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__sub__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rsub__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__mul__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rmul__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__div__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rdiv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__truediv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rtruediv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__floordiv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rfloordiv__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__mod__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rmod__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__lt__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__le__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__gt__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__ge__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__ne__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__eq__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__and__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rand__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__or__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__ror__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__xor__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rxor__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__getitem__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__pow__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rpow__&quot;</span><span class="p">,</span>
      <span class="c1"># Unary.</span>
      <span class="s2">&quot;__invert__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__neg__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__abs__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__matmul__&quot;</span><span class="p">,</span>
      <span class="s2">&quot;__rmatmul__&quot;</span>
  <span class="p">}</span>

  <span class="c1"># Whether to allow hashing or numpy-style equality</span>
  <span class="n">_USE_EQUALITY</span> <span class="o">=</span> <span class="n">tf2</span><span class="o">.</span><span class="n">enabled</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">value_index</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new `Tensor`.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: An `Operation`. `Operation` that computes this tensor.</span>
<span class="sd">      value_index: An `int`. Index of the operation&#39;s endpoint that produces</span>
<span class="sd">        this tensor.</span>
<span class="sd">      dtype: A `DType`. Type of elements stored in this tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If the op is not an `Operation`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op needs to be an Operation: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span> <span class="o">=</span> <span class="n">value_index</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># This will be set by self._as_tf_output().</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tf_output</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># This will be set by self.shape().</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># List of operations that use this Tensor as input.  We maintain this list</span>
    <span class="c1"># to easily navigate a computation graph.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_consumers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_id</span> <span class="o">=</span> <span class="n">uid</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_create_with_tf_output</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">value_index</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">tf_output</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">value_index</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">_tf_output</span> <span class="o">=</span> <span class="n">tf_output</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` that produces this tensor as an output.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `DType` of elements in this tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` that contains this tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">graph</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The string name of this tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Operation was not named: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">:</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of the device on which this tensor will be produced, or None.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">device</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `TensorShape` that represents the shape of this tensor.</span>

<span class="sd">    The shape is computed using shape inference functions that are</span>
<span class="sd">    registered in the Op for each `Operation`.  See</span>
<span class="sd">    `tf.TensorShape`</span>
<span class="sd">    for more details of what a shape represents.</span>

<span class="sd">    The inferred shape of a tensor is used to provide shape</span>
<span class="sd">    information without having to execute the underlying kernel. This</span>
<span class="sd">    can be used for debugging and providing early error messages. For</span>
<span class="sd">    example:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span>
<span class="sd">    &gt;&gt;&gt; print(c.shape) # will be TensorShape([2, 3])</span>
<span class="sd">    (2, 3)</span>

<span class="sd">    &gt;&gt;&gt; d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])</span>
<span class="sd">    &gt;&gt;&gt; print(d.shape)</span>
<span class="sd">    (4, 2)</span>

<span class="sd">    # Raises a ValueError, because `c` and `d` do not have compatible</span>
<span class="sd">    # inner dimensions.</span>
<span class="sd">    &gt;&gt;&gt; e = tf.matmul(c, d)</span>
<span class="sd">    Traceback (most recent call last):</span>
<span class="sd">        ...</span>
<span class="sd">    tensorflow.python.framework.errors_impl.InvalidArgumentError: Matrix</span>
<span class="sd">    size-incompatible: In[0]: [2,3], In[1]: [4,2] [Op:MatMul] name: MatMul/</span>

<span class="sd">    # This works because we have compatible shapes.</span>
<span class="sd">    &gt;&gt;&gt; f = tf.matmul(c, d, transpose_a=True, transpose_b=True)</span>
<span class="sd">    &gt;&gt;&gt; print(f.shape)</span>
<span class="sd">    (3, 4)</span>

<span class="sd">    ```</span>

<span class="sd">    In some cases, the inferred shape may have unknown dimensions. If</span>
<span class="sd">    the caller has additional information about the values of these</span>
<span class="sd">    dimensions, `Tensor.set_shape()` can be used to augment the</span>
<span class="sd">    inferred shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `tf.TensorShape` representing the shape of this tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_api_shape</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span>

  <span class="k">def</span> <span class="nf">_c_api_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the TensorShape of this tensor according to the C API.&quot;&quot;&quot;</span>
    <span class="n">c_graph</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">shape_vec</span><span class="p">,</span> <span class="n">unknown_shape</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GraphGetTensorShapeHelper</span><span class="p">(</span>
        <span class="n">c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">unknown_shape</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">shape_vec</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">shape_vec</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">shape_vec</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Tensor._shape is private, use Tensor.shape &quot;</span>
                    <span class="s2">&quot;instead. Tensor._shape will eventually be removed.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

  <span class="nd">@_shape</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor._shape cannot be assigned, use Tensor.set_shape instead.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_disallow_when_autograph_disabled</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">):</span>
    <span class="k">raise</span> <span class="n">errors</span><span class="o">.</span><span class="n">OperatorNotAllowedInGraphError</span><span class="p">(</span>
        <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not allowed: AutoGraph is disabled in this function.&quot;</span>
        <span class="s2">&quot; Try decorating it directly with @tf.function.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">task</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_disallow_when_autograph_enabled</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">):</span>
    <span class="k">raise</span> <span class="n">errors</span><span class="o">.</span><span class="n">OperatorNotAllowedInGraphError</span><span class="p">(</span>
        <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not allowed: AutoGraph did not convert this function. Try&quot;</span>
        <span class="s2">&quot; decorating it directly with @tf.function.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">task</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_disallow_in_graph_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">):</span>
    <span class="k">raise</span> <span class="n">errors</span><span class="o">.</span><span class="n">OperatorNotAllowedInGraphError</span><span class="p">(</span>
        <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not allowed in Graph execution. Use Eager execution or decorate&quot;</span>
        <span class="s2">&quot; this function with @tf.function.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">task</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_disallow_bool_casting</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">control_status_ctx</span><span class="p">()</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">Status</span><span class="o">.</span><span class="n">DISABLED</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_when_autograph_disabled</span><span class="p">(</span>
          <span class="s2">&quot;using a `tf.Tensor` as a Python `bool`&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">control_status_ctx</span><span class="p">()</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">Status</span><span class="o">.</span><span class="n">ENABLED</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_when_autograph_enabled</span><span class="p">(</span>
          <span class="s2">&quot;using a `tf.Tensor` as a Python `bool`&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Default: V1-style Graph execution.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_in_graph_mode</span><span class="p">(</span><span class="s2">&quot;using a `tf.Tensor` as a Python `bool`&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_disallow_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">control_status_ctx</span><span class="p">()</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">Status</span><span class="o">.</span><span class="n">DISABLED</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_when_autograph_disabled</span><span class="p">(</span><span class="s2">&quot;iterating over `tf.Tensor`&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">control_status_ctx</span><span class="p">()</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">Status</span><span class="o">.</span><span class="n">ENABLED</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_when_autograph_enabled</span><span class="p">(</span><span class="s2">&quot;iterating over `tf.Tensor`&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Default: V1-style Graph execution.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_in_graph_mode</span><span class="p">(</span><span class="s2">&quot;iterating over `tf.Tensor`&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_iteration</span><span class="p">()</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot iterate over a tensor with unknown shape.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Cannot iterate over a scalar tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot iterate over a tensor with unknown first dimension.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_TensorIterator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">_shape_as_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">_shape_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Integer rank of this Tensor, if known, else None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Integer rank or None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>

<div class="viewcode-block" id="Tensor.get_shape"><a class="viewcode-back" href="../../../../index.html#tensorflow.Tensor.get_shape">[docs]</a>  <span class="k">def</span> <span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Alias of `tf.Tensor.shape`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span></div>

<div class="viewcode-block" id="Tensor.set_shape"><a class="viewcode-back" href="../../../../index.html#tensorflow.Tensor.set_shape">[docs]</a>  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates the shape of this tensor.</span>

<span class="sd">    This method can be called multiple times, and will merge the given</span>
<span class="sd">    `shape` with the current shape of this tensor. It can be used to</span>
<span class="sd">    provide additional information about the shape of this tensor that</span>
<span class="sd">    cannot be inferred from the graph alone. For example, this can be used</span>
<span class="sd">    to provide additional information about the shapes of images:</span>

<span class="sd">    ```python</span>
<span class="sd">    _, image_data = tf.compat.v1.TFRecordReader(...).read(...)</span>
<span class="sd">    image = tf.image.decode_png(image_data, channels=3)</span>

<span class="sd">    # The height and width dimensions of `image` are data dependent, and</span>
<span class="sd">    # cannot be computed without executing the op.</span>
<span class="sd">    print(image.shape)</span>
<span class="sd">    ==&gt; TensorShape([Dimension(None), Dimension(None), Dimension(3)])</span>

<span class="sd">    # We know that each image in this dataset is 28 x 28 pixels.</span>
<span class="sd">    image.set_shape([28, 28, 3])</span>
<span class="sd">    print(image.shape)</span>
<span class="sd">    ==&gt; TensorShape([Dimension(28), Dimension(28), Dimension(3)])</span>
<span class="sd">    ```</span>

<span class="sd">    NOTE: This shape is not enforced at runtime. Setting incorrect shapes can</span>
<span class="sd">    result in inconsistencies between the statically-known graph and the runtime</span>
<span class="sd">    value of tensors. For runtime validation of the shape, use `tf.ensure_shape`</span>
<span class="sd">    instead.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: A `TensorShape` representing the shape of this tensor, a</span>
<span class="sd">        `TensorShapeProto`, a list, a tuple, or None.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `shape` is not compatible with the current shape of</span>
<span class="sd">        this tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Reset cached shape.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape_val</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># We want set_shape to be reflected in the C API graph for when we run it.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">):</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dim_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">unknown_shape</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">unknown_shape</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">dim_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">dim_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GraphSetTensorShape_wrapper</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">(),</span>
          <span class="n">dim_list</span><span class="p">,</span>
          <span class="n">unknown_shape</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># Convert to ValueError for backwards compatibility.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">value_index</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The index of this tensor in the outputs of its `Operation`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span>

<div class="viewcode-block" id="Tensor.consumers"><a class="viewcode-back" href="../../../../index.html#tensorflow.Tensor.consumers">[docs]</a>  <span class="k">def</span> <span class="nf">consumers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of `Operation`s that consume this tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Operation`s.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">consumer_names</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationOutputConsumers_wrapper</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">consumer_names</span>
    <span class="p">]</span></div>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">_as_node_def_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return a value to use for the NodeDef &quot;input&quot; attribute.</span>

<span class="sd">    The returned string can be used in a NodeDef &quot;input&quot; attribute</span>
<span class="sd">    to indicate that the NodeDef uses this Tensor as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if this Tensor&#39;s Operation does not have a name.</span>

<span class="sd">    Returns:</span>
<span class="sd">      a string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Operation was not named: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">:</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_index</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_as_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="c1"># NOTE: Beyond preventing unnecessary (re-)allocation, the cached object</span>
    <span class="c1"># also guarantees that a dictionary of tf_output objects will retain a</span>
    <span class="c1"># deterministic (yet unsorted) order which prevents memory blowup in the</span>
    <span class="c1"># cache of executor(s) stored for every session.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tf_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tf_output</span> <span class="o">=</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_output</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tf_output</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;Tensor(</span><span class="se">\&quot;</span><span class="si">%s</span><span class="se">\&quot;</span><span class="si">%s%s%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="p">(</span><span class="s2">&quot;, shape=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">(</span><span class="s2">&quot;, dtype=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">(</span><span class="s2">&quot;, device=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;&lt;tf.Tensor &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span>
                                                   <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">g</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="ow">and</span> <span class="n">executing_eagerly_outside_functions</span><span class="p">()</span> <span class="ow">and</span>
        <span class="p">(</span><span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">g</span><span class="o">.</span><span class="n">building_function</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Tensor is unhashable. &quot;</span>
                      <span class="s2">&quot;Instead, use tensor.ref() as the key.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># TODO(b/77597810): get rid of Tensor copies.</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
    <span class="n">result</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="c1"># NOTE(mrry): This enables the Tensor&#39;s overloaded &quot;right&quot; binary</span>
  <span class="c1"># operators to run when the left operand is an ndarray, because it</span>
  <span class="c1"># accords the Tensor class higher priority than an ndarray, or a</span>
  <span class="c1"># numpy matrix.</span>
  <span class="c1"># TODO(mrry): Convert this to using numpy&#39;s __numpy_ufunc__</span>
  <span class="c1"># mechanism, which allows more control over how Tensors interact</span>
  <span class="c1"># with ndarrays.</span>
  <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">100</span>

  <span class="k">def</span> <span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Cannot convert a symbolic Tensor (</span><span class="si">{}</span><span class="s2">) to a numpy&quot;</span>
                              <span class="s2">&quot; array.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;len is not well defined for symbolic Tensors. (</span><span class="si">{}</span><span class="s2">) &quot;</span>
                    <span class="s2">&quot;Please call `x.shape` rather than `len(x)` for &quot;</span>
                    <span class="s2">&quot;shape information.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_override_operator</span><span class="p">(</span><span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="n">_override_helper</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dummy method to prevent a tensor from being used as a Python `bool`.</span>

<span class="sd">    This overload raises a `TypeError` when the user inadvertently</span>
<span class="sd">    treats a `Tensor` as a boolean (most commonly in an `if` or `while`</span>
<span class="sd">    statement), in code that was not converted by AutoGraph. For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    if tf.constant(True):  # Will raise.</span>
<span class="sd">      # ...</span>

<span class="sd">    if tf.constant(5) &lt; tf.constant(7):  # Will raise.</span>
<span class="sd">      # ...</span>
<span class="sd">    ```</span>

<span class="sd">    Raises:</span>
<span class="sd">      `TypeError`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_bool_casting</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__nonzero__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dummy method to prevent a tensor from being used as a Python `bool`.</span>

<span class="sd">    This is the Python 2.x counterpart to `__bool__()` above.</span>

<span class="sd">    Raises:</span>
<span class="sd">      `TypeError`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_disallow_bool_casting</span><span class="p">()</span>

<div class="viewcode-block" id="Tensor.eval"><a class="viewcode-back" href="../../../../index.html#tensorflow.Tensor.eval">[docs]</a>  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates this tensor in a `Session`.</span>

<span class="sd">    Note: If you are not using `compat.v1` libraries, you should not need this,</span>
<span class="sd">    (or `feed_dict` or `Session`).  In eager execution (or within `tf.function`)</span>
<span class="sd">    you do not need to call `eval`.</span>

<span class="sd">    Calling this method will execute all preceding operations that</span>
<span class="sd">    produce the inputs needed for the operation that produces this</span>
<span class="sd">    tensor.</span>

<span class="sd">    *N.B.* Before invoking `Tensor.eval()`, its graph must have been</span>
<span class="sd">    launched in a session, and either a default session must be</span>
<span class="sd">    available, or `session` must be specified explicitly.</span>

<span class="sd">    Args:</span>
<span class="sd">      feed_dict: A dictionary that maps `Tensor` objects to feed values. See</span>
<span class="sd">        `tf.Session.run` for a description of the valid feed values.</span>
<span class="sd">      session: (Optional.) The `Session` to be used to evaluate this tensor. If</span>
<span class="sd">        none, the default session will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A numpy array corresponding to the value of this tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_eval_using_default_session</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.experimental_ref"><a class="viewcode-back" href="../../../../index.html#tensorflow.Tensor.experimental_ref">[docs]</a>  <span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use ref() instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">experimental_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span></div>

<div class="viewcode-block" id="Tensor.ref"><a class="viewcode-back" href="../../../../index.html#tensorflow.Tensor.ref">[docs]</a>  <span class="k">def</span> <span class="nf">ref</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># tf.Variable also has the same ref() API.  If you update the</span>
    <span class="c1"># documentation here, please update tf.Variable.ref() as well.</span>
    <span class="sd">&quot;&quot;&quot;Returns a hashable reference object to this Tensor.</span>

<span class="sd">    The primary use case for this API is to put tensors in a set/dictionary.</span>
<span class="sd">    We can&#39;t put tensors in a set/dictionary as `tensor.__hash__()` is no longer</span>
<span class="sd">    available starting Tensorflow 2.0.</span>

<span class="sd">    The following will raise an exception starting 2.0</span>

<span class="sd">    &gt;&gt;&gt; x = tf.constant(5)</span>
<span class="sd">    &gt;&gt;&gt; y = tf.constant(10)</span>
<span class="sd">    &gt;&gt;&gt; z = tf.constant(10)</span>
<span class="sd">    &gt;&gt;&gt; tensor_set = {x, y, z}</span>
<span class="sd">    Traceback (most recent call last):</span>
<span class="sd">      ...</span>
<span class="sd">    TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.</span>
<span class="sd">    &gt;&gt;&gt; tensor_dict = {x: &#39;five&#39;, y: &#39;ten&#39;}</span>
<span class="sd">    Traceback (most recent call last):</span>
<span class="sd">      ...</span>
<span class="sd">    TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.</span>

<span class="sd">    Instead, we can use `tensor.ref()`.</span>

<span class="sd">    &gt;&gt;&gt; tensor_set = {x.ref(), y.ref(), z.ref()}</span>
<span class="sd">    &gt;&gt;&gt; x.ref() in tensor_set</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; tensor_dict = {x.ref(): &#39;five&#39;, y.ref(): &#39;ten&#39;, z.ref(): &#39;ten&#39;}</span>
<span class="sd">    &gt;&gt;&gt; tensor_dict[y.ref()]</span>
<span class="sd">    &#39;ten&#39;</span>

<span class="sd">    Also, the reference object provides `.deref()` function that returns the</span>
<span class="sd">    original Tensor.</span>

<span class="sd">    &gt;&gt;&gt; x = tf.constant(5)</span>
<span class="sd">    &gt;&gt;&gt; x.ref().deref()</span>
<span class="sd">    &lt;tf.Tensor: shape=(), dtype=int32, numpy=5&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">object_identity</span><span class="o">.</span><span class="n">Reference</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div></div>


<span class="c1"># TODO(agarwal): consider getting rid of this.</span>
<span class="k">class</span> <span class="nc">_EagerTensorBase</span><span class="p">(</span><span class="n">Tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for EagerTensor.&quot;&quot;&quot;</span>

  <span class="c1"># __complex__, __int__, __float__ and __index__ may copy the tensor to CPU and</span>
  <span class="c1"># only work for scalars; values are cast as per numpy.</span>
  <span class="k">def</span> <span class="nf">__complex__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">complex</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__long__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">long</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__float__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__index__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">()</span><span class="o">.</span><span class="fm">__index__</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">())</span>

  <span class="n">__nonzero__</span> <span class="o">=</span> <span class="fm">__bool__</span>

  <span class="k">def</span> <span class="nf">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">()</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="n">format_spec</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__reduce__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">convert_to_tensor</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">(),)</span>

  <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Eager Tensors are immutable so it&#39;s safe to return themselves as a copy.</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
    <span class="c1"># Eager Tensors are immutable so it&#39;s safe to return themselves as a copy.</span>
    <span class="k">del</span> <span class="n">memo</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;tf.Tensor(</span><span class="si">%s</span><span class="s2">, shape=</span><span class="si">%s</span><span class="s2">, dtype=</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">numpy_text</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;&lt;tf.Tensor: shape=</span><span class="si">%s</span><span class="s2">, dtype=</span><span class="si">%s</span><span class="s2">, numpy=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">numpy_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_repr</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the length of the first dimension in the Tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Scalar tensor has no `len()`&quot;</span><span class="p">)</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">except</span> <span class="n">core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">_status_to_exception</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_numpy_internal</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_numpy_internal</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">_status_to_exception</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Note: using the intern table directly here as this is</span>
    <span class="c1"># performance-sensitive in some models.</span>
    <span class="k">return</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">_INTERN_TABLE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_datatype_enum</span><span class="p">()]</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Copy of the contents of this Tensor into a NumPy array or scalar.</span>

<span class="sd">    Unlike NumPy arrays, Tensors are immutable, so this method has to copy</span>
<span class="sd">    the contents to ensure safety. Use `memoryview` to get a readonly</span>
<span class="sd">    view of the contents without doing a copy:</span>

<span class="sd">    &gt;&gt;&gt; t = tf.constant([42])</span>
<span class="sd">    &gt;&gt;&gt; np.array(memoryview(t))</span>
<span class="sd">    array([42], dtype=int32)</span>

<span class="sd">    Note that `memoryview` is only zero-copy for Tensors on CPU. If a Tensor</span>
<span class="sd">    is on GPU, it will have to be transferred to CPU first in order for</span>
<span class="sd">    `memoryview` to work.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A NumPy array of the same shape and dtype or a NumPy scalar, if this</span>
<span class="sd">      Tensor has rank 0.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the dtype of this Tensor does not have a compatible</span>
<span class="sd">        NumPy dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.</span>
    <span class="n">maybe_arr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_numpy</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">maybe_arr</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">maybe_arr</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="k">else</span> <span class="n">maybe_arr</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">backing_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the name of the device holding this tensor&#39;s memory.</span>

<span class="sd">    `.backing_device` is usually the same as `.device`, which returns</span>
<span class="sd">    the device on which the kernel of the operation that produced this tensor</span>
<span class="sd">    ran. However, some operations can produce tensors on a different device</span>
<span class="sd">    (e.g., an operation that executes on the GPU but produces output tensors</span>
<span class="sd">    in host memory).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_datatype_enum</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_shape_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The shape of this Tensor, as a tuple.</span>

<span class="sd">    This is more performant than tuple(shape().as_list()) as it avoids</span>
<span class="sd">    two list and one object creation. Marked private for now as from an API</span>
<span class="sd">    perspective, it would be better to have a single performant way of</span>
<span class="sd">    getting a shape rather than exposing shape() and shape_tuple()</span>
<span class="sd">    (and heaven forbid, shape_list() etc. as well!). Punting on that for now,</span>
<span class="sd">    but ideally one would work things out and remove the need for this method.</span>

<span class="sd">    Returns:</span>
<span class="sd">      tuple with the shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Integer rank of this Tensor.</span>

<span class="sd">    Unlike regular Tensors, the rank is always known for EagerTensors.</span>

<span class="sd">    This is more performant than len(self._shape_tuple())</span>

<span class="sd">    Returns:</span>
<span class="sd">      Integer rank</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_num_elements</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Number of elements of this Tensor.</span>

<span class="sd">    Unlike regular Tensors, the number of elements is always known for</span>
<span class="sd">    EagerTensors.</span>

<span class="sd">    This is more performant than tensor.shape.num_elements</span>

<span class="sd">    Returns:</span>
<span class="sd">      Long - num elements in the tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_copy_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_override_operator</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">_EagerTensorBase</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_copy_nograd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Copies tensor to dest device, but doesn&#39;t record the operation.&quot;&quot;&quot;</span>
    <span class="c1"># Creates a new tensor on the dest device.</span>
    <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_name</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">ctx</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
      <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy_to_device</span><span class="p">(</span><span class="n">device_name</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">_status_to_exception</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_tensor</span>

  <span class="k">def</span> <span class="nf">_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Copies tensor to dest device.&quot;&quot;&quot;</span>
    <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy_nograd</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">device_name</span><span class="p">)</span>
    <span class="c1"># Record the copy on tape and define backprop copy as well.</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="n">self_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>

      <span class="k">def</span> <span class="nf">grad_fun</span><span class="p">(</span><span class="n">dresult</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">dresult</span><span class="o">.</span><span class="n">_copy</span><span class="p">(</span><span class="n">device_name</span><span class="o">=</span><span class="n">self_device</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dresult</span><span class="p">,</span> <span class="s2">&quot;_copy&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">dresult</span>
        <span class="p">]</span>

      <span class="n">tape</span><span class="o">.</span><span class="n">record_operation</span><span class="p">(</span><span class="s2">&quot;_copy&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">new_tensor</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="p">],</span> <span class="n">grad_fun</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_tensor</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># pylint: disable=access-member-before-definition</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="c1"># `_tensor_shape` is declared and defined in the definition of</span>
        <span class="c1"># `EagerTensor`, in C.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">())</span>
      <span class="k">except</span> <span class="n">core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">_status_to_exception</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_shape</span>

  <span class="k">def</span> <span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Alias of Tensor.shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

  <span class="k">def</span> <span class="nf">_shape_as_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The shape of the tensor as a list.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">())</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of Tensor dimensions.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>

  <span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use tf.identity instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A copy of this Tensor with contents backed by host memory.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">(),</span> <span class="s2">&quot;CPU:0&quot;</span><span class="p">)</span>

  <span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use tf.identity instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gpu_index</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A copy of this Tensor with contents backed by memory on the GPU.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      gpu_index: Identifies which GPU to place the contents on the returned</span>
<span class="sd">        Tensor in.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A GPU-memory backed Tensor object initialized with the same contents</span>
<span class="sd">      as this Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_copy</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">(),</span> <span class="s2">&quot;GPU:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gpu_index</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Tensor&#39;s shape </span><span class="si">%s</span><span class="s2"> is not compatible with supplied shape </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>

  <span class="c1"># Methods not supported / implemented for Eager Tensors.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.op is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.graph is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.name is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">value_index</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.value_index is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">consumers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;Tensor.consumers is meaningless when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_add_consumer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">consumer</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;_add_consumer not supported when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_as_node_def_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;_as_node_def_input not supported when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_as_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;_as_tf_output not supported when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;eval is not supported when eager execution is enabled, &quot;</span>
        <span class="s2">&quot;is .numpy() what you&#39;re looking for?&quot;</span><span class="p">)</span>


<span class="c1"># This call creates an EagerTensor class, as a subclass of _EagerTensorBase, and</span>
<span class="c1"># registers it with the current module.</span>
<span class="n">EagerTensor</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_InitEagerTensor</span><span class="p">(</span><span class="n">_EagerTensorBase</span><span class="p">)</span>


<span class="n">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;convert_to_tensor&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">convert_to_tensor_v1</span><span class="p">(</span><span class="n">value</span><span class="p">,</span>
                         <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">dtype_hint</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given `value` to a `Tensor`.</span>

<span class="sd">  This function converts Python objects of various types to `Tensor`</span>
<span class="sd">  objects. It accepts `Tensor` objects, numpy arrays, Python lists,</span>
<span class="sd">  and Python scalars. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  import numpy as np</span>

<span class="sd">  def my_func(arg):</span>
<span class="sd">    arg = tf.convert_to_tensor(arg, dtype=tf.float32)</span>
<span class="sd">    return tf.matmul(arg, arg) + arg</span>

<span class="sd">  # The following calls are equivalent.</span>
<span class="sd">  value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))</span>
<span class="sd">  value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])</span>
<span class="sd">  value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))</span>
<span class="sd">  ```</span>

<span class="sd">  This function can be useful when composing a new operation in Python</span>
<span class="sd">  (such as `my_func` in the example above). All standard Python op</span>
<span class="sd">  constructors apply this function to each of their Tensor-valued</span>
<span class="sd">  inputs, which allows those ops to accept numpy arrays, Python lists,</span>
<span class="sd">  and scalars in addition to `Tensor` objects.</span>

<span class="sd">  Note: This function diverges from default Numpy behavior for `float` and</span>
<span class="sd">    `string` types when `None` is present in a Python list or scalar. Rather</span>
<span class="sd">    than silently converting `None` values, an error will be thrown.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: An object whose type has a registered `Tensor` conversion function.</span>
<span class="sd">    dtype: Optional element type for the returned tensor. If missing, the type</span>
<span class="sd">      is inferred from the type of `value`.</span>
<span class="sd">    name: Optional name to use if a new `Tensor` is created.</span>
<span class="sd">    preferred_dtype: Optional element type for the returned tensor, used when</span>
<span class="sd">      dtype is None. In some cases, a caller may not have a dtype in mind when</span>
<span class="sd">      converting to a tensor, so preferred_dtype can be used as a soft</span>
<span class="sd">      preference.  If the conversion to `preferred_dtype` is not possible, this</span>
<span class="sd">      argument has no effect.</span>
<span class="sd">    dtype_hint: same meaning as preferred_dtype, and overrides it.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for `value` to `dtype`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid value.</span>
<span class="sd">    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">preferred_dtype</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;dtype_hint&quot;</span><span class="p">,</span> <span class="n">dtype_hint</span><span class="p">,</span> <span class="s2">&quot;preferred_dtype&quot;</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">convert_to_tensor_v2</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;convert_to_tensor&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">convert_to_tensor_v2</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype_hint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given `value` to a `Tensor`.</span>

<span class="sd">  This function converts Python objects of various types to `Tensor`</span>
<span class="sd">  objects. It accepts `Tensor` objects, numpy arrays, Python lists,</span>
<span class="sd">  and Python scalars. For example:</span>

<span class="sd">  &gt;&gt;&gt; def my_func(arg):</span>
<span class="sd">  ...   arg = tf.convert_to_tensor(arg, dtype=tf.float32)</span>
<span class="sd">  ...   return arg</span>

<span class="sd">  &gt;&gt;&gt; # The following calls are equivalent.</span>
<span class="sd">  &gt;&gt;&gt; value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))</span>
<span class="sd">  &gt;&gt;&gt; print(value_1)</span>
<span class="sd">  tf.Tensor(</span>
<span class="sd">    [[1. 2.]</span>
<span class="sd">     [3. 4.]], shape=(2, 2), dtype=float32)</span>
<span class="sd">  &gt;&gt;&gt; value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])</span>
<span class="sd">  &gt;&gt;&gt; print(value_2)</span>
<span class="sd">  tf.Tensor(</span>
<span class="sd">    [[1. 2.]</span>
<span class="sd">     [3. 4.]], shape=(2, 2), dtype=float32)</span>
<span class="sd">  &gt;&gt;&gt; value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))</span>
<span class="sd">  &gt;&gt;&gt; print(value_3)</span>
<span class="sd">  tf.Tensor(</span>
<span class="sd">    [[1. 2.]</span>
<span class="sd">     [3. 4.]], shape=(2, 2), dtype=float32)</span>

<span class="sd">  This function can be useful when composing a new operation in Python</span>
<span class="sd">  (such as `my_func` in the example above). All standard Python op</span>
<span class="sd">  constructors apply this function to each of their Tensor-valued</span>
<span class="sd">  inputs, which allows those ops to accept numpy arrays, Python lists,</span>
<span class="sd">  and scalars in addition to `Tensor` objects.</span>

<span class="sd">  Note: This function diverges from default Numpy behavior for `float` and</span>
<span class="sd">    `string` types when `None` is present in a Python list or scalar. Rather</span>
<span class="sd">    than silently converting `None` values, an error will be thrown.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: An object whose type has a registered `Tensor` conversion function.</span>
<span class="sd">    dtype: Optional element type for the returned tensor. If missing, the type</span>
<span class="sd">      is inferred from the type of `value`.</span>
<span class="sd">    dtype_hint: Optional element type for the returned tensor, used when dtype</span>
<span class="sd">      is None. In some cases, a caller may not have a dtype in mind when</span>
<span class="sd">      converting to a tensor, so dtype_hint can be used as a soft preference.</span>
<span class="sd">      If the conversion to `dtype_hint` is not possible, this argument has no</span>
<span class="sd">      effect.</span>
<span class="sd">    name: Optional name to use if a new `Tensor` is created.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for `value` to `dtype`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid value.</span>
<span class="sd">    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">convert_to_tensor</span><span class="p">(</span>
      <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">dtype_hint</span><span class="p">,</span>
      <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="k">return</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: &quot;</span> <span class="o">%</span> <span class="n">name</span>


<div class="viewcode-block" id="convert_to_tensor"><a class="viewcode-back" href="../../../../index.html#tensorflow.convert_to_tensor">[docs]</a><span class="k">def</span> <span class="nf">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dtype_hint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">accepted_result_types</span><span class="o">=</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,)):</span>
  <span class="sd">&quot;&quot;&quot;Implementation of the public convert_to_tensor.&quot;&quot;&quot;</span>
  <span class="c1"># TODO(b/142518781): Fix all call-sites and remove redundant arg</span>
  <span class="n">preferred_dtype</span> <span class="o">=</span> <span class="n">preferred_dtype</span> <span class="ow">or</span> <span class="n">dtype_hint</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">EagerTensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Attempting to capture an EagerTensor without &quot;</span>
                           <span class="s2">&quot;building a function.&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">graph</span><span class="o">.</span><span class="n">capture</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Tensor conversion requested dtype </span><span class="si">%s</span><span class="s2"> for Tensor with dtype </span><span class="si">%s</span><span class="s2">: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">value</span>

  <span class="k">if</span> <span class="n">preferred_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">preferred_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">preferred_dtype</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">conversion_func</span> <span class="ow">in</span> <span class="n">tensor_conversion_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)):</span>
    <span class="c1"># If dtype is None but preferred_dtype is not None, we try to</span>
    <span class="c1"># cast to preferred_dtype first.</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">preferred_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">conversion_func</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">preferred_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>
      <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="c1"># Could not coerce the conversion to use the preferred dtype.</span>
        <span class="k">pass</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">NotImplemented</span> <span class="ow">and</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">!=</span> <span class="n">preferred_dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;convert_to_tensor did not convert to &quot;</span>
                          <span class="s2">&quot;the preferred dtype: </span><span class="si">%s</span><span class="s2"> vs </span><span class="si">%s</span><span class="s2"> &quot;</span> <span class="o">%</span>
                          <span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">conversion_func</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="bp">NotImplemented</span><span class="p">:</span>
      <span class="k">continue</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">accepted_result_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">Conversion function </span><span class="si">%r</span><span class="s2"> for type </span><span class="si">%s</span><span class="s2"> returned non-Tensor: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">conversion_func</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">ret</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">Conversion function </span><span class="si">%r</span><span class="s2"> for type </span><span class="si">%s</span><span class="s2"> returned incompatible &quot;</span>
          <span class="s2">&quot;dtype: requested = </span><span class="si">%s</span><span class="s2">, actual = </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">conversion_func</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
           <span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span>
  <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">Cannot convert </span><span class="si">%r</span><span class="s2"> with type </span><span class="si">%s</span><span class="s2"> to Tensor: &quot;</span>
                  <span class="s2">&quot;no conversion function registered.&quot;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">_error_prefix</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">value</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span></div>


<span class="n">internal_convert_to_tensor</span> <span class="o">=</span> <span class="n">convert_to_tensor</span>


<span class="k">def</span> <span class="nf">internal_convert_n_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span>
                                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Tensor` objects.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of objects that can be consumed by `tf.convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` objects.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is created, in</span>
<span class="sd">      which case element `i` will be given the name `name + &#39;_&#39; + i`.</span>
<span class="sd">    as_ref: True if the caller wants the results as ref tensors.</span>
<span class="sd">    preferred_dtype: Optional element type for the returned tensors, used when</span>
<span class="sd">      dtype is None. In some cases, a caller may not have a dtype in mind when</span>
<span class="sd">      converting to a tensor, so preferred_dtype can be used as a soft</span>
<span class="sd">      preference.  If the conversion to `preferred_dtype` is not possible, this</span>
<span class="sd">      argument has no effect.</span>
<span class="sd">    ctx: The value of context.context().</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` and/or `IndexedSlices` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">collections_abc</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;values must be a sequence.&quot;</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">convert_to_tensor</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
            <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">,</span>
            <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">preferred_dtype</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">convert_n_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Tensor` objects.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of objects that can be consumed by `tf.convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` objects.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is created, in</span>
<span class="sd">      which case element `i` will be given the name `name + &#39;_&#39; + i`.</span>
<span class="sd">    preferred_dtype: Optional element type for the returned tensors, used when</span>
<span class="sd">      dtype is None. In some cases, a caller may not have a dtype in mind when</span>
<span class="sd">      converting to a tensor, so preferred_dtype can be used as a soft</span>
<span class="sd">      preference.  If the conversion to `preferred_dtype` is not possible, this</span>
<span class="sd">      argument has no effect.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` and/or `IndexedSlices` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">internal_convert_n_to_tensor</span><span class="p">(</span>
      <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
      <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">preferred_dtype</span><span class="p">,</span>
      <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given object to a `Tensor` or `CompositeTensor`.</span>

<span class="sd">  If `value` is a `CompositeTensor` it is returned unmodified. Otherwise, it</span>
<span class="sd">  is converted to a `Tensor` using `convert_to_tensor()`.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A `CompositeTensor` or an object that can be consumed by</span>
<span class="sd">      `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` or</span>
<span class="sd">      `CompositeTensor`.</span>
<span class="sd">    name: (Optional.) A name to use if a new `Tensor` is created.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `CompositeTensor`, based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `dtype` does not match the element type of `value`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span>
      <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">internal_convert_to_tensor_or_composite</span><span class="p">(</span><span class="n">value</span><span class="p">,</span>
                                            <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                            <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                            <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts the given object to a `Tensor` or `CompositeTensor`.</span>

<span class="sd">  If `value` is a `CompositeTensor` it is returned unmodified.  Otherwise, it</span>
<span class="sd">  is converted to a `Tensor` using `convert_to_tensor()`.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A `CompositeTensor`, or an object that can be consumed by</span>
<span class="sd">      `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor` or</span>
<span class="sd">      `CompositeTensor`.</span>
<span class="sd">    name: (Optional.) A name to use if a new `Tensor` is created.</span>
<span class="sd">    as_ref: True if the caller wants the results as ref tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `CompositeTensor`, based on `value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `dtype` does not match the element type of `value`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">):</span>
    <span class="n">value_dtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">value_dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Tensor conversion requested dtype </span><span class="si">%s</span><span class="s2"> for Tensor with dtype </span><span class="si">%s</span><span class="s2">: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">value</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="n">value</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">,</span>
        <span class="n">accepted_result_types</span><span class="o">=</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">composite_tensor</span><span class="o">.</span><span class="n">CompositeTensor</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">internal_convert_n_to_tensor_or_composite</span><span class="p">(</span><span class="n">values</span><span class="p">,</span>
                                              <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                              <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                              <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Tensor` or `CompositeTensor` objects.</span>

<span class="sd">  Any `CompositeTensor` objects in `values` are returned unmodified.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of `None`, `CompositeTensor`, or objects that can be consumed</span>
<span class="sd">      by `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor`s or</span>
<span class="sd">      `CompositeTensor`s.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is created, in</span>
<span class="sd">      which case element `i` will be given the name `name + &#39;_&#39; + i`.</span>
<span class="sd">    as_ref: True if the caller wants the results as ref tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor`, `CompositeTensor`, and/or `None` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">collections_abc</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;values must be a sequence.&quot;</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">n</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">internal_convert_to_tensor_or_composite</span><span class="p">(</span>
              <span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">convert_n_to_tensor_or_composite</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts `values` to a list of `Output` or `CompositeTensor` objects.</span>

<span class="sd">  Any `CompositeTensor` objects in `values` are returned unmodified.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: A list of `None`, `CompositeTensor``, or objects that can be</span>
<span class="sd">      consumed by `convert_to_tensor()`.</span>
<span class="sd">    dtype: (Optional.) The required `DType` of the returned `Tensor`s or</span>
<span class="sd">      `CompositeTensor`s.</span>
<span class="sd">    name: (Optional.) A name prefix to used when a new `Tensor` is created, in</span>
<span class="sd">      which case element `i` will be given the name `name + &#39;_&#39; + i`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor` and/or `CompositeTensor` objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If no conversion function is registered for an element in</span>
<span class="sd">      `values`.</span>
<span class="sd">    RuntimeError: If a registered conversion function returns an invalid</span>
<span class="sd">      value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">internal_convert_n_to_tensor_or_composite</span><span class="p">(</span>
      <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_device_string</span><span class="p">(</span><span class="n">dev_spec</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">pydev</span><span class="o">.</span><span class="n">is_device_spec</span><span class="p">(</span><span class="n">dev_spec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dev_spec</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">dev_spec</span>


<span class="k">def</span> <span class="nf">_NodeDef</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a NodeDef proto.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_type: Value for the &quot;op&quot; attribute of the NodeDef proto.</span>
<span class="sd">    name: Value for the &quot;name&quot; attribute of the NodeDef proto.</span>
<span class="sd">    attrs: Dictionary where the key is the attribute name (a string)</span>
<span class="sd">      and the value is the respective &quot;attr&quot; attribute of the NodeDef proto (an</span>
<span class="sd">      AttrValue).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A node_def_pb2.NodeDef protocol buffer.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">op_type</span><span class="p">),</span>
                                  <span class="n">name</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">attrs</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">attrs</span><span class="p">):</span>
      <span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">node_def</span>


<span class="c1"># Copied from core/framework/node_def_util.cc</span>
<span class="c1"># TODO(mrry,josh11b): Consolidate this validation in C++ code.</span>
<span class="n">_VALID_OP_NAME_REGEX</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;^[A-Za-z0-9.][A-Za-z0-9_.</span><span class="se">\\</span><span class="s2">-/&gt;]*$&quot;</span><span class="p">)</span>
<span class="n">_VALID_SCOPE_NAME_REGEX</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;^[A-Za-z0-9_.</span><span class="se">\\</span><span class="s2">-/&gt;]*$&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_c_op</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">control_inputs</span><span class="p">,</span> <span class="n">op_def</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a TF_Operation.</span>

<span class="sd">  Args:</span>
<span class="sd">    graph: a `Graph`.</span>
<span class="sd">    node_def: `node_def_pb2.NodeDef` for the operation to create.</span>
<span class="sd">    inputs: A flattened list of `Tensor`s. This function handles grouping</span>
<span class="sd">      tensors into lists as per attributes in the `node_def`.</span>
<span class="sd">    control_inputs: A list of `Operation`s to set as control dependencies.</span>
<span class="sd">    op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not</span>
<span class="sd">      specified, is looked up from the `graph` using `node_def.op`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped TF_Operation*.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">op_def</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">op_def</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">_get_op_def</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="c1"># TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.</span>
  <span class="c1"># Refactor so we don&#39;t have to do this here.</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">_reconstruct_sequence_inputs</span><span class="p">(</span><span class="n">op_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">)</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">op_desc</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_NewOperation</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>
                                              <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">op</span><span class="p">),</span>
                                              <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">node_def</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_SetDevice</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
  <span class="c1"># Add inputs</span>
  <span class="k">for</span> <span class="n">op_input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_input</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_AddInputList</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span>
                                        <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">op_input</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_AddInput</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="n">op_input</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>

  <span class="c1"># Add control inputs</span>
  <span class="k">for</span> <span class="n">control_input</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_AddControlInput</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="n">control_input</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
  <span class="c1"># pylint: enable=protected-access</span>

  <span class="c1"># Add attrs</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">attr_value</span> <span class="ow">in</span> <span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">serialized</span> <span class="o">=</span> <span class="n">attr_value</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
    <span class="c1"># TODO(skyewm): this creates and deletes a new TF_Status for every attr.</span>
    <span class="c1"># It might be worth creating a convenient way to re-use the same status.</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_SetAttrValueProto</span><span class="p">(</span><span class="n">op_desc</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                           <span class="n">serialized</span><span class="p">)</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">c_op</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_FinishOperation</span><span class="p">(</span><span class="n">op_desc</span><span class="p">)</span>
  <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># Convert to ValueError for backwards compatibility.</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">c_op</span>


<div class="viewcode-block" id="Operation"><a class="viewcode-back" href="../../../../index.html#tensorflow.Operation">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;Operation&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Operation</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Represents a graph node that performs computation on tensors.</span>

<span class="sd">  An `Operation` is a node in a `tf.Graph` that takes zero or more `Tensor`</span>
<span class="sd">  objects as input, and produces zero or more `Tensor` objects as output.</span>
<span class="sd">  Objects of type `Operation` are created by calling a Python op constructor</span>
<span class="sd">  (such as `tf.matmul`) within a `tf.function` or under a `tf.Graph.as_default`</span>
<span class="sd">  context manager.</span>

<span class="sd">  For example, within a `tf.function`, `c = tf.matmul(a, b)` creates an</span>
<span class="sd">  `Operation` of type &quot;MatMul&quot; that takes tensors `a` and `b` as input, and</span>
<span class="sd">  produces `c` as output.</span>

<span class="sd">  If a `tf.compat.v1.Session` is used, an `Operation` of a `tf.Graph` can be</span>
<span class="sd">  executed by passing it to `tf.Session.run`. `op.run()` is a shortcut for</span>
<span class="sd">  calling `tf.compat.v1.get_default_session().run(op)`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">node_def</span><span class="p">,</span>
               <span class="n">g</span><span class="p">,</span>
               <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">output_types</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">control_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">input_types</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">original_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">op_def</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates an `Operation`.</span>

<span class="sd">    NOTE: This constructor validates the name of the `Operation` (passed</span>
<span class="sd">    as `node_def.name`). Valid `Operation` names match the following</span>
<span class="sd">    regular expression:</span>

<span class="sd">        [A-Za-z0-9.][A-Za-z0-9_.\\-/]*</span>

<span class="sd">    Args:</span>
<span class="sd">      node_def: `node_def_pb2.NodeDef`.  `NodeDef` for the `Operation`. Used for</span>
<span class="sd">        attributes of `node_def_pb2.NodeDef`, typically `name`, `op`, and</span>
<span class="sd">        `device`.  The `input` attribute is irrelevant here as it will be</span>
<span class="sd">        computed when generating the model.</span>
<span class="sd">      g: `Graph`. The parent graph.</span>
<span class="sd">      inputs: list of `Tensor` objects. The inputs to this `Operation`.</span>
<span class="sd">      output_types: list of `DType` objects.  List of the types of the `Tensors`</span>
<span class="sd">        computed by this operation.  The length of this list indicates the</span>
<span class="sd">        number of output endpoints of the `Operation`.</span>
<span class="sd">      control_inputs: list of operations or tensors from which to have a control</span>
<span class="sd">        dependency.</span>
<span class="sd">      input_types: List of `DType` objects representing the types of the tensors</span>
<span class="sd">        accepted by the `Operation`.  By default uses `[x.dtype.base_dtype for x</span>
<span class="sd">        in inputs]`.  Operations that expect reference-typed inputs must specify</span>
<span class="sd">        these explicitly.</span>
<span class="sd">      original_op: Optional. Used to associate the new `Operation` with an</span>
<span class="sd">        existing `Operation` (for example, a replica with the op that was</span>
<span class="sd">        replicated).</span>
<span class="sd">      op_def: Optional. The `op_def_pb2.OpDef` proto that describes the op type</span>
<span class="sd">        that this `Operation` represents.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if control inputs are not Operations or Tensors,</span>
<span class="sd">        or if `node_def` is not a `NodeDef`,</span>
<span class="sd">        or if `g` is not a `Graph`,</span>
<span class="sd">        or if `inputs` are not tensors,</span>
<span class="sd">        or if `inputs` and `input_types` are incompatible.</span>
<span class="sd">      ValueError: if the `node_def` name is not valid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># For internal use only: `node_def` can be set to a TF_Operation to create</span>
    <span class="c1"># an Operation for that op. This is useful for creating Operations for ops</span>
    <span class="c1"># indirectly created by C API methods, e.g. the ops created by</span>
    <span class="c1"># TF_ImportGraphDef. When `node_def` is a TF_Operation, all optional fields</span>
    <span class="c1"># should be None.</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_def</span><span class="p">,</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">node_def</span><span class="o">.</span><span class="n">ByteSize</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">31</span><span class="p">)</span> <span class="ow">or</span> <span class="n">node_def</span><span class="o">.</span><span class="n">ByteSize</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot create a tensor proto whose content is larger than 2GB.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">_VALID_OP_NAME_REGEX</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; is not a valid node name&quot;</span> <span class="o">%</span> <span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">c_op</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">node_def</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;TF_Operation&quot;</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">output_types</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">control_inputs</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">input_types</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">original_op</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">op_def</span> <span class="ow">is</span> <span class="kc">None</span>
      <span class="n">c_op</span> <span class="o">=</span> <span class="n">node_def</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;node_def needs to be a NodeDef: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">node_def</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">Graph</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;g needs to be a Graph: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">g</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">g</span>

    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;inputs needs to be a list of Tensors: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;input needs to be a Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_types</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">input_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
          <span class="n">x</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_types</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;In op &#39;</span><span class="si">%s</span><span class="s2">&#39;, input types (</span><span class="si">%s</span><span class="s2">) are not compatible &quot;</span>
                        <span class="s2">&quot;with expected types (</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span>
                        <span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">input_types</span><span class="p">))</span>

    <span class="c1"># Build the list of control inputs.</span>
    <span class="n">control_input_ops</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">control_inputs</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
        <span class="n">control_op</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
          <span class="n">control_op</span> <span class="o">=</span> <span class="n">c</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">IndexedSlices</span><span class="p">)):</span>
          <span class="n">control_op</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">op</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Control input must be an Operation, &quot;</span>
                          <span class="s2">&quot;a Tensor, or IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">c</span><span class="p">)</span>
        <span class="n">control_input_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">control_op</span><span class="p">)</span>

    <span class="c1"># This will be set by self.inputs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># pylint: disable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_original_op</span> <span class="o">=</span> <span class="n">original_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_traceback</span> <span class="o">=</span> <span class="n">tf_stack</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">()</span>

    <span class="c1"># List of _UserDevSpecs holding code location of device context manager</span>
    <span class="c1"># invocations and the users original argument to them.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_code_locations</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Dict mapping op name to file and line information for op colocation</span>
    <span class="c1"># context managers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_code_locations</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>

    <span class="c1"># Gradient function for this op. There are three ways to specify gradient</span>
    <span class="c1"># function, and first available gradient gets used, in the following order.</span>
    <span class="c1"># 1. self._gradient_function</span>
    <span class="c1"># 2. Gradient name registered by &quot;_gradient_op_type&quot; attribute.</span>
    <span class="c1"># 3. Gradient name registered by op.type.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_function</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Initialize self._c_op.</span>
    <span class="k">if</span> <span class="n">c_op</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span> <span class="o">=</span> <span class="n">c_op</span>
      <span class="n">op_def</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">_get_op_def</span><span class="p">(</span><span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationOpType</span><span class="p">(</span><span class="n">c_op</span><span class="p">))</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">op_def</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op_def</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_op_def</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span> <span class="o">=</span> <span class="n">_create_c_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="p">,</span> <span class="n">node_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span>
                                <span class="n">control_input_ops</span><span class="p">,</span> <span class="n">op_def</span><span class="p">)</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_is_stateful</span> <span class="o">=</span> <span class="n">op_def</span><span class="o">.</span><span class="n">is_stateful</span>

    <span class="c1"># Initialize self._outputs.</span>
    <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationNumOutputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">):</span>
      <span class="n">tf_output</span> <span class="o">=</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_output</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
      <span class="n">output_type</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationOutputType</span><span class="p">(</span><span class="n">tf_output</span><span class="p">)</span>
      <span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_create_with_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">tf_output</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_id_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_add_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">c_op</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_post_processing</span><span class="p">(</span><span class="n">input_tensors</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_control_flow_post_processing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add this op to its control flow context.</span>

<span class="sd">    This may add new ops and change this op&#39;s inputs. self.inputs must be</span>
<span class="sd">    available before calling this method.</span>

<span class="sd">    Args:</span>
<span class="sd">      input_tensors: (Optional.) A list of `Tensors` corresponding to the inputs</span>
<span class="sd">        of this op, which should be equivalent to `self.inputs`. Pass this</span>
<span class="sd">        argument to avoid evaluating `self.inputs` unnecessarily.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_tensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">input_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span>
    <span class="k">for</span> <span class="n">input_tensor</span> <span class="ow">in</span> <span class="n">input_tensors</span><span class="p">:</span>
      <span class="n">control_flow_util</span><span class="o">.</span><span class="n">CheckInputFromValidContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span><span class="o">.</span><span class="n">AddOp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="Operation.colocation_groups"><a class="viewcode-back" href="../../../../index.html#tensorflow.Operation.colocation_groups">[docs]</a>  <span class="k">def</span> <span class="nf">colocation_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the list of colocation groups of the op.&quot;&quot;&quot;</span>
    <span class="n">default_colocation_group</span> <span class="o">=</span> <span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="s2">&quot;loc:@</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)]</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">class_attr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;_class&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
      <span class="c1"># This op has no explicit colocation group, so it is itself its</span>
      <span class="c1"># own root of a colocation group.</span>
      <span class="k">return</span> <span class="n">default_colocation_group</span>

    <span class="n">attr_groups</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">class_name</span> <span class="k">for</span> <span class="n">class_name</span> <span class="ow">in</span> <span class="n">class_attr</span>
        <span class="k">if</span> <span class="n">class_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;loc:@&quot;</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># If there are no colocation groups in the explicit _class field,</span>
    <span class="c1"># return the default colocation group.</span>
    <span class="k">return</span> <span class="n">attr_groups</span> <span class="k">if</span> <span class="n">attr_groups</span> <span class="k">else</span> <span class="n">default_colocation_group</span></div>

<div class="viewcode-block" id="Operation.values"><a class="viewcode-back" href="../../../../index.html#tensorflow.Operation.values">[docs]</a>  <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;DEPRECATED: Use outputs.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_get_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the control flow context of this op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span>

  <span class="k">def</span> <span class="nf">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the current control flow context of this op.</span>

<span class="sd">    Args:</span>
<span class="sd">      ctx: a context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="n">ctx</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The full name of this operation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The unique integer id of this operation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_id_value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of the device to which this op has been assigned, if any.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The string name of the device to which this op has been</span>
<span class="sd">      assigned, or an empty string if it has not been assigned to a</span>
<span class="sd">      device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationDevice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_assignments</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Code locations for device context managers active at op creation.</span>

<span class="sd">    This property will return a list of traceable_stack.TraceableObject</span>
<span class="sd">    instances where .obj is a string representing the assigned device</span>
<span class="sd">    (or information about the function that would be applied to this op</span>
<span class="sd">    to compute the desired device) and the filename and lineno members</span>
<span class="sd">    record the location of the relevant device context manager.</span>

<span class="sd">    For example, suppose file_a contained these lines:</span>

<span class="sd">      file_a.py:</span>
<span class="sd">        15: with tf.device(&#39;/gpu:0&#39;):</span>
<span class="sd">        16:   node_b = tf.constant(4, name=&#39;NODE_B&#39;)</span>

<span class="sd">    Then a TraceableObject t_obj representing the device context manager</span>
<span class="sd">    would have these member values:</span>

<span class="sd">      t_obj.obj -&gt; &#39;/gpu:0&#39;</span>
<span class="sd">      t_obj.filename = &#39;file_a.py&#39;</span>
<span class="sd">      t_obj.lineno = 15</span>

<span class="sd">    and node_b.op._device_assignments would return the list [t_obj].</span>

<span class="sd">    Returns:</span>
<span class="sd">      [str: traceable_stack.TraceableObject, ...] as per this method&#39;s</span>
<span class="sd">      description, above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_code_locations</span> <span class="ow">or</span> <span class="p">[]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_colocation_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Code locations for colocation context managers active at op creation.</span>

<span class="sd">    This property will return a dictionary for which the keys are nodes with</span>
<span class="sd">    which this Operation is colocated, and for which the values are</span>
<span class="sd">    traceable_stack.TraceableObject instances.  The TraceableObject instances</span>
<span class="sd">    record the location of the relevant colocation context manager but have the</span>
<span class="sd">    &quot;obj&quot; field set to None to prevent leaking private data.</span>

<span class="sd">    For example, suppose file_a contained these lines:</span>

<span class="sd">      file_a.py:</span>
<span class="sd">        14: node_a = tf.constant(3, name=&#39;NODE_A&#39;)</span>
<span class="sd">        15: with tf.compat.v1.colocate_with(node_a):</span>
<span class="sd">        16:   node_b = tf.constant(4, name=&#39;NODE_B&#39;)</span>

<span class="sd">    Then a TraceableObject t_obj representing the colocation context manager</span>
<span class="sd">    would have these member values:</span>

<span class="sd">      t_obj.obj -&gt; None</span>
<span class="sd">      t_obj.filename = &#39;file_a.py&#39;</span>
<span class="sd">      t_obj.lineno = 15</span>

<span class="sd">    and node_b.op._colocation_dict would return the dictionary</span>

<span class="sd">      { &#39;NODE_A&#39;: t_obj }</span>

<span class="sd">    Returns:</span>
<span class="sd">      {str: traceable_stack.TraceableObject} as per this method&#39;s description,</span>
<span class="sd">      above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">locations_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_code_locations</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="k">return</span> <span class="n">locations_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_output_types</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;List this operation&#39;s output types.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of the types of the Tensors computed by this operation.</span>
<span class="sd">      Each element in the list is an integer whose value is one of</span>
<span class="sd">      the TF_DataType enums defined in pywrap_tf_session.h</span>
<span class="sd">      The length of this list indicates the number of output endpoints</span>
<span class="sd">      of the operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationNumOutputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="n">output_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">int</span><span class="p">(</span><span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationOutputType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tf_output</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">output_types</span>

  <span class="k">def</span> <span class="nf">_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create and return a new TF_Output for output_idx&#39;th output of this op.&quot;&quot;&quot;</span>
    <span class="n">tf_output</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_Output</span><span class="p">()</span>
    <span class="n">tf_output</span><span class="o">.</span><span class="n">oper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span>
    <span class="n">tf_output</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">output_idx</span>
    <span class="k">return</span> <span class="n">tf_output</span>

  <span class="k">def</span> <span class="nf">_tf_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create and return a new TF_Input for input_idx&#39;th input of this op.&quot;&quot;&quot;</span>
    <span class="n">tf_input</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_Input</span><span class="p">()</span>
    <span class="n">tf_input</span><span class="o">.</span><span class="n">oper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span>
    <span class="n">tf_input</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">input_idx</span>
    <span class="k">return</span> <span class="n">tf_input</span>

  <span class="k">def</span> <span class="nf">_set_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
    <span class="sd">&quot;&quot;&quot;Set the device of this operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      device: string or device..  The device to set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_device_from_string</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">_device_string</span><span class="p">(</span><span class="n">device</span><span class="p">)))</span>

  <span class="k">def</span> <span class="nf">_set_device_from_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_str</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fast path to set device if the type is known to be a string.</span>

<span class="sd">    This function is called frequently enough during graph construction that</span>
<span class="sd">    there are non-trivial performance gains if the caller can guarantee that</span>
<span class="sd">    the specified device is already a string.</span>

<span class="sd">    Args:</span>
<span class="sd">      device_str: A string specifying where to place this op.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">SetRequestedDevice</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">device_str</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_update_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update the input to this operation at the given index.</span>

<span class="sd">    NOTE: This is for TF internal use only. Please don&#39;t use it.</span>

<span class="sd">    Args:</span>
<span class="sd">      index: the index of the input to update.</span>
<span class="sd">      tensor: the Tensor to be used as the input at the given index.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if tensor is not a Tensor,</span>
<span class="sd">        or if input tensor type is not convertible to dtype.</span>
<span class="sd">      ValueError: if the Tensor is from a different graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;tensor must be a Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tensor</span><span class="p">)</span>
    <span class="n">_assert_same_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>

    <span class="c1"># Reset cached inputs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">UpdateEdge</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">tensor</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">(),</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tf_input</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_add_while_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See AddWhileInputHack in python_api.h.</span>

<span class="sd">    NOTE: This is for TF internal use only. Please don&#39;t use it.</span>

<span class="sd">    Args:</span>
<span class="sd">      tensors: list of Tensors</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if tensor is not a Tensor,</span>
<span class="sd">        or if input tensor type is not convertible to dtype.</span>
<span class="sd">      ValueError: if the Tensor is from a different graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;tensor must be a Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tensor</span><span class="p">)</span>
      <span class="n">_assert_same_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>

      <span class="c1"># Reset cached inputs.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">AddWhileInputHack</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">tensor</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">(),</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_add_control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ops</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a list of new control inputs to this operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      ops: the list of Operations to add as control input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if ops is not a list of Operations.</span>
<span class="sd">      ValueError: if any op in ops is from a different graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">ops</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op must be an Operation: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">AddControlInput</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_add_control_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add a new control input to this operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: the Operation to add as control input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if op is not an Operation.</span>
<span class="sd">      ValueError: if op is from a different graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op must be an Operation: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">op</span><span class="p">)</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">AddControlInput</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">op</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_remove_all_control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Removes any control inputs to this operation.&quot;&quot;&quot;</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">RemoveAllControlInputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_add_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds new Tensors to self.outputs.</span>

<span class="sd">    Note: this is generally unsafe to use. This is used in certain situations in</span>
<span class="sd">    conjunction with _set_type_list_attr.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      types: list of DTypes</span>
<span class="sd">      shapes: list of TensorShapes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">types</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span>
    <span class="n">orig_num_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">types</span><span class="p">)):</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">orig_num_outputs</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">types</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
      <span class="n">t</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">shapes</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;&lt;tf.Operation &#39;</span><span class="si">%s</span><span class="s2">&#39; type=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The list of `Tensor` objects representing the outputs of this op.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The sequence of `Tensor` objects representing the data inputs of this op.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
          <span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_tensor_by_tf_output</span><span class="p">,</span>
              <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">GetOperationInputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)))</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_val</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_input_types</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationNumInputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="n">input_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span>
            <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationInputType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tf_input</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">input_types</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` objects on which this op has a control dependency.</span>

<span class="sd">    Before this op is executed, TensorFlow will ensure that the</span>
<span class="sd">    operations in `self.control_inputs` have finished executing. This</span>
<span class="sd">    mechanism can be used to run ops sequentially for performance</span>
<span class="sd">    reasons, or to ensure that the side effects of an op are observed</span>
<span class="sd">    in the correct order.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Operation` objects.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">control_c_ops</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationGetControlInputs_wrapper</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span>
            <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="n">c_op</span><span class="p">))</span> <span class="k">for</span> <span class="n">c_op</span> <span class="ow">in</span> <span class="n">control_c_ops</span>
    <span class="p">]</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_control_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` objects which have a control dependency on this op.</span>

<span class="sd">    Before any of the ops in self._control_outputs can execute tensorflow will</span>
<span class="sd">    ensure self has finished executing.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of `Operation` objects.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">control_c_ops</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationGetControlOutputs_wrapper</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span>
            <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="n">c_op</span><span class="p">))</span> <span class="k">for</span> <span class="n">c_op</span> <span class="ow">in</span> <span class="n">control_c_ops</span>
    <span class="p">]</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The type of the op (e.g. `&quot;MatMul&quot;`).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationOpType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` that contains this operation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">node_def</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns the `NodeDef` representation of this operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A</span>
<span class="sd">      [`NodeDef`](https://www.tensorflow.org/code/tensorflow/core/framework/node_def.proto)</span>
<span class="sd">      protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationToNodeDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">()</span>
    <span class="n">node_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">node_def</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op_def</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns the `OpDef` proto that represents the type of this op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An</span>
<span class="sd">      [`OpDef`](https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto)</span>
<span class="sd">      protocol buffer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_op_def</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">traceback</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the call stack from when this operation was constructed.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_traceback</span>

  <span class="k">def</span> <span class="nf">_set_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private method used to set an attribute in the node_def.&quot;&quot;&quot;</span>
    <span class="n">buf</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_NewBufferFromString</span><span class="p">(</span>
        <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">attr_value</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()))</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_set_attr_with_buf</span><span class="p">(</span><span class="n">attr_name</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_DeleteBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_set_attr_with_buf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_buf</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set an attr in the node_def with a pre-allocated buffer.&quot;&quot;&quot;</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">SetAttr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span>
                              <span class="n">attr_buf</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="k">def</span> <span class="nf">_set_func_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private method used to set a function attribute in the node_def.&quot;&quot;&quot;</span>
    <span class="n">func</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">NameAttrList</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_set_func_list_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">func_names</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private method used to set a list(function) attribute in the node_def.&quot;&quot;&quot;</span>
    <span class="n">funcs</span> <span class="o">=</span> <span class="p">[</span><span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">NameAttrList</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">func_name</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">func_names</span><span class="p">]</span>
    <span class="n">funcs_list</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">funcs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="nb">list</span><span class="o">=</span><span class="n">funcs_list</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_set_type_list_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">types</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private method used to set a list(type) attribute in the node_def.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">types</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">types</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">DType</span><span class="p">):</span>
      <span class="n">types</span> <span class="o">=</span> <span class="p">[</span><span class="n">dt</span><span class="o">.</span><span class="n">as_datatype_enum</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">types</span><span class="p">]</span>
    <span class="n">types_list</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="n">types</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="nb">list</span><span class="o">=</span><span class="n">types_list</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_set_shape_list_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private method used to set a list(shape) attribute in the node_def.&quot;&quot;&quot;</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">as_proto</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">]</span>
    <span class="n">shapes_list</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shapes</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="nb">list</span><span class="o">=</span><span class="n">shapes_list</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_clear_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private method used to clear an attribute in the node_def.&quot;&quot;&quot;</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">ClearAttr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

<div class="viewcode-block" id="Operation.get_attr"><a class="viewcode-back" href="../../../../index.html#tensorflow.Operation.get_attr">[docs]</a>  <span class="k">def</span> <span class="nf">get_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the value of the attr of this op with the given `name`.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the attr to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The value of the attr, as a Python object.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If this op does not have an attr with the given `name`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fields</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;func&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
        <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationGetAttrValueProto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># Convert to ValueError for backwards compatibility.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">()</span>
    <span class="n">x</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="n">oneof_value</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">oneof_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">oneof_value</span> <span class="o">==</span> <span class="s2">&quot;list&quot;</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">list</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">f</span> <span class="o">==</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">type</span><span class="p">]</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">list</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>
      <span class="k">return</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">oneof_value</span> <span class="o">==</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">oneof_value</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">,</span> <span class="s2">&quot;Unsupported field type in &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">oneof_value</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_get_attr_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `DType` value of the attr of this op with the given `name`.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">dtype_enum</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationGetAttrType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">_DTYPES_INTERN_TABLE</span><span class="p">[</span><span class="n">dtype_enum</span><span class="p">]</span>
    <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># Convert to ValueError for backwards compatibility.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_get_attr_bool</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `bool` value of the attr of this op with the given `name`.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationGetAttrBool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># Convert to ValueError for backwards compatibility.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_get_attr_int</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `int` value of the attr of this op with the given `name`.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationGetAttrInt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_op</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># Convert to ValueError for backwards compatibility.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>

<div class="viewcode-block" id="Operation.run"><a class="viewcode-back" href="../../../../index.html#tensorflow.Operation.run">[docs]</a>  <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Runs this operation in a `Session`.</span>

<span class="sd">    Calling this method will execute all preceding operations that</span>
<span class="sd">    produce the inputs needed for this operation.</span>

<span class="sd">    *N.B.* Before invoking `Operation.run()`, its graph must have been</span>
<span class="sd">    launched in a session, and either a default session must be</span>
<span class="sd">    available, or `session` must be specified explicitly.</span>

<span class="sd">    Args:</span>
<span class="sd">      feed_dict: A dictionary that maps `Tensor` objects to feed values. See</span>
<span class="sd">        `tf.Session.run` for a description of the valid feed values.</span>
<span class="sd">      session: (Optional.) The `Session` to be used to run to this operation. If</span>
<span class="sd">        none, the default session will be used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_run_using_default_session</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="p">)</span></div></div>

<span class="n">_gradient_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;gradient&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="RegisterGradient"><a class="viewcode-back" href="../../../../index.html#tensorflow.RegisterGradient">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;RegisterGradient&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">RegisterGradient</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A decorator for registering the gradient function for an op type.</span>

<span class="sd">  This decorator is only used when defining a new op type. For an op</span>
<span class="sd">  with `m` inputs and `n` outputs, the gradient function is a function</span>
<span class="sd">  that takes the original `Operation` and `n` `Tensor` objects</span>
<span class="sd">  (representing the gradients with respect to each output of the op),</span>
<span class="sd">  and returns `m` `Tensor` objects (representing the partial gradients</span>
<span class="sd">  with respect to each input of the op).</span>

<span class="sd">  For example, assuming that operations of type `&quot;Sub&quot;` take two</span>
<span class="sd">  inputs `x` and `y`, and return a single output `x - y`, the</span>
<span class="sd">  following gradient function would be registered:</span>

<span class="sd">  ```python</span>
<span class="sd">  @tf.RegisterGradient(&quot;Sub&quot;)</span>
<span class="sd">  def _sub_grad(unused_op, grad):</span>
<span class="sd">    return grad, tf.negative(grad)</span>
<span class="sd">  ```</span>

<span class="sd">  The decorator argument `op_type` is the string type of an</span>
<span class="sd">  operation. This corresponds to the `OpDef.name` field for the proto</span>
<span class="sd">  that defines the operation.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new decorator with `op_type` as the Operation type.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_type: The string type of an operation. This corresponds to the</span>
<span class="sd">        `OpDef.name` field for the proto that defines the operation.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `op_type` is not string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must be a string&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span> <span class="o">=</span> <span class="n">op_type</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Registers the function `f` as gradient function for `op_type`.&quot;&quot;&quot;</span>
    <span class="n">_gradient_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span></div>


<div class="viewcode-block" id="no_gradient"><a class="viewcode-back" href="../../../../index.html#tensorflow.no_gradient">[docs]</a><span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;NotDifferentiable&quot;</span><span class="p">,</span> <span class="s2">&quot;NoGradient&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;no_gradient&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;no_gradient&quot;</span><span class="p">,</span> <span class="s2">&quot;NotDifferentiable&quot;</span><span class="p">,</span> <span class="s2">&quot;NoGradient&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">no_gradient</span><span class="p">(</span><span class="n">op_type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Specifies that ops of type `op_type` is not differentiable.</span>

<span class="sd">  This function should *not* be used for operations that have a</span>
<span class="sd">  well-defined gradient that is not yet implemented.</span>

<span class="sd">  This function is only used when defining a new op type. It may be</span>
<span class="sd">  used for ops such as `tf.size()` that are not differentiable.  For</span>
<span class="sd">  example:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.no_gradient(&quot;Size&quot;)</span>
<span class="sd">  ```</span>

<span class="sd">  The gradient computed for &#39;op_type&#39; will then propagate zeros.</span>

<span class="sd">  For ops that have a well-defined gradient but are not yet implemented,</span>
<span class="sd">  no declaration should be made, and an error *must* be thrown if</span>
<span class="sd">  an attempt to request its gradient is made.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_type: The string type of an operation. This corresponds to the</span>
<span class="sd">      `OpDef.name` field for the proto that defines the operation.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `op_type` is not a string.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must be a string&quot;</span><span class="p">)</span>
  <span class="n">_gradient_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_type</span><span class="p">)</span></div>


<span class="c1"># Aliases for the old names, will be eventually removed.</span>
<span class="n">NoGradient</span> <span class="o">=</span> <span class="n">no_gradient</span>
<span class="n">NotDifferentiable</span> <span class="o">=</span> <span class="n">no_gradient</span>


<span class="k">def</span> <span class="nf">get_gradient_function</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the function that computes gradients for &quot;op&quot;.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="n">gradient_function</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_gradient_function</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">gradient_function</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gradient_function</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">op_type</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;_gradient_op_type&quot;</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="n">op_type</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span>
  <span class="k">return</span> <span class="n">_gradient_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">op_type</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">set_shape_and_handle_data_for_outputs</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;No op. TODO(b/74620627): Remove this.&quot;&quot;&quot;</span>
  <span class="k">pass</span>


<span class="k">class</span> <span class="nc">OpStats</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A holder for statistics about an operator.</span>

<span class="sd">  This class holds information about the resource requirements for an op,</span>
<span class="sd">  including the size of its weight parameters on-disk and how many FLOPS it</span>
<span class="sd">  requires to execute forward inference.</span>

<span class="sd">  If you define a new operation, you can create a function that will return a</span>
<span class="sd">  set of information about its usage of the CPU and disk space when serialized.</span>
<span class="sd">  The function itself takes a Graph object that&#39;s been set up so you can call</span>
<span class="sd">  methods like get_tensor_by_name to help calculate the results, and a NodeDef</span>
<span class="sd">  argument.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets up the initial placeholders for the statistics.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">statistic_type</span> <span class="o">=</span> <span class="n">statistic_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">statistic_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span>

  <span class="nd">@statistic_type</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">statistic_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span> <span class="o">=</span> <span class="n">statistic_type</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value</span>

  <span class="nd">@value</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_value</span> <span class="o">=</span> <span class="n">value</span>

  <span class="k">def</span> <span class="nf">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">other</span><span class="o">.</span><span class="n">statistic_type</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">statistic_type</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t add an OpStat of type </span><span class="si">%s</span><span class="s2"> to one of </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">statistic_type</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">statistic_type</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">value</span>
    <span class="k">elif</span> <span class="n">other</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_value</span> <span class="o">+=</span> <span class="n">other</span><span class="o">.</span><span class="n">value</span>
    <span class="k">return</span> <span class="bp">self</span>


<span class="n">_stats_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;statistical functions&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RegisterStatistics</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A decorator for registering the statistics function for an op type.</span>

<span class="sd">  This decorator can be defined for an op type so that it gives a</span>
<span class="sd">  report on the resources used by an instance of an operator, in the</span>
<span class="sd">  form of an OpStats object.</span>

<span class="sd">  Well-known types of statistics include these so far:</span>

<span class="sd">  - flops: When running a graph, the bulk of the computation happens doing</span>
<span class="sd">    numerical calculations like matrix multiplications. This type allows a node</span>
<span class="sd">    to return how many floating-point operations it takes to complete. The</span>
<span class="sd">    total number of FLOPs for a graph is a good guide to its expected latency.</span>

<span class="sd">  You can add your own statistics just by picking a new type string, registering</span>
<span class="sd">  functions for the ops you care about, and then calling get_stats_for_node_def.</span>

<span class="sd">  If a statistic for an op is registered multiple times, a KeyError will be</span>
<span class="sd">  raised.</span>

<span class="sd">  Since the statistics is counted on a per-op basis. It is not suitable for</span>
<span class="sd">  model parameters (capacity), which is expected to be counted only once, even</span>
<span class="sd">  if it is shared by multiple ops. (e.g. RNN)</span>

<span class="sd">  For example, you can define a new metric called doohickey for a Foo operation</span>
<span class="sd">  by placing this in your code:</span>

<span class="sd">  ```python</span>
<span class="sd">  @ops.RegisterStatistics(&quot;Foo&quot;, &quot;doohickey&quot;)</span>
<span class="sd">  def _calc_foo_bojangles(unused_graph, unused_node_def):</span>
<span class="sd">    return ops.OpStats(&quot;doohickey&quot;, 20)</span>
<span class="sd">  ```</span>

<span class="sd">  Then in client code you can retrieve the value by making this call:</span>

<span class="sd">  ```python</span>
<span class="sd">  doohickey = ops.get_stats_for_node_def(graph, node_def, &quot;doohickey&quot;)</span>
<span class="sd">  ```</span>

<span class="sd">  If the NodeDef is for an op with a registered doohickey function, you&#39;ll get</span>
<span class="sd">  back the calculated amount in doohickey.value, or None if it&#39;s not defined.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Saves the `op_type` as the `Operation` type.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must be a string.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;,&quot;</span> <span class="ow">in</span> <span class="n">op_type</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type must not contain a comma.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span> <span class="o">=</span> <span class="n">op_type</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">statistic_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;statistic_type must be a string.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;,&quot;</span> <span class="ow">in</span> <span class="n">statistic_type</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;statistic_type must not contain a comma.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span> <span class="o">=</span> <span class="n">statistic_type</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Registers &quot;f&quot; as the statistics function for &quot;op_type&quot;.&quot;&quot;&quot;</span>
    <span class="n">_stats_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_type</span> <span class="o">+</span> <span class="s2">&quot;,&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_statistic_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">get_stats_for_node_def</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">statistic_type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Looks up the node&#39;s statistics function in the registry and calls it.</span>

<span class="sd">  This function takes a Graph object and a NodeDef from a GraphDef, and if</span>
<span class="sd">  there&#39;s an associated statistics method, calls it and returns a result. If no</span>
<span class="sd">  function has been registered for the particular node type, it returns an empty</span>
<span class="sd">  statistics object.</span>

<span class="sd">  Args:</span>
<span class="sd">    graph: A Graph object that&#39;s been set up with the node&#39;s graph.</span>
<span class="sd">    node: A NodeDef describing the operator.</span>
<span class="sd">    statistic_type: A string identifying the statistic we&#39;re interested in.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An OpStats object containing information about resource usage.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">stats_func</span> <span class="o">=</span> <span class="n">_stats_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">+</span> <span class="s2">&quot;,&quot;</span> <span class="o">+</span> <span class="n">statistic_type</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">stats_func</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">OpStats</span><span class="p">(</span><span class="n">statistic_type</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the name of an op given the name of its scope.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: the name of the scope.</span>

<span class="sd">  Returns:</span>
<span class="sd">    the name of the op (equal to scope name minus any trailing slash).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">name</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="ow">and</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">name</span>


<span class="n">_MUTATION_LOCK_GROUP</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_SESSION_RUN_LOCK_GROUP</span> <span class="o">=</span> <span class="mi">1</span>


<div class="viewcode-block" id="Graph"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Graph</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A TensorFlow computation, represented as a dataflow graph.</span>

<span class="sd">  Graphs are used by `tf.function`s to represent the function&#39;s computations.</span>
<span class="sd">  Each graph contains a set of `tf.Operation` objects, which represent units of</span>
<span class="sd">  computation; and `tf.Tensor` objects, which represent the units of data that</span>
<span class="sd">  flow between operations.</span>

<span class="sd">  ### Using graphs directly (deprecated)</span>

<span class="sd">  A `tf.Graph` can be constructed and used directly without a `tf.function`, as</span>
<span class="sd">  was required in TensorFlow 1, but this is deprecated and it is recommended to</span>
<span class="sd">  use a `tf.function` instead. If a graph is directly used, other deprecated</span>
<span class="sd">  TensorFlow 1 classes are also required to execute the graph, such as a</span>
<span class="sd">  `tf.compat.v1.Session`.</span>

<span class="sd">  A default graph can be registered with the `tf.Graph.as_default` context</span>
<span class="sd">  manager. Then, operations will be added to the graph instead of being executed</span>
<span class="sd">  eagerly. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  g = tf.Graph()</span>
<span class="sd">  with g.as_default():</span>
<span class="sd">    # Define operations and tensors in `g`.</span>
<span class="sd">    c = tf.constant(30.0)</span>
<span class="sd">    assert c.graph is g</span>
<span class="sd">  ```</span>

<span class="sd">  `tf.compat.v1.get_default_graph()` can be used to obtain the default graph.</span>

<span class="sd">  Important note: This class *is not* thread-safe for graph construction. All</span>
<span class="sd">  operations should be created from a single thread, or external</span>
<span class="sd">  synchronization must be provided. Unless otherwise specified, all methods</span>
<span class="sd">  are not thread-safe.</span>

<span class="sd">  A `Graph` instance supports an arbitrary number of &quot;collections&quot;</span>
<span class="sd">  that are identified by name. For convenience when building a large</span>
<span class="sd">  graph, collections can store groups of related objects: for</span>
<span class="sd">  example, the `tf.Variable` uses a collection (named</span>
<span class="sd">  `tf.GraphKeys.GLOBAL_VARIABLES`) for</span>
<span class="sd">  all variables that are created during the construction of a graph. The caller</span>
<span class="sd">  may define additional collections by specifying a new name.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new, empty Graph.&quot;&quot;&quot;</span>
    <span class="c1"># Protects core state that can be returned via public accessors.</span>
    <span class="c1"># Thread-safety is provided on a best-effort basis to support buggy</span>
    <span class="c1"># programs, and is not guaranteed by the public `tf.Graph` API.</span>
    <span class="c1">#</span>
    <span class="c1"># NOTE(mrry): This does not protect the various stacks. A warning will</span>
    <span class="c1"># be reported if these are used from multiple threads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>
    <span class="c1"># The group lock synchronizes Session.run calls with methods that create</span>
    <span class="c1"># and mutate ops (e.g. Graph.create_op()). This synchronization is</span>
    <span class="c1"># necessary because it&#39;s illegal to modify an operation after it&#39;s been run.</span>
    <span class="c1"># The group lock allows any number of threads to mutate ops at the same time</span>
    <span class="c1"># but if any modification is going on, all Session.run calls have to wait.</span>
    <span class="c1"># Similarly, if one or more Session.run calls are going on, all mutate ops</span>
    <span class="c1"># have to wait until all Session.run calls have finished.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_group_lock</span> <span class="o">=</span> <span class="n">lock_util</span><span class="o">.</span><span class="n">GroupLock</span><span class="p">(</span><span class="n">num_groups</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_version</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># GUARDED_BY(self._lock)</span>
    <span class="c1"># Maps a name used in the graph to the next id to use for that name.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>
    <span class="c1"># Functions that will be applied to choose a device if none is specified.</span>
    <span class="c1"># In TF2.x or after switch_to_thread_local(),</span>
    <span class="c1"># self._thread_local._device_function_stack is used instead.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>
    <span class="c1"># Default original_op applied to new ops.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Current control flow context. It could be either CondContext or</span>
    <span class="c1"># WhileContext defined in ops/control_flow_ops.py</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># A new node will depend of the union of all of the nodes in the stack.</span>
    <span class="c1"># In TF2.x or after switch_to_thread_local(),</span>
    <span class="c1"># self._thread_local._control_dependencies_stack is used instead.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Arbitrary collections of objects.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># The graph-level random seed</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># A dictionary of attributes that should be applied to all ops.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from op type to the kernel label that should be used.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from op type to an alternative op type that should be used when</span>
    <span class="c1"># computing gradients.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from op type to a gradient function that should be used instead.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_function_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># True if the graph is considered &quot;finalized&quot;.  In that case no</span>
    <span class="c1"># new operations can be added.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Functions defined in the graph</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">()</span>
    <span class="c1"># Default GraphDef versions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_def_versions</span> <span class="o">=</span> <span class="n">versions_pb2</span><span class="o">.</span><span class="n">VersionDef</span><span class="p">(</span>
        <span class="n">producer</span><span class="o">=</span><span class="n">versions</span><span class="o">.</span><span class="n">GRAPH_DEF_VERSION</span><span class="p">,</span>
        <span class="n">min_consumer</span><span class="o">=</span><span class="n">versions</span><span class="o">.</span><span class="n">GRAPH_DEF_VERSION_MIN_CONSUMER</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Stack of colocate_with ops. In TF2.x or after switch_to_thread_local(),</span>
    <span class="c1"># self._thread_local._colocation_stack is used instead.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>
    <span class="c1"># Set of tensors that are dangerous to feed!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfeedable_tensors</span> <span class="o">=</span> <span class="n">object_identity</span><span class="o">.</span><span class="n">ObjectIdentitySet</span><span class="p">()</span>
    <span class="c1"># Set of operations that are dangerous to fetch!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="c1"># A map of tensor handle placeholder to tensor dtype.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_feeders</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from tensor handle to its read op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_readers</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from tensor handle to its move op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_movers</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># A map from tensor handle to its delete op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_deleters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Allow optimizers and other objects to pseudo-uniquely key graphs (this key</span>
    <span class="c1"># will be shared when defining function graphs, for example, so optimizers</span>
    <span class="c1"># being called inside function definitions behave as if they were seeing the</span>
    <span class="c1"># actual outside graph).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_key</span> <span class="o">=</span> <span class="s2">&quot;grap-key-</span><span class="si">%d</span><span class="s2">/&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">uid</span><span class="p">(),)</span>
    <span class="c1"># A string with the last reduction method passed to</span>
    <span class="c1"># losses.compute_weighted_loss(), or None. This is required only for</span>
    <span class="c1"># backward compatibility with Estimator and optimizer V1 use cases.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_loss_reduction</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Flag that is used to indicate whether loss has been scaled by optimizer.</span>
    <span class="c1"># If this flag has been set, then estimator uses it to scale losss back</span>
    <span class="c1"># before reporting. This is required only for backward compatibility with</span>
    <span class="c1"># Estimator and optimizer V1 use cases.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_loss_scaled_by_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="c1"># Set to True if this graph is being built in an</span>
    <span class="c1"># AutomaticControlDependencies context.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_add_control_dependencies</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Cache for OpDef protobufs retrieved via the C API.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_op_def_cache</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Cache for constant results of `broadcast_gradient_args()`. The keys are</span>
    <span class="c1"># tuples of fully-defined shapes: (x_shape_tuple, y_shape_tuple), and the</span>
    <span class="c1"># values are tuples of reduction indices: (rx, ry).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bcast_grad_args_cache</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Cache for constant results of `reduced_shape()`. The keys are pairs of</span>
    <span class="c1"># tuples: (input_shape_tuple, reduction_indices_tuple), and the values</span>
    <span class="c1"># are pairs of tuples: (output_shape_kept_dims, tile_scaling).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reduced_shape_cache</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># TODO(skyewm): fold as much of the above as possible into the C</span>
    <span class="c1"># implementation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_scoped_c_graph</span> <span class="o">=</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">ScopedTFGraph</span><span class="p">()</span>
    <span class="c1"># The C API requires all ops to have shape functions. Disable this</span>
    <span class="c1"># requirement (many custom ops do not have shape functions, and we don&#39;t</span>
    <span class="c1"># want to break these existing cases).</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">SetRequireShapeInferenceFns</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tf2</span><span class="o">.</span><span class="n">enabled</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">switch_to_thread_local</span><span class="p">()</span>

  <span class="c1"># Note: this method is private because the API of tf.Graph() is public and</span>
  <span class="c1"># frozen, and this functionality is still not ready for public visibility.</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_variable_creator_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">creator</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scope which defines a variable creation function.</span>

<span class="sd">    Args:</span>
<span class="sd">      creator: A callable taking `next_creator` and `kwargs`. See the</span>
<span class="sd">        `tf.variable_creator_scope` docstring.</span>
<span class="sd">      priority: Creators with a higher `priority` are called first. Within the</span>
<span class="sd">        same priority, creators are called inner-to-outer.</span>

<span class="sd">    Yields:</span>
<span class="sd">      `_variable_creator_scope` is a context manager with a side effect, but</span>
<span class="sd">      doesn&#39;t return a value.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If variable creator scopes are not properly nested.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This step keeps a reference to the existing stack, and it also initializes</span>
    <span class="c1"># self._thread_local._variable_creator_stack if it doesn&#39;t exist yet.</span>
    <span class="n">old</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_creator_stack</span>
    <span class="n">new</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old</span><span class="p">)</span>
    <span class="n">new</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">priority</span><span class="p">,</span> <span class="n">creator</span><span class="p">))</span>
    <span class="c1"># Sorting is stable, so we&#39;ll put higher-priority creators later in the list</span>
    <span class="c1"># but otherwise maintain registration order.</span>
    <span class="n">new</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="o">=</span> <span class="n">new</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Exiting variable_creator_scope without proper nesting.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="o">=</span> <span class="n">old</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="c1"># Note: this method is private because the API of tf.Graph() is public and</span>
  <span class="c1"># frozen, and this functionality is still not ready for public visibility.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_variable_creator_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_variable_creator_stack&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="c1"># This previously returned a copy of the stack instead of the stack itself,</span>
    <span class="c1"># to guard against accidental mutation. Consider, however, code that wants</span>
    <span class="c1"># to save and restore the variable creator stack:</span>
    <span class="c1">#     def f():</span>
    <span class="c1">#       original_stack = graph._variable_creator_stack</span>
    <span class="c1">#       graph._variable_creator_stack = new_stack</span>
    <span class="c1">#       ...  # Some code</span>
    <span class="c1">#       graph._variable_creator_stack = original_stack</span>
    <span class="c1">#</span>
    <span class="c1"># And lets say you have some code that calls this function with some</span>
    <span class="c1"># variable_creator:</span>
    <span class="c1">#     def g():</span>
    <span class="c1">#       with variable_scope.variable_creator_scope(creator):</span>
    <span class="c1">#         f()</span>
    <span class="c1"># When exiting the variable creator scope, it would see a different stack</span>
    <span class="c1"># object than it expected leading to a &quot;Exiting variable_creator_scope</span>
    <span class="c1"># without proper nesting&quot; error.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="nd">@_variable_creator_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_variable_creator_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_creator_stack</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_variable_creator_stack</span> <span class="o">=</span> <span class="n">variable_creator_stack</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_check_not_finalized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Check if the graph is finalized.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If the graph finalized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Graph is finalized and cannot be modified.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_add_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds &#39;op&#39; to the graph and returns the unique ID for the added Operation.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: the Operation to add.</span>
<span class="sd">      op_name: the name of the Operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An integer that is a unique ID for the added Operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">op_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span><span class="p">[</span><span class="n">op_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">op</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">op_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">op</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_version</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">,</span> <span class="n">op_id</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">op_id</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_c_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scoped_c_graph</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scoped_c_graph</span><span class="o">.</span><span class="n">graph</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">version</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a version number that increases as ops are added to the graph.</span>

<span class="sd">    Note that this is unrelated to the</span>
<span class="sd">    `tf.Graph.graph_def_versions`.</span>

<span class="sd">    Returns:</span>
<span class="sd">       An integer version that increases as ops are added to the graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_version</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_version</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph_def_versions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;The GraphDef version information of this graph.</span>

<span class="sd">    For details on the meaning of each version, see</span>
<span class="sd">    [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `VersionDef`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
      <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GraphVersions</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="n">version_def</span> <span class="o">=</span> <span class="n">versions_pb2</span><span class="o">.</span><span class="n">VersionDef</span><span class="p">()</span>
    <span class="n">version_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">version_def</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The graph-level random seed of this graph.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span>

  <span class="nd">@seed</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="n">seed</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">finalized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True if this graph has been finalized.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span>

<div class="viewcode-block" id="Graph.finalize"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.finalize">[docs]</a>  <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Finalizes this graph, making it read-only.</span>

<span class="sd">    After calling `g.finalize()`, no new operations can be added to</span>
<span class="sd">    `g`.  This method is used to ensure that no operations are added</span>
<span class="sd">    to a graph when it is shared between multiple threads, for example</span>
<span class="sd">    when using a `tf.compat.v1.train.QueueRunner`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span> <span class="o">=</span> <span class="kc">True</span></div>

  <span class="k">def</span> <span class="nf">_unsafe_unfinalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Opposite of `finalize`.</span>

<span class="sd">    Internal interface.</span>

<span class="sd">    NOTE: Unfinalizing a graph could have negative impact on performance,</span>
<span class="sd">    especially in a multi-threaded environment.  Unfinalizing a graph</span>
<span class="sd">    when it is in use by a Session may lead to undefined behavior. Ensure</span>
<span class="sd">    that all sessions using a graph are closed before calling this method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">_get_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the current control flow context.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span>

  <span class="k">def</span> <span class="nf">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the current control flow context.</span>

<span class="sd">    Args:</span>
<span class="sd">      ctx: a context object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="o">=</span> <span class="n">ctx</span>

  <span class="k">def</span> <span class="nf">_copy_functions_to_graph_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_def</span><span class="p">,</span> <span class="n">starting_bytesize</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;If this graph contains functions, copy them to `graph_def`.&quot;&quot;&quot;</span>
    <span class="n">bytesize</span> <span class="o">=</span> <span class="n">starting_bytesize</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
      <span class="n">bytesize</span> <span class="o">+=</span> <span class="n">f</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">ByteSize</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">bytesize</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">31</span><span class="p">)</span> <span class="ow">or</span> <span class="n">bytesize</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;GraphDef cannot be larger than 2GB.&quot;</span><span class="p">)</span>
      <span class="n">graph_def</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">definition</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">grad_func_name</span><span class="p">:</span>
        <span class="n">grad_def</span> <span class="o">=</span> <span class="n">function_pb2</span><span class="o">.</span><span class="n">GradientDef</span><span class="p">()</span>
        <span class="n">grad_def</span><span class="o">.</span><span class="n">function_name</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span>
        <span class="n">grad_def</span><span class="o">.</span><span class="n">gradient_func</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">grad_func_name</span>
        <span class="n">graph_def</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">grad_def</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">_as_graph_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns a serialized `GraphDef` representation of this graph.</span>

<span class="sd">    The serialized `GraphDef` can be imported into another `Graph`</span>
<span class="sd">    (using `tf.import_graph_def`) or used with the</span>
<span class="sd">    [C++ Session API](../../../../api_docs/cc/index.md).</span>

<span class="sd">    This method is thread-safe.</span>

<span class="sd">    Args:</span>
<span class="sd">      from_version: Optional.  If this is set, returns a `GraphDef` containing</span>
<span class="sd">        only the nodes that were added to this graph since its `version`</span>
<span class="sd">        property had the given value.</span>
<span class="sd">      add_shapes: If true, adds an &quot;_output_shapes&quot; list attr to each node with</span>
<span class="sd">        the inferred shapes of each of its outputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple containing a</span>
<span class="sd">      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)</span>
<span class="sd">      protocol buffer, and the version of the graph to which that</span>
<span class="sd">      `GraphDef` corresponds.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the `graph_def` would be too large.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
        <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GraphToGraphDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_pb2</span><span class="o">.</span><span class="n">GraphDef</span><span class="p">()</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
      <span class="c1"># Strip the experimental library field iff it&#39;s empty.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">graph</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="p">:</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">ClearField</span><span class="p">(</span><span class="s2">&quot;library&quot;</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">add_shapes</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">:</span>
          <span class="n">op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
          <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
            <span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;_output_shapes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_proto</span><span class="p">()</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">function_def</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="p">:</span>
          <span class="n">defined_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="p">[</span><span class="n">function_def</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
          <span class="k">try</span><span class="p">:</span>
            <span class="n">func_graph</span> <span class="o">=</span> <span class="n">defined_function</span><span class="o">.</span><span class="n">graph</span>
          <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="c1"># _DefinedFunction doesn&#39;t have a graph, _EagerDefinedFunction</span>
            <span class="c1"># does. Both rely on ops.py, so we can&#39;t really isinstance check</span>
            <span class="c1"># them.</span>
            <span class="k">continue</span>
          <span class="n">input_shapes</span> <span class="o">=</span> <span class="n">function_def</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;_input_shapes&quot;</span><span class="p">]</span>
          <span class="k">try</span><span class="p">:</span>
            <span class="n">func_graph_inputs</span> <span class="o">=</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">inputs</span>
          <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">continue</span>
          <span class="c1"># TODO(b/141471245): Fix the inconsistency when inputs of func graph</span>
          <span class="c1"># are appended during gradient computation of while/cond.</span>
          <span class="k">for</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">func_graph_inputs</span><span class="p">,</span>
                                     <span class="n">function_def</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">input_arg</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">resource</span><span class="p">:</span>
              <span class="c1"># TODO(allenl): Save and restore handle data, then save the</span>
              <span class="c1"># resource placeholder&#39;s shape. Right now some shape functions get</span>
              <span class="c1"># confused if we set the shape of the resource placeholder (to a</span>
              <span class="c1"># scalar of course) and there isn&#39;t any handle data.</span>
              <span class="n">input_shapes</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">add</span><span class="p">()</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span>
                  <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">as_proto</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">input_shapes</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">add</span><span class="p">()</span><span class="o">.</span><span class="n">CopyFrom</span><span class="p">(</span>
                  <span class="n">input_tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_proto</span><span class="p">())</span>
          <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">function_def</span><span class="o">.</span><span class="n">node_def</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
              <span class="n">op</span> <span class="o">=</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">get_operation_by_name</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
              <span class="k">continue</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span>

            <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;StatefulPartitionedCall&quot;</span><span class="p">:</span>
              <span class="c1"># Filter out any extra outputs (possibly added by function</span>
              <span class="c1"># backpropagation rewriting).</span>
              <span class="n">num_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;Tout&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
              <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="n">num_outputs</span><span class="p">]</span>

            <span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;_output_shapes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_proto</span><span class="p">()</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_version</span>

<div class="viewcode-block" id="Graph.as_graph_def"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.as_graph_def">[docs]</a>  <span class="k">def</span> <span class="nf">as_graph_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns a serialized `GraphDef` representation of this graph.</span>

<span class="sd">    The serialized `GraphDef` can be imported into another `Graph`</span>
<span class="sd">    (using `tf.import_graph_def`) or used with the</span>
<span class="sd">    [C++ Session API](../../api_docs/cc/index.md).</span>

<span class="sd">    This method is thread-safe.</span>

<span class="sd">    Args:</span>
<span class="sd">      from_version: Optional.  If this is set, returns a `GraphDef` containing</span>
<span class="sd">        only the nodes that were added to this graph since its `version`</span>
<span class="sd">        property had the given value.</span>
<span class="sd">      add_shapes: If true, adds an &quot;_output_shapes&quot; list attr to each node with</span>
<span class="sd">        the inferred shapes of each of its outputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A</span>
<span class="sd">      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)</span>
<span class="sd">      protocol buffer.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the `graph_def` would be too large.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=line-too-long</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_graph_def</span><span class="p">(</span><span class="n">from_version</span><span class="p">,</span> <span class="n">add_shapes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>

  <span class="k">def</span> <span class="nf">_is_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tests whether &#39;name&#39; is registered in this graph&#39;s function library.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: string op name.</span>

<span class="sd">    Returns:</span>
<span class="sd">      bool indicating whether or not &#39;name&#39; is registered in function library.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span>

  <span class="k">def</span> <span class="nf">_get_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the function definition for &#39;name&#39;.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: string function name.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The function def proto.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_add_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">function</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a function to the graph.</span>

<span class="sd">    After the function has been added, you can call to the function by</span>
<span class="sd">    passing the function name in place of an op name to</span>
<span class="sd">    `Graph.create_op()`.</span>

<span class="sd">    Args:</span>
<span class="sd">      function: A `_DefinedFunction` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if another function is defined with the same name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>

    <span class="n">name</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">name</span>
    <span class="c1"># Sanity checks on gradient definition.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">function</span><span class="o">.</span><span class="n">grad_func_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">function</span><span class="o">.</span><span class="n">python_grad_func</span> <span class="ow">is</span>
                                                  <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Gradient defined twice for function </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># Add function to graph</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">function</span><span class="o">.</span><span class="n">_grad_func</span><span class="o">.</span><span class="n">_c_func</span><span class="o">.</span><span class="n">func</span> <span class="k">if</span> <span class="n">function</span><span class="o">.</span><span class="n">_grad_func</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GraphCopyFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">function</span><span class="o">.</span><span class="n">_c_func</span><span class="o">.</span><span class="n">func</span><span class="p">,</span>
                                           <span class="n">gradient</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_functions</span><span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">)]</span> <span class="o">=</span> <span class="n">function</span>

    <span class="c1"># Need a new-enough consumer to support the functions we add to the graph.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_def_versions</span><span class="o">.</span><span class="n">min_consumer</span> <span class="o">&lt;</span> <span class="mi">12</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_def_versions</span><span class="o">.</span><span class="n">min_consumer</span> <span class="o">=</span> <span class="mi">12</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">building_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns True iff this graph represents a function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_building_function</span>

  <span class="c1"># Helper functions to create operations.</span>
<div class="viewcode-block" id="Graph.create_op"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.create_op">[docs]</a>  <span class="nd">@deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
                   <span class="s2">&quot;Shapes are always computed; don&#39;t use the compute_shapes &quot;</span>
                   <span class="s2">&quot;as it has no effect.&quot;</span><span class="p">,</span> <span class="s2">&quot;compute_shapes&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">create_op</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">op_type</span><span class="p">,</span>
      <span class="n">inputs</span><span class="p">,</span>
      <span class="n">dtypes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
      <span class="n">input_types</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">attrs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">op_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">compute_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="n">compute_device</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an `Operation` in this graph.</span>

<span class="sd">    This is a low-level interface for creating an `Operation`. Most</span>
<span class="sd">    programs will not call this method directly, and instead use the</span>
<span class="sd">    Python op constructors, such as `tf.constant()`, which add ops to</span>
<span class="sd">    the default graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_type: The `Operation` type to create. This corresponds to the</span>
<span class="sd">        `OpDef.name` field for the proto that defines the operation.</span>
<span class="sd">      inputs: A list of `Tensor` objects that will be inputs to the `Operation`.</span>
<span class="sd">      dtypes: (Optional) A list of `DType` objects that will be the types of the</span>
<span class="sd">        tensors that the operation produces.</span>
<span class="sd">      input_types: (Optional.) A list of `DType`s that will be the types of the</span>
<span class="sd">        tensors that the operation consumes. By default, uses the base `DType`</span>
<span class="sd">        of each input in `inputs`. Operations that expect reference-typed inputs</span>
<span class="sd">        must specify `input_types` explicitly.</span>
<span class="sd">      name: (Optional.) A string name for the operation. If not specified, a</span>
<span class="sd">        name is generated based on `op_type`.</span>
<span class="sd">      attrs: (Optional.) A dictionary where the key is the attribute name (a</span>
<span class="sd">        string) and the value is the respective `attr` attribute of the</span>
<span class="sd">        `NodeDef` proto that will represent the operation (an `AttrValue`</span>
<span class="sd">        proto).</span>
<span class="sd">      op_def: (Optional.) The `OpDef` proto that describes the `op_type` that</span>
<span class="sd">        the operation will have.</span>
<span class="sd">      compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always</span>
<span class="sd">        computed).</span>
<span class="sd">      compute_device: (Optional.) If True, device functions will be executed to</span>
<span class="sd">        compute the device property of the Operation.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if any of the inputs is not a `Tensor`.</span>
<span class="sd">      ValueError: if colocation conflicts with existing device assignment.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An `Operation` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">compute_shapes</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input #</span><span class="si">%d</span><span class="s2"> is not a tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_op_internal</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">dtypes</span><span class="p">,</span> <span class="n">input_types</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                                    <span class="n">attrs</span><span class="p">,</span> <span class="n">op_def</span><span class="p">,</span> <span class="n">compute_device</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_create_op_internal</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">op_type</span><span class="p">,</span>
      <span class="n">inputs</span><span class="p">,</span>
      <span class="n">dtypes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
      <span class="n">input_types</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">attrs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">op_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">compute_device</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an `Operation` in this graph.</span>

<span class="sd">    Implements `Graph.create_op()` without the overhead of the deprecation</span>
<span class="sd">    wrapper.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_type: The `Operation` type to create. This corresponds to the</span>
<span class="sd">        `OpDef.name` field for the proto that defines the operation.</span>
<span class="sd">      inputs: A list of `Tensor` objects that will be inputs to the `Operation`.</span>
<span class="sd">      dtypes: (Optional) A list of `DType` objects that will be the types of the</span>
<span class="sd">        tensors that the operation produces.</span>
<span class="sd">      input_types: (Optional.) A list of `DType`s that will be the types of the</span>
<span class="sd">        tensors that the operation consumes. By default, uses the base `DType`</span>
<span class="sd">        of each input in `inputs`. Operations that expect reference-typed inputs</span>
<span class="sd">        must specify `input_types` explicitly.</span>
<span class="sd">      name: (Optional.) A string name for the operation. If not specified, a</span>
<span class="sd">        name is generated based on `op_type`.</span>
<span class="sd">      attrs: (Optional.) A dictionary where the key is the attribute name (a</span>
<span class="sd">        string) and the value is the respective `attr` attribute of the</span>
<span class="sd">        `NodeDef` proto that will represent the operation (an `AttrValue`</span>
<span class="sd">        proto).</span>
<span class="sd">      op_def: (Optional.) The `OpDef` proto that describes the `op_type` that</span>
<span class="sd">        the operation will have.</span>
<span class="sd">      compute_device: (Optional.) If True, device functions will be executed to</span>
<span class="sd">        compute the device property of the Operation.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if colocation conflicts with existing device assignment.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An `Operation` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">op_type</span>
    <span class="c1"># If a names ends with a &#39;/&#39; it is a &quot;name scope&quot; and we use it as-is,</span>
    <span class="c1"># after removing the trailing &#39;/&#39;.</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="n">node_def</span> <span class="o">=</span> <span class="n">_NodeDef</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">attrs</span><span class="p">)</span>

    <span class="n">input_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">control_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_for_inputs</span><span class="p">(</span><span class="n">input_ops</span><span class="p">)</span>
    <span class="c1"># _create_op_helper mutates the new Operation. `_mutation_lock` ensures a</span>
    <span class="c1"># Session.run call cannot occur between creating and mutating the op.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mutation_lock</span><span class="p">():</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">Operation</span><span class="p">(</span>
          <span class="n">node_def</span><span class="p">,</span>
          <span class="bp">self</span><span class="p">,</span>
          <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
          <span class="n">output_types</span><span class="o">=</span><span class="n">dtypes</span><span class="p">,</span>
          <span class="n">control_inputs</span><span class="o">=</span><span class="n">control_inputs</span><span class="p">,</span>
          <span class="n">input_types</span><span class="o">=</span><span class="n">input_types</span><span class="p">,</span>
          <span class="n">original_op</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span><span class="p">,</span>
          <span class="n">op_def</span><span class="o">=</span><span class="n">op_def</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_create_op_helper</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="n">compute_device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_create_op_from_tf_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_op</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an `Operation` in this graph from the supplied TF_Operation.</span>

<span class="sd">    This method is like create_op() except the new Operation is constructed</span>
<span class="sd">    using `c_op`. The returned Operation will have `c_op` as its _c_op</span>
<span class="sd">    field. This is used to create Operation objects around TF_Operations created</span>
<span class="sd">    indirectly by the C API (e.g. by TF_ImportGraphDef, TF_FinishWhile).</span>

<span class="sd">    This function does not call Operation._control_flow_post_processing or</span>
<span class="sd">    Graph._control_dependencies_for_inputs (since the inputs may not be</span>
<span class="sd">    available yet). The caller is responsible for calling these methods.</span>

<span class="sd">    Args:</span>
<span class="sd">      c_op: a wrapped TF_Operation</span>
<span class="sd">      compute_device: (Optional.) If True, device functions will be executed to</span>
<span class="sd">        compute the device property of the Operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An `Operation` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">Operation</span><span class="p">(</span><span class="n">c_op</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="c1"># If a name_scope was created with ret.name but no nodes were created in it,</span>
    <span class="c1"># the name will still appear in _names_in_use even though the name hasn&#39;t</span>
    <span class="c1"># been used. This is ok, just leave _names_in_use as-is in this case.</span>
    <span class="c1"># TODO(skyewm): make the C API guarantee no name conflicts.</span>
    <span class="n">name_key</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">name_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">[</span><span class="n">name_key</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_create_op_helper</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="n">compute_device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_create_op_helper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Common logic for creating an op in this graph.&quot;&quot;&quot;</span>
    <span class="c1"># Apply any additional attributes requested. Do not overwrite any existing</span>
    <span class="c1"># attributes.</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
          <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;Callable for scope map key &#39;</span><span class="si">%s</span><span class="s2">&#39; must return either None or &quot;</span>
                <span class="s2">&quot;an AttrValue protocol buffer; but it returned: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">value</span><span class="p">:</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="c1"># Apply a kernel label if one has been specified for this op type.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">kernel_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">]</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;_kernel&quot;</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
                   <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">kernel_label</span><span class="p">)))</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="n">op</span><span class="o">.</span><span class="n">_gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_function_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="c1"># Apply the overriding op type for gradients if one has been specified for</span>
    <span class="c1"># this op type.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">mapped_op_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">type</span><span class="p">]</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;_gradient_op_type&quot;</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
                   <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">mapped_op_type</span><span class="p">)))</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_record_op_seen_by_control_dependencies</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">compute_device</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_apply_device_functions</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

    <span class="c1"># Snapshot the colocation stack metadata before we might generate error</span>
    <span class="c1"># messages using it.  Note that this snapshot depends on the actual stack</span>
    <span class="c1"># and is independent of the op&#39;s _class attribute.</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">op</span><span class="o">.</span><span class="n">_colocation_code_locations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot_colocation_stack_metadata</span><span class="p">()</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="p">:</span>
      <span class="n">all_colocation_groups</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">colocation_op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">peek_objs</span><span class="p">():</span>
        <span class="n">all_colocation_groups</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">colocation_op</span><span class="o">.</span><span class="n">colocation_groups</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">colocation_op</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
          <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_set_device</span><span class="p">(</span><span class="n">colocation_op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
          <span class="c1"># pylint: enable=protected-access</span>

      <span class="n">all_colocation_groups</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">all_colocation_groups</span><span class="p">))</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span>
          <span class="s2">&quot;_class&quot;</span><span class="p">,</span>
          <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span>
              <span class="nb">list</span><span class="o">=</span><span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">all_colocation_groups</span><span class="p">)))</span>
      <span class="c1"># pylint: enable=protected-access</span>

    <span class="c1"># Sets &quot;container&quot; attribute if</span>
    <span class="c1"># (1) self._container is not None</span>
    <span class="c1"># (2) &quot;is_stateful&quot; is set in OpDef</span>
    <span class="c1"># (3) &quot;container&quot; attribute is in OpDef</span>
    <span class="c1"># (4) &quot;container&quot; attribute is None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="ow">and</span> <span class="n">op</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">container_attr</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;container&quot;</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="c1"># &quot;container&quot; attribute is not in OpDef</span>
        <span class="k">pass</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">container_attr</span><span class="p">:</span>
          <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="s2">&quot;container&quot;</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
              <span class="n">s</span><span class="o">=</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_container</span><span class="p">)))</span>

  <span class="k">def</span> <span class="nf">_add_new_tf_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compute_devices</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates `Operations` in this graph for any new TF_Operations.</span>

<span class="sd">    This is useful for when TF_Operations are indirectly created by the C API</span>
<span class="sd">    outside of the Operation constructor (e.g. by TF_ImportGraphDef,</span>
<span class="sd">    TF_FinishWhile). This ensures there are corresponding Operations for all</span>
<span class="sd">    TF_Operations in the underlying TF_Graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      compute_devices: (Optional.) If True, device functions will be executed to</span>
<span class="sd">        compute the device properties of each new Operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of the new `Operation` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>

    <span class="c1"># Create all Operation objects before accessing their inputs since an op may</span>
    <span class="c1"># be created before its inputs.</span>
    <span class="n">new_ops</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_create_op_from_tf_operation</span><span class="p">(</span><span class="n">c_op</span><span class="p">,</span> <span class="n">compute_device</span><span class="o">=</span><span class="n">compute_devices</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">c_op</span> <span class="ow">in</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">new_tf_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">new_ops</span><span class="p">:</span>
      <span class="n">new_control_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_for_inputs</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_add_control_inputs</span><span class="p">(</span><span class="n">new_control_inputs</span><span class="p">)</span>
      <span class="n">op</span><span class="o">.</span><span class="n">_control_flow_post_processing</span><span class="p">()</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="k">return</span> <span class="n">new_ops</span>

<div class="viewcode-block" id="Graph.as_graph_element"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.as_graph_element">[docs]</a>  <span class="k">def</span> <span class="nf">as_graph_element</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_operation</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the object referred to by `obj`, as an `Operation` or `Tensor`.</span>

<span class="sd">    This function validates that `obj` represents an element of this</span>
<span class="sd">    graph, and gives an informative error message if it is not.</span>

<span class="sd">    This function is the canonical way to get/validate an object of</span>
<span class="sd">    one of the allowed types from an external argument reference in the</span>
<span class="sd">    Session API.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      obj: A `Tensor`, an `Operation`, or the name of a tensor or operation. Can</span>
<span class="sd">        also be any object with an `_as_graph_element()` method that returns a</span>
<span class="sd">        value of one of these types. Note: `_as_graph_element` will be called</span>
<span class="sd">        inside the graph&#39;s lock and so may not modify the graph.</span>
<span class="sd">      allow_tensor: If true, `obj` may refer to a `Tensor`.</span>
<span class="sd">      allow_operation: If true, `obj` may refer to an `Operation`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Tensor` or `Operation` in the Graph corresponding to `obj`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `obj` is not a type we support attempting to convert</span>
<span class="sd">        to types.</span>
<span class="sd">      ValueError: If `obj` is of an appropriate type but invalid. For</span>
<span class="sd">        example, an invalid string.</span>
<span class="sd">      KeyError: If `obj` is not an object in the graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_graph_element_locked</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="p">,</span> <span class="n">allow_operation</span><span class="p">)</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_graph_element_locked</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="p">,</span> <span class="n">allow_operation</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_as_graph_element_locked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="p">,</span> <span class="n">allow_operation</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See `Graph.as_graph_element()` for details.&quot;&quot;&quot;</span>
    <span class="c1"># The vast majority of this function is figuring</span>
    <span class="c1"># out what an API user might be doing wrong, so</span>
    <span class="c1"># that we can give helpful error messages.</span>
    <span class="c1">#</span>
    <span class="c1"># Ideally, it would be nice to split it up, but we</span>
    <span class="c1"># need context to generate nice error messages.</span>

    <span class="k">if</span> <span class="n">allow_tensor</span> <span class="ow">and</span> <span class="n">allow_operation</span><span class="p">:</span>
      <span class="n">types_str</span> <span class="o">=</span> <span class="s2">&quot;Tensor or Operation&quot;</span>
    <span class="k">elif</span> <span class="n">allow_tensor</span><span class="p">:</span>
      <span class="n">types_str</span> <span class="o">=</span> <span class="s2">&quot;Tensor&quot;</span>
    <span class="k">elif</span> <span class="n">allow_operation</span><span class="p">:</span>
      <span class="n">types_str</span> <span class="o">=</span> <span class="s2">&quot;Operation&quot;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;allow_tensor and allow_operation can&#39;t both be False.&quot;</span><span class="p">)</span>

    <span class="n">temp_obj</span> <span class="o">=</span> <span class="n">_as_graph_element</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">temp_obj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">obj</span> <span class="o">=</span> <span class="n">temp_obj</span>

    <span class="c1"># If obj appears to be a name...</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">bytes_or_text_types</span><span class="p">):</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>

      <span class="k">if</span> <span class="s2">&quot;:&quot;</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">allow_tensor</span><span class="p">:</span>
        <span class="c1"># Looks like a Tensor name and can be a Tensor.</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">op_name</span><span class="p">,</span> <span class="n">out_n</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
          <span class="n">out_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">out_n</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> looks a like a Tensor name, but is &quot;</span>
                           <span class="s2">&quot;not a valid one. Tensor names must be of the &quot;</span>
                           <span class="s2">&quot;form </span><span class="se">\&quot;</span><span class="s2">&lt;op_name&gt;:&lt;output_index&gt;</span><span class="se">\&quot;</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">op_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">:</span>
          <span class="n">op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">op_name</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to a Tensor which does not &quot;</span>
                         <span class="s2">&quot;exist. The operation, </span><span class="si">%s</span><span class="s2">, does not exist in the &quot;</span>
                         <span class="s2">&quot;graph.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">op_name</span><span class="p">)))</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">out_n</span><span class="p">]</span>
        <span class="k">except</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to a Tensor which does not &quot;</span>
                         <span class="s2">&quot;exist. The operation, </span><span class="si">%s</span><span class="s2">, exists but only has &quot;</span>
                         <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> outputs.&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">op_name</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">)))</span>

      <span class="k">elif</span> <span class="s2">&quot;:&quot;</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">allow_tensor</span><span class="p">:</span>
        <span class="c1"># Looks like a Tensor name but can&#39;t be a Tensor.</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Name </span><span class="si">%s</span><span class="s2"> appears to refer to a Tensor, not a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">types_str</span><span class="p">))</span>

      <span class="k">elif</span> <span class="s2">&quot;:&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">allow_operation</span><span class="p">:</span>
        <span class="c1"># Looks like an Operation name and can be an Operation.</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to an Operation not in the &quot;</span>
                         <span class="s2">&quot;graph.&quot;</span> <span class="o">%</span> <span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

      <span class="k">elif</span> <span class="s2">&quot;:&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">allow_operation</span><span class="p">:</span>
        <span class="c1"># Looks like an Operation name but can&#39;t be an Operation.</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">:</span>
          <span class="c1"># Yep, it&#39;s an Operation name</span>
          <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> refers to an Operation, not a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">types_str</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The name </span><span class="si">%s</span><span class="s2"> looks like an (invalid) Operation name, &quot;</span>
                     <span class="s2">&quot;not a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">types_str</span><span class="p">))</span>
        <span class="n">err_msg</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot; Tensor names must be of the form &quot;</span>
                    <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">&lt;op_name&gt;:&lt;output_index&gt;</span><span class="se">\&quot;</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">allow_tensor</span><span class="p">:</span>
      <span class="c1"># Actually obj is just the object it&#39;s referring to.</span>
      <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tensor </span><span class="si">%s</span><span class="s2"> is not an element of this graph.&quot;</span> <span class="o">%</span> <span class="n">obj</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">obj</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Operation</span><span class="p">)</span> <span class="ow">and</span> <span class="n">allow_operation</span><span class="p">:</span>
      <span class="c1"># Actually obj is just the object it&#39;s referring to.</span>
      <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Operation </span><span class="si">%s</span><span class="s2"> is not an element of this graph.&quot;</span> <span class="o">%</span> <span class="n">obj</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">obj</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># We give up!</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Can not convert a </span><span class="si">%s</span><span class="s2"> into a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">types_str</span><span class="p">))</span>

<div class="viewcode-block" id="Graph.get_operations"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.get_operations">[docs]</a>  <span class="k">def</span> <span class="nf">get_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the list of operations in the graph.</span>

<span class="sd">    You can modify the operations in place, but modifications</span>
<span class="sd">    to the list such as inserts/delete have no effect on the</span>
<span class="sd">    list of operations known to the graph.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of Operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_id</span><span class="o">.</span><span class="n">values</span><span class="p">())</span></div>

<div class="viewcode-block" id="Graph.get_operation_by_name"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.get_operation_by_name">[docs]</a>  <span class="k">def</span> <span class="nf">get_operation_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Operation` with the given `name`.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the `Operation` to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Operation` with the given `name`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `name` is not a string.</span>
<span class="sd">      KeyError: If `name` does not correspond to an operation in this graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Operation names are strings (or similar), not </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                      <span class="nb">type</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">allow_operation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_get_operation_by_name_unsafe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Operation` with the given `name`.</span>

<span class="sd">    This is a internal unsafe version of get_operation_by_name. It skips many</span>
<span class="sd">    checks and does not have user friendly error messages but runs considerably</span>
<span class="sd">    faster. This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the `Operation` to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Operation` with the given `name`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `name` does not correspond to an operation in this graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finalized</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_name</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_get_operation_by_tf_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tf_oper</span><span class="p">):</span>
    <span class="n">op_name</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_OperationName</span><span class="p">(</span><span class="n">tf_oper</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation_by_name_unsafe</span><span class="p">(</span><span class="n">op_name</span><span class="p">)</span>

<div class="viewcode-block" id="Graph.get_tensor_by_name"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.get_tensor_by_name">[docs]</a>  <span class="k">def</span> <span class="nf">get_tensor_by_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Tensor` with the given `name`.</span>

<span class="sd">    This method may be called concurrently from multiple threads.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name of the `Tensor` to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Tensor` with the given `name`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `name` is not a string.</span>
<span class="sd">      KeyError: If `name` does not correspond to a tensor in this graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Names should be strings.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Tensor names are strings (or similar), not </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                      <span class="nb">type</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_operation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_get_tensor_by_tf_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tf_output</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `Tensor` representing `tf_output`.</span>

<span class="sd">    Note that there is only one such `Tensor`, i.e. multiple calls to this</span>
<span class="sd">    function with the same TF_Output value will always return the same `Tensor`</span>
<span class="sd">    object.</span>

<span class="sd">    Args:</span>
<span class="sd">      tf_output: A wrapped `TF_Output` (the C API equivalent of `Tensor`).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The `Tensor` that represents `tf_output`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_operation_by_tf_operation</span><span class="p">(</span><span class="n">tf_output</span><span class="o">.</span><span class="n">oper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">tf_output</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_last_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_id_counter</span>

  <span class="k">def</span> <span class="nf">_get_op_def</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
    <span class="sd">&quot;&quot;&quot;Returns the `OpDef` proto for `type`. `type` is a string.&quot;&quot;&quot;</span>
    <span class="c1"># NOTE: No locking is required because the lookup and insertion operations</span>
    <span class="c1"># on Python dictionaries are atomic.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_def_cache</span><span class="p">[</span><span class="nb">type</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">c_api_util</span><span class="o">.</span><span class="n">tf_buffer</span><span class="p">()</span> <span class="k">as</span> <span class="n">buf</span><span class="p">:</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GraphGetOpDef</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="nb">type</span><span class="p">),</span>
                                           <span class="n">buf</span><span class="p">)</span>
        <span class="c1"># pylint: enable=protected-access</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
      <span class="n">op_def</span> <span class="o">=</span> <span class="n">op_def_pb2</span><span class="o">.</span><span class="n">OpDef</span><span class="p">()</span>
      <span class="n">op_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_op_def_cache</span><span class="p">[</span><span class="nb">type</span><span class="p">]</span> <span class="o">=</span> <span class="n">op_def</span>
      <span class="k">return</span> <span class="n">op_def</span>

<div class="viewcode-block" id="Graph.as_default"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.as_default">[docs]</a>  <span class="k">def</span> <span class="nf">as_default</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that makes this `Graph` the default graph.</span>

<span class="sd">    This method should be used if you want to create multiple graphs</span>
<span class="sd">    in the same process. For convenience, a global default graph is</span>
<span class="sd">    provided, and all ops will be added to this graph if you do not</span>
<span class="sd">    create a new graph explicitly.</span>

<span class="sd">    Use this method with the `with` keyword to specify that ops created within</span>
<span class="sd">    the scope of a block should be added to this graph. In this case, once</span>
<span class="sd">    the scope of the `with` is exited, the previous default graph is set again</span>
<span class="sd">    as default. There is a stack, so it&#39;s ok to have multiple nested levels</span>
<span class="sd">    of `as_default` calls.</span>

<span class="sd">    The default graph is a property of the current thread. If you</span>
<span class="sd">    create a new thread, and wish to use the default graph in that</span>
<span class="sd">    thread, you must explicitly add a `with g.as_default():` in that</span>
<span class="sd">    thread&#39;s function.</span>

<span class="sd">    The following code examples are equivalent:</span>

<span class="sd">    ```python</span>
<span class="sd">    # 1. Using Graph.as_default():</span>
<span class="sd">    g = tf.Graph()</span>
<span class="sd">    with g.as_default():</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      assert c.graph is g</span>

<span class="sd">    # 2. Constructing and making default:</span>
<span class="sd">    with tf.Graph().as_default() as g:</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      assert c.graph is g</span>
<span class="sd">    ```</span>

<span class="sd">    If eager execution is enabled ops created under this context manager will be</span>
<span class="sd">    added to the graph instead of executed eagerly.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager for using this graph as the default graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">collections</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the names of the collections known to this graph.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">)</span>

<div class="viewcode-block" id="Graph.add_to_collection"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.add_to_collection">[docs]</a>  <span class="k">def</span> <span class="nf">add_to_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stores `value` in the collection with the given `name`.</span>

<span class="sd">    Note that collections are not sets, so it is possible to add a value to</span>
<span class="sd">    a collection several times.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. The `GraphKeys` class contains many</span>
<span class="sd">        standard names for collections.</span>
<span class="sd">      value: The value to add to the collection.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># pylint: disable=g-doc-exception</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Graph.add_to_collections"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.add_to_collections">[docs]</a>  <span class="k">def</span> <span class="nf">add_to_collections</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stores `value` in the collections given by `names`.</span>

<span class="sd">    Note that collections are not sets, so it is possible to add a value to</span>
<span class="sd">    a collection several times. This function makes sure that duplicates in</span>
<span class="sd">    `names` are ignored, but it will not check for pre-existing membership of</span>
<span class="sd">    `value` in any of the collections in `names`.</span>

<span class="sd">    `names` can be any iterable, but if `names` is a string, it is treated as a</span>
<span class="sd">    single collection name.</span>

<span class="sd">    Args:</span>
<span class="sd">      names: The keys for the collections to add to. The `GraphKeys` class</span>
<span class="sd">        contains many standard names for collections.</span>
<span class="sd">      value: The value to add to the collections.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure names are unique, but treat strings as a single collection name</span>
    <span class="n">names</span> <span class="o">=</span> <span class="p">(</span><span class="n">names</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="k">else</span> <span class="nb">set</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Graph.get_collection_ref"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.get_collection_ref">[docs]</a>  <span class="k">def</span> <span class="nf">get_collection_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of values in the collection with the given `name`.</span>

<span class="sd">    If the collection exists, this returns the list itself, which can</span>
<span class="sd">    be modified in place to change the collection.  If the collection does</span>
<span class="sd">    not exist, it is created as an empty list and the list is returned.</span>

<span class="sd">    This is different from `get_collection()` which always returns a copy of</span>
<span class="sd">    the collection list if it exists and never creates an empty collection.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">        contains many standard names for collections.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The list of values in the collection with the given `name`, or an empty</span>
<span class="sd">      list if no value has been added to that collection.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># pylint: disable=g-doc-exception</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="n">coll_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">coll_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">coll_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">coll_list</span>
      <span class="k">return</span> <span class="n">coll_list</span></div>

<div class="viewcode-block" id="Graph.get_collection"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.get_collection">[docs]</a>  <span class="k">def</span> <span class="nf">get_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of values in the collection with the given `name`.</span>

<span class="sd">    This is different from `get_collection_ref()` which always returns the</span>
<span class="sd">    actual collection list if it exists in that it returns a new list each time</span>
<span class="sd">    it is called.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">        contains many standard names for collections.</span>
<span class="sd">      scope: (Optional.) A string. If supplied, the resulting list is filtered</span>
<span class="sd">        to include only items whose `name` attribute matches `scope` using</span>
<span class="sd">        `re.match`. Items without a `name` attribute are never returned if a</span>
<span class="sd">        scope is supplied. The choice of `re.match` means that a `scope` without</span>
<span class="sd">        special tokens filters by prefix.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The list of values in the collection with the given `name`, or</span>
<span class="sd">      an empty list if no value has been added to that collection. The</span>
<span class="sd">      list contains the values in the order under which they were</span>
<span class="sd">      collected.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># pylint: disable=g-doc-exception</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="n">collection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">collection</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
      <span class="k">if</span> <span class="n">scope</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">collection</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">regex</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">scope</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">collection</span><span class="p">:</span>
          <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">regex</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
              <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
          <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="c1"># Collection items with no name are ignored.</span>
            <span class="k">pass</span>
        <span class="k">return</span> <span class="n">c</span></div>

<div class="viewcode-block" id="Graph.get_all_collection_keys"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.get_all_collection_keys">[docs]</a>  <span class="k">def</span> <span class="nf">get_all_collection_keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of collections used in this graph.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)]</span></div>

<div class="viewcode-block" id="Graph.clear_collection"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.clear_collection">[docs]</a>  <span class="k">def</span> <span class="nf">clear_collection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clears all values in a collection.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The key for the collection. The `GraphKeys` class contains many</span>
<span class="sd">        standard names for collections.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_not_finalized</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">:</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collections</span><span class="p">[</span><span class="n">name</span><span class="p">]</span></div>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_original_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Python &#39;with&#39; handler to help annotate ops with their originator.</span>

<span class="sd">    An op may have an &#39;original_op&#39; property that indicates the op on which</span>
<span class="sd">    it was based. For example a replica op is based on the op that was</span>
<span class="sd">    replicated and a gradient op is based on the op that was differentiated.</span>

<span class="sd">    All ops created in the scope of this &#39;with&#39; handler will have</span>
<span class="sd">    the given &#39;op&#39; as their original op.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: The Operation that all ops created in this scope will have as their</span>
<span class="sd">        original op.</span>

<span class="sd">    Yields:</span>
<span class="sd">      Nothing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">old_original_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_default_original_op</span> <span class="o">=</span> <span class="n">old_original_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_name_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># This may be called from a thread where name_stack doesn&#39;t yet exist.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_name_stack&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_name_stack</span>

  <span class="nd">@_name_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_name_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name_stack</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="n">name_stack</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield,line-too-long</span>
<div class="viewcode-block" id="Graph.name_scope"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.name_scope">[docs]</a>  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that creates hierarchical names for operations.</span>

<span class="sd">    A graph maintains a stack of name scopes. A `with name_scope(...):`</span>
<span class="sd">    statement pushes a new name onto the stack for the lifetime of the context.</span>

<span class="sd">    The `name` argument will be interpreted as follows:</span>

<span class="sd">    * A string (not ending with &#39;/&#39;) will create a new name scope, in which</span>
<span class="sd">      `name` is appended to the prefix of all operations created in the</span>
<span class="sd">      context. If `name` has been used before, it will be made unique by</span>
<span class="sd">      calling `self.unique_name(name)`.</span>
<span class="sd">    * A scope previously captured from a `with g.name_scope(...) as</span>
<span class="sd">      scope:` statement will be treated as an &quot;absolute&quot; name scope, which</span>
<span class="sd">      makes it possible to re-enter existing scopes.</span>
<span class="sd">    * A value of `None` or the empty string will reset the current name scope</span>
<span class="sd">      to the top-level (empty) name scope.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with tf.Graph().as_default() as g:</span>
<span class="sd">      c = tf.constant(5.0, name=&quot;c&quot;)</span>
<span class="sd">      assert c.op.name == &quot;c&quot;</span>
<span class="sd">      c_1 = tf.constant(6.0, name=&quot;c&quot;)</span>
<span class="sd">      assert c_1.op.name == &quot;c_1&quot;</span>

<span class="sd">      # Creates a scope called &quot;nested&quot;</span>
<span class="sd">      with g.name_scope(&quot;nested&quot;) as scope:</span>
<span class="sd">        nested_c = tf.constant(10.0, name=&quot;c&quot;)</span>
<span class="sd">        assert nested_c.op.name == &quot;nested/c&quot;</span>

<span class="sd">        # Creates a nested scope called &quot;inner&quot;.</span>
<span class="sd">        with g.name_scope(&quot;inner&quot;):</span>
<span class="sd">          nested_inner_c = tf.constant(20.0, name=&quot;c&quot;)</span>
<span class="sd">          assert nested_inner_c.op.name == &quot;nested/inner/c&quot;</span>

<span class="sd">        # Create a nested scope called &quot;inner_1&quot;.</span>
<span class="sd">        with g.name_scope(&quot;inner&quot;):</span>
<span class="sd">          nested_inner_1_c = tf.constant(30.0, name=&quot;c&quot;)</span>
<span class="sd">          assert nested_inner_1_c.op.name == &quot;nested/inner_1/c&quot;</span>

<span class="sd">          # Treats `scope` as an absolute name scope, and</span>
<span class="sd">          # switches to the &quot;nested/&quot; scope.</span>
<span class="sd">          with g.name_scope(scope):</span>
<span class="sd">            nested_d = tf.constant(40.0, name=&quot;d&quot;)</span>
<span class="sd">            assert nested_d.op.name == &quot;nested/d&quot;</span>

<span class="sd">            with g.name_scope(&quot;&quot;):</span>
<span class="sd">              e = tf.constant(50.0, name=&quot;e&quot;)</span>
<span class="sd">              assert e.op.name == &quot;e&quot;</span>
<span class="sd">    ```</span>

<span class="sd">    The name of the scope itself can be captured by `with</span>
<span class="sd">    g.name_scope(...) as scope:`, which stores the name of the scope</span>
<span class="sd">    in the variable `scope`. This value can be used to name an</span>
<span class="sd">    operation that represents the overall result of executing the ops</span>
<span class="sd">    in a scope. For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.constant(...)</span>
<span class="sd">    with g.name_scope(&#39;my_layer&#39;) as scope:</span>
<span class="sd">      weights = tf.Variable(..., name=&quot;weights&quot;)</span>
<span class="sd">      biases = tf.Variable(..., name=&quot;biases&quot;)</span>
<span class="sd">      affine = tf.matmul(inputs, weights) + biases</span>
<span class="sd">      output = tf.nn.relu(affine, name=scope)</span>
<span class="sd">    ```</span>

<span class="sd">    NOTE: This constructor validates the given `name`. Valid scope</span>
<span class="sd">    names match one of the following regular expressions:</span>

<span class="sd">        [A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)</span>
<span class="sd">        [A-Za-z0-9_.\\-/]* (for other scopes)</span>

<span class="sd">    Args:</span>
<span class="sd">      name: A name for the scope.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that installs `name` as a new name scope.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `name` is not a valid scope name, according to the rules</span>
<span class="sd">        above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">bytes_or_text_types</span><span class="p">):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span><span class="p">:</span>
        <span class="c1"># Scopes created in a nested scope may have initial characters</span>
        <span class="c1"># that are illegal as the initial character of an op name</span>
        <span class="c1"># (viz. &#39;-&#39;, &#39;\&#39;, &#39;/&#39;, and &#39;_&#39;).</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_VALID_SCOPE_NAME_REGEX</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; is not a valid scope name&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Scopes created in the root must match the more restrictive</span>
        <span class="c1"># op name regex, which constrains the initial character.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_VALID_OP_NAME_REGEX</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; is not a valid scope name&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">old_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>  <span class="c1"># Both for name=None and name=&quot;&quot; we re-set to empty scope.</span>
      <span class="n">new_stack</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">new_stack</span> <span class="o">=</span> <span class="n">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">new_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="n">new_stack</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">new_stack</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">new_stack</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">=</span> <span class="n">old_stack</span></div>

  <span class="c1"># pylint: enable=g-doc-return-or-yield,line-too-long</span>

<div class="viewcode-block" id="Graph.unique_name"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.unique_name">[docs]</a>  <span class="k">def</span> <span class="nf">unique_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">mark_as_used</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return a unique operation name for `name`.</span>

<span class="sd">    Note: You rarely need to call `unique_name()` directly.  Most of</span>
<span class="sd">    the time you just need to create `with g.name_scope()` blocks to</span>
<span class="sd">    generate structured names.</span>

<span class="sd">    `unique_name` is used to generate structured names, separated by</span>
<span class="sd">    `&quot;/&quot;`, to help identify operations when debugging a graph.</span>
<span class="sd">    Operation names are displayed in error messages reported by the</span>
<span class="sd">    TensorFlow runtime, and in various visualization tools such as</span>
<span class="sd">    TensorBoard.</span>

<span class="sd">    If `mark_as_used` is set to `True`, which is the default, a new</span>
<span class="sd">    unique name is created and marked as in use. If it&#39;s set to `False`,</span>
<span class="sd">    the unique name is returned without actually being marked as used.</span>
<span class="sd">    This is useful when the caller simply wants to know what the name</span>
<span class="sd">    to be created will be.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name for an operation.</span>
<span class="sd">      mark_as_used: Whether to mark this name as being used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A string to be passed to `create_op()` that will be used</span>
<span class="sd">      to name the operation being created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">name</span>

    <span class="c1"># For the sake of checking for names in use, we treat names as case</span>
    <span class="c1"># insensitive (e.g. foo = Foo).</span>
    <span class="n">name_key</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name_key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Increment the number for &quot;name_key&quot;.</span>
    <span class="k">if</span> <span class="n">mark_as_used</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">[</span><span class="n">name_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">base_name_key</span> <span class="o">=</span> <span class="n">name_key</span>
      <span class="c1"># Make sure the composed name key is not already used.</span>
      <span class="k">while</span> <span class="n">name_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">:</span>
        <span class="n">name_key</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">base_name_key</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="c1"># Mark the composed name_key as used in case someone wants</span>
      <span class="c1"># to call unique_name(&quot;name_1&quot;).</span>
      <span class="k">if</span> <span class="n">mark_as_used</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_names_in_use</span><span class="p">[</span><span class="n">name_key</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

      <span class="c1"># Return the new name with the original capitalization of the given name.</span>
      <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">name</span></div>

<div class="viewcode-block" id="Graph.get_name_scope"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.get_name_scope">[docs]</a>  <span class="k">def</span> <span class="nf">get_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the current name scope.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with tf.name_scope(&#39;scope1&#39;):</span>
<span class="sd">      with tf.name_scope(&#39;scope2&#39;):</span>
<span class="sd">        print(tf.compat.v1.get_default_graph().get_name_scope())</span>
<span class="sd">    ```</span>
<span class="sd">    would print the string `scope1/scope2`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A string representing the current name scope.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_stack</span></div>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_colocate_with_for_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">,</span>
                                  <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">gradient_uid</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span><span class="o">.</span><span class="n">EnterGradientColocation</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_control_flow_context</span><span class="o">.</span><span class="n">ExitGradientColocation</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span>

<div class="viewcode-block" id="Graph.colocate_with"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.colocate_with">[docs]</a>  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies an op to colocate with.</span>

<span class="sd">    Note: this function is not for public use, only for internal libraries.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    a = tf.Variable([1.0])</span>
<span class="sd">    with g.colocate_with(a):</span>
<span class="sd">      b = tf.constant(1.0)</span>
<span class="sd">      c = tf.add(a, b)</span>
<span class="sd">    ```</span>

<span class="sd">    `b` and `c` will always be colocated with `a`, no matter where `a`</span>
<span class="sd">    is eventually placed.</span>

<span class="sd">    **NOTE** Using a colocation scope resets any existing device constraints.</span>

<span class="sd">    If `op` is `None` then `ignore_existing` must be `True` and the new</span>
<span class="sd">    scope resets all colocation and device constraints.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: The op to colocate all created ops with, or `None`.</span>
<span class="sd">      ignore_existing: If true, only applies colocation of this op within the</span>
<span class="sd">        context, rather than applying all colocation properties on the stack.</span>
<span class="sd">        If `op` is `None`, this value must be `True`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if op is None but ignore_existing is False.</span>

<span class="sd">    Yields:</span>
<span class="sd">      A context manager that specifies the op with which to colocate</span>
<span class="sd">      newly created ops.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ignore_existing</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trying to reset colocation (op is None) but &quot;</span>
                       <span class="s2">&quot;ignore_existing is not True&quot;</span><span class="p">)</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">_op_to_colocate_with</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="c1"># By default, colocate_with resets the device function stack,</span>
    <span class="c1"># since colocate_with is typically used in specific internal</span>
    <span class="c1"># library functions where colocation is intended to be &quot;stronger&quot;</span>
    <span class="c1"># than device functions.</span>
    <span class="c1">#</span>
    <span class="c1"># In the future, a caller may specify that device_functions win</span>
    <span class="c1"># over colocation, in which case we can add support.</span>
    <span class="n">device_fn_tmp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">ignore_existing</span><span class="p">:</span>
      <span class="n">current_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">traceable_stack</span><span class="o">.</span><span class="n">TraceableStack</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># offset refers to the stack frame used for storing code location.</span>
      <span class="c1"># We use 4, the sum of 1 to use our caller&#39;s stack frame and 3</span>
      <span class="c1"># to jump over layers of context managers above us.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">push_obj</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Restore device function stack</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">device_fn_tmp</span>
      <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">pop_obj</span><span class="p">()</span>

      <span class="c1"># Reset the colocation stack if requested.</span>
      <span class="k">if</span> <span class="n">ignore_existing</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">current_stack</span></div>

  <span class="k">def</span> <span class="nf">_add_device_to_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name_or_function</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add device to stack manually, separate from a context manager.&quot;&quot;&quot;</span>
    <span class="n">total_offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">offset</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">_UserDeviceSpec</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">push_obj</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">total_offset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">spec</span>

<div class="viewcode-block" id="Graph.device"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.device">[docs]</a>  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_name_or_function</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies the default device to use.</span>

<span class="sd">    The `device_name_or_function` argument may either be a device name</span>
<span class="sd">    string, a device function, or None:</span>

<span class="sd">    * If it is a device name string, all operations constructed in</span>
<span class="sd">      this context will be assigned to the device with that name, unless</span>
<span class="sd">      overridden by a nested `device()` context.</span>
<span class="sd">    * If it is a function, it will be treated as a function from</span>
<span class="sd">      Operation objects to device name strings, and invoked each time</span>
<span class="sd">      a new Operation is created. The Operation will be assigned to</span>
<span class="sd">      the device with the returned name.</span>
<span class="sd">    * If it is None, all `device()` invocations from the enclosing context</span>
<span class="sd">      will be ignored.</span>

<span class="sd">    For information about the valid syntax of device name strings, see</span>
<span class="sd">    the documentation in</span>
<span class="sd">    [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.device(&#39;/device:GPU:0&#39;):</span>
<span class="sd">      # All operations constructed in this context will be placed</span>
<span class="sd">      # on GPU 0.</span>
<span class="sd">      with g.device(None):</span>
<span class="sd">        # All operations constructed in this context will have no</span>
<span class="sd">        # assigned device.</span>

<span class="sd">    # Defines a function from `Operation` to device string.</span>
<span class="sd">    def matmul_on_gpu(n):</span>
<span class="sd">      if n.type == &quot;MatMul&quot;:</span>
<span class="sd">        return &quot;/device:GPU:0&quot;</span>
<span class="sd">      else:</span>
<span class="sd">        return &quot;/cpu:0&quot;</span>

<span class="sd">    with g.device(matmul_on_gpu):</span>
<span class="sd">      # All operations of type &quot;MatMul&quot; constructed in this context</span>
<span class="sd">      # will be placed on GPU 0; all other operations will be placed</span>
<span class="sd">      # on CPU 0.</span>
<span class="sd">    ```</span>

<span class="sd">    **N.B.** The device scope may be overridden by op wrappers or</span>
<span class="sd">    other library code. For example, a variable assignment op</span>
<span class="sd">    `v.assign()` must be colocated with the `tf.Variable` `v`, and</span>
<span class="sd">    incompatible device scopes will be ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">      device_name_or_function: The device name or function to use in the</span>
<span class="sd">        context.</span>

<span class="sd">    Yields:</span>
<span class="sd">      A context manager that specifies the default device to use for newly</span>
<span class="sd">      created ops.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If device scopes are not properly nested.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_add_device_to_stack</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">old_top_of_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_top_obj</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">new_top_of_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_top_obj</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">old_top_of_stack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_top_of_stack</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Exiting device scope without proper scope nesting.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">pop_obj</span><span class="p">()</span></div>

  <span class="k">def</span> <span class="nf">_apply_device_functions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies the current device function stack to the given operation.&quot;&quot;&quot;</span>
    <span class="c1"># Apply any device functions in LIFO order, so that the most recently</span>
    <span class="c1"># pushed function has the first chance to apply a device to the op.</span>
    <span class="c1"># We apply here because the result can depend on the Operation&#39;s</span>
    <span class="c1"># signature, which is computed in the Operation constructor.</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">prior_device_string</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">device_spec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_objs</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">device_spec</span><span class="o">.</span><span class="n">is_null_merge</span><span class="p">:</span>
        <span class="k">continue</span>

      <span class="k">if</span> <span class="n">device_spec</span><span class="o">.</span><span class="n">function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">break</span>

      <span class="n">device_string</span> <span class="o">=</span> <span class="n">device_spec</span><span class="o">.</span><span class="n">string_merge</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

      <span class="c1"># Take advantage of the fact that None is a singleton and Python interns</span>
      <span class="c1"># strings, since identity checks are faster than equality checks.</span>
      <span class="k">if</span> <span class="n">device_string</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">prior_device_string</span><span class="p">:</span>
        <span class="n">op</span><span class="o">.</span><span class="n">_set_device_from_string</span><span class="p">(</span><span class="n">device_string</span><span class="p">)</span>
        <span class="n">prior_device_string</span> <span class="o">=</span> <span class="n">device_string</span>
    <span class="n">op</span><span class="o">.</span><span class="n">_device_code_locations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot_device_function_stack_metadata</span><span class="p">()</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
<div class="viewcode-block" id="Graph.container"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.container">[docs]</a>  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">container</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">container_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies the resource container to use.</span>

<span class="sd">    Stateful operations, such as variables and queues, can maintain their</span>
<span class="sd">    states on devices so that they can be shared by multiple processes.</span>
<span class="sd">    A resource container is a string name under which these stateful</span>
<span class="sd">    operations are tracked. These resources can be released or cleared</span>
<span class="sd">    with `tf.Session.reset()`.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.container(&#39;experiment0&#39;):</span>
<span class="sd">      # All stateful Operations constructed in this context will be placed</span>
<span class="sd">      # in resource container &quot;experiment0&quot;.</span>
<span class="sd">      v1 = tf.Variable([1.0])</span>
<span class="sd">      v2 = tf.Variable([2.0])</span>
<span class="sd">      with g.container(&quot;experiment1&quot;):</span>
<span class="sd">        # All stateful Operations constructed in this context will be</span>
<span class="sd">        # placed in resource container &quot;experiment1&quot;.</span>
<span class="sd">        v3 = tf.Variable([3.0])</span>
<span class="sd">        q1 = tf.queue.FIFOQueue(10, tf.float32)</span>
<span class="sd">      # All stateful Operations constructed in this context will be</span>
<span class="sd">      # be created in the &quot;experiment0&quot;.</span>
<span class="sd">      v4 = tf.Variable([4.0])</span>
<span class="sd">      q1 = tf.queue.FIFOQueue(20, tf.float32)</span>
<span class="sd">      with g.container(&quot;&quot;):</span>
<span class="sd">        # All stateful Operations constructed in this context will be</span>
<span class="sd">        # be placed in the default resource container.</span>
<span class="sd">        v5 = tf.Variable([5.0])</span>
<span class="sd">        q3 = tf.queue.FIFOQueue(30, tf.float32)</span>

<span class="sd">    # Resets container &quot;experiment0&quot;, after which the state of v1, v2, v4, q1</span>
<span class="sd">    # will become undefined (such as uninitialized).</span>
<span class="sd">    tf.Session.reset(target, [&quot;experiment0&quot;])</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      container_name: container name string.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager for defining resource containers for stateful ops,</span>
<span class="sd">        yields the container name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original_container</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_container</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="o">=</span> <span class="n">container_name</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">_container</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_container</span> <span class="o">=</span> <span class="n">original_container</span></div>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

  <span class="k">class</span> <span class="nc">_ControlDependenciesController</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Context manager for `control_dependencies()`.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">control_inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Create a new `_ControlDependenciesController`.</span>

<span class="sd">      A `_ControlDependenciesController` is the context manager for</span>
<span class="sd">      `with tf.control_dependencies()` blocks.  These normally nest,</span>
<span class="sd">      as described in the documentation for `control_dependencies()`.</span>

<span class="sd">      The `control_inputs` argument list control dependencies that must be</span>
<span class="sd">      added to the current set of control dependencies.  Because of</span>
<span class="sd">      uniquification the set can be empty even if the caller passed a list of</span>
<span class="sd">      ops.  The special value `None` indicates that we want to start a new</span>
<span class="sd">      empty set of control dependencies instead of extending the current set.</span>

<span class="sd">      In that case we also clear the current control flow context, which is an</span>
<span class="sd">      additional mechanism to add control dependencies.</span>

<span class="sd">      Args:</span>
<span class="sd">        graph: The graph that this controller is managing.</span>
<span class="sd">        control_inputs: List of ops to use as control inputs in addition to the</span>
<span class="sd">          current control dependencies.  None to indicate that the dependencies</span>
<span class="sd">          should be cleared.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">graph</span>
      <span class="k">if</span> <span class="n">control_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_inputs_val</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_inputs_val</span> <span class="o">=</span> <span class="n">control_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_seen_nodes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_old_stack</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_old_control_flow_context</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># pylint: disable=protected-access</span>

    <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span><span class="p">:</span>
        <span class="c1"># Clear the control_dependencies graph.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_old_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_control_dependencies_stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Clear the control_flow_context too.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_old_control_flow_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_push_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_type</span><span class="p">,</span> <span class="n">unused_value</span><span class="p">,</span> <span class="n">unused_traceback</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_pop_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_stack</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_set_control_flow_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_old_control_flow_context</span><span class="p">)</span>

<span class="c1"># pylint: enable=protected-access</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">control_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_inputs_val</span>

    <span class="k">def</span> <span class="nf">add_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_seen_nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">op_in_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seen_nodes</span>

  <span class="k">def</span> <span class="nf">_push_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">controller</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">controller</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_pop_control_dependencies_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">controller</span><span class="p">):</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="n">controller</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_current_control_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">controller</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">controller</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">:</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_control_dependencies_for_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ops</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For an op that takes `input_ops` as inputs, compute control inputs.</span>

<span class="sd">    The returned control dependencies should yield an execution that</span>
<span class="sd">    is equivalent to adding all control inputs in</span>
<span class="sd">    self._control_dependencies_stack to a newly created op. However,</span>
<span class="sd">    this function attempts to prune the returned control dependencies</span>
<span class="sd">    by observing that nodes created within the same `with</span>
<span class="sd">    control_dependencies(...):` block may have data dependencies that make</span>
<span class="sd">    the explicit approach redundant.</span>

<span class="sd">    Args:</span>
<span class="sd">      input_ops: The data input ops for an op to be created.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of control inputs for the op to be created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">controller</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">:</span>
      <span class="c1"># If any of the input_ops already depends on the inputs from controller,</span>
      <span class="c1"># we say that the new op is dominated (by that input), and we therefore</span>
      <span class="c1"># do not need to add control dependencies for this controller&#39;s inputs.</span>
      <span class="n">dominated</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">input_ops</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">controller</span><span class="o">.</span><span class="n">op_in_group</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
          <span class="n">dominated</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="k">break</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">dominated</span><span class="p">:</span>
        <span class="c1"># Don&#39;t add a control input if we already have a data dependency on i.</span>
        <span class="c1"># NOTE(mrry): We do not currently track transitive data dependencies,</span>
        <span class="c1">#   so we may add redundant control inputs.</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">controller</span><span class="o">.</span><span class="n">control_inputs</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">input_ops</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_record_op_seen_by_control_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Record that the given op depends on all registered control dependencies.</span>

<span class="sd">    Args:</span>
<span class="sd">      op: An Operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">controller</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_dependencies_stack</span><span class="p">:</span>
      <span class="n">controller</span><span class="o">.</span><span class="n">add_op</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

<div class="viewcode-block" id="Graph.control_dependencies"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.control_dependencies">[docs]</a>  <span class="k">def</span> <span class="nf">control_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a context manager that specifies control dependencies.</span>

<span class="sd">    Use with the `with` keyword to specify that all operations constructed</span>
<span class="sd">    within the context should have control dependencies on</span>
<span class="sd">    `control_inputs`. For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.control_dependencies([a, b, c]):</span>
<span class="sd">      # `d` and `e` will only run after `a`, `b`, and `c` have executed.</span>
<span class="sd">      d = ...</span>
<span class="sd">      e = ...</span>
<span class="sd">    ```</span>

<span class="sd">    Multiple calls to `control_dependencies()` can be nested, and in</span>
<span class="sd">    that case a new `Operation` will have control dependencies on the union</span>
<span class="sd">    of `control_inputs` from all active contexts.</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.control_dependencies([a, b]):</span>
<span class="sd">      # Ops constructed here run after `a` and `b`.</span>
<span class="sd">      with g.control_dependencies([c, d]):</span>
<span class="sd">        # Ops constructed here run after `a`, `b`, `c`, and `d`.</span>
<span class="sd">    ```</span>

<span class="sd">    You can pass None to clear the control dependencies:</span>

<span class="sd">    ```python</span>
<span class="sd">    with g.control_dependencies([a, b]):</span>
<span class="sd">      # Ops constructed here run after `a` and `b`.</span>
<span class="sd">      with g.control_dependencies(None):</span>
<span class="sd">        # Ops constructed here run normally, not waiting for either `a` or `b`.</span>
<span class="sd">        with g.control_dependencies([c, d]):</span>
<span class="sd">          # Ops constructed here run after `c` and `d`, also not waiting</span>
<span class="sd">          # for either `a` or `b`.</span>
<span class="sd">    ```</span>

<span class="sd">    *N.B.* The control dependencies context applies *only* to ops that</span>
<span class="sd">    are constructed within the context. Merely using an op or tensor</span>
<span class="sd">    in the context does not add a control dependency. The following</span>
<span class="sd">    example illustrates this point:</span>

<span class="sd">    ```python</span>
<span class="sd">    # WRONG</span>
<span class="sd">    def my_func(pred, tensor):</span>
<span class="sd">      t = tf.matmul(tensor, tensor)</span>
<span class="sd">      with tf.control_dependencies([pred]):</span>
<span class="sd">        # The matmul op is created outside the context, so no control</span>
<span class="sd">        # dependency will be added.</span>
<span class="sd">        return t</span>

<span class="sd">    # RIGHT</span>
<span class="sd">    def my_func(pred, tensor):</span>
<span class="sd">      with tf.control_dependencies([pred]):</span>
<span class="sd">        # The matmul op is created in the context, so a control dependency</span>
<span class="sd">        # will be added.</span>
<span class="sd">        return tf.matmul(tensor, tensor)</span>
<span class="sd">    ```</span>

<span class="sd">    Also note that though execution of ops created under this scope will trigger</span>
<span class="sd">    execution of the dependencies, the ops created under this scope might still</span>
<span class="sd">    be pruned from a normal tensorflow graph. For example, in the following</span>
<span class="sd">    snippet of code the dependencies are never executed:</span>

<span class="sd">    ```python</span>
<span class="sd">      loss = model.loss()</span>
<span class="sd">      with tf.control_dependencies(dependencies):</span>
<span class="sd">        loss = loss + tf.constant(1)  # note: dependencies ignored in the</span>
<span class="sd">                                      # backward pass</span>
<span class="sd">      return tf.gradients(loss, model.variables)</span>
<span class="sd">    ```</span>

<span class="sd">    This is because evaluating the gradient graph does not require evaluating</span>
<span class="sd">    the constant(1) op created in the forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">      control_inputs: A list of `Operation` or `Tensor` objects which must be</span>
<span class="sd">        executed or computed before running the operations defined in the</span>
<span class="sd">        context.  Can also be `None` to clear the control dependencies.</span>

<span class="sd">    Returns:</span>
<span class="sd">     A context manager that specifies control dependencies for all</span>
<span class="sd">     operations constructed within the context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `control_inputs` is not a list of `Operation` or</span>
<span class="sd">        `Tensor` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">control_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ControlDependenciesController</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># First convert the inputs to ops, and deduplicate them.</span>
    <span class="c1"># NOTE(mrry): Other than deduplication, we do not currently track direct</span>
    <span class="c1">#   or indirect dependencies between control_inputs, which may result in</span>
    <span class="c1">#   redundant control inputs.</span>
    <span class="n">control_ops</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_control_dependencies</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
      <span class="c1"># The hasattr(handle) is designed to match ResourceVariables. This is so</span>
      <span class="c1"># control dependencies on a variable or on an unread variable don&#39;t</span>
      <span class="c1"># trigger reads.</span>
      <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">IndexedSlices</span><span class="p">)</span> <span class="ow">or</span>
          <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="s2">&quot;_handle&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="s2">&quot;op&quot;</span><span class="p">))):</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">op</span>
      <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">op</span>
      <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Control input must be Operation or Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">c</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">current</span><span class="p">:</span>
        <span class="n">control_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="n">current</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ControlDependenciesController</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_ops</span><span class="p">)</span></div>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_attr_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: A context manager for setting attributes on operators.</span>

<span class="sd">    This context manager can be used to add additional</span>
<span class="sd">    attributes to operators within the scope of the context.</span>

<span class="sd">    For example:</span>

<span class="sd">       with ops.Graph().as_default() as g:</span>
<span class="sd">         f_1 = Foo()  # No extra attributes</span>
<span class="sd">         with g._attr_scope({&quot;_a&quot;: tf.attr_value_pb2.AttrValue(b=False)}):</span>
<span class="sd">           f_2 = Foo()  # Additional attribute _a=False</span>
<span class="sd">           with g._attr_scope({&quot;_a&quot;: tf.attr_value_pb2.AttrValue(b=True)}):</span>
<span class="sd">             f_3 = Foo()  # Additional attribute _a=False</span>
<span class="sd">             with g._attr_scope({&quot;_a&quot;: None}):</span>
<span class="sd">               f_4 = Foo()  # No additional attributes.</span>

<span class="sd">    Args:</span>
<span class="sd">      attr_map: A dictionary mapping attr name strings to AttrValue protocol</span>
<span class="sd">        buffers or None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that sets the kernel label to be used for one or more</span>
<span class="sd">      ops created in that context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If attr_map is not a dictionary mapping</span>
<span class="sd">        strings to AttrValue protobufs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_map</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;attr_map must be a dictionary mapping &quot;</span>
                      <span class="s2">&quot;strings to AttrValue protocol buffers&quot;</span><span class="p">)</span>
    <span class="c1"># The saved_attrs dictionary stores any currently-set labels that</span>
    <span class="c1"># will be overridden by this context manager.</span>
    <span class="n">saved_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Install the given attribute</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attr_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span>
              <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">))</span> <span class="ow">or</span>
               <span class="n">callable</span><span class="p">(</span><span class="n">attr</span><span class="p">))):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;attr_map must be a dictionary mapping &quot;</span>
                        <span class="s2">&quot;strings to AttrValue protocol buffers or &quot;</span>
                        <span class="s2">&quot;callables that emit AttrValue protocol buffers&quot;</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">saved_attrs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">pass</span>
      <span class="k">if</span> <span class="n">attr</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">attr</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>  <span class="c1"># The code within the context runs here.</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Remove the attributes set for this context, and restore any saved</span>
      <span class="c1"># attributes.</span>
      <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attr_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_attrs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
          <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attr_scope_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_kernel_label_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_to_kernel_label_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: A context manager for setting kernel labels.</span>

<span class="sd">    This context manager can be used to select particular</span>
<span class="sd">    implementations of kernels within the scope of the context.</span>

<span class="sd">    For example:</span>

<span class="sd">        with ops.Graph().as_default() as g:</span>
<span class="sd">          f_1 = Foo()  # Uses the default registered kernel for the Foo op.</span>
<span class="sd">          with g.kernel_label_map({&quot;Foo&quot;: &quot;v_2&quot;}):</span>
<span class="sd">            f_2 = Foo()  # Uses the registered kernel with label &quot;v_2&quot;</span>
<span class="sd">                         # for the Foo op.</span>
<span class="sd">            with g.kernel_label_map({&quot;Foo&quot;: &quot;v_3&quot;}):</span>
<span class="sd">              f_3 = Foo()  # Uses the registered kernel with label &quot;v_3&quot;</span>
<span class="sd">                           # for the Foo op.</span>
<span class="sd">              with g.kernel_label_map({&quot;Foo&quot;: &quot;&quot;}):</span>
<span class="sd">                f_4 = Foo()  # Uses the default registered kernel</span>
<span class="sd">                             # for the Foo op.</span>

<span class="sd">    Args:</span>
<span class="sd">      op_to_kernel_label_map: A dictionary mapping op type strings to kernel</span>
<span class="sd">        label strings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that sets the kernel label to be used for one or more</span>
<span class="sd">      ops created in that context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If op_to_kernel_label_map is not a dictionary mapping</span>
<span class="sd">        strings to strings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_to_kernel_label_map</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_to_kernel_label_map must be a dictionary mapping &quot;</span>
                      <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
    <span class="c1"># The saved_labels dictionary stores any currently-set labels that</span>
    <span class="c1"># will be overridden by this context manager.</span>
    <span class="n">saved_labels</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Install the given label</span>
    <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">op_to_kernel_label_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span>
              <span class="nb">isinstance</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_to_kernel_label_map must be a dictionary mapping &quot;</span>
                        <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">saved_labels</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">pass</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>  <span class="c1"># The code within the context runs here.</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Remove the labels set for this context, and restore any saved labels.</span>
      <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">op_to_kernel_label_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_labels</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
          <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_to_kernel_label_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_override_gradient_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient_function_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Specify gradient function for the given op type.&quot;&quot;&quot;</span>

    <span class="c1"># This is an internal API and we don&#39;t need nested context for this.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_function_map</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_function_map</span> <span class="o">=</span> <span class="n">gradient_function_map</span>
    <span class="k">yield</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_function_map</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="c1"># pylint: disable=g-doc-return-or-yield</span>
<div class="viewcode-block" id="Graph.gradient_override_map"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.gradient_override_map">[docs]</a>  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">gradient_override_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_type_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;EXPERIMENTAL: A context manager for overriding gradient functions.</span>

<span class="sd">    This context manager can be used to override the gradient function</span>
<span class="sd">    that will be used for ops within the scope of the context.</span>

<span class="sd">    For example:</span>

<span class="sd">    ```python</span>
<span class="sd">    @tf.RegisterGradient(&quot;CustomSquare&quot;)</span>
<span class="sd">    def _custom_square_grad(op, grad):</span>
<span class="sd">      # ...</span>

<span class="sd">    with tf.Graph().as_default() as g:</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      s_1 = tf.square(c)  # Uses the default gradient for tf.square.</span>
<span class="sd">      with g.gradient_override_map({&quot;Square&quot;: &quot;CustomSquare&quot;}):</span>
<span class="sd">        s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the</span>
<span class="sd">                              # gradient of s_2.</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      op_type_map: A dictionary mapping op type strings to alternative op type</span>
<span class="sd">        strings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A context manager that sets the alternative op type to be used for one</span>
<span class="sd">      or more ops created in that context.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `op_type_map` is not a dictionary mapping strings to</span>
<span class="sd">        strings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type_map</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type_map must be a dictionary mapping &quot;</span>
                      <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
    <span class="c1"># The saved_mappings dictionary stores any currently-set mappings that</span>
    <span class="c1"># will be overridden by this context manager.</span>
    <span class="n">saved_mappings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Install the given label</span>
    <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">mapped_op_type</span> <span class="ow">in</span> <span class="n">op_type_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span>
              <span class="nb">isinstance</span><span class="p">(</span><span class="n">mapped_op_type</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;op_type_map must be a dictionary mapping &quot;</span>
                        <span class="s2">&quot;strings to strings&quot;</span><span class="p">)</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">saved_mappings</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">pass</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapped_op_type</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>  <span class="c1"># The code within the context runs here.</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># Remove the labels set for this context, and restore any saved labels.</span>
      <span class="k">for</span> <span class="n">op_type</span><span class="p">,</span> <span class="n">mapped_op_type</span> <span class="ow">in</span> <span class="n">op_type_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_mappings</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
          <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_override_map</span><span class="p">[</span><span class="n">op_type</span><span class="p">]</span></div>

  <span class="c1"># pylint: enable=g-doc-return-or-yield</span>

<div class="viewcode-block" id="Graph.prevent_feeding"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.prevent_feeding">[docs]</a>  <span class="k">def</span> <span class="nf">prevent_feeding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Marks the given `tensor` as unfeedable in this graph.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfeedable_tensors</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span></div>

<div class="viewcode-block" id="Graph.is_feedable"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.is_feedable">[docs]</a>  <span class="k">def</span> <span class="nf">is_feedable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if and only if `tensor` is feedable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unfeedable_tensors</span></div>

<div class="viewcode-block" id="Graph.prevent_fetching"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.prevent_fetching">[docs]</a>  <span class="k">def</span> <span class="nf">prevent_fetching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Marks the given `op` as unfetchable in this graph.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">op</span><span class="p">)</span></div>

<div class="viewcode-block" id="Graph.is_fetchable"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.is_fetchable">[docs]</a>  <span class="k">def</span> <span class="nf">is_fetchable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_or_op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if and only if `tensor_or_op` is fetchable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_or_op</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tensor_or_op</span><span class="o">.</span><span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tensor_or_op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unfetchable_ops</span></div>

<div class="viewcode-block" id="Graph.switch_to_thread_local"><a class="viewcode-back" href="../../../../index.html#tensorflow.Graph.switch_to_thread_local">[docs]</a>  <span class="k">def</span> <span class="nf">switch_to_thread_local</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make device, colocation and dependencies stacks thread-local.</span>

<span class="sd">    Device, colocation and dependencies stacks are not thread-local be default.</span>
<span class="sd">    If multiple threads access them, then the state is shared.  This means that</span>
<span class="sd">    one thread may affect the behavior of another thread.</span>

<span class="sd">    After this method is called, the stacks become thread-local.  If multiple</span>
<span class="sd">    threads access them, then the state is not shared.  Each thread uses its own</span>
<span class="sd">    value; a thread doesn&#39;t affect other threads by mutating such a stack.</span>

<span class="sd">    The initial value for every thread&#39;s stack is set to the current value</span>
<span class="sd">    of the stack when `switch_to_thread_local()` was first called.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span> <span class="o">=</span> <span class="kc">True</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_function_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># This may be called from a thread where device_function_stack doesn&#39;t yet</span>
      <span class="c1"># exist.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_device_function_stack&quot;</span><span class="p">):</span>
        <span class="n">stack_copy_for_this_thread</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">stack_copy_for_this_thread</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_device_function_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_functions_outer_to_inner</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">user_device_specs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_objs</span><span class="p">()</span>
    <span class="n">device_functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">spec</span><span class="o">.</span><span class="n">function</span> <span class="k">for</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">user_device_specs</span><span class="p">]</span>
    <span class="n">device_functions_outer_to_inner</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">device_functions</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">device_functions_outer_to_inner</span>

  <span class="k">def</span> <span class="nf">_snapshot_device_function_stack_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return device function stack as a list of TraceableObjects.</span>

<span class="sd">    Returns:</span>
<span class="sd">      [traceable_stack.TraceableObject, ...] where each TraceableObject&#39;s .obj</span>
<span class="sd">      member is a displayable name for the user&#39;s argument to Graph.device, and</span>
<span class="sd">      the filename and lineno members point to the code location where</span>
<span class="sd">      Graph.device was called directly or indirectly by the user.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">snapshot</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="o">.</span><span class="n">peek_traceable_objs</span><span class="p">():</span>
      <span class="n">obj_copy</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">copy_metadata</span><span class="p">()</span>
      <span class="n">obj_copy</span><span class="o">.</span><span class="n">obj</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">obj</span><span class="o">.</span><span class="n">display_name</span>
      <span class="n">snapshot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obj_copy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">snapshot</span>

  <span class="nd">@_device_function_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_device_function_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_function_stack</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">device_function_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_device_function_stack</span> <span class="o">=</span> <span class="n">device_function_stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_colocation_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return thread-local copy of colocation stack.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># This may be called from a thread where colocation_stack doesn&#39;t yet</span>
      <span class="c1"># exist.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_colocation_stack&quot;</span><span class="p">):</span>
        <span class="n">stack_copy_for_this_thread</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">stack_copy_for_this_thread</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_colocation_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span>

  <span class="k">def</span> <span class="nf">_snapshot_colocation_stack_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return colocation stack metadata as a dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="n">traceable_obj</span><span class="o">.</span><span class="n">obj</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">traceable_obj</span><span class="o">.</span><span class="n">copy_metadata</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">traceable_obj</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_colocation_stack</span><span class="o">.</span><span class="n">peek_traceable_objs</span><span class="p">()</span>
    <span class="p">}</span>

  <span class="nd">@_colocation_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_colocation_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">colocation_stack</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_colocation_stack</span> <span class="o">=</span> <span class="n">colocation_stack</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_colocation_stack</span> <span class="o">=</span> <span class="n">colocation_stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_control_dependencies_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="c1"># This may be called from a thread where control_dependencies_stack</span>
      <span class="c1"># doesn&#39;t yet exist.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_control_dependencies_stack&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span><span class="p">[:])</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_control_dependencies_stack</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span>

  <span class="nd">@_control_dependencies_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_control_dependencies_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_dependencies</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stack_state_is_thread_local</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_control_dependencies_stack</span> <span class="o">=</span> <span class="n">control_dependencies</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_control_dependencies_stack</span> <span class="o">=</span> <span class="n">control_dependencies</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_distribution_strategy_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A stack to maintain distribution strategy context for each thread.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_distribution_strategy_stack&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_distribution_strategy_stack</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_distribution_strategy_stack</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="nd">@_distribution_strategy_stack</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_distribution_strategy_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_distribution_strategy_stack</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_distribution_strategy_stack</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">_distribution_strategy_stack</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_global_distribute_strategy_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For implementing `tf.distribute.set_strategy()`.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;distribute_strategy_scope&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">distribute_strategy_scope</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">distribute_strategy_scope</span>

  <span class="nd">@_global_distribute_strategy_scope</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_global_distribute_strategy_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distribute_strategy_scope</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">distribute_strategy_scope</span> <span class="o">=</span> <span class="p">(</span><span class="n">distribute_strategy_scope</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_auto_cast_variable_read_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The dtype that instances of `AutoCastVariable` will be casted to.</span>

<span class="sd">    This is None if `AutoCastVariables` should not be casted.</span>

<span class="sd">    See `AutoCastVariable` for more information.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The dtype that instances of `AutoCastVariable` will be casted to.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="p">,</span> <span class="s2">&quot;_auto_cast_variable_read_dtype&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_auto_cast_variable_read_dtype</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_auto_cast_variable_read_dtype</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="nd">@_auto_cast_variable_read_dtype</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">_auto_cast_variable_read_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dtype</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local</span><span class="o">.</span><span class="n">_auto_cast_variable_read_dtype</span> <span class="o">=</span> <span class="n">dtype</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_enable_auto_casting_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Context manager to automatically cast AutoCastVariables.</span>

<span class="sd">    If an AutoCastVariable `var` is used under this context manager, it will be</span>
<span class="sd">    casted to `dtype` before being used.</span>

<span class="sd">    See `AutoCastVariable` for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">      dtype: The dtype that AutoCastVariables should be casted to.</span>

<span class="sd">    Yields:</span>
<span class="sd">      Nothing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prev_read_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_cast_variable_read_dtype</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_auto_cast_variable_read_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_auto_cast_variable_read_dtype</span> <span class="o">=</span> <span class="n">prev_read_dtype</span>

  <span class="k">def</span> <span class="nf">_mutation_lock</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a lock to guard code that creates &amp; mutates ops.</span>

<span class="sd">    See the comment for self._group_lock for more info.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_lock</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="n">_MUTATION_LOCK_GROUP</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_session_run_lock</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a lock to guard code for Session.run.</span>

<span class="sd">    See the comment for self._group_lock for more info.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_lock</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="n">_SESSION_RUN_LOCK_GROUP</span><span class="p">)</span></div>


<span class="c1"># TODO(agarwal): currently device directives in an outer eager scope will not</span>
<span class="c1"># apply to inner graph mode code. Fix that.</span>


<div class="viewcode-block" id="device"><a class="viewcode-back" href="../../../../index.html#tensorflow.device">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.device()` using the default graph.</span>

<span class="sd">  See `tf.Graph.device` for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    device_name_or_function: The device name or function to use in the context.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A context manager that specifies the default device to use for newly</span>
<span class="sd">    created ops.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If eager execution is enabled and a function is passed in.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;tf.device does not support functions when eager execution &quot;</span>
          <span class="s2">&quot;is enabled.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">context</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>
    <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">combined</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
          <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">):</span>
            <span class="k">yield</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">yield</span>
    <span class="k">return</span> <span class="n">combined</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">device_v2</span><span class="p">(</span><span class="n">device_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Specifies the device for ops created/executed in this context.</span>

<span class="sd">  This function specifies the device to be used for ops created/executed in a</span>
<span class="sd">  particular context. Nested contexts will inherit and also create/execute</span>
<span class="sd">  their ops on the specified device. If a specific device is not required,</span>
<span class="sd">  consider not using this function so that a device can be automatically</span>
<span class="sd">  assigned.  In general the use of this function is optional. `device_name` can</span>
<span class="sd">  be fully specified, as in &quot;/job:worker/task:1/device:cpu:0&quot;, or partially</span>
<span class="sd">  specified, containing only a subset of the &quot;/&quot;-separated fields. Any fields</span>
<span class="sd">  which are specified will override device annotations from outer scopes.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.device(&#39;/job:foo&#39;):</span>
<span class="sd">    # ops created here have devices with /job:foo</span>
<span class="sd">    with tf.device(&#39;/job:bar/task:0/device:gpu:2&#39;):</span>
<span class="sd">      # ops created here have the fully specified device above</span>
<span class="sd">    with tf.device(&#39;/device:gpu:1&#39;):</span>
<span class="sd">      # ops created here have the device &#39;/job:foo/device:gpu:1&#39;</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    device_name: The device name to use in the context.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A context manager that specifies the default device to use for newly</span>
<span class="sd">    created ops.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If a function is passed in.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">device_name</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;tf.device does not support functions.&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;container&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">container</span><span class="p">(</span><span class="n">container_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.container()` using the default graph.</span>

<span class="sd">  Args:</span>
<span class="sd">    container_name: The container string to use in the context.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A context manager that specifies the default container to use for newly</span>
<span class="sd">    created stateful ops.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">container</span><span class="p">(</span><span class="n">container_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_colocate_with_for_gradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">internal_convert_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">device</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">NullContextmanager</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">default_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">EagerTensor</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Encountered an Eager-defined Tensor during graph &quot;</span>
                         <span class="s2">&quot;construction, but a function was not being built.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">_colocate_with_for_gradient</span><span class="p">(</span>
        <span class="n">op</span><span class="p">,</span> <span class="n">gradient_uid</span><span class="o">=</span><span class="n">gradient_uid</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="n">ignore_existing</span><span class="p">)</span>


<span class="c1"># Internal interface to colocate_with. colocate_with has been deprecated from</span>
<span class="c1"># public API. There are still a few internal uses of colocate_with. Add internal</span>
<span class="c1"># only API for those uses to avoid deprecation warning.</span>
<span class="k">def</span> <span class="nf">colocate_with</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_colocate_with_for_gradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="n">ignore_existing</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Colocations handled automatically by placer.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;colocate_with&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">_colocate_with</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">colocate_with</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="p">)</span>


<div class="viewcode-block" id="control_dependencies"><a class="viewcode-back" href="../../../../index.html#tensorflow.control_dependencies">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;control_dependencies&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">control_dependencies</span><span class="p">(</span><span class="n">control_inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.control_dependencies()` using the default graph.</span>

<span class="sd">  See `tf.Graph.control_dependencies`</span>
<span class="sd">  for more details.</span>

<span class="sd">  When eager execution is enabled, any callable object in the `control_inputs`</span>
<span class="sd">  list will be called.</span>

<span class="sd">  Args:</span>
<span class="sd">    control_inputs: A list of `Operation` or `Tensor` objects which must be</span>
<span class="sd">      executed or computed before running the operations defined in the context.</span>
<span class="sd">      Can also be `None` to clear the control dependencies. If eager execution</span>
<span class="sd">      is enabled, any callable object in the `control_inputs` list will be</span>
<span class="sd">      called.</span>

<span class="sd">  Returns:</span>
<span class="sd">   A context manager that specifies control dependencies for all</span>
<span class="sd">   operations constructed within the context.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">control_inputs</span><span class="p">:</span>
      <span class="c1"># Execute any pending callables.</span>
      <span class="k">for</span> <span class="n">control</span> <span class="ow">in</span> <span class="n">control_inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">control</span><span class="p">):</span>
          <span class="n">control</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">NullContextmanager</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">control_inputs</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_DefaultStack</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A thread-local stack of objects for providing implicit defaults.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">get_default</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">is_cleared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">enforce_nesting</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span>

  <span class="nd">@enforce_nesting</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">enforce_nesting</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">default</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A context manager for manipulating a default stack.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">default</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span> <span class="n">default</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># stack may be empty if reset() was called</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enforce_nesting</span><span class="p">:</span>
          <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">default</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;Nesting violated for default stack of </span><span class="si">%s</span><span class="s2"> objects&quot;</span> <span class="o">%</span>
                <span class="nb">type</span><span class="p">(</span><span class="n">default</span><span class="p">))</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">default</span><span class="p">)</span>


<span class="n">_default_session_stack</span> <span class="o">=</span> <span class="n">_DefaultStack</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">default_session</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Python &quot;with&quot; handler for defining a default session.</span>

<span class="sd">  This function provides a means of registering a session for handling</span>
<span class="sd">  Tensor.eval() and Operation.run() calls. It is primarily intended for use</span>
<span class="sd">  by session.Session, but can be used with any object that implements</span>
<span class="sd">  the Session.run() interface.</span>

<span class="sd">  Use with the &quot;with&quot; keyword to specify that Tensor.eval() and Operation.run()</span>
<span class="sd">  invocations within the scope of a block should be executed by a particular</span>
<span class="sd">  session.</span>

<span class="sd">  The default session applies to the current thread only, so it is always</span>
<span class="sd">  possible to inspect the call stack and determine the scope of a default</span>
<span class="sd">  session. If you create a new thread, and wish to use the default session</span>
<span class="sd">  in that thread, you must explicitly add a &quot;with ops.default_session(sess):&quot;</span>
<span class="sd">  block in that thread&#39;s function.</span>

<span class="sd">  Example:</span>
<span class="sd">    The following code examples are equivalent:</span>

<span class="sd">    # 1. Using the Session object directly:</span>
<span class="sd">    sess = ...</span>
<span class="sd">    c = tf.constant(5.0)</span>
<span class="sd">    sess.run(c)</span>

<span class="sd">    # 2. Using default_session():</span>
<span class="sd">    sess = ...</span>
<span class="sd">    with ops.default_session(sess):</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      result = c.eval()</span>

<span class="sd">    # 3. Overriding default_session():</span>
<span class="sd">    sess = ...</span>
<span class="sd">    with ops.default_session(sess):</span>
<span class="sd">      c = tf.constant(5.0)</span>
<span class="sd">      with ops.default_session(...):</span>
<span class="sd">        c.eval(session=sess)</span>

<span class="sd">  Args:</span>
<span class="sd">    session: The session to be installed as the default session.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A context manager for the default session.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_default_session_stack</span><span class="o">.</span><span class="n">get_controller</span><span class="p">(</span><span class="n">session</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;get_default_session&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">get_default_session</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the default session for the current thread.</span>

<span class="sd">  The returned `Session` will be the innermost session on which a</span>
<span class="sd">  `Session` or `Session.as_default()` context has been entered.</span>

<span class="sd">  NOTE: The default session is a property of the current thread. If you</span>
<span class="sd">  create a new thread, and wish to use the default session in that</span>
<span class="sd">  thread, you must explicitly add a `with sess.as_default():` in that</span>
<span class="sd">  thread&#39;s function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The default `Session` being used in the current thread.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_default_session_stack</span><span class="o">.</span><span class="n">get_default</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_eval_using_default_session</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Uses the default session to evaluate one or more tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensors: A single Tensor, or a list of Tensor objects.</span>
<span class="sd">    feed_dict: A dictionary that maps Tensor objects (or tensor names) to lists,</span>
<span class="sd">      numpy ndarrays, TensorProtos, or strings.</span>
<span class="sd">    graph: The graph in which the tensors are defined.</span>
<span class="sd">    session: (Optional) A different session to use to evaluate &quot;tensors&quot;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Either a single numpy ndarray if &quot;tensors&quot; is a single tensor; or a list</span>
<span class="sd">    of numpy ndarrays that each correspond to the respective element in</span>
<span class="sd">    &quot;tensors&quot;.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If no default session is available; the default session</span>
<span class="sd">      does not have &quot;graph&quot; as its graph; or if &quot;session&quot; is specified,</span>
<span class="sd">      and it does not have &quot;graph&quot; as its graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">get_default_session</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot evaluate tensor using `eval()`: No default &quot;</span>
                       <span class="s2">&quot;session is registered. Use `with &quot;</span>
                       <span class="s2">&quot;sess.as_default()` or pass an explicit session to &quot;</span>
                       <span class="s2">&quot;`eval(session=sess)`&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the default session to evaluate tensor: &quot;</span>
                       <span class="s2">&quot;the tensor&#39;s graph is different from the session&#39;s &quot;</span>
                       <span class="s2">&quot;graph. Pass an explicit session to &quot;</span>
                       <span class="s2">&quot;`eval(session=sess)`.&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the given session to evaluate tensor: &quot;</span>
                       <span class="s2">&quot;the tensor&#39;s graph is different from the session&#39;s &quot;</span>
                       <span class="s2">&quot;graph.&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_run_using_default_session</span><span class="p">(</span><span class="n">operation</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Uses the default session to run &quot;operation&quot;.</span>

<span class="sd">  Args:</span>
<span class="sd">    operation: The Operation to be run.</span>
<span class="sd">    feed_dict: A dictionary that maps Tensor objects (or tensor names) to lists,</span>
<span class="sd">      numpy ndarrays, TensorProtos, or strings.</span>
<span class="sd">    graph: The graph in which &quot;operation&quot; is defined.</span>
<span class="sd">    session: (Optional) A different session to use to run &quot;operation&quot;.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If no default session is available; the default session</span>
<span class="sd">      does not have &quot;graph&quot; as its graph; or if &quot;session&quot; is specified,</span>
<span class="sd">      and it does not have &quot;graph&quot; as its graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">get_default_session</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot execute operation using `run()`: No default &quot;</span>
                       <span class="s2">&quot;session is registered. Use `with &quot;</span>
                       <span class="s2">&quot;sess.as_default():` or pass an explicit session to &quot;</span>
                       <span class="s2">&quot;`run(session=sess)`&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the default session to execute operation: &quot;</span>
                       <span class="s2">&quot;the operation&#39;s graph is different from the &quot;</span>
                       <span class="s2">&quot;session&#39;s graph. Pass an explicit session to &quot;</span>
                       <span class="s2">&quot;run(session=sess).&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot use the given session to execute operation: &quot;</span>
                       <span class="s2">&quot;the operation&#39;s graph is different from the session&#39;s &quot;</span>
                       <span class="s2">&quot;graph.&quot;</span><span class="p">)</span>
  <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">operation</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_DefaultGraphStack</span><span class="p">(</span><span class="n">_DefaultStack</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="sd">&quot;&quot;&quot;A thread-local stack of objects for providing an implicit default graph.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">get_default</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Override that returns a global default if the stack is empty.&quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_default</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetGlobalDefaultGraph</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret</span>

  <span class="k">def</span> <span class="nf">_GetGlobalDefaultGraph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># TODO(mrry): Perhaps log that the default graph is being used, or set</span>
      <span class="c1">#   provide some other feedback to prevent confusion when a mixture of</span>
      <span class="c1">#   the global default graph and an explicit graph are combined in the</span>
      <span class="c1">#   same process.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span>

  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">default</span><span class="p">):</span>
    <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">default</span><span class="o">.</span><span class="n">building_function</span><span class="p">,</span>
                                            <span class="n">default</span><span class="o">.</span><span class="n">as_default</span><span class="p">,</span>
                                            <span class="n">default</span><span class="o">.</span><span class="n">_device_function_stack</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">super</span><span class="p">(</span><span class="n">_DefaultGraphStack</span><span class="p">,</span>
                 <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_controller</span><span class="p">(</span><span class="n">default</span><span class="p">)</span> <span class="k">as</span> <span class="n">g</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">graph_mode</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">g</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># If an exception is raised here it may be hiding a related exception in</span>
      <span class="c1"># the try-block (just above).</span>
      <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="n">_default_graph_stack</span> <span class="o">=</span> <span class="n">_DefaultGraphStack</span><span class="p">()</span>


<span class="c1"># Shared helper used in init_scope and executing_eagerly_outside_functions</span>
<span class="c1"># to obtain the outermost context that is not building a function, and the</span>
<span class="c1"># innermost non empty device stack.</span>
<span class="k">def</span> <span class="nf">_get_outer_context_and_inner_device_stack</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the outermost context not building a function.&quot;&quot;&quot;</span>
  <span class="n">default_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
  <span class="n">outer_context</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">innermost_nonempty_device_stack</span> <span class="o">=</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">_device_function_stack</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">stack</span><span class="p">:</span>
    <span class="c1"># If the default graph stack is empty, then we cannot be building a</span>
    <span class="c1"># function. Install the global graph (which, in this case, is also the</span>
    <span class="c1"># default graph) as the outer context.</span>
    <span class="k">if</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The global graph is building a function.&quot;</span><span class="p">)</span>
    <span class="n">outer_context</span> <span class="o">=</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">as_default</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Find a context that is not building a function.</span>
    <span class="k">for</span> <span class="n">stack_entry</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">context_switches</span><span class="o">.</span><span class="n">stack</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">innermost_nonempty_device_stack</span><span class="p">:</span>
        <span class="n">innermost_nonempty_device_stack</span> <span class="o">=</span> <span class="n">stack_entry</span><span class="o">.</span><span class="n">device_stack</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">stack_entry</span><span class="o">.</span><span class="n">is_building_function</span><span class="p">:</span>
        <span class="n">outer_context</span> <span class="o">=</span> <span class="n">stack_entry</span><span class="o">.</span><span class="n">enter_context_fn</span>
        <span class="k">break</span>

    <span class="k">if</span> <span class="n">outer_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># As a last resort, obtain the global default graph; this graph doesn&#39;t</span>
      <span class="c1"># necessarily live on the graph stack (and hence it doesn&#39;t necessarily</span>
      <span class="c1"># live on the context stack), but it is stored in the graph stack&#39;s</span>
      <span class="c1"># encapsulating object.</span>
      <span class="n">outer_context</span> <span class="o">=</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">_GetGlobalDefaultGraph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">if</span> <span class="n">outer_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Sanity check; this shouldn&#39;t be triggered.</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;All graphs are building functions, and no &quot;</span>
                       <span class="s2">&quot;eager context was previously active.&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">outer_context</span><span class="p">,</span> <span class="n">innermost_nonempty_device_stack</span>


<span class="c1"># pylint: disable=g-doc-return-or-yield,line-too-long</span>
<div class="viewcode-block" id="init_scope"><a class="viewcode-back" href="../../../../index.html#tensorflow.init_scope">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;init_scope&quot;</span><span class="p">)</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">init_scope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;A context manager that lifts ops out of control-flow scopes and function-building graphs.</span>

<span class="sd">  There is often a need to lift variable initialization ops out of control-flow</span>
<span class="sd">  scopes, function-building graphs, and gradient tapes. Entering an</span>
<span class="sd">  `init_scope` is a mechanism for satisfying these desiderata. In particular,</span>
<span class="sd">  entering an `init_scope` has three effects:</span>

<span class="sd">    (1) All control dependencies are cleared the moment the scope is entered;</span>
<span class="sd">        this is equivalent to entering the context manager returned from</span>
<span class="sd">        `control_dependencies(None)`, which has the side-effect of exiting</span>
<span class="sd">        control-flow scopes like `tf.cond` and `tf.while_loop`.</span>

<span class="sd">    (2) All operations that are created while the scope is active are lifted</span>
<span class="sd">        into the lowest context on the `context_stack` that is not building a</span>
<span class="sd">        graph function. Here, a context is defined as either a graph or an eager</span>
<span class="sd">        context. Every context switch, i.e., every installation of a graph as</span>
<span class="sd">        the default graph and every switch into eager mode, is logged in a</span>
<span class="sd">        thread-local stack called `context_switches`; the log entry for a</span>
<span class="sd">        context switch is popped from the stack when the context is exited.</span>
<span class="sd">        Entering an `init_scope` is equivalent to crawling up</span>
<span class="sd">        `context_switches`, finding the first context that is not building a</span>
<span class="sd">        graph function, and entering it. A caveat is that if graph mode is</span>
<span class="sd">        enabled but the default graph stack is empty, then entering an</span>
<span class="sd">        `init_scope` will simply install a fresh graph as the default one.</span>

<span class="sd">    (3) The gradient tape is paused while the scope is active.</span>

<span class="sd">  When eager execution is enabled, code inside an init_scope block runs with</span>
<span class="sd">  eager execution enabled even when tracing a `tf.function`. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.compat.v1.enable_eager_execution()</span>

<span class="sd">  @tf.function</span>
<span class="sd">  def func():</span>
<span class="sd">    # A function constructs TensorFlow graphs,</span>
<span class="sd">    # it does not execute eagerly.</span>
<span class="sd">    assert not tf.executing_eagerly()</span>
<span class="sd">    with tf.init_scope():</span>
<span class="sd">      # Initialization runs with eager execution enabled</span>
<span class="sd">      assert tf.executing_eagerly()</span>
<span class="sd">  ```</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: if graph state is incompatible with this initialization.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># pylint: enable=g-doc-return-or-yield,line-too-long</span>

  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="c1"># Fastpath.</span>
    <span class="k">with</span> <span class="n">tape</span><span class="o">.</span><span class="n">stop_recording</span><span class="p">():</span>
      <span class="k">yield</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Retrieve the active name scope: entering an `init_scope` preserves</span>
    <span class="c1"># the name scope of the current context.</span>
    <span class="n">scope</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">scope</span> <span class="ow">and</span> <span class="n">scope</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="c1"># Names that end with trailing slashes are treated by `name_scope` as</span>
      <span class="c1"># absolute.</span>
      <span class="n">scope</span> <span class="o">=</span> <span class="n">scope</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span>

    <span class="n">outer_context</span><span class="p">,</span> <span class="n">innermost_nonempty_device_stack</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">_get_outer_context_and_inner_device_stack</span><span class="p">())</span>

    <span class="n">outer_graph</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">outer_device_stack</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">outer_context</span><span class="p">(),</span> <span class="n">name_scope</span><span class="p">(</span>
          <span class="n">scope</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">control_dependencies</span><span class="p">(</span>
              <span class="kc">None</span><span class="p">),</span> <span class="n">tape</span><span class="o">.</span><span class="n">stop_recording</span><span class="p">():</span>
        <span class="n">context_manager</span> <span class="o">=</span> <span class="n">NullContextmanager</span>
        <span class="n">context_manager_input</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="c1"># The device stack is preserved when lifting into a graph. Eager</span>
          <span class="c1"># execution doesn&#39;t implement device stacks and in particular it</span>
          <span class="c1"># doesn&#39;t support device functions, so in general it&#39;s not possible</span>
          <span class="c1"># to do the same when lifting into the eager context.</span>
          <span class="n">outer_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
          <span class="n">outer_device_stack</span> <span class="o">=</span> <span class="n">outer_graph</span><span class="o">.</span><span class="n">_device_function_stack</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">outer_graph</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">innermost_nonempty_device_stack</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">elif</span> <span class="n">innermost_nonempty_device_stack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">for</span> <span class="n">device_spec</span> <span class="ow">in</span> <span class="n">innermost_nonempty_device_stack</span><span class="o">.</span><span class="n">peek_objs</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">device_spec</span><span class="o">.</span><span class="n">function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
              <span class="k">break</span>
            <span class="k">if</span> <span class="n">device_spec</span><span class="o">.</span><span class="n">raw_string</span><span class="p">:</span>
              <span class="n">context_manager</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">device</span>
              <span class="n">context_manager_input</span> <span class="o">=</span> <span class="n">device_spec</span><span class="o">.</span><span class="n">raw_string</span>
              <span class="k">break</span>
            <span class="c1"># It is currently not possible to have a device function in V2,</span>
            <span class="c1"># but in V1 we are unable to apply device functions in eager mode.</span>
            <span class="c1"># This means that we will silently skip some of the entries on the</span>
            <span class="c1"># device stack in V1 + eager mode.</span>

        <span class="k">with</span> <span class="n">context_manager</span><span class="p">(</span><span class="n">context_manager_input</span><span class="p">):</span>
          <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="c1"># If an exception is raised here it may be hiding a related exception in</span>
      <span class="c1"># try-block (just above).</span>
      <span class="k">if</span> <span class="n">outer_graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outer_graph</span><span class="o">.</span><span class="n">_device_function_stack</span> <span class="o">=</span> <span class="n">outer_device_stack</span>  <span class="c1"># pylint: disable=protected-access</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;executing_eagerly_outside_functions&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">executing_eagerly_outside_functions</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns True if executing eagerly, even if inside a graph function.</span>

<span class="sd">  This function will check the outermost context for the program and see if</span>
<span class="sd">  it is in eager mode. It is useful comparing to `tf.executing_eagerly()`,</span>
<span class="sd">  which checks the current context and will return `False` within a</span>
<span class="sd">  `tf.function` body. It can be used to build library that behave differently</span>
<span class="sd">  in eager runtime and v1 session runtime (deprecated).</span>

<span class="sd">  Example:</span>

<span class="sd">  &gt;&gt;&gt; tf.compat.v1.enable_eager_execution()</span>
<span class="sd">  &gt;&gt;&gt; @tf.function</span>
<span class="sd">  ... def func():</span>
<span class="sd">  ...   # A function constructs TensorFlow graphs, it does not execute eagerly,</span>
<span class="sd">  ...   # but the outer most context is still eager.</span>
<span class="sd">  ...   assert not tf.executing_eagerly()</span>
<span class="sd">  ...   return tf.compat.v1.executing_eagerly_outside_functions()</span>
<span class="sd">  &gt;&gt;&gt; func()</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=bool, numpy=True&gt;</span>

<span class="sd">  Returns:</span>
<span class="sd">    boolean, whether the outermost context is in eager mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">outer_context</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_outer_context_and_inner_device_stack</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">outer_context</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">inside_function</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">building_function</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;enable_eager_execution&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">enable_eager_execution</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_policy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">execution_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Enables eager execution for the lifetime of this program.</span>

<span class="sd">  Eager execution provides an imperative interface to TensorFlow. With eager</span>
<span class="sd">  execution enabled, TensorFlow functions execute operations immediately (as</span>
<span class="sd">  opposed to adding to a graph to be executed later in a `tf.compat.v1.Session`)</span>
<span class="sd">  and</span>
<span class="sd">  return concrete values (as opposed to symbolic references to a node in a</span>
<span class="sd">  computational graph).</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.compat.v1.enable_eager_execution()</span>

<span class="sd">  # After eager execution is enabled, operations are executed as they are</span>
<span class="sd">  # defined and Tensor objects hold concrete values, which can be accessed as</span>
<span class="sd">  # numpy.ndarray`s through the numpy() method.</span>
<span class="sd">  assert tf.multiply(6, 7).numpy() == 42</span>
<span class="sd">  ```</span>

<span class="sd">  Eager execution cannot be enabled after TensorFlow APIs have been used to</span>
<span class="sd">  create or execute graphs. It is typically recommended to invoke this function</span>
<span class="sd">  at program startup and not in a library (as most libraries should be usable</span>
<span class="sd">  both with and without eager execution).</span>

<span class="sd">  Args:</span>
<span class="sd">    config: (Optional.) A `tf.compat.v1.ConfigProto` to use to configure the</span>
<span class="sd">      environment in which operations are executed. Note that</span>
<span class="sd">      `tf.compat.v1.ConfigProto` is also used to configure graph execution (via</span>
<span class="sd">      `tf.compat.v1.Session`) and many options within `tf.compat.v1.ConfigProto`</span>
<span class="sd">      are not implemented (or are irrelevant) when eager execution is enabled.</span>
<span class="sd">    device_policy: (Optional.) Policy controlling how operations requiring</span>
<span class="sd">      inputs on a specific device (e.g., a GPU 0) handle inputs on a different</span>
<span class="sd">      device  (e.g. GPU 1 or CPU). When set to None, an appropriate value will</span>
<span class="sd">      be picked automatically. The value picked may change between TensorFlow</span>
<span class="sd">      releases.</span>
<span class="sd">      Valid values:</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT: raises an error if the</span>
<span class="sd">        placement is not correct.</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_WARN: copies the tensors which are not</span>
<span class="sd">        on the right device but logs a warning.</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_SILENT: silently copies the tensors.</span>
<span class="sd">        Note that this may hide performance problems as there is no notification</span>
<span class="sd">        provided when operations are blocked on the tensor being copied between</span>
<span class="sd">        devices.</span>
<span class="sd">      - tf.contrib.eager.DEVICE_PLACEMENT_SILENT_FOR_INT32: silently copies</span>
<span class="sd">        int32 tensors, raising errors on the other ones.</span>
<span class="sd">    execution_mode: (Optional.) Policy controlling how operations dispatched are</span>
<span class="sd">      actually executed. When set to None, an appropriate value will be picked</span>
<span class="sd">      automatically. The value picked may change between TensorFlow releases.</span>
<span class="sd">      Valid values:</span>
<span class="sd">      - tf.contrib.eager.SYNC: executes each operation synchronously.</span>
<span class="sd">      - tf.contrib.eager.ASYNC: executes each operation asynchronously. These</span>
<span class="sd">        operations may return &quot;non-ready&quot; handles.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If eager execution is enabled after creating/executing a</span>
<span class="sd">     TensorFlow graph, or if options provided conflict with a previous call</span>
<span class="sd">     to this function.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_api_usage_gauge</span><span class="o">.</span><span class="n">get_cell</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">default_execution_mode</span> <span class="o">!=</span> <span class="n">context</span><span class="o">.</span><span class="n">EAGER_MODE</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">enable_eager_execution_internal</span><span class="p">(</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
        <span class="n">device_policy</span><span class="o">=</span><span class="n">device_policy</span><span class="p">,</span>
        <span class="n">execution_mode</span><span class="o">=</span><span class="n">execution_mode</span><span class="p">,</span>
        <span class="n">server_def</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;disable_eager_execution&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">disable_eager_execution</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Disables eager execution.</span>

<span class="sd">  This function can only be called before any Graphs, Ops, or Tensors have been</span>
<span class="sd">  created. It can be used at the beginning of the program for complex migration</span>
<span class="sd">  projects from TensorFlow 1.x to 2.x.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_api_usage_gauge</span><span class="o">.</span><span class="n">get_cell</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">context</span><span class="o">.</span><span class="n">default_execution_mode</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span>
  <span class="n">c</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context_safe</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">c</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">is_eager</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">enable_eager_execution_internal</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">device_policy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">execution_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">server_def</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Enables eager execution for the lifetime of this program.</span>

<span class="sd">  Most of the doc string for enable_eager_execution is relevant here as well.</span>

<span class="sd">  Args:</span>
<span class="sd">    config: See enable_eager_execution doc string</span>
<span class="sd">    device_policy: See enable_eager_execution doc string</span>
<span class="sd">    execution_mode: See enable_eager_execution doc string</span>
<span class="sd">    server_def: (Optional.) A tensorflow::ServerDef proto. Enables execution on</span>
<span class="sd">      remote devices. GrpcServers need to be started by creating an identical</span>
<span class="sd">      server_def to this, and setting the appropriate task_indexes, so that the</span>
<span class="sd">      servers can communicate. It will then be possible to execute operations on</span>
<span class="sd">      remote devices.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">config_pb2</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;config must be a tf.ConfigProto, but got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                    <span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">device_policy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_EXPLICIT</span><span class="p">,</span>
                           <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_WARN</span><span class="p">,</span>
                           <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_SILENT</span><span class="p">,</span>
                           <span class="n">context</span><span class="o">.</span><span class="n">DEVICE_PLACEMENT_SILENT_FOR_INT32</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;device_policy must be one of None, tf.contrib.eager.DEVICE_PLACEMENT_*&quot;</span>
    <span class="p">)</span>
  <span class="k">if</span> <span class="n">execution_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">SYNC</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">ASYNC</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;execution_mode must be one of None, tf.contrib.eager.SYNC, &quot;</span>
        <span class="s2">&quot;tf.contrib.eager.ASYNC&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">default_execution_mode</span> <span class="o">==</span> <span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">:</span>
    <span class="n">graph_mode_has_been_used</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">_global_default_graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="n">graph_mode_has_been_used</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;tf.enable_eager_execution must be called at program startup.&quot;</span><span class="p">)</span>
  <span class="n">context</span><span class="o">.</span><span class="n">default_execution_mode</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">EAGER_MODE</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">_context_lock</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">context</span><span class="o">.</span><span class="n">_set_context_locked</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">Context</span><span class="p">(</span>
          <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
          <span class="n">device_policy</span><span class="o">=</span><span class="n">device_policy</span><span class="p">,</span>
          <span class="n">execution_mode</span><span class="o">=</span><span class="n">execution_mode</span><span class="p">,</span>
          <span class="n">server_def</span><span class="o">=</span><span class="n">server_def</span><span class="p">))</span>
    <span class="k">elif</span> <span class="p">((</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_config</span><span class="p">)</span> <span class="ow">or</span>
          <span class="p">(</span><span class="n">device_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
           <span class="n">device_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_device_policy</span><span class="p">)</span> <span class="ow">or</span>
          <span class="p">(</span><span class="n">execution_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
           <span class="n">execution_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_execution_mode</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Trying to change the options of an active eager&quot;</span>
          <span class="s2">&quot; execution. Context config: </span><span class="si">%s</span><span class="s2">, specified config:&quot;</span>
          <span class="s2">&quot; </span><span class="si">%s</span><span class="s2">. Context device policy: </span><span class="si">%s</span><span class="s2">, specified device&quot;</span>
          <span class="s2">&quot; policy: </span><span class="si">%s</span><span class="s2">. Context execution mode: </span><span class="si">%s</span><span class="s2">, &quot;</span>
          <span class="s2">&quot; specified execution mode </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_config</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_device_policy</span><span class="p">,</span>
           <span class="n">device_policy</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_execution_mode</span><span class="p">,</span> <span class="n">execution_mode</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># We already created everything, so update the thread local data.</span>
      <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="o">.</span><span class="n">_thread_local_data</span><span class="o">.</span><span class="n">is_eager</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="c1"># Monkey patch to get rid of an unnecessary conditional since the context is</span>
  <span class="c1"># now initialized.</span>
  <span class="n">context</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context_safe</span>


<span class="k">def</span> <span class="nf">eager_run</span><span class="p">(</span><span class="n">main</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">argv</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Runs the program with an optional main function and argv list.</span>

<span class="sd">  The program will run with eager execution enabled.</span>

<span class="sd">  Example:</span>
<span class="sd">  ```python</span>
<span class="sd">  import tensorflow as tf</span>
<span class="sd">  # Import subject to future changes:</span>
<span class="sd">  from tensorflow.contrib.eager.python import tfe</span>

<span class="sd">  def main(_):</span>
<span class="sd">    u = tf.constant(6.0)</span>
<span class="sd">    v = tf.constant(7.0)</span>
<span class="sd">    print(u * v)</span>

<span class="sd">  if __name__ == &quot;__main__&quot;:</span>
<span class="sd">    tfe.run()</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    main: the main function to run.</span>
<span class="sd">    argv: the arguments to pass to it.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">enable_eager_execution</span><span class="p">()</span>
  <span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">,</span> <span class="n">argv</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;reset_default_graph&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">reset_default_graph</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Clears the default graph stack and resets the global default graph.</span>

<span class="sd">  NOTE: The default graph is a property of the current thread. This</span>
<span class="sd">  function applies only to the current thread.  Calling this function while</span>
<span class="sd">  a `tf.compat.v1.Session` or `tf.compat.v1.InteractiveSession` is active will</span>
<span class="sd">  result in undefined</span>
<span class="sd">  behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects</span>
<span class="sd">  after calling this function will result in undefined behavior.</span>
<span class="sd">  Raises:</span>
<span class="sd">    AssertionError: If this function is called within a nested graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">is_cleared</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Do not use tf.reset_default_graph() to clear &quot;</span>
                         <span class="s2">&quot;nested graphs. If you need a cleared graph, &quot;</span>
                         <span class="s2">&quot;exit the nesting and create a new graph.&quot;</span><span class="p">)</span>
  <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;get_default_graph&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">get_default_graph</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the default graph for the current thread.</span>

<span class="sd">  The returned graph will be the innermost graph on which a</span>
<span class="sd">  `Graph.as_default()` context has been entered, or a global default</span>
<span class="sd">  graph if none has been explicitly created.</span>

<span class="sd">  NOTE: The default graph is a property of the current thread. If you</span>
<span class="sd">  create a new thread, and wish to use the default graph in that</span>
<span class="sd">  thread, you must explicitly add a `with g.as_default():` in that</span>
<span class="sd">  thread&#39;s function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The default `Graph` being used in the current thread.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">get_default</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">has_default_graph</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns True if there is a default graph.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">_default_graph_stack</span><span class="o">.</span><span class="n">stack</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span>


<span class="k">def</span> <span class="nf">get_name_scope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the current name scope in the default_graph.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  with tf.name_scope(&#39;scope1&#39;):</span>
<span class="sd">    with tf.name_scope(&#39;scope2&#39;):</span>
<span class="sd">      print(tf.get_name_scope())</span>
<span class="sd">  ```</span>
<span class="sd">  would print the string `scope1/scope2`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A string representing the current name scope.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span><span class="o">.</span><span class="n">scope_name</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_assert_same_graph</span><span class="p">(</span><span class="n">original_item</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fail if the 2 items are from different graphs.</span>

<span class="sd">  Args:</span>
<span class="sd">    original_item: Original item to check against.</span>
<span class="sd">    item: Item to check.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if graphs do not match.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">original_item</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">item</span><span class="o">.</span><span class="n">graph</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> must be from the same graph as </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">original_item</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_get_graph_from_inputs</span><span class="p">(</span><span class="n">op_input_list</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the appropriate graph to use for the given inputs.</span>

<span class="sd">  This library method provides a consistent algorithm for choosing the graph</span>
<span class="sd">  in which an Operation should be constructed:</span>

<span class="sd">  1. If the default graph is being used to construct a function, we</span>
<span class="sd">     use the default graph.</span>
<span class="sd">  2. If the &quot;graph&quot; is specified explicitly, we validate that all of the inputs</span>
<span class="sd">     in &quot;op_input_list&quot; are compatible with that graph.</span>
<span class="sd">  3. Otherwise, we attempt to select a graph from the first Operation-</span>
<span class="sd">     or Tensor-valued input in &quot;op_input_list&quot;, and validate that all other</span>
<span class="sd">     such inputs are in the same graph.</span>
<span class="sd">  4. If the graph was not specified and it could not be inferred from</span>
<span class="sd">     &quot;op_input_list&quot;, we attempt to use the default graph.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_input_list: A list of inputs to an operation, which may include `Tensor`,</span>
<span class="sd">      `Operation`, and other objects that may be converted to a graph element.</span>
<span class="sd">    graph: (Optional) The explicit graph to use.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If op_input_list is not a list or tuple, or if graph is not a</span>
<span class="sd">      Graph.</span>
<span class="sd">    ValueError: If a graph is explicitly passed and not all inputs are from it,</span>
<span class="sd">      or if the inputs are from multiple graphs, or we could not find a graph</span>
<span class="sd">      and there was no default graph.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The appropriate graph to use for the given inputs.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">current_default_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">current_default_graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">current_default_graph</span>

  <span class="n">op_input_list</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">op_input_list</span><span class="p">)</span>  <span class="c1"># Handle generators correctly</span>
  <span class="k">if</span> <span class="n">graph</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">Graph</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input graph needs to be a Graph: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">graph</span><span class="p">)</span>

  <span class="c1"># 1. We validate that all of the inputs are from the same graph. This is</span>
  <span class="c1">#    either the supplied graph parameter, or the first one selected from one</span>
  <span class="c1">#    the graph-element-valued inputs. In the latter case, we hold onto</span>
  <span class="c1">#    that input in original_graph_element so we can provide a more</span>
  <span class="c1">#    informative error if a mismatch is found.</span>
  <span class="n">original_graph_element</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">for</span> <span class="n">op_input</span> <span class="ow">in</span> <span class="n">op_input_list</span><span class="p">:</span>
    <span class="c1"># Determine if this is a valid graph_element.</span>
    <span class="c1"># TODO(josh11b): Note that we exclude subclasses of Tensor. Need to clean this</span>
    <span class="c1"># up.</span>
    <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">op_input</span><span class="p">,</span> <span class="p">(</span><span class="n">Operation</span><span class="p">,</span> <span class="n">_TensorLike</span><span class="p">))</span> <span class="ow">and</span>
        <span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">op_input</span><span class="p">)</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">)):</span>  <span class="c1"># pylint: disable=unidiomatic-typecheck</span>
      <span class="n">graph_element</span> <span class="o">=</span> <span class="n">op_input</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">graph_element</span> <span class="o">=</span> <span class="n">_as_graph_element</span><span class="p">(</span><span class="n">op_input</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">graph_element</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
        <span class="n">original_graph_element</span> <span class="o">=</span> <span class="n">graph_element</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_element</span><span class="o">.</span><span class="n">graph</span>
      <span class="k">elif</span> <span class="n">original_graph_element</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_assert_same_graph</span><span class="p">(</span><span class="n">original_graph_element</span><span class="p">,</span> <span class="n">graph_element</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">graph_element</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">graph</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> is not from the passed-in graph.&quot;</span> <span class="o">%</span> <span class="n">graph_element</span><span class="p">)</span>

  <span class="c1"># 2. If all else fails, we use the default graph, which is always there.</span>
  <span class="k">return</span> <span class="n">graph</span> <span class="ow">or</span> <span class="n">current_default_graph</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;GraphKeys&quot;</span><span class="p">])</span>
<span class="k">class</span> <span class="nc">GraphKeys</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Standard names to use for graph collections.</span>

<span class="sd">  The standard library uses various well-known names to collect and</span>
<span class="sd">  retrieve values associated with a graph. For example, the</span>
<span class="sd">  `tf.Optimizer` subclasses default to optimizing the variables</span>
<span class="sd">  collected under `tf.GraphKeys.TRAINABLE_VARIABLES` if none is</span>
<span class="sd">  specified, but it is also possible to pass an explicit list of</span>
<span class="sd">  variables.</span>

<span class="sd">  The following standard keys are defined:</span>

<span class="sd">  * `GLOBAL_VARIABLES`: the default collection of `Variable` objects, shared</span>
<span class="sd">    across distributed environment (model variables are subset of these). See</span>
<span class="sd">    `tf.compat.v1.global_variables`</span>
<span class="sd">    for more details.</span>
<span class="sd">    Commonly, all `TRAINABLE_VARIABLES` variables will be in `MODEL_VARIABLES`,</span>
<span class="sd">    and all `MODEL_VARIABLES` variables will be in `GLOBAL_VARIABLES`.</span>
<span class="sd">  * `LOCAL_VARIABLES`: the subset of `Variable` objects that are local to each</span>
<span class="sd">    machine. Usually used for temporarily variables, like counters.</span>
<span class="sd">    Note: use `tf.contrib.framework.local_variable` to add to this collection.</span>
<span class="sd">  * `MODEL_VARIABLES`: the subset of `Variable` objects that are used in the</span>
<span class="sd">    model for inference (feed forward). Note: use</span>
<span class="sd">    `tf.contrib.framework.model_variable` to add to this collection.</span>
<span class="sd">  * `TRAINABLE_VARIABLES`: the subset of `Variable` objects that will</span>
<span class="sd">    be trained by an optimizer. See</span>
<span class="sd">    `tf.compat.v1.trainable_variables`</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `SUMMARIES`: the summary `Tensor` objects that have been created in the</span>
<span class="sd">    graph. See</span>
<span class="sd">    `tf.compat.v1.summary.merge_all`</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `QUEUE_RUNNERS`: the `QueueRunner` objects that are used to</span>
<span class="sd">    produce input for a computation. See</span>
<span class="sd">    `tf.compat.v1.train.start_queue_runners`</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `MOVING_AVERAGE_VARIABLES`: the subset of `Variable` objects that will also</span>
<span class="sd">    keep moving averages.  See</span>
<span class="sd">    `tf.compat.v1.moving_average_variables`</span>
<span class="sd">    for more details.</span>
<span class="sd">  * `REGULARIZATION_LOSSES`: regularization losses collected during graph</span>
<span class="sd">    construction.</span>

<span class="sd">  The following standard keys are _defined_, but their collections are **not**</span>
<span class="sd">  automatically populated as many of the others are:</span>

<span class="sd">  * `WEIGHTS`</span>
<span class="sd">  * `BIASES`</span>
<span class="sd">  * `ACTIVATIONS`</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Key to collect Variable objects that are global (shared across machines).</span>
  <span class="c1"># Default collection for all variables, except local ones.</span>
  <span class="n">GLOBAL_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;variables&quot;</span>
  <span class="c1"># Key to collect local variables that are local to the machine and are not</span>
  <span class="c1"># saved/restored.</span>
  <span class="n">LOCAL_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;local_variables&quot;</span>
  <span class="c1"># Key to collect local variables which are used to accumulate interal state</span>
  <span class="c1"># to be used in tf.metrics.*.</span>
  <span class="n">METRIC_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;metric_variables&quot;</span>
  <span class="c1"># Key to collect model variables defined by layers.</span>
  <span class="n">MODEL_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;model_variables&quot;</span>
  <span class="c1"># Key to collect Variable objects that will be trained by the</span>
  <span class="c1"># optimizers.</span>
  <span class="n">TRAINABLE_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;trainable_variables&quot;</span>
  <span class="c1"># Key to collect summaries.</span>
  <span class="n">SUMMARIES</span> <span class="o">=</span> <span class="s2">&quot;summaries&quot;</span>
  <span class="c1"># Key to collect QueueRunners.</span>
  <span class="n">QUEUE_RUNNERS</span> <span class="o">=</span> <span class="s2">&quot;queue_runners&quot;</span>
  <span class="c1"># Key to collect table initializers.</span>
  <span class="n">TABLE_INITIALIZERS</span> <span class="o">=</span> <span class="s2">&quot;table_initializer&quot;</span>
  <span class="c1"># Key to collect asset filepaths. An asset represents an external resource</span>
  <span class="c1"># like a vocabulary file.</span>
  <span class="n">ASSET_FILEPATHS</span> <span class="o">=</span> <span class="s2">&quot;asset_filepaths&quot;</span>
  <span class="c1"># Key to collect Variable objects that keep moving averages.</span>
  <span class="n">MOVING_AVERAGE_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;moving_average_variables&quot;</span>
  <span class="c1"># Key to collect regularization losses at graph construction.</span>
  <span class="n">REGULARIZATION_LOSSES</span> <span class="o">=</span> <span class="s2">&quot;regularization_losses&quot;</span>
  <span class="c1"># Key to collect concatenated sharded variables.</span>
  <span class="n">CONCATENATED_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;concatenated_variables&quot;</span>
  <span class="c1"># Key to collect savers.</span>
  <span class="n">SAVERS</span> <span class="o">=</span> <span class="s2">&quot;savers&quot;</span>
  <span class="c1"># Key to collect weights</span>
  <span class="n">WEIGHTS</span> <span class="o">=</span> <span class="s2">&quot;weights&quot;</span>
  <span class="c1"># Key to collect biases</span>
  <span class="n">BIASES</span> <span class="o">=</span> <span class="s2">&quot;biases&quot;</span>
  <span class="c1"># Key to collect activations</span>
  <span class="n">ACTIVATIONS</span> <span class="o">=</span> <span class="s2">&quot;activations&quot;</span>
  <span class="c1"># Key to collect update_ops</span>
  <span class="n">UPDATE_OPS</span> <span class="o">=</span> <span class="s2">&quot;update_ops&quot;</span>
  <span class="c1"># Key to collect losses</span>
  <span class="n">LOSSES</span> <span class="o">=</span> <span class="s2">&quot;losses&quot;</span>
  <span class="c1"># Key to collect BaseSaverBuilder.SaveableObject instances for checkpointing.</span>
  <span class="n">SAVEABLE_OBJECTS</span> <span class="o">=</span> <span class="s2">&quot;saveable_objects&quot;</span>
  <span class="c1"># Key to collect all shared resources used by the graph which need to be</span>
  <span class="c1"># initialized once per cluster.</span>
  <span class="n">RESOURCES</span> <span class="o">=</span> <span class="s2">&quot;resources&quot;</span>
  <span class="c1"># Key to collect all shared resources used in this graph which need to be</span>
  <span class="c1"># initialized once per session.</span>
  <span class="n">LOCAL_RESOURCES</span> <span class="o">=</span> <span class="s2">&quot;local_resources&quot;</span>
  <span class="c1"># Trainable resource-style variables.</span>
  <span class="n">TRAINABLE_RESOURCE_VARIABLES</span> <span class="o">=</span> <span class="s2">&quot;trainable_resource_variables&quot;</span>

  <span class="c1"># Key to indicate various ops.</span>
  <span class="n">INIT_OP</span> <span class="o">=</span> <span class="s2">&quot;init_op&quot;</span>
  <span class="n">LOCAL_INIT_OP</span> <span class="o">=</span> <span class="s2">&quot;local_init_op&quot;</span>
  <span class="n">READY_OP</span> <span class="o">=</span> <span class="s2">&quot;ready_op&quot;</span>
  <span class="n">READY_FOR_LOCAL_INIT_OP</span> <span class="o">=</span> <span class="s2">&quot;ready_for_local_init_op&quot;</span>
  <span class="n">SUMMARY_OP</span> <span class="o">=</span> <span class="s2">&quot;summary_op&quot;</span>
  <span class="n">GLOBAL_STEP</span> <span class="o">=</span> <span class="s2">&quot;global_step&quot;</span>

  <span class="c1"># Used to count the number of evaluations performed during a single evaluation</span>
  <span class="c1"># run.</span>
  <span class="n">EVAL_STEP</span> <span class="o">=</span> <span class="s2">&quot;eval_step&quot;</span>
  <span class="n">TRAIN_OP</span> <span class="o">=</span> <span class="s2">&quot;train_op&quot;</span>

  <span class="c1"># Key for control flow context.</span>
  <span class="n">COND_CONTEXT</span> <span class="o">=</span> <span class="s2">&quot;cond_context&quot;</span>
  <span class="n">WHILE_CONTEXT</span> <span class="o">=</span> <span class="s2">&quot;while_context&quot;</span>

  <span class="c1"># Used to store v2 summary names.</span>
  <span class="n">_SUMMARY_COLLECTION</span> <span class="o">=</span> <span class="s2">&quot;_SUMMARY_V2&quot;</span>

  <span class="c1"># List of all collections that keep track of variables.</span>
  <span class="n">_VARIABLE_COLLECTIONS</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
      <span class="n">LOCAL_VARIABLES</span><span class="p">,</span>
      <span class="n">METRIC_VARIABLES</span><span class="p">,</span>
      <span class="n">MODEL_VARIABLES</span><span class="p">,</span>
      <span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
      <span class="n">MOVING_AVERAGE_VARIABLES</span><span class="p">,</span>
      <span class="n">CONCATENATED_VARIABLES</span><span class="p">,</span>
      <span class="n">TRAINABLE_RESOURCE_VARIABLES</span><span class="p">,</span>
  <span class="p">]</span>

  <span class="c1"># Key for streaming model ports.</span>
  <span class="c1"># NOTE(yuanbyu): internal and experimental.</span>
  <span class="n">_STREAMING_MODEL_PORTS</span> <span class="o">=</span> <span class="s2">&quot;streaming_model_ports&quot;</span>

  <span class="nd">@decorator_utils</span><span class="o">.</span><span class="n">classproperty</span>
  <span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">VARIABLES</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>  <span class="c1"># pylint: disable=no-self-argument</span>
    <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span>


<span class="k">def</span> <span class="nf">dismantle_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Cleans up reference cycles from a `Graph`.</span>

<span class="sd">  Helpful for making sure the garbage collector doesn&#39;t need to run after a</span>
<span class="sd">  temporary `Graph` is no longer needed.</span>

<span class="sd">  Args:</span>
<span class="sd">    graph: A `Graph` object to destroy. Neither it nor any of its ops are usable</span>
<span class="sd">      after this function runs.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">memory</span><span class="o">.</span><span class="n">dismantle_ordered_dict</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">_functions</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="c1"># Now clean up Operation&lt;-&gt;Graph reference cycles by clearing all of the</span>
  <span class="c1"># attributes for the Graph and its ops.</span>
  <span class="n">graph_operations</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_operations</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">graph_operations</span><span class="p">:</span>
    <span class="n">op</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">graph</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="p">{}</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;add_to_collection&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.add_to_collection()` using the default graph.</span>

<span class="sd">  See `tf.Graph.add_to_collection`</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: The key for the collection. For example, the `GraphKeys` class</span>
<span class="sd">      contains many standard names for collections.</span>
<span class="sd">    value: The value to add to the collection.  @compatibility(eager)</span>
<span class="sd">      Collections are only supported in eager when variables are created inside</span>
<span class="sd">      an EagerVariableStore (e.g. as part of a layer or template).</span>
<span class="sd">      @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;add_to_collections&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">add_to_collections</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.add_to_collections()` using the default graph.</span>

<span class="sd">  See `tf.Graph.add_to_collections`</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    names: The key for the collections. The `GraphKeys` class contains many</span>
<span class="sd">      standard names for collections.</span>
<span class="sd">    value: The value to add to the collections.  @compatibility(eager)</span>
<span class="sd">      Collections are only supported in eager when variables are created inside</span>
<span class="sd">      an EagerVariableStore (e.g. as part of a layer or template).</span>
<span class="sd">      @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;get_collection_ref&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">get_collection_ref</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.get_collection_ref()` using the default graph.</span>

<span class="sd">  See `tf.Graph.get_collection_ref`</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    key: The key for the collection. For example, the `GraphKeys` class contains</span>
<span class="sd">      many standard names for collections.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The list of values in the collection with the given `name`, or an empty</span>
<span class="sd">    list if no value has been added to that collection.  Note that this returns</span>
<span class="sd">    the collection list itself, which can be modified in place to change the</span>
<span class="sd">    collection.</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Collections are not supported when eager execution is enabled.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;get_collection&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">get_collection</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper for `Graph.get_collection()` using the default graph.</span>

<span class="sd">  See `tf.Graph.get_collection`</span>
<span class="sd">  for more details.</span>

<span class="sd">  Args:</span>
<span class="sd">    key: The key for the collection. For example, the `GraphKeys` class contains</span>
<span class="sd">      many standard names for collections.</span>
<span class="sd">    scope: (Optional.) If supplied, the resulting list is filtered to include</span>
<span class="sd">      only items whose `name` attribute matches using `re.match`. Items without</span>
<span class="sd">      a `name` attribute are never returned if a scope is supplied and the</span>
<span class="sd">      choice or `re.match` means that a `scope` without special tokens filters</span>
<span class="sd">      by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The list of values in the collection with the given `name`, or</span>
<span class="sd">    an empty list if no value has been added to that collection. The</span>
<span class="sd">    list contains the values in the order under which they were</span>
<span class="sd">    collected.</span>

<span class="sd">  @compatibility(eager)</span>
<span class="sd">  Collections are not supported when eager execution is enabled.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_all_collection_keys</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns a list of collections used in the default graph.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_all_collection_keys</span><span class="p">()</span>


<div class="viewcode-block" id="name_scope"><a class="viewcode-back" href="../../../../index.html#tensorflow.name_scope">[docs]</a><span class="k">def</span> <span class="nf">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Internal-only entry point for `name_scope*`.</span>

<span class="sd">  Internal ops do not use the public API and instead rely on</span>
<span class="sd">  `ops.name_scope` regardless of the execution mode. This function</span>
<span class="sd">  dispatches to the correct `name_scope*` implementation based on</span>
<span class="sd">  the arguments provided and the current mode. Specifically,</span>

<span class="sd">  * if `values` contains a graph tensor `Graph.name_scope` is used;</span>
<span class="sd">  * `name_scope_v1` is used in graph mode;</span>
<span class="sd">  * `name_scope_v2` -- in eager mode.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: The name argument that is passed to the op function.</span>
<span class="sd">    default_name: The default name to use if the `name` argument is `None`.</span>
<span class="sd">    values: The list of `Tensor` arguments that are passed to the op function.</span>
<span class="sd">    skip_on_eager: Indicates to return NullContextmanager if executing eagerly.</span>
<span class="sd">      By default this is True since naming tensors and operations in eager mode</span>
<span class="sd">      have little use and cause unnecessary performance overhead. However, it is</span>
<span class="sd">      important to preserve variable names since they are often useful for</span>
<span class="sd">      debugging and saved models.</span>

<span class="sd">  Returns:</span>
<span class="sd">    `name_scope*` context manager.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
  <span class="n">in_eager_mode</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">in_eager_mode</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">internal_name_scope_v1</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">skip_on_eager</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">NullContextmanager</span><span class="p">()</span>

  <span class="n">name</span> <span class="o">=</span> <span class="n">default_name</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span>
  <span class="k">if</span> <span class="n">values</span><span class="p">:</span>
    <span class="c1"># The presence of a graph tensor in `values` overrides the context.</span>
    <span class="c1"># TODO(slebedev): this is Keras-specific and should be removed.</span>
    <span class="c1"># pylint: disable=unidiomatic-typecheck</span>
    <span class="n">graph_value</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">value</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">values</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">),</span>
                       <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># pylint: enable=unidiomatic-typecheck</span>
    <span class="k">if</span> <span class="n">graph_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">graph_value</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">name_scope_v2</span><span class="p">(</span><span class="n">name</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">internal_name_scope_v1</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;Graph-only version of `name_scope_v1`.&quot;&quot;&quot;</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the context manager.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name argument that is passed to the op function.</span>
<span class="sd">      default_name: The default name to use if the `name` argument is `None`.</span>
<span class="sd">      values: The list of `Tensor` arguments that are passed to the op function.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `default_name` is passed in but not a string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">default_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;`default_name` type (</span><span class="si">%s</span><span class="s2">) is not a string type. You likely meant to &quot;</span>
          <span class="s2">&quot;pass this into the `values` kwarg.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">default_name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">default_name</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span> <span class="o">=</span> <span class="n">default_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="n">values</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Start the scope block.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The scope name.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if neither `name` nor `default_name` is provided</span>
<span class="sd">        but `values` are.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># We only raise an error if values is not None (provided) because</span>
      <span class="c1"># currently tf.name_scope(None) (values=None then) is sometimes used as</span>
      <span class="c1"># an idiom to reset to top scope.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;At least one of name (</span><span class="si">%s</span><span class="s2">) and default_name (</span><span class="si">%s</span><span class="s2">) must be provided.&quot;</span>
          <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_name</span><span class="p">))</span>

    <span class="n">g</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">g</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
      <span class="c1"># Specialize based on the knowledge that `_get_graph_from_inputs()`</span>
      <span class="c1"># ignores `inputs` when building a function.</span>
      <span class="n">g_from_inputs</span> <span class="o">=</span> <span class="n">_get_graph_from_inputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">g_from_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">g</span><span class="p">:</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">g_from_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
    <span class="k">except</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())</span>
      <span class="k">raise</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">exc_info</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">exc_info</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_g_manager</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">exc_info</span><span class="p">)</span>


<span class="c1"># Named like a function for backwards compatibility with the</span>
<span class="c1"># @tf_contextlib.contextmanager version, which was switched to a class to avoid</span>
<span class="c1"># some object creation overhead.</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;name_scope&quot;</span><span class="p">])</span>
<span class="k">class</span> <span class="nc">name_scope_v1</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;A context manager for use when defining a Python op.</span>

<span class="sd">  This context manager validates that the given `values` are from the</span>
<span class="sd">  same graph, makes that graph the default graph, and pushes a</span>
<span class="sd">  name scope in that graph (see</span>
<span class="sd">  `tf.Graph.name_scope`</span>
<span class="sd">  for more details on that).</span>

<span class="sd">  For example, to define a new Python op called `my_op`:</span>

<span class="sd">  ```python</span>
<span class="sd">  def my_op(a, b, c, name=None):</span>
<span class="sd">    with tf.name_scope(name, &quot;MyOp&quot;, [a, b, c]) as scope:</span>
<span class="sd">      a = tf.convert_to_tensor(a, name=&quot;a&quot;)</span>
<span class="sd">      b = tf.convert_to_tensor(b, name=&quot;b&quot;)</span>
<span class="sd">      c = tf.convert_to_tensor(c, name=&quot;c&quot;)</span>
<span class="sd">      # Define some computation that uses `a`, `b`, and `c`.</span>
<span class="sd">      return foo_op(..., name=scope)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the context manager.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The name argument that is passed to the op function.</span>
<span class="sd">      default_name: The default name to use if the `name` argument is `None`.</span>
<span class="sd">      values: The list of `Tensor` arguments that are passed to the op function.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `default_name` is passed in but not a string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span> <span class="o">=</span> <span class="n">name_scope</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">default_name</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">exc_info</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="o">*</span><span class="n">exc_info</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">enter_eager_name_scope</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Updates the eager context to enter the given name scope.&quot;&quot;&quot;</span>
  <span class="n">old_name</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">scope_name</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">scope_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">):</span>
      <span class="c1"># A trailing slash breaks out of nested name scopes, indicating a</span>
      <span class="c1"># fully specified scope name, for compatibility with Graph.name_scope.</span>
      <span class="n">scope_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">scope_name</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span>
      <span class="k">if</span> <span class="n">old_name</span><span class="p">:</span>
        <span class="n">scope_name</span> <span class="o">=</span> <span class="n">old_name</span> <span class="o">+</span> <span class="n">scope_name</span>
  <span class="n">ctx</span><span class="o">.</span><span class="n">scope_name</span> <span class="o">=</span> <span class="n">scope_name</span>
  <span class="k">return</span> <span class="n">scope_name</span><span class="p">,</span> <span class="n">old_name</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;name_scope&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">class</span> <span class="nc">name_scope_v2</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A context manager for use when defining a Python op.</span>

<span class="sd">  This context manager pushes a name scope, which will make the name of all</span>
<span class="sd">  operations added within it have a prefix.</span>

<span class="sd">  For example, to define a new Python op called `my_op`:</span>

<span class="sd">  ```python</span>
<span class="sd">  def my_op(a, b, c, name=None):</span>
<span class="sd">    with tf.name_scope(&quot;MyOp&quot;) as scope:</span>
<span class="sd">      a = tf.convert_to_tensor(a, name=&quot;a&quot;)</span>
<span class="sd">      b = tf.convert_to_tensor(b, name=&quot;b&quot;)</span>
<span class="sd">      c = tf.convert_to_tensor(c, name=&quot;c&quot;)</span>
<span class="sd">      # Define some computation that uses `a`, `b`, and `c`.</span>
<span class="sd">      return foo_op(..., name=scope)</span>
<span class="sd">  ```</span>

<span class="sd">  When executed, the Tensors `a`, `b`, `c`, will have names `MyOp/a`, `MyOp/b`,</span>
<span class="sd">  and `MyOp/c`.</span>

<span class="sd">  If the scope name already exists, the name will be made unique by appending</span>
<span class="sd">  `_n`. For example, calling `my_op` the second time will generate `MyOp_1/a`,</span>
<span class="sd">  etc.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the context manager.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: The prefix to use on all names created within the name scope.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If name is None, or not a string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;name for name_scope must be a string.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_exit_fns</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Start the scope block.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The scope name.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if neither `name` nor `default_name` is provided</span>
<span class="sd">        but `values` are.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="n">scope_name</span><span class="p">,</span> <span class="n">old_scope_name</span> <span class="o">=</span> <span class="n">enter_eager_name_scope</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_exit_fns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="o">*</span><span class="n">a</span><span class="p">:</span> <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="s2">&quot;scope_name&quot;</span><span class="p">,</span> <span class="n">old_scope_name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">scope</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>
      <span class="n">scope_name</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_exit_fns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scope_name</span>

  <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">):</span>
    <span class="n">exit_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exit_fns</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">exit_fn</span><span class="p">(</span><span class="n">type_arg</span><span class="p">,</span> <span class="n">value_arg</span><span class="p">,</span> <span class="n">traceback_arg</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>  <span class="c1"># False values do not suppress exceptions</span>


<span class="k">def</span> <span class="nf">strip_name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes name scope from a name.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A `string` name.</span>
<span class="sd">    export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Name with name scope removed, or the original name if export_scope</span>
<span class="sd">    is None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">export_scope</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">export_scope</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">export_scope</span> <span class="o">=</span> <span class="n">export_scope</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># Strips export_scope/, export_scope///,</span>
      <span class="c1"># ^export_scope/, loc:@export_scope/.</span>
      <span class="n">str_to_replace</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;([\^]|loc:@|^)&quot;</span> <span class="o">+</span> <span class="n">export_scope</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;[\/]+(.*)&quot;</span>
      <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">str_to_replace</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\1\2&quot;</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># If the name is not of a type we can process, simply return it.</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">name</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">prepend_name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">import_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Prepends name scope to a name.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A `string` name.</span>
<span class="sd">    import_scope: Optional `string`. Name scope to add.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Name with name scope added, or the original name if import_scope</span>
<span class="sd">    is None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">import_scope</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">import_scope</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/&quot;</span><span class="p">:</span>
      <span class="n">import_scope</span> <span class="o">=</span> <span class="n">import_scope</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">str_to_replace</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;([\^]|loc:@|^)(.*)&quot;</span>
      <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">str_to_replace</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\1&quot;</span> <span class="o">+</span> <span class="n">import_scope</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;/\2&quot;</span><span class="p">,</span>
                    <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="c1"># If the name is not of a type we can process, simply return it.</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">name</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">name</span>


<span class="c1"># pylint: disable=g-doc-return-or-yield</span>
<span class="c1"># pylint: disable=not-context-manager</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;op_scope&quot;</span><span class="p">])</span>
<span class="nd">@tf_contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">op_scope</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;DEPRECATED. Same as name_scope above, just different argument order.&quot;&quot;&quot;</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;tf.op_scope(values, name, default_name) is deprecated,&quot;</span>
               <span class="s2">&quot; use tf.name_scope(name, default_name, values)&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="n">default_name</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="k">yield</span> <span class="n">scope</span>


<span class="n">_proto_function_registry</span> <span class="o">=</span> <span class="n">registry</span><span class="o">.</span><span class="n">Registry</span><span class="p">(</span><span class="s2">&quot;proto functions&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">register_proto_function</span><span class="p">(</span><span class="n">collection_name</span><span class="p">,</span>
                            <span class="n">proto_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">to_proto</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">from_proto</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Registers `to_proto` and `from_proto` functions for collection_name.</span>

<span class="sd">  `to_proto` function converts a Python object to the corresponding protocol</span>
<span class="sd">  buffer, and returns the protocol buffer.</span>

<span class="sd">  `from_proto` function converts protocol buffer into a Python object, and</span>
<span class="sd">  returns the object..</span>

<span class="sd">  Args:</span>
<span class="sd">    collection_name: Name of the collection.</span>
<span class="sd">    proto_type: Protobuf type, such as `saver_pb2.SaverDef`,</span>
<span class="sd">      `variable_pb2.VariableDef`, `queue_runner_pb2.QueueRunnerDef`..</span>
<span class="sd">    to_proto: Function that implements Python object to protobuf conversion.</span>
<span class="sd">    from_proto: Function that implements protobuf to Python object conversion.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">to_proto</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">to_proto</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;to_proto must be callable.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">from_proto</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">from_proto</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;from_proto must be callable.&quot;</span><span class="p">)</span>

  <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">register</span><span class="p">((</span><span class="n">proto_type</span><span class="p">,</span> <span class="n">to_proto</span><span class="p">,</span> <span class="n">from_proto</span><span class="p">),</span>
                                    <span class="n">collection_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_collection_proto_type</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the proto_type for collection_name.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">get_to_proto_function</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the to_proto function for collection_name.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">get_from_proto_function</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the from_proto function for collection_name.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_proto_function_registry</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
  <span class="k">except</span> <span class="ne">LookupError</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_operation_conversion_error</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Produce a nice error if someone converts an Operation to a Tensor.&quot;&quot;&quot;</span>
  <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">((</span><span class="s2">&quot;Can&#39;t convert Operation &#39;</span><span class="si">%s</span><span class="s2">&#39; to Tensor &quot;</span>
                   <span class="s2">&quot;(target dtype=</span><span class="si">%r</span><span class="s2">, name=</span><span class="si">%r</span><span class="s2">, as_ref=</span><span class="si">%r</span><span class="s2">)&quot;</span><span class="p">)</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_op_to_colocate_with</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Operation object corresponding to v to use for colocation constraints.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Operation</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v</span>
  <span class="c1"># We always want to colocate with the reference op.</span>
  <span class="c1"># When &#39;v&#39; is a ResourceVariable, the reference op is the handle creating op.</span>
  <span class="c1">#</span>
  <span class="c1"># What this should be is:</span>
  <span class="c1"># if isinstance(v, ResourceVariable):</span>
  <span class="c1">#   return v.handle.op</span>
  <span class="c1"># However, that would require a circular import dependency.</span>
  <span class="c1"># As of October 2018, there were attempts underway to remove</span>
  <span class="c1"># colocation constraints altogether. Assuming that will</span>
  <span class="c1"># happen soon, perhaps this hack to work around the circular</span>
  <span class="c1"># import dependency is acceptable.</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;handle&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">graph</span><span class="o">.</span><span class="n">capture</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">handle</span><span class="p">)</span><span class="o">.</span><span class="n">op</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">op</span>
  <span class="k">return</span> <span class="n">internal_convert_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">op</span>


<span class="k">def</span> <span class="nf">_is_keras_symbolic_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;keras_graph&quot;</span>


<span class="n">tensor_conversion_registry</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">(</span>
    <span class="n">Operation</span><span class="p">,</span> <span class="n">_operation_conversion_error</span><span class="p">)</span>


<span class="c1"># These symbols were originally defined in this module; import them for</span>
<span class="c1"># backwards compatibility until all references have been updated to access</span>
<span class="c1"># them from the indexed_slices.py module.</span>
<span class="n">IndexedSlices</span> <span class="o">=</span> <span class="n">indexed_slices</span><span class="o">.</span><span class="n">IndexedSlices</span>
<span class="n">IndexedSlicesValue</span> <span class="o">=</span> <span class="n">indexed_slices</span><span class="o">.</span><span class="n">IndexedSlicesValue</span>
<span class="n">convert_to_tensor_or_indexed_slices</span> <span class="o">=</span> \
    <span class="n">indexed_slices</span><span class="o">.</span><span class="n">convert_to_tensor_or_indexed_slices</span>
<span class="n">convert_n_to_tensor_or_indexed_slices</span> <span class="o">=</span> \
    <span class="n">indexed_slices</span><span class="o">.</span><span class="n">convert_n_to_tensor_or_indexed_slices</span>
<span class="n">internal_convert_to_tensor_or_indexed_slices</span> <span class="o">=</span> \
    <span class="n">indexed_slices</span><span class="o">.</span><span class="n">internal_convert_to_tensor_or_indexed_slices</span>
<span class="n">internal_convert_n_to_tensor_or_indexed_slices</span> <span class="o">=</span> \
    <span class="n">indexed_slices</span><span class="o">.</span><span class="n">internal_convert_n_to_tensor_or_indexed_slices</span>
<span class="n">register_tensor_conversion_function</span> <span class="o">=</span> \
    <span class="n">tensor_conversion_registry</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span>


<span class="c1"># Helper functions for op wrapper modules generated by `python_op_gen`.</span>


<span class="k">def</span> <span class="nf">to_raw_op</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Make a given op wrapper function `f` raw.</span>

<span class="sd">  Raw op wrappers can only be called with keyword arguments.</span>

<span class="sd">  Args:</span>
<span class="sd">    f: An op wrapper function to make raw.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Raw `f`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Copy `f` to get a new `__dict__`, otherwise `tf_export` will fail</span>
  <span class="c1"># due to double-registration.</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="vm">__code__</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="vm">__globals__</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="vm">__defaults__</span><span class="p">,</span>
                         <span class="n">f</span><span class="o">.</span><span class="vm">__closure__</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">kwarg_only</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">raise_from_not_ok_status</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
  <span class="n">message</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot; name: &quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">_status_to_exception</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">message</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
  <span class="c1"># pylint: enable=protected-access</span>


<span class="k">def</span> <span class="nf">add_exit_callback_to_default_func_graph</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add a callback to run when the default function graph goes out of scope.</span>

<span class="sd">  Usage:</span>

<span class="sd">  ```python</span>
<span class="sd">  @tf.function</span>
<span class="sd">  def fn(x, v):</span>
<span class="sd">    expensive = expensive_object(v)</span>
<span class="sd">    add_exit_callback_to_default_func_graph(lambda: expensive.release())</span>
<span class="sd">    return g(x, expensive)</span>

<span class="sd">  fn(x=tf.constant(...), v=...)</span>
<span class="sd">  # `expensive` has been released.</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    fn: A callable that takes no arguments and whose output is ignored.</span>
<span class="sd">      To be executed when exiting func graph scope.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If executed when the current default graph is not a FuncGraph,</span>
<span class="sd">      or not currently executing in function creation mode (e.g., if inside</span>
<span class="sd">      an init_scope).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">default_graph</span> <span class="o">=</span> <span class="n">get_default_graph</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">_building_function</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Cannot add scope exit callbacks when not building a function.  &quot;</span>
        <span class="s2">&quot;Default graph: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">default_graph</span><span class="p">))</span>
  <span class="n">default_graph</span><span class="o">.</span><span class="n">_add_scope_exit_callback</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">_reconstruct_sequence_inputs</span><span class="p">(</span><span class="n">op_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Regroups a flat list of input tensors into scalar and sequence inputs.</span>

<span class="sd">  Args:</span>
<span class="sd">    op_def: The `op_def_pb2.OpDef` (for knowing the input types)</span>
<span class="sd">    inputs: a list of input `Tensor`s to the op.</span>
<span class="sd">    attrs: mapping from attr name to `attr_value_pb2.AttrValue` (these define</span>
<span class="sd">      how long each sequence is)</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Tensor`s (corresponding to scalar inputs) and lists of</span>
<span class="sd">    `Tensor`s (corresponding to sequence inputs).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">grouped_inputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">input_arg</span> <span class="ow">in</span> <span class="n">op_def</span><span class="o">.</span><span class="n">input_arg</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">input_arg</span><span class="o">.</span><span class="n">number_attr</span><span class="p">:</span>
      <span class="n">input_len</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">[</span><span class="n">input_arg</span><span class="o">.</span><span class="n">number_attr</span><span class="p">]</span><span class="o">.</span><span class="n">i</span>
      <span class="n">is_sequence</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">elif</span> <span class="n">input_arg</span><span class="o">.</span><span class="n">type_list_attr</span><span class="p">:</span>
      <span class="n">input_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">attrs</span><span class="p">[</span><span class="n">input_arg</span><span class="o">.</span><span class="n">type_list_attr</span><span class="p">]</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
      <span class="n">is_sequence</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_len</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="n">is_sequence</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">is_sequence</span><span class="p">:</span>
      <span class="n">grouped_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">input_len</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">grouped_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="n">input_len</span>

  <span class="k">assert</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">grouped_inputs</span>


<span class="k">class</span> <span class="nc">_TensorIterator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Iterates over the leading dim of a Tensor. Performs no error checks.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">dim0</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">tensor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_limit</span> <span class="o">=</span> <span class="n">dim0</span>

  <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_limit</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">StopIteration</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_index</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="nb">next</span> <span class="o">=</span> <span class="fm">__next__</span>  <span class="c1"># python2.x compatibility.</span>


<span class="k">def</span> <span class="nf">set_int_list_attr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">ints</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;TF internal method used to set a list(int) attribute in the node_def.&quot;&quot;&quot;</span>
  <span class="n">ints_list</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="n">ints</span><span class="p">)</span>
  <span class="n">op</span><span class="o">.</span><span class="n">_set_attr</span><span class="p">(</span><span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span><span class="nb">list</span><span class="o">=</span><span class="n">ints_list</span><span class="p">))</span>  <span class="c1"># pylint:disable=protected-access</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>