

<!DOCTYPE html>
<html class="writer-html5" lang="Chinese" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>TensorFlow 机器学习 Cookbook (version : 0.1.3) &mdash; tensorflow 0.1.3 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/GCC.png"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="TensorFlow如何工作" href="01_Introduction/index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >
          

          
            <a href="#" class="icon icon-home" alt="Documentation Home"> tensorflow
          

          
            
            <img src="_static/GCC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html">TensorFlow如何工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id1">变量和张量的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id2">使用占位符和变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id3">矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id4">操作符的声明</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id5">载入激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id6">数据资源</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id7">资源库</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id8">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id2">分层嵌套操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id3">多层操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id4">载入损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id5">载入反向传播</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id6">随机和批量训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id7">结合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id8">模型评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html">矩阵转置</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#id2">矩阵分解法</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#id3">线性回归的损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#deming">Deming回归(全回归)</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#id4">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id2">线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id3">回归线性回归</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id5">多类支持向量机</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id6">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id2">载入操作门</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id3">门运算和激活函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id4">载入一层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id5">载入多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id6">使用多层神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id7">线性模型预测改善</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id8">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id9">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#id3">本章学习模块</a></li>
</ul>
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
<p class="caption"><span class="caption-text">TF Cookbook</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html">书籍介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id2">第一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id3">第二章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id4">第三章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id5">第四章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id6">第五章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id7">第六章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id8">第七章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id9">第八章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id10">第九章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id11">第十章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id12">第十一章</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id13">索引</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
        
      <li>TensorFlow 机器学习 Cookbook (version : 0.1.3)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p><a class="reference external" href="https://tensorflow-ml.readthedocs.io/zh/latest/?badge=latest"><img alt="Documentation Status" src="https://readthedocs.org/projects/tensorflow-ml/badge/?version=latest" /></a> <a class="reference external" href="http://choosealicense.com/licenses/mit/"><img alt="MIT License" src="https://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" /></a> <a class="reference external" href="https://auth.huaweicloud.com/authui/login.html?service=https%3A%2F%2Fconsole.huaweicloud.com%2Fconsole%2F%3Flocale%3Dzh-cn#/login"><img alt="Huawei Clodu" src="https://img.shields.io/badge/platform-huawei%20cloud-blue" /></a> <a class="reference external" href="https://github.com/tensorflow/tensorflow"><img alt="TensorFlow" src="https://img.shields.io/badge/tensorflow-2.2-brightgreen.svg" /></a> <a class="reference external" href="https://www.python.org/"><img alt="Python version" src="https://img.shields.io/badge/python-3.7,%203.8-blue.svg" /></a> Sep 24, 2020</p>
<hr class="docutils" />
<p>Copyright © Wei MEI, MLMS™—all rights reserved.
🀤</p>
<div class="section" id="tensorflow-cookbook-version-version">
<h1>TensorFlow 机器学习 Cookbook (version : 0.1.3)<a class="headerlink" href="#tensorflow-cookbook-version-version" title="Permalink to this headline">¶</a></h1>
<dl class="simple">
<dt>TensorFlow (i.e., TF)::</dt><dd><p>在2015年的时候已经成为开源项目, 自从那之后它已经成为Github中starred最多的机器学习库. TensorFlow的受欢迎度主要归功于它能帮助程序员创造计算图(computational graphs), 自动微分 (automatic differentation) 和 可定制性 (customizability). 由于这些特性，TensorFlow是一个强有力的灵活性高的工具,  用于解决很多机器学习的问题.</p>
</dd>
</dl>
<p>本教程阐述很多机器学习算法, 以及如何把它们应用到实际情况中, 以及如何诠释所得到的结果.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<ul class="simple">
<li><p><a class="reference internal" href="#label1"><span class="std std-ref">第一章: 从TensorFlow开始 (Getting Started)</span></a>, 介绍主要tensorflow的对象与概念. 我们介绍张量, 变量和占位符. 我们也会展示如何在tensorflow中使用矩阵和其他的数学操作. 在本章的末尾，我们会展示如何获取数据资源.</p></li>
<li><p><a class="reference internal" href="#label2"><span class="std std-ref">第二章: TensorFlow方式 (TF Way)</span></a>, 阐述如何用多种方式将第一章中所有的算法成分关联成一个计算图并创造出一个简单的分类器. 在阐述的过程中, 我们会介绍计算图 (computational graphs), 损失函数 (loss functions), 反向传播 (back propagation), 以及训练数据.</p></li>
<li><p><a class="reference internal" href="#label3"><span class="std std-ref">第三章: 线性回归 (Linear Regression)</span></a>, 本章着重强调如何使用tensorflow来探索不同的线性回归技巧, 比如Deming, lasso, ridge, elastic net 和 logistic regression. 我们会在计算图中展示如何应用它们.</p></li>
<li><p><a class="reference internal" href="#label4"><span class="std std-ref">第四章: 支持向量机 (Support Vector Machine)</span></a>, 介绍支持向量机 (SVMs) 然后展示如何用tensorflow去运用线性SVMs, 非线性SVMs和多类SVMs.</p></li>
<li><p><a class="reference internal" href="#label5"><span class="std std-ref">第五章: 最近邻方法 (NNM)</span></a>, 展示如何运用数值度量，文本度量和比例距离函数使用最近邻技巧. 我们使用最近邻技巧来完成地址记录匹配和从MNIST数据库中对手写数字进行分类.</p></li>
<li><p><a class="reference internal" href="#label6"><span class="std std-ref">第六章: 神经网络 (Neural Networks)</span></a>, 介绍了从操作门 (operational gates) 和激活函数 (activation function) 的概念开始, 在tensorflow中如何运用神经网络. 然后我们展示一个很浅神经元然后展示如何建立不同类型的层. 在本章的末尾, 我们会教tensorflow通过神经网络的方法玩井字棋(tic-tac-toe).</p></li>
<li><p><a class="reference internal" href="#label7"><span class="std std-ref">第七章: 自然语言处理 (NLP)</span></a>, 本章展示了运用tensorflow不同文本的处理方法. 我们会展示如何在文本处理中使用Bag of Words (BoW) 模型和TF-IDF (Term Frequency-Inverse Document Frequency) 模型. 我们然后会用CBOW (Continuous Bag of Words) 和Skip-Gram模型来介绍神经元完了文本表达, 然后运用这些技巧到Word2Vec和Doc2Vec上, 用于解决实际结果预测.</p></li>
<li><p><a class="reference internal" href="#label8"><span class="std std-ref">第八章: 卷积神经网络 (CNN)</span></a>, 通过展示如何通过使用卷积神经网络 (convolutional neural networks) CNNs模型将神经网络运用到图像处理上. 我们诠释了如何为MNIST数字识别构建一个简单卷积神经网络模型, 然后在CIFAR-10任务中把它扩展到颜色识别. 我们也会展示如何把之前训练过得图像识别模型扩展到自定义任务当中. 在本章的末尾，我们会在tensorflow中解释 stylenet/neural style和deep-dream 算法.</p></li>
<li><p><a class="reference internal" href="#label9"><span class="std std-ref">第九章: 递归神经网络 (RNN)</span></a>, 会展示如何在tensorflow中运用递归神经元(recurrent neural networks). 我们会展示如何进行垃圾文本预测, 然后将递归神经网络模型扩展到基于莎士比亚文本生成. 我们也会训练段对段模型 (sequence to sequence model), 用于德语英语的翻译. 在本章的末尾, 我们也会展示Siamese递归神经网络用于地址记录匹配的用法.</p></li>
<li><p><a class="reference internal" href="#label10"><span class="std std-ref">第十章: TensorFlow的应用技巧</span></a>, 本章将会给出将TensorFlow应用到开发环境中, 如何利用多过程设备(比如GPUs), 然后将TensorFlow分布在多个机器上.</p></li>
<li><p><a class="reference internal" href="#label11"><span class="std std-ref">第十一章: TensorFlow的更多功能</span></a>, 通过阐述如何运行k-means, genetic算法来展示TensorFlow的多面性, 解决系统的常微分方程. 我们也展示Tensorboard的多处使用, 以及如何显示计算图度量.</p></li>
</ul>
</div>
<hr class="docutils" />
<p>让我们从一个TensorFlow的视频开始吧！</p>
<video poster="_static/images/GCC.png" width="690" height="402" controls="controls">
    <source src="_static/videos/Intro2ML/TFIntro1.mp4" type="video/mp4">
</video></div>
<div class="section" id="id1">
<h1>目录<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound" id="label1">
<p class="caption"><span class="caption-text">从TensorFlow开始 (Getting Started)</span><a class="headerlink" href="#label1" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html">TensorFlow如何工作</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html">引言</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id2">TensorFlow是如何运行的</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id3">通用TensorFlow算法概览</a><ul>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id4">导入或产生数据</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id5">转换和规范化数据</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id6">设置算法参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id7">变量和占位符的初始化</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id8">定义模型结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id9">声明损失函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id10">模型的初始化和训练</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id11">模型的评估(可选)</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id12">预测新结果(可选)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id13">总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/01_How_TensorFlow_Works/index.html#id14">你知道吗？</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id1">变量和张量的声明</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html">计算图</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id2">创建张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id3">创建0填充张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id4">创建1填充张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id5">创建常数填充张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id6">由给定的数创建一个张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id7">创建相似类型的张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id8">创建序列张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id9">创建随机张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id10">创建一个符合正态分布的张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id11">创建有界限的正态分布张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id12">张量乱序化</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id13">张量裁剪</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#tensorflow">TensorFlow机器学习的应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/02_Creating_and_Using_Tensors/index.html#id14">本节学习模块</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id2">使用占位符和变量</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html">创建变量和占位符</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html#id2">创建特定的变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html#id3">基于其他张量的形状创建张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html#id4">常数填充变量张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html#range">基于序列和range来创建变量张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html#id5">随机数变量张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html#tensorboard">在TensorBoard中进行变量创建的可视化</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/03_Using_Variables_and_Placeholders/index.html#tensorflow-pythonjavascript">TensorFlow, Python和Javascript</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id3">矩阵</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html">创建一个矩阵</a><ul>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id2">对角矩阵</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id3">随机矩阵</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id4">常数矩阵</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id5">随机矩阵</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#convert-to-tensor"><code class="code docutils literal notranslate"><span class="pre">convert_to_tensor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id6">非传统意义上的矩阵</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id7">矩阵加减法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id8">加法</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id9">减法</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id10">乘法</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id11">矩阵的转置</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#inverse">矩阵的逆(inverse)</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id12">矩阵的本征值与向量</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#pythoncolab">Python和Colab的初级知识</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/04_Working_with_Matrices/index.html#id13">本章学习模块</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id4">操作符的声明</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html"><code class="code docutils literal notranslate"><span class="pre">div()</span></code> 函数及其相关的函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html#mod"><code class="code docutils literal notranslate"><span class="pre">mod()</span></code> 函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html#cross"><code class="code docutils literal notranslate"><span class="pre">cross()</span></code> 函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html#id1">常用的数学函数列表</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html#id2">特殊数学函数列表</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html#id3">自定义函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html#id4">谷歌工程师的机器学习访谈</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/05_Declaring_Operations/index.html#id5">本节学习模块</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id5">载入激活函数</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html">线性整流函数(Rectifed Linear Unit)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#relun">ReLUn函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#s-sigmoid">S型函数(Sigmoid)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#tanh"><span class="math notranslate nohighlight">\(\tanh\)</span> 函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#softsign">softsign 函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#softplus">softplus 函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#exponential-linear-unit-elu">Exponential Linear Unit(ELU)函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#id1">总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#id2">机器学习是什么鬼?</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/06_Implementing_Activation_Functions/index.html#id3">本节学习模块</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id6">数据资源</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/index.html">Iris Dataset(鸢尾属植物数据集)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/index.html#low-birthrate-dataset-hosted-on-github">Low Birthrate Dataset (Hosted on Github)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/index.html#university-of-california-at-irvine">波士顿房价数据库(University of California at Irvine)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/index.html#mnist-handwriting-dataset-yann-lecun">MNIST Handwriting Dataset (手写数据库, Yann LeCun)</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/appendix.html">MNIST 手写数据代码补充</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/appendix.html#ham-spam-text-dataset-uci">Ham/Spam Text Dataset(垃圾邮件分类, UCI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/appendix.html#stanford">电影评论数据库 (Stanford)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/appendix.html#id3">莎士比亚全集 (古登堡计划)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/appendix.html#manythings-tatoeba">英语-德语 文本翻译数据库 (Manythings/Tatoeba)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/07_Working_with_Data_Sources/appendix.html#cifar-10">CIFAR-10 数据库</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id7">资源库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/08_Additional_Resources/index.html">官方资源</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/08_Additional_Resources/index.html#github">Github学习指导和例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/08_Additional_Resources/index.html#id2">深度学习资源</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/08_Additional_Resources/index.html#id3">额外资源</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/08_Additional_Resources/index.html#arxiv">Arxiv 文章(免费)</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/08_Additional_Resources/index.html#colab">Colab如何将训练麻瓜?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01_Introduction/index.html#id8">本章学习模块</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#module-tensorflow.zeros"><em>tensorflow.zeros</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#module-tensorflow.ones"><em>tensorflow.ones</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#module-tensorflow.fill"><em>tensorflow.fill</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#tensorflow-constant"><em>tensorflow.constant</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#tensorflow-zeros-like"><em>tensorflow.zeros_like</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#module-tensorflow.ones_like"><em>tensorflow.ones_like</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#module-tensorflow.linspace"><em>tensorflow.linspace</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#module-tensorflow.range"><em>tensorflow.range</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#tensorflow-random-uniform-initializer"><em>tensorflow.random_uniform_initializer</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="01_Introduction/index.html#module-tensorflow.random_normal_initializer"><em>tensorflow.random_normal_initializer</em></a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label2">
<p class="caption"><span class="caption-text">TensorFlow方式 (TensorFlow Way)</span><a class="headerlink" href="#label2" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html">计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="02_TensorFlow_Way/01_Operations_as_a_Computational_Graph/index.html">计算图中操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_TensorFlow_Way/01_Operations_as_a_Computational_Graph/index.html#dense-layer">密集层(Dense Layer)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id2">分层嵌套操作</a><ul>
<li class="toctree-l2"><a class="reference internal" href="02_TensorFlow_Way/02_Layering_Nested_Operations/index.html">准备</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id3">多层操作</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id4">载入损失函数</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id5">载入反向传播</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id6">随机和批量训练</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id7">结合训练</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id8">模型评估</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02_TensorFlow_Way/index.html#id9">本章学习模块</a><ul>
<li class="toctree-l2"><a class="reference internal" href="02_TensorFlow_Way/index.html#module-tensorflow.zeros"><em>tensorflow.zeros</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="02_TensorFlow_Way/index.html#module-tensorflow.ones"><em>tensorflow.ones</em></a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label3">
<p class="caption"><span class="caption-text">线性回归 (Linear Regression)</span><a class="headerlink" href="#label3" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html">矩阵转置</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#id2">矩阵分解法</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#tensorflow">TensorFLow的线性回归</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#id3">线性回归的损失函数</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#deming">Deming回归(全回归)</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#lasso-ridge">套索(Lasso)回归和岭(Ridge)回归</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#elastic-net">弹性网(Elastic Net)回归</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#logistic">逻辑(Logistic)回归</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Linear_Regression/index.html#id4">本章学习模块</a><ul>
<li class="toctree-l2"><a class="reference internal" href="03_Linear_Regression/index.html#module-tensorflow.zeros"><em>tensorflow.zeros</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="03_Linear_Regression/index.html#module-tensorflow.ones"><em>tensorflow.ones</em></a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label4">
<p class="caption"><span class="caption-text">支持向量机(Support Vector Machines)</span><a class="headerlink" href="#label4" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html">引言</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id2">线性支持向量机</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id3">回归线性回归</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#tensorflow">TensorFlow中的核</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id4">非线性支持向量机</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id5">多类支持向量机</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="04_Support_Vector_Machines/index.html#id6">本章学习模块</a><ul>
<li class="toctree-l2"><a class="reference internal" href="04_Support_Vector_Machines/index.html#module-tensorflow.zeros"><em>tensorflow.zeros</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="04_Support_Vector_Machines/index.html#module-tensorflow.ones"><em>tensorflow.ones</em></a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label5">
<p class="caption"><span class="caption-text">最近邻法 (Nearest Neighbor Methods)</span><a class="headerlink" href="#label5" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html">引言</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id2">最近邻法的使用</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id3">文本距离函数</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id4">计算混合距离函数</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id5">地址匹配</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id6">图像处理的近邻法</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#id7">本章学习模块</a><ul>
<li class="toctree-l2"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#module-tensorflow.zeros"><em>tensorflow.zeros</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="05_Nearest_Neighbor_Methods/index.html#module-tensorflow.ones"><em>tensorflow.ones</em></a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label6">
<p class="caption"><span class="caption-text">神经元网络 (Neural Networks)</span><a class="headerlink" href="#label6" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html">引言</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id2">载入操作门</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id3">门运算和激活函数</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id4">载入一层神经网络</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id5">载入多层神经网络</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id6">使用多层神经网络</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id7">线性模型预测改善</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id8">神经网络学习井字棋</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="06_Neural_Networks/index.html#id9">本章学习模块</a><ul>
<li class="toctree-l2"><a class="reference internal" href="06_Neural_Networks/index.html#module-tensorflow.zeros"><em>tensorflow.zeros</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="06_Neural_Networks/index.html#module-tensorflow.ones"><em>tensorflow.ones</em></a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label7">
<p class="caption"><span class="caption-text">自然语言处理(NLP)</span><a class="headerlink" href="#label7" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html">引言</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#bag-of-words">词袋 (Bag of Words)</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#tf-idf">词频-逆文本频率 (TF-IDF)</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#skip-gram">运用Skip-Gram</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#cbow-continuous-bag-fo-words">CBOW (Continuous Bag fo Words)</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#word2vec">Word2Vec应用实例</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#doc2vec-sentiment-analysis">Doc2Vec情感分析 (Sentiment Analysis)</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#id2">神经网络学习井字棋</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="07_Natural_Language_Processing/index.html#id3">本章学习模块</a><ul>
<li class="toctree-l2"><a class="reference internal" href="07_Natural_Language_Processing/index.html#module-tensorflow.zeros"><em>tensorflow.zeros</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="07_Natural_Language_Processing/index.html#module-tensorflow.ones"><em>tensorflow.ones</em></a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label8">
<p class="caption"><span class="caption-text">卷积神经网络(CNN)</span><a class="headerlink" href="#label8" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#simple-cnns">简单卷积神经网络 (Simple CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#advanced-cnns">高级卷积神经网络 (Advanced CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#id2">重新训练一个存在架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#stylenet-neural-style">使用Stylenet/Neural-Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Convolutional_Neural_Networks/index.html#deep-dream">运用Deep Dream</a></li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label9">
<p class="caption"><span class="caption-text">递归神经网络(RNN)</span><a class="headerlink" href="#label9" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html">引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#id2">卷积神经网络模型用于垃圾信息检测</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#lstm">LSTM模型用于文本生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#id3">堆叠多层LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#seq2seq">创建段对段模型翻译 (Seq2Seq)</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Recurrent_Neural_Networks/index.html#siamese">训练Siamese相似度测量</a></li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label10">
<p class="caption"><span class="caption-text">TensorFlow的应用技巧</span><a class="headerlink" href="#label10" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html">单元测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#id2">使用多个执行器 (设备)</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#tensorflow">TensorFlow平行化</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#id3">TensorFlow开发贴士</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_Taking_TensorFlow_to_Production/index.html#id4">TensorFlow开发实例</a></li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label11">
<p class="caption"><span class="caption-text">TensorFlow的更多功能</span><a class="headerlink" href="#label11" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html">计算图可视化(用Tensorboard)</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#id1">遗传算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#k-means">K-means聚类分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#id2">解决体系常微分方程</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#id3">随机森林</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_More_with_TensorFlow/index.html#tensorflowkeras">TensorFlow中的Keras</a></li>
</ul>
</div>
<div class="toctree-wrapper compound" id="label12">
<p class="caption"><span class="caption-text">TF Cookbook</span><a class="headerlink" href="#label12" title="Permalink to this toctree">¶</a></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html">书籍介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="images/bookcoverindex.html">封面</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id2">第一章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id3">第二章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id4">第三章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id5">第四章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id6">第五章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id7">第六章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id8">第七章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id9">第八章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id10">第九章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id11">第十章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id12">第十一章</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bookindex.html#id13">索引</a><ul class="simple">
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="license">
<h1>许可证(License)<a class="headerlink" href="#license" title="Permalink to this headline">¶</a></h1>
<p>MIT许可证请参见 <a class="reference external" href="https://github.com/nickcafferry/tensorflow/blob/master/LICENSE">MIT LICENSE</a></p>
</div>
<div class="section" id="tensorflow">
<h1>TensorFlow模块介绍<a class="headerlink" href="#tensorflow" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-tensorflow"></span><p>Top-level module of TensorFlow. By convention, we refer to this module as
<cite>tf</cite> instead of <cite>tensorflow</cite>, following the common practice of importing
TensorFlow via the command <cite>import tensorflow as tf</cite>.</p>
<p>The primary function of this module is to import all of the public TensorFlow
interfaces into a single place. The interfaces themselves are located in
sub-modules, as described below.</p>
<p>Note that the file <cite>__init__.py</cite> in the TensorFlow source code tree is actually
only a placeholder to enable test cases to run. The TensorFlow build replaces
this file with a file generated from [<cite>api_template.__init__.py</cite>](<a class="reference external" href="https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py">https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py</a>)</p>
<dl class="py class">
<dt id="tensorflow.AggregationMethod">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">AggregationMethod</code><a class="reference internal" href="_modules/tensorflow/python/ops/gradients_util.html#AggregationMethod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.AggregationMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class listing aggregation methods used to combine gradients.</p>
<p>Computing partial derivatives can require aggregating gradient
contributions. This class lists the various methods that can
be used to combine gradients in the graph.</p>
<p>The following aggregation methods are part of the stable API for
aggregating gradients:</p>
<ul class="simple">
<li><p><cite>ADD_N</cite>: All of the gradient terms are summed as part of one
operation using the &quot;AddN&quot; op (see <cite>tf.add_n</cite>). This
method has the property that all gradients must be ready and
buffered separately in memory before any aggregation is performed.</p></li>
<li><p><cite>DEFAULT</cite>: The system-chosen default aggregation method.</p></li>
</ul>
<p>The following aggregation methods are experimental and may not
be supported in future releases:</p>
<ul class="simple">
<li><p><cite>EXPERIMENTAL_TREE</cite>: Gradient terms are summed in pairs using
using the &quot;AddN&quot; op. This method of summing gradients may reduce
performance, but it can improve memory utilization because the
gradients can be released earlier.</p></li>
</ul>
<dl class="py attribute">
<dt id="tensorflow.AggregationMethod.ADD_N">
<code class="sig-name descname">ADD_N</code><em class="property"> = 0</em><a class="headerlink" href="#tensorflow.AggregationMethod.ADD_N" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="tensorflow.AggregationMethod.DEFAULT">
<code class="sig-name descname">DEFAULT</code><em class="property"> = 0</em><a class="headerlink" href="#tensorflow.AggregationMethod.DEFAULT" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="tensorflow.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N">
<code class="sig-name descname">EXPERIMENTAL_ACCUMULATE_N</code><em class="property"> = 2</em><a class="headerlink" href="#tensorflow.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="tensorflow.AggregationMethod.EXPERIMENTAL_TREE">
<code class="sig-name descname">EXPERIMENTAL_TREE</code><em class="property"> = 1</em><a class="headerlink" href="#tensorflow.AggregationMethod.EXPERIMENTAL_TREE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="tensorflow.Assert">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">Assert</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">condition</span></em>, <em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">summarize</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/control_flow_ops.html#Assert"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Assert" title="Permalink to this definition">¶</a></dt>
<dd><p>Asserts that the given condition is true.</p>
<p>If <cite>condition</cite> evaluates to false, print the list of tensors in <cite>data</cite>.
<cite>summarize</cite> determines how many entries of the tensors to print.</p>
<p>NOTE: In graph mode, to ensure that Assert executes, one usually attaches
a dependency:</p>
<p><a href="#id2"><span class="problematic" id="id3">``</span></a><a href="#id4"><span class="problematic" id="id5">`</span></a>python
# Ensure maximum element of x is smaller or equal to 1
assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])
with tf.control_dependencies([assert_op]):</p>
<blockquote>
<div><p>... code using x ...</p>
</div></blockquote>
<p><a href="#id6"><span class="problematic" id="id7">``</span></a><a href="#id8"><span class="problematic" id="id9">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>condition</strong> -- The condition to evaluate.</p></li>
<li><p><strong>data</strong> -- The tensors to print out when condition is false.</p></li>
<li><p><strong>summarize</strong> -- Print this many entries of each tensor.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An <cite>Operation</cite> that, when executed, raises a
<cite>tf.errors.InvalidArgumentError</cite> if <cite>condition</cite> is not true.
&#64;compatibility(eager)
returns None
&#64;end_compatibility</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>assert_op</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>&#64;compatibility</strong><strong>(</strong><strong>eager</strong><strong>)</strong> -- </p></li>
<li><p><strong>tf.errors.InvalidArgumentError</strong> -- </p></li>
<li><p><strong>&#64;end_compatibility</strong> -- </p></li>
</ul>
</dd>
</dl>
<p><strong>NOTE</strong> The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.</p>
</dd></dl>

<dl class="py class">
<dt id="tensorflow.CriticalSection">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">CriticalSection</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">shared_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">critical_section_def</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">import_scope</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/critical_section_ops.html#CriticalSection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.CriticalSection" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Critical section.</p>
<p>A <cite>CriticalSection</cite> object is a resource in the graph which executes subgraphs
in <strong>serial</strong> order.  A common example of a subgraph one may wish to run
exclusively is the one given by the following function:</p>
<p><a href="#id10"><span class="problematic" id="id11">``</span></a><a href="#id12"><span class="problematic" id="id13">`</span></a>python
v = resource_variable_ops.ResourceVariable(0.0, name=&quot;v&quot;)</p>
<dl>
<dt>def count():</dt><dd><p>value = v.read_value()
with tf.control_dependencies([value]):</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.control_dependencies([v.assign_add(1)]):</dt><dd><p>return tf.identity(value)</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
<p><a href="#id14"><span class="problematic" id="id15">``</span></a><a href="#id16"><span class="problematic" id="id17">`</span></a></p>
<p>Here, a snapshot of <cite>v</cite> is captured in <cite>value</cite>; and then <cite>v</cite> is updated.
The snapshot value is returned.</p>
<p>If multiple workers or threads all execute <cite>count</cite> in parallel, there is no
guarantee that access to the variable <cite>v</cite> is atomic at any point within
any thread's calculation of <cite>count</cite>.  In fact, even implementing an atomic
counter that guarantees that the user will see each value <cite>0, 1, ...,</cite> is
currently impossible.</p>
<p>The solution is to ensure any access to the underlying resource <cite>v</cite> is
only processed through a critical section:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">cs</span> <span class="pre">=</span> <span class="pre">CriticalSection()</span>
<span class="pre">f1</span> <span class="pre">=</span> <span class="pre">cs.execute(count)</span>
<span class="pre">f2</span> <span class="pre">=</span> <span class="pre">cs.execute(count)</span>
<span class="pre">output</span> <span class="pre">=</span> <span class="pre">f1</span> <span class="pre">+</span> <span class="pre">f2</span>
<span class="pre">session.run(output)</span>
<span class="pre">`</span></code>
The functions <cite>f1</cite> and <cite>f2</cite> will be executed serially, and updates to <cite>v</cite>
will be atomic.</p>
<p><strong>NOTES</strong></p>
<p>All resource objects, including the critical section and any captured
variables of functions executed on that critical section, will be
colocated to the same device (host and cpu/gpu).</p>
<p>When using multiple critical sections on the same resources, there is no
guarantee of exclusive access to those resources.  This behavior is disallowed
by default (but see the kwarg <cite>exclusive_resource_access</cite>).</p>
<p>For example, running the same function in two separate critical sections
will not ensure serial execution:</p>
<p><a href="#id18"><span class="problematic" id="id19">``</span></a><a href="#id20"><span class="problematic" id="id21">`</span></a>python
v = tf.compat.v1.get_variable(&quot;v&quot;, initializer=0.0, use_resource=True)
def accumulate(up):</p>
<blockquote>
<div><p>x = v.read_value()
with tf.control_dependencies([x]):</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.control_dependencies([v.assign_add(up)]):</dt><dd><p>return tf.identity(x)</p>
</dd>
</dl>
</div></blockquote>
</div></blockquote>
<dl class="simple">
<dt>ex1 = CriticalSection().execute(</dt><dd><p>accumulate, 1.0, exclusive_resource_access=False)</p>
</dd>
<dt>ex2 = CriticalSection().execute(</dt><dd><p>accumulate, 1.0, exclusive_resource_access=False)</p>
</dd>
</dl>
<p>bad_sum = ex1 + ex2
sess.run(v.initializer)
sess.run(bad_sum)  # May return 0.0
<a href="#id22"><span class="problematic" id="id23">``</span></a><a href="#id24"><span class="problematic" id="id25">`</span></a></p>
<p>Creates a critical section.</p>
<dl class="py method">
<dt id="tensorflow.CriticalSection.execute">
<code class="sig-name descname">execute</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em>, <em class="sig-param"><span class="n">exclusive_resource_access</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/critical_section_ops.html#CriticalSection.execute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.CriticalSection.execute" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute function <cite>fn()</cite> inside the critical section.</p>
<p><cite>fn</cite> should not accept any arguments.  To add extra arguments to when
calling <cite>fn</cite> in the critical section, create a lambda:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">critical_section.execute(lambda:</span> <span class="pre">fn(*my_args,</span> <span class="pre">**my_kwargs))</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- The function to execute.  Must return at least one tensor.</p></li>
<li><p><strong>exclusive_resource_access</strong> -- Whether the resources required by
<cite>fn</cite> should be exclusive to this <cite>CriticalSection</cite>.  Default: <cite>True</cite>.
You may want to set this to <cite>False</cite> if you will be accessing a
resource in read-only mode in two different CriticalSections.</p></li>
<li><p><strong>name</strong> -- The name to use when creating the execute operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tensors returned from <cite>fn()</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> -- If <cite>fn</cite> attempts to lock this <cite>CriticalSection</cite> in any nested
    or lazy way that may cause a deadlock.</p></li>
<li><p><strong>ValueError</strong> -- If <cite>exclusive_resource_access == True</cite> and
    another <cite>CriticalSection</cite> has an execution requesting the same
    resources as <cite>fn`</cite>.  Note, even if <cite>exclusive_resource_access</cite> is
    <cite>True</cite>, if another execution in another <cite>CriticalSection</cite> was created
    without <cite>exclusive_resource_access=True</cite>, a <cite>ValueError</cite> will be raised.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.CriticalSection.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#tensorflow.CriticalSection.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.DType">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">DType</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span><span class="p">:</span> <span class="n">tensorflow.python._dtypes.DType</span></em>, <em class="sig-param"><span class="n">arg0</span><span class="p">:</span> <span class="n">object</span></em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/tensorflow/python/framework/dtypes.html#DType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.DType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python._dtypes.DType</span></code></p>
<p>Represents the type of the elements in a <cite>Tensor</cite>.</p>
<p>The following <cite>DType</cite> objects are defined:</p>
<ul class="simple">
<li><p><cite>tf.float16</cite>: 16-bit half-precision floating-point.</p></li>
<li><p><cite>tf.float32</cite>: 32-bit single-precision floating-point.</p></li>
<li><p><cite>tf.float64</cite>: 64-bit double-precision floating-point.</p></li>
<li><p><cite>tf.bfloat16</cite>: 16-bit truncated floating-point.</p></li>
<li><p><cite>tf.complex64</cite>: 64-bit single-precision complex.</p></li>
<li><p><cite>tf.complex128</cite>: 128-bit double-precision complex.</p></li>
<li><p><cite>tf.int8</cite>: 8-bit signed integer.</p></li>
<li><p><cite>tf.uint8</cite>: 8-bit unsigned integer.</p></li>
<li><p><cite>tf.uint16</cite>: 16-bit unsigned integer.</p></li>
<li><p><cite>tf.uint32</cite>: 32-bit unsigned integer.</p></li>
<li><p><cite>tf.uint64</cite>: 64-bit unsigned integer.</p></li>
<li><p><cite>tf.int16</cite>: 16-bit signed integer.</p></li>
<li><p><cite>tf.int32</cite>: 32-bit signed integer.</p></li>
<li><p><cite>tf.int64</cite>: 64-bit signed integer.</p></li>
<li><p><cite>tf.bool</cite>: Boolean.</p></li>
<li><p><cite>tf.string</cite>: String.</p></li>
<li><p><cite>tf.qint8</cite>: Quantized 8-bit signed integer.</p></li>
<li><p><cite>tf.quint8</cite>: Quantized 8-bit unsigned integer.</p></li>
<li><p><cite>tf.qint16</cite>: Quantized 16-bit signed integer.</p></li>
<li><p><cite>tf.quint16</cite>: Quantized 16-bit unsigned integer.</p></li>
<li><p><cite>tf.qint32</cite>: Quantized 32-bit signed integer.</p></li>
<li><p><cite>tf.resource</cite>: Handle to a mutable resource.</p></li>
<li><p><cite>tf.variant</cite>: Values of arbitrary types.</p></li>
</ul>
<p>The <cite>tf.as_dtype()</cite> function converts numpy types and string type
names to a <cite>DType</cite> object.</p>
<dl class="py method">
<dt id="tensorflow.DType.as_numpy_dtype">
<em class="property">property </em><code class="sig-name descname">as_numpy_dtype</code><a class="headerlink" href="#tensorflow.DType.as_numpy_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Python <cite>type</cite> object based on this <cite>DType</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.DType.base_dtype">
<em class="property">property </em><code class="sig-name descname">base_dtype</code><a class="headerlink" href="#tensorflow.DType.base_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a non-reference <cite>DType</cite> based on this <cite>DType</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.DType.is_compatible_with">
<code class="sig-name descname">is_compatible_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/dtypes.html#DType.is_compatible_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.DType.is_compatible_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the <cite>other</cite> DType will be converted to this DType.</p>
<p>The conversion rules are as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">DType(T)</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">.is_compatible_with(DType(T))</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">==</span> <span class="pre">True</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- A <cite>DType</cite> (or object that may be converted to a <cite>DType</cite>).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if a Tensor of the <cite>other</cite> <cite>DType</cite> will be implicitly converted to
this <cite>DType</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.DType.limits">
<em class="property">property </em><code class="sig-name descname">limits</code><a class="headerlink" href="#tensorflow.DType.limits" title="Permalink to this definition">¶</a></dt>
<dd><p>Return intensity limits, i.e.</p>
<p>(min, max) tuple, of the dtype.
:param clip_negative: bool, optional If True, clip the negative range (i.e.</p>
<blockquote>
<div><p>return 0 for min intensity) even if the image dtype allows negative
values. Returns</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>min</strong> -- tuple Lower and upper intensity limits.</p></li>
<li><p><strong>max</strong> -- tuple Lower and upper intensity limits.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.DType.max">
<em class="property">property </em><code class="sig-name descname">max</code><a class="headerlink" href="#tensorflow.DType.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the maximum representable value in this data type.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if this is a non-numeric, unordered, or quantized type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.DType.min">
<em class="property">property </em><code class="sig-name descname">min</code><a class="headerlink" href="#tensorflow.DType.min" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the minimum representable value in this data type.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if this is a non-numeric, unordered, or quantized type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.DType.real_dtype">
<em class="property">property </em><code class="sig-name descname">real_dtype</code><a class="headerlink" href="#tensorflow.DType.real_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <cite>DType</cite> corresponding to this <cite>DType</cite>'s real part.</p>
</dd></dl>

</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.DeviceSpec">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">DeviceSpec</code><a class="headerlink" href="#tensorflow.DeviceSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.device_spec.DeviceSpecV2</span></code></p>
</dd></dl>

<dl class="py class">
<dt id="tensorflow.GradientTape">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">GradientTape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">persistent</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">watch_accessed_variables</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Record operations for automatic differentiation.</p>
<p>Operations are recorded if they are executed within this context manager and
at least one of their inputs is being &quot;watched&quot;.</p>
<p>Trainable variables (created by <cite>tf.Variable</cite> or <cite>tf.compat.v1.get_variable</cite>,
where <cite>trainable=True</cite> is default in both cases) are automatically watched.
Tensors can be manually watched by invoking the <cite>watch</cite> method on this context
manager.</p>
<p>For example, consider the function <cite>y = x * x</cite>. The gradient at <cite>x = 3.0</cite> can
be computed as:</p>
<p><a href="#id26"><span class="problematic" id="id27">``</span></a><a href="#id28"><span class="problematic" id="id29">`</span></a>python
x = tf.constant(3.0)
with tf.GradientTape() as g:</p>
<blockquote>
<div><p>g.watch(x)
y = x * x</p>
</div></blockquote>
<p>dy_dx = g.gradient(y, x) # Will compute to 6.0
<a href="#id30"><span class="problematic" id="id31">``</span></a><a href="#id32"><span class="problematic" id="id33">`</span></a></p>
<p>GradientTapes can be nested to compute higher-order derivatives. For example,</p>
<p><a href="#id34"><span class="problematic" id="id35">``</span></a><a href="#id36"><span class="problematic" id="id37">`</span></a>python
x = tf.constant(3.0)
with tf.GradientTape() as g:</p>
<blockquote>
<div><p>g.watch(x)
with tf.GradientTape() as gg:</p>
<blockquote>
<div><p>gg.watch(x)
y = x * x</p>
</div></blockquote>
<p>dy_dx = gg.gradient(y, x)     # Will compute to 6.0</p>
</div></blockquote>
<p>d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0
<a href="#id38"><span class="problematic" id="id39">``</span></a><a href="#id40"><span class="problematic" id="id41">`</span></a></p>
<p>By default, the resources held by a GradientTape are released as soon as
GradientTape.gradient() method is called. To compute multiple gradients over
the same computation, create a persistent gradient tape. This allows multiple
calls to the gradient() method as resources are released when the tape object
is garbage collected. For example:</p>
<p><a href="#id42"><span class="problematic" id="id43">``</span></a><a href="#id44"><span class="problematic" id="id45">`</span></a>python
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as g:</p>
<blockquote>
<div><p>g.watch(x)
y = x * x
z = y * y</p>
</div></blockquote>
<p>dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = g.gradient(y, x)  # 6.0
del g  # Drop the reference to the tape
<a href="#id46"><span class="problematic" id="id47">``</span></a><a href="#id48"><span class="problematic" id="id49">`</span></a></p>
<p>By default GradientTape will automatically watch any trainable variables that
are accessed inside the context. If you want fine grained control over which
variables are watched you can disable automatic tracking by passing
<cite>watch_accessed_variables=False</cite> to the tape constructor:</p>
<p><a href="#id50"><span class="problematic" id="id51">``</span></a><a href="#id52"><span class="problematic" id="id53">`</span></a>python
with tf.GradientTape(watch_accessed_variables=False) as tape:</p>
<blockquote>
<div><p>tape.watch(variable_a)
y = variable_a ** 2  # Gradients will be available for <cite>variable_a</cite>.
z = variable_b ** 3  # No gradients will be available since <cite>variable_b</cite> is</p>
<blockquote>
<div><p># not being watched.</p>
</div></blockquote>
</div></blockquote>
<p><a href="#id54"><span class="problematic" id="id55">``</span></a><a href="#id56"><span class="problematic" id="id57">`</span></a></p>
<p>Note that when using models you should ensure that your variables exist when
using <cite>watch_accessed_variables=False</cite>. Otherwise it's quite easy to make your
first iteration not have any gradients:</p>
<p><a href="#id58"><span class="problematic" id="id59">``</span></a><a href="#id60"><span class="problematic" id="id61">`</span></a>python
a = tf.keras.layers.Dense(32)
b = tf.keras.layers.Dense(32)</p>
<dl>
<dt>with tf.GradientTape(watch_accessed_variables=False) as tape:</dt><dd><dl class="simple">
<dt>tape.watch(a.variables)  # Since <cite>a.build</cite> has not been called at this point</dt><dd><p># <cite>a.variables</cite> will return an empty list and the
# tape will not be watching anything.</p>
</dd>
</dl>
<p>result = b(a(inputs))
tape.gradient(result, a.variables)  # The result of this computation will be</p>
<blockquote>
<div><p># a list of <a href="#id62"><span class="problematic" id="id63">`</span></a>None`s since a's variables
# are not being watched.</p>
</div></blockquote>
</dd>
</dl>
<p><a href="#id64"><span class="problematic" id="id65">``</span></a><a href="#id66"><span class="problematic" id="id67">`</span></a></p>
<p>Note that only tensors with real or complex dtypes are differentiable.</p>
<p>Creates a new GradientTape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>persistent</strong> -- Boolean controlling whether a persistent gradient tape
is created. False by default, which means at most one call can
be made to the gradient() method on this object.</p></li>
<li><p><strong>watch_accessed_variables</strong> -- Boolean controlling whether the tape will
automatically <cite>watch</cite> any (trainable) variables accessed while the tape
is active. Defaults to True meaning gradients can be requested from any
result computed in the tape derived from reading a trainable <cite>Variable</cite>.
If False users must explicitly <cite>watch</cite> any <a href="#id68"><span class="problematic" id="id69">`</span></a>Variable`s they want to
request gradients from.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.GradientTape.batch_jacobian">
<code class="sig-name descname">batch_jacobian</code><span class="sig-paren">(</span><em class="sig-param">target</em>, <em class="sig-param">source</em>, <em class="sig-param">unconnected_gradients=&lt;UnconnectedGradients.NONE: 'none'&gt;</em>, <em class="sig-param">parallel_iterations=None</em>, <em class="sig-param">experimental_use_pfor=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape.batch_jacobian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape.batch_jacobian" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes and stacks per-example jacobians.</p>
<p>See [wikipedia article](<a class="reference external" href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant</a>) for the
definition of a Jacobian. This function is essentially an efficient
implementation of the following:</p>
<p><cite>tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])</cite>.</p>
<p>Note that compared to <cite>GradientTape.jacobian</cite> which computes gradient of
each output value w.r.t each input value, this function is useful when
<cite>target[i,...]</cite> is independent of <cite>source[j,...]</cite> for <cite>j != i</cite>. This
assumption allows more efficient computation as compared to
<cite>GradientTape.jacobian</cite>. The output, as well as intermediate activations,
are lower dimensional and avoid a bunch of redundant zeros which would
result in the jacobian computation given the independence assumption.</p>
<p>Example usage:</p>
<p><a href="#id70"><span class="problematic" id="id71">``</span></a><a href="#id72"><span class="problematic" id="id73">`</span></a>python
with tf.GradientTape() as g:</p>
<blockquote>
<div><p>x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)
g.watch(x)
y = x * x</p>
</div></blockquote>
<p>batch_jacobian = g.batch_jacobian(y, x)
# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]
<a href="#id74"><span class="problematic" id="id75">``</span></a><a href="#id76"><span class="problematic" id="id77">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> -- A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].
<cite>target[i,...]</cite> should only depend on <cite>source[i,...]</cite>.</p></li>
<li><p><strong>source</strong> -- A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].</p></li>
<li><p><strong>unconnected_gradients</strong> -- a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
'UnconnectedGradients' and it defaults to 'none'.</p></li>
<li><p><strong>parallel_iterations</strong> -- A knob to control how many iterations are dispatched
in parallel. This knob can be used to control the total memory usage.</p></li>
<li><p><strong>experimental_use_pfor</strong> -- If true, uses pfor for computing the Jacobian. Else
uses a tf.while_loop.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor <cite>t</cite> with shape [b, y_1, ..., y_n, x1, ..., x_m] where <cite>t[i, ...]</cite>
is the jacobian of <cite>target[i, ...]</cite> w.r.t. <cite>source[i, ...]</cite>, i.e. stacked
per-example jacobians.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>RuntimeError</strong> -- If called on a non-persistent tape with eager execution
    enabled and without enabling experimental_use_pfor.</p></li>
<li><p><strong>ValueError</strong> -- If vectorization of jacobian computation fails or if first
    dimension of <cite>target</cite> and <cite>source</cite> do not match.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.GradientTape.gradient">
<code class="sig-name descname">gradient</code><span class="sig-paren">(</span><em class="sig-param">target</em>, <em class="sig-param">sources</em>, <em class="sig-param">output_gradients=None</em>, <em class="sig-param">unconnected_gradients=&lt;UnconnectedGradients.NONE: 'none'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape.gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape.gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the gradient using operations recorded in context of this tape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> -- a list or nested structure of Tensors or Variables to be
differentiated.</p></li>
<li><p><strong>sources</strong> -- a list or nested structure of Tensors or Variables. <cite>target</cite>
will be differentiated against elements in <cite>sources</cite>.</p></li>
<li><p><strong>output_gradients</strong> -- a list of gradients, one for each element of
target. Defaults to None.</p></li>
<li><p><strong>unconnected_gradients</strong> -- a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
'UnconnectedGradients' and it defaults to 'none'.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list or nested structure of Tensors (or IndexedSlices, or None),
one for each element in <cite>sources</cite>. Returned structure is the same as
the structure of <cite>sources</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>RuntimeError</strong> -- if called inside the context of the tape, or if called more
    than once on a non-persistent tape.</p></li>
<li><p><strong>ValueError</strong> -- if the target is a variable or if unconnected gradients is
    called with an unknown value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.GradientTape.jacobian">
<code class="sig-name descname">jacobian</code><span class="sig-paren">(</span><em class="sig-param">target</em>, <em class="sig-param">sources</em>, <em class="sig-param">unconnected_gradients=&lt;UnconnectedGradients.NONE: 'none'&gt;</em>, <em class="sig-param">parallel_iterations=None</em>, <em class="sig-param">experimental_use_pfor=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape.jacobian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape.jacobian" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the jacobian using operations recorded in context of this tape.</p>
<p>See [wikipedia article](<a class="reference external" href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant</a>) for the
definition of a Jacobian.</p>
<p>Example usage:</p>
<p><a href="#id78"><span class="problematic" id="id79">``</span></a><a href="#id80"><span class="problematic" id="id81">`</span></a>python
with tf.GradientTape() as g:</p>
<blockquote>
<div><p>x  = tf.constant([1.0, 2.0])
g.watch(x)
y = x * x</p>
</div></blockquote>
<p>jacobian = g.jacobian(y, x)
# jacobian value is [[2., 0.], [0., 4.]]
<a href="#id82"><span class="problematic" id="id83">``</span></a><a href="#id84"><span class="problematic" id="id85">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> -- Tensor to be differentiated.</p></li>
<li><p><strong>sources</strong> -- a list or nested structure of Tensors or Variables. <cite>target</cite>
will be differentiated against elements in <cite>sources</cite>.</p></li>
<li><p><strong>unconnected_gradients</strong> -- a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
'UnconnectedGradients' and it defaults to 'none'.</p></li>
<li><p><strong>parallel_iterations</strong> -- A knob to control how many iterations are dispatched
in parallel. This knob can be used to control the total memory usage.</p></li>
<li><p><strong>experimental_use_pfor</strong> -- If true, vectorizes the jacobian computation. Else
falls back to a sequential while_loop. Vectorization can sometimes fail
or lead to excessive memory usage. This option can be used to disable
vectorization in such cases.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list or nested structure of Tensors (or None), one for each element in
<cite>sources</cite>. Returned structure is the same as the structure of <cite>sources</cite>.
Note if any gradient is sparse (IndexedSlices), jacobian function
currently makes it dense and returns a Tensor instead. This may change in
the future.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>RuntimeError</strong> -- If called on a non-persistent tape with eager execution
    enabled and without enabling experimental_use_pfor.</p></li>
<li><p><strong>ValueError</strong> -- If vectorization of jacobian computation fails.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.GradientTape.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears all information stored in this tape.</p>
<p>Equivalent to exiting and reentering the tape context manager with a new
tape. For example, the two following code blocks are equivalent:</p>
<p><a href="#id86"><span class="problematic" id="id87">``</span></a>`
with tf.GradientTape() as t:</p>
<blockquote>
<div><p>loss = loss_fn()</p>
</div></blockquote>
<dl class="simple">
<dt>with tf.GradientTape() as t:</dt><dd><p>loss += other_loss_fn()</p>
</dd>
</dl>
<p>t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn</p>
<p># The following is equivalent to the above
with tf.GradientTape() as t:</p>
<blockquote>
<div><p>loss = loss_fn()
t.reset()
loss += other_loss_fn()</p>
</div></blockquote>
<p>t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn
<a href="#id88"><span class="problematic" id="id89">``</span></a><a href="#id90"><span class="problematic" id="id91">`</span></a></p>
<p>This is useful if you don't want to exit the context manager for the tape,
or can't because the desired reset point is inside a control flow construct:</p>
<p><a href="#id92"><span class="problematic" id="id93">``</span></a>`
with tf.GradientTape() as t:</p>
<blockquote>
<div><p>loss = ...
if loss &gt; k:</p>
<blockquote>
<div><p>t.reset()</p>
</div></blockquote>
</div></blockquote>
<p><a href="#id94"><span class="problematic" id="id95">``</span></a><a href="#id96"><span class="problematic" id="id97">`</span></a></p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.GradientTape.stop_recording">
<code class="sig-name descname">stop_recording</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape.stop_recording"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape.stop_recording" title="Permalink to this definition">¶</a></dt>
<dd><p>Temporarily stops recording operations on this tape.</p>
<p>Operations executed while this context manager is active will not be
recorded on the tape. This is useful for reducing the memory used by tracing
all computations.</p>
<p>For example:</p>
<dl>
<dt><a href="#id98"><span class="problematic" id="id99">``</span></a><a href="#id100"><span class="problematic" id="id101">`</span></a></dt><dd><dl>
<dt>with tf.GradientTape(persistent=True) as t:</dt><dd><p>loss = compute_loss(model)
with t.stop_recording():</p>
<blockquote>
<div><p># The gradient computation below is not traced, saving memory.
grads = t.gradient(loss, model.variables)</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<p><a href="#id102"><span class="problematic" id="id103">``</span></a><a href="#id104"><span class="problematic" id="id105">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p>None</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>RuntimeError</strong> -- if the tape is not currently recording.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.GradientTape.watch">
<code class="sig-name descname">watch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape.watch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape.watch" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensures that <cite>tensor</cite> is being traced by this tape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> -- a Tensor or list of Tensors.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- if it encounters something that is not a tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.GradientTape.watched_variables">
<code class="sig-name descname">watched_variables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/backprop.html#GradientTape.watched_variables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.GradientTape.watched_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns variables watched by this tape in order of construction.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.Graph">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">Graph</code><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A TensorFlow computation, represented as a dataflow graph.</p>
<p>Graphs are used by <cite>tf.function`s to represent the function's computations.
Each graph contains a set of `tf.Operation</cite> objects, which represent units of
computation; and <cite>tf.Tensor</cite> objects, which represent the units of data that
flow between operations.</p>
<p>### Using graphs directly (deprecated)</p>
<p>A <cite>tf.Graph</cite> can be constructed and used directly without a <cite>tf.function</cite>, as
was required in TensorFlow 1, but this is deprecated and it is recommended to
use a <cite>tf.function</cite> instead. If a graph is directly used, other deprecated
TensorFlow 1 classes are also required to execute the graph, such as a
<cite>tf.compat.v1.Session</cite>.</p>
<p>A default graph can be registered with the <cite>tf.Graph.as_default</cite> context
manager. Then, operations will be added to the graph instead of being executed
eagerly. For example:</p>
<p><a href="#id106"><span class="problematic" id="id107">``</span></a><a href="#id108"><span class="problematic" id="id109">`</span></a>python
g = tf.Graph()
with g.as_default():</p>
<blockquote>
<div><p># Define operations and tensors in <cite>g</cite>.
c = tf.constant(30.0)
assert c.graph is g</p>
</div></blockquote>
<p><a href="#id110"><span class="problematic" id="id111">``</span></a><a href="#id112"><span class="problematic" id="id113">`</span></a></p>
<p><cite>tf.compat.v1.get_default_graph()</cite> can be used to obtain the default graph.</p>
<p>Important note: This class <em>is not</em> thread-safe for graph construction. All
operations should be created from a single thread, or external
synchronization must be provided. Unless otherwise specified, all methods
are not thread-safe.</p>
<p>A <cite>Graph</cite> instance supports an arbitrary number of &quot;collections&quot;
that are identified by name. For convenience when building a large
graph, collections can store groups of related objects: for
example, the <cite>tf.Variable</cite> uses a collection (named
<cite>tf.GraphKeys.GLOBAL_VARIABLES</cite>) for
all variables that are created during the construction of a graph. The caller
may define additional collections by specifying a new name.</p>
<p>Creates a new, empty Graph.</p>
<dl class="py method">
<dt id="tensorflow.Graph.add_to_collection">
<code class="sig-name descname">add_to_collection</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.add_to_collection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.add_to_collection" title="Permalink to this definition">¶</a></dt>
<dd><p>Stores <cite>value</cite> in the collection with the given <cite>name</cite>.</p>
<p>Note that collections are not sets, so it is possible to add a value to
a collection several times.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> -- The key for the collection. The <cite>GraphKeys</cite> class contains many
standard names for collections.</p></li>
<li><p><strong>value</strong> -- The value to add to the collection.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.add_to_collections">
<code class="sig-name descname">add_to_collections</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">names</span></em>, <em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.add_to_collections"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.add_to_collections" title="Permalink to this definition">¶</a></dt>
<dd><p>Stores <cite>value</cite> in the collections given by <cite>names</cite>.</p>
<p>Note that collections are not sets, so it is possible to add a value to
a collection several times. This function makes sure that duplicates in
<cite>names</cite> are ignored, but it will not check for pre-existing membership of
<cite>value</cite> in any of the collections in <cite>names</cite>.</p>
<p><cite>names</cite> can be any iterable, but if <cite>names</cite> is a string, it is treated as a
single collection name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>names</strong> -- The keys for the collections to add to. The <cite>GraphKeys</cite> class
contains many standard names for collections.</p></li>
<li><p><strong>value</strong> -- The value to add to the collections.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.as_default">
<code class="sig-name descname">as_default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.as_default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.as_default" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a context manager that makes this <cite>Graph</cite> the default graph.</p>
<p>This method should be used if you want to create multiple graphs
in the same process. For convenience, a global default graph is
provided, and all ops will be added to this graph if you do not
create a new graph explicitly.</p>
<p>Use this method with the <cite>with</cite> keyword to specify that ops created within
the scope of a block should be added to this graph. In this case, once
the scope of the <cite>with</cite> is exited, the previous default graph is set again
as default. There is a stack, so it's ok to have multiple nested levels
of <cite>as_default</cite> calls.</p>
<p>The default graph is a property of the current thread. If you
create a new thread, and wish to use the default graph in that
thread, you must explicitly add a <cite>with g.as_default():</cite> in that
thread's function.</p>
<p>The following code examples are equivalent:</p>
<p><a href="#id114"><span class="problematic" id="id115">``</span></a><a href="#id116"><span class="problematic" id="id117">`</span></a>python
# 1. Using Graph.as_default():
g = tf.Graph()
with g.as_default():</p>
<blockquote>
<div><p>c = tf.constant(5.0)
assert c.graph is g</p>
</div></blockquote>
<p># 2. Constructing and making default:
with tf.Graph().as_default() as g:</p>
<blockquote>
<div><p>c = tf.constant(5.0)
assert c.graph is g</p>
</div></blockquote>
<p><a href="#id118"><span class="problematic" id="id119">``</span></a><a href="#id120"><span class="problematic" id="id121">`</span></a></p>
<p>If eager execution is enabled ops created under this context manager will be
added to the graph instead of executed eagerly.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A context manager for using this graph as the default graph.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.as_graph_def">
<code class="sig-name descname">as_graph_def</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">from_version</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_shapes</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.as_graph_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.as_graph_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a serialized <cite>GraphDef</cite> representation of this graph.</p>
<p>The serialized <cite>GraphDef</cite> can be imported into another <cite>Graph</cite>
(using <cite>tf.import_graph_def</cite>) or used with the
[C++ Session API](../../api_docs/cc/index.md).</p>
<p>This method is thread-safe.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>from_version</strong> -- Optional.  If this is set, returns a <cite>GraphDef</cite> containing
only the nodes that were added to this graph since its <cite>version</cite>
property had the given value.</p></li>
<li><p><strong>add_shapes</strong> -- If true, adds an &quot;_output_shapes&quot; list attr to each node with
the inferred shapes of each of its outputs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A
[<cite>GraphDef</cite>](<a class="reference external" href="https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto">https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto</a>)
protocol buffer.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If the <cite>graph_def</cite> would be too large.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.as_graph_element">
<code class="sig-name descname">as_graph_element</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obj</span></em>, <em class="sig-param"><span class="n">allow_tensor</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">allow_operation</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.as_graph_element"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.as_graph_element" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the object referred to by <cite>obj</cite>, as an <cite>Operation</cite> or <cite>Tensor</cite>.</p>
<p>This function validates that <cite>obj</cite> represents an element of this
graph, and gives an informative error message if it is not.</p>
<p>This function is the canonical way to get/validate an object of
one of the allowed types from an external argument reference in the
Session API.</p>
<p>This method may be called concurrently from multiple threads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obj</strong> -- A <cite>Tensor</cite>, an <cite>Operation</cite>, or the name of a tensor or operation. Can
also be any object with an <cite>_as_graph_element()</cite> method that returns a
value of one of these types. Note: <cite>_as_graph_element</cite> will be called
inside the graph's lock and so may not modify the graph.</p></li>
<li><p><strong>allow_tensor</strong> -- If true, <cite>obj</cite> may refer to a <cite>Tensor</cite>.</p></li>
<li><p><strong>allow_operation</strong> -- If true, <cite>obj</cite> may refer to an <cite>Operation</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The <cite>Tensor</cite> or <cite>Operation</cite> in the Graph corresponding to <cite>obj</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If <cite>obj</cite> is not a type we support attempting to convert
    to types.</p></li>
<li><p><strong>ValueError</strong> -- If <cite>obj</cite> is of an appropriate type but invalid. For
    example, an invalid string.</p></li>
<li><p><strong>KeyError</strong> -- If <cite>obj</cite> is not an object in the graph.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.building_function">
<em class="property">property </em><code class="sig-name descname">building_function</code><a class="headerlink" href="#tensorflow.Graph.building_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True iff this graph represents a function.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.clear_collection">
<code class="sig-name descname">clear_collection</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.clear_collection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.clear_collection" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears all values in a collection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- The key for the collection. The <cite>GraphKeys</cite> class contains many
standard names for collections.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.collections">
<em class="property">property </em><code class="sig-name descname">collections</code><a class="headerlink" href="#tensorflow.Graph.collections" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the names of the collections known to this graph.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.colocate_with">
<code class="sig-name descname">colocate_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op</span></em>, <em class="sig-param"><span class="n">ignore_existing</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.colocate_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.colocate_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a context manager that specifies an op to colocate with.</p>
<p>Note: this function is not for public use, only for internal libraries.</p>
<p>For example:</p>
<p><a href="#id122"><span class="problematic" id="id123">``</span></a><a href="#id124"><span class="problematic" id="id125">`</span></a>python
a = tf.Variable([1.0])
with g.colocate_with(a):</p>
<blockquote>
<div><p>b = tf.constant(1.0)
c = tf.add(a, b)</p>
</div></blockquote>
<p><a href="#id126"><span class="problematic" id="id127">``</span></a><a href="#id128"><span class="problematic" id="id129">`</span></a></p>
<p><cite>b</cite> and <cite>c</cite> will always be colocated with <cite>a</cite>, no matter where <cite>a</cite>
is eventually placed.</p>
<p><strong>NOTE</strong> Using a colocation scope resets any existing device constraints.</p>
<p>If <cite>op</cite> is <cite>None</cite> then <cite>ignore_existing</cite> must be <cite>True</cite> and the new
scope resets all colocation and device constraints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op</strong> -- The op to colocate all created ops with, or <cite>None</cite>.</p></li>
<li><p><strong>ignore_existing</strong> -- If true, only applies colocation of this op within the
context, rather than applying all colocation properties on the stack.
If <cite>op</cite> is <cite>None</cite>, this value must be <cite>True</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- if op is None but ignore_existing is False.</p>
</dd>
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p>A context manager that specifies the op with which to colocate
newly created ops.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.container">
<code class="sig-name descname">container</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">container_name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.container"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.container" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a context manager that specifies the resource container to use.</p>
<p>Stateful operations, such as variables and queues, can maintain their
states on devices so that they can be shared by multiple processes.
A resource container is a string name under which these stateful
operations are tracked. These resources can be released or cleared
with <cite>tf.Session.reset()</cite>.</p>
<p>For example:</p>
<p><a href="#id130"><span class="problematic" id="id131">``</span></a><a href="#id132"><span class="problematic" id="id133">`</span></a>python
with g.container('experiment0'):</p>
<blockquote>
<div><p># All stateful Operations constructed in this context will be placed
# in resource container &quot;experiment0&quot;.
v1 = tf.Variable([1.0])
v2 = tf.Variable([2.0])
with g.container(&quot;experiment1&quot;):</p>
<blockquote>
<div><p># All stateful Operations constructed in this context will be
# placed in resource container &quot;experiment1&quot;.
v3 = tf.Variable([3.0])
q1 = tf.queue.FIFOQueue(10, tf.float32)</p>
</div></blockquote>
<p># All stateful Operations constructed in this context will be
# be created in the &quot;experiment0&quot;.
v4 = tf.Variable([4.0])
q1 = tf.queue.FIFOQueue(20, tf.float32)
with g.container(&quot;&quot;):</p>
<blockquote>
<div><p># All stateful Operations constructed in this context will be
# be placed in the default resource container.
v5 = tf.Variable([5.0])
q3 = tf.queue.FIFOQueue(30, tf.float32)</p>
</div></blockquote>
</div></blockquote>
<p># Resets container &quot;experiment0&quot;, after which the state of v1, v2, v4, q1
# will become undefined (such as uninitialized).
tf.Session.reset(target, [&quot;experiment0&quot;])
<a href="#id134"><span class="problematic" id="id135">``</span></a><a href="#id136"><span class="problematic" id="id137">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>container_name</strong> -- container name string.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A context manager for defining resource containers for stateful ops,</dt><dd><p>yields the container name.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.control_dependencies">
<code class="sig-name descname">control_dependencies</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">control_inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.control_dependencies"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.control_dependencies" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a context manager that specifies control dependencies.</p>
<p>Use with the <cite>with</cite> keyword to specify that all operations constructed
within the context should have control dependencies on
<cite>control_inputs</cite>. For example:</p>
<p><a href="#id138"><span class="problematic" id="id139">``</span></a><a href="#id140"><span class="problematic" id="id141">`</span></a>python
with g.control_dependencies([a, b, c]):</p>
<blockquote>
<div><p># <cite>d</cite> and <cite>e</cite> will only run after <cite>a</cite>, <cite>b</cite>, and <cite>c</cite> have executed.
d = ...
e = ...</p>
</div></blockquote>
<p><a href="#id142"><span class="problematic" id="id143">``</span></a><a href="#id144"><span class="problematic" id="id145">`</span></a></p>
<p>Multiple calls to <cite>control_dependencies()</cite> can be nested, and in
that case a new <cite>Operation</cite> will have control dependencies on the union
of <cite>control_inputs</cite> from all active contexts.</p>
<p><a href="#id146"><span class="problematic" id="id147">``</span></a><a href="#id148"><span class="problematic" id="id149">`</span></a>python
with g.control_dependencies([a, b]):</p>
<blockquote>
<div><p># Ops constructed here run after <cite>a</cite> and <cite>b</cite>.
with g.control_dependencies([c, d]):</p>
<blockquote>
<div><p># Ops constructed here run after <cite>a</cite>, <cite>b</cite>, <cite>c</cite>, and <cite>d</cite>.</p>
</div></blockquote>
</div></blockquote>
<p><a href="#id150"><span class="problematic" id="id151">``</span></a><a href="#id152"><span class="problematic" id="id153">`</span></a></p>
<p>You can pass None to clear the control dependencies:</p>
<p><a href="#id154"><span class="problematic" id="id155">``</span></a><a href="#id156"><span class="problematic" id="id157">`</span></a>python
with g.control_dependencies([a, b]):</p>
<blockquote>
<div><p># Ops constructed here run after <cite>a</cite> and <cite>b</cite>.
with g.control_dependencies(None):</p>
<blockquote>
<div><p># Ops constructed here run normally, not waiting for either <cite>a</cite> or <cite>b</cite>.
with g.control_dependencies([c, d]):</p>
<blockquote>
<div><p># Ops constructed here run after <cite>c</cite> and <cite>d</cite>, also not waiting
# for either <cite>a</cite> or <cite>b</cite>.</p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p><a href="#id158"><span class="problematic" id="id159">``</span></a><a href="#id160"><span class="problematic" id="id161">`</span></a></p>
<p><em>N.B.</em> The control dependencies context applies <em>only</em> to ops that
are constructed within the context. Merely using an op or tensor
in the context does not add a control dependency. The following
example illustrates this point:</p>
<p><a href="#id162"><span class="problematic" id="id163">``</span></a><a href="#id164"><span class="problematic" id="id165">`</span></a>python
# WRONG
def my_func(pred, tensor):</p>
<blockquote>
<div><p>t = tf.matmul(tensor, tensor)
with tf.control_dependencies([pred]):</p>
<blockquote>
<div><p># The matmul op is created outside the context, so no control
# dependency will be added.
return t</p>
</div></blockquote>
</div></blockquote>
<p># RIGHT
def my_func(pred, tensor):</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.control_dependencies([pred]):</dt><dd><p># The matmul op is created in the context, so a control dependency
# will be added.
return tf.matmul(tensor, tensor)</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id166"><span class="problematic" id="id167">``</span></a><a href="#id168"><span class="problematic" id="id169">`</span></a></p>
<p>Also note that though execution of ops created under this scope will trigger
execution of the dependencies, the ops created under this scope might still
be pruned from a normal tensorflow graph. For example, in the following
snippet of code the dependencies are never executed:</p>
<dl>
<dt><a href="#id170"><span class="problematic" id="id171">``</span></a><a href="#id172"><span class="problematic" id="id173">`</span></a>python</dt><dd><p>loss = model.loss()
with tf.control_dependencies(dependencies):</p>
<blockquote>
<div><dl class="simple">
<dt>loss = loss + tf.constant(1)  # note: dependencies ignored in the</dt><dd><p># backward pass</p>
</dd>
</dl>
</div></blockquote>
<p>return tf.gradients(loss, model.variables)</p>
</dd>
</dl>
<p><a href="#id174"><span class="problematic" id="id175">``</span></a><a href="#id176"><span class="problematic" id="id177">`</span></a></p>
<p>This is because evaluating the gradient graph does not require evaluating
the constant(1) op created in the forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>control_inputs</strong> -- A list of <cite>Operation</cite> or <cite>Tensor</cite> objects which must be
executed or computed before running the operations defined in the
context.  Can also be <cite>None</cite> to clear the control dependencies.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A context manager that specifies control dependencies for all
operations constructed within the context.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- If <cite>control_inputs</cite> is not a list of <cite>Operation</cite> or
    <cite>Tensor</cite> objects.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.create_op">
<code class="sig-name descname">create_op</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op_type</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">dtypes</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_types</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attrs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">op_def</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">compute_shapes</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">compute_device</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.create_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.create_op" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an <cite>Operation</cite> in this graph. (deprecated arguments)</p>
<p>Warning: SOME ARGUMENTS ARE DEPRECATED: <cite>(compute_shapes)</cite>. They will be removed in a future version.
Instructions for updating:
Shapes are always computed; don't use the compute_shapes as it has no effect.</p>
<p>This is a low-level interface for creating an <cite>Operation</cite>. Most
programs will not call this method directly, and instead use the
Python op constructors, such as <cite>tf.constant()</cite>, which add ops to
the default graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op_type</strong> -- The <cite>Operation</cite> type to create. This corresponds to the
<cite>OpDef.name</cite> field for the proto that defines the operation.</p></li>
<li><p><strong>inputs</strong> -- A list of <cite>Tensor</cite> objects that will be inputs to the <cite>Operation</cite>.</p></li>
<li><p><strong>dtypes</strong> -- (Optional) A list of <cite>DType</cite> objects that will be the types of the
tensors that the operation produces.</p></li>
<li><p><strong>input_types</strong> -- (Optional.) A list of <cite>DType`s that will be the types of the
tensors that the operation consumes. By default, uses the base `DType</cite>
of each input in <cite>inputs</cite>. Operations that expect reference-typed inputs
must specify <cite>input_types</cite> explicitly.</p></li>
<li><p><strong>name</strong> -- (Optional.) A string name for the operation. If not specified, a
name is generated based on <cite>op_type</cite>.</p></li>
<li><p><strong>attrs</strong> -- (Optional.) A dictionary where the key is the attribute name (a
string) and the value is the respective <cite>attr</cite> attribute of the
<cite>NodeDef</cite> proto that will represent the operation (an <cite>AttrValue</cite>
proto).</p></li>
<li><p><strong>op_def</strong> -- (Optional.) The <cite>OpDef</cite> proto that describes the <cite>op_type</cite> that
the operation will have.</p></li>
<li><p><strong>compute_shapes</strong> -- (Optional.) Deprecated. Has no effect (shapes are always
computed).</p></li>
<li><p><strong>compute_device</strong> -- (Optional.) If True, device functions will be executed to
compute the device property of the Operation.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> -- if any of the inputs is not a <cite>Tensor</cite>.</p></li>
<li><p><strong>ValueError</strong> -- if colocation conflicts with existing device assignment.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An <cite>Operation</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.device">
<code class="sig-name descname">device</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device_name_or_function</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a context manager that specifies the default device to use.</p>
<p>The <cite>device_name_or_function</cite> argument may either be a device name
string, a device function, or None:</p>
<ul class="simple">
<li><p>If it is a device name string, all operations constructed in
this context will be assigned to the device with that name, unless
overridden by a nested <cite>device()</cite> context.</p></li>
<li><p>If it is a function, it will be treated as a function from
Operation objects to device name strings, and invoked each time
a new Operation is created. The Operation will be assigned to
the device with the returned name.</p></li>
<li><p>If it is None, all <cite>device()</cite> invocations from the enclosing context
will be ignored.</p></li>
</ul>
<p>For information about the valid syntax of device name strings, see
the documentation in
[<cite>DeviceNameUtils</cite>](<a class="reference external" href="https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h">https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h</a>).</p>
<p>For example:</p>
<p><a href="#id178"><span class="problematic" id="id179">``</span></a><a href="#id180"><span class="problematic" id="id181">`</span></a>python
with g.device('/device:GPU:0'):</p>
<blockquote>
<div><p># All operations constructed in this context will be placed
# on GPU 0.
with g.device(None):</p>
<blockquote>
<div><p># All operations constructed in this context will have no
# assigned device.</p>
</div></blockquote>
</div></blockquote>
<p># Defines a function from <cite>Operation</cite> to device string.
def matmul_on_gpu(n):</p>
<blockquote>
<div><dl class="simple">
<dt>if n.type == &quot;MatMul&quot;:</dt><dd><p>return &quot;/device:GPU:0&quot;</p>
</dd>
<dt>else:</dt><dd><p>return &quot;/cpu:0&quot;</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>with g.device(matmul_on_gpu):</dt><dd><p># All operations of type &quot;MatMul&quot; constructed in this context
# will be placed on GPU 0; all other operations will be placed
# on CPU 0.</p>
</dd>
</dl>
<p><a href="#id182"><span class="problematic" id="id183">``</span></a><a href="#id184"><span class="problematic" id="id185">`</span></a></p>
<p><strong>N.B.</strong> The device scope may be overridden by op wrappers or
other library code. For example, a variable assignment op
<cite>v.assign()</cite> must be colocated with the <cite>tf.Variable</cite> <cite>v</cite>, and
incompatible device scopes will be ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device_name_or_function</strong> -- The device name or function to use in the
context.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p>A context manager that specifies the default device to use for newly
created ops.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> -- If device scopes are not properly nested.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.finalize">
<code class="sig-name descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.finalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.finalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Finalizes this graph, making it read-only.</p>
<p>After calling <cite>g.finalize()</cite>, no new operations can be added to
<cite>g</cite>.  This method is used to ensure that no operations are added
to a graph when it is shared between multiple threads, for example
when using a <cite>tf.compat.v1.train.QueueRunner</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.finalized">
<em class="property">property </em><code class="sig-name descname">finalized</code><a class="headerlink" href="#tensorflow.Graph.finalized" title="Permalink to this definition">¶</a></dt>
<dd><p>True if this graph has been finalized.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.get_all_collection_keys">
<code class="sig-name descname">get_all_collection_keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.get_all_collection_keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.get_all_collection_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of collections used in this graph.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.get_collection">
<code class="sig-name descname">get_collection</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.get_collection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.get_collection" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of values in the collection with the given <cite>name</cite>.</p>
<p>This is different from <cite>get_collection_ref()</cite> which always returns the
actual collection list if it exists in that it returns a new list each time
it is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> -- The key for the collection. For example, the <cite>GraphKeys</cite> class
contains many standard names for collections.</p></li>
<li><p><strong>scope</strong> -- (Optional.) A string. If supplied, the resulting list is filtered
to include only items whose <cite>name</cite> attribute matches <cite>scope</cite> using
<cite>re.match</cite>. Items without a <cite>name</cite> attribute are never returned if a
scope is supplied. The choice of <cite>re.match</cite> means that a <cite>scope</cite> without
special tokens filters by prefix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of values in the collection with the given <cite>name</cite>, or
an empty list if no value has been added to that collection. The
list contains the values in the order under which they were
collected.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.get_collection_ref">
<code class="sig-name descname">get_collection_ref</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.get_collection_ref"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.get_collection_ref" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of values in the collection with the given <cite>name</cite>.</p>
<p>If the collection exists, this returns the list itself, which can
be modified in place to change the collection.  If the collection does
not exist, it is created as an empty list and the list is returned.</p>
<p>This is different from <cite>get_collection()</cite> which always returns a copy of
the collection list if it exists and never creates an empty collection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- The key for the collection. For example, the <cite>GraphKeys</cite> class
contains many standard names for collections.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of values in the collection with the given <cite>name</cite>, or an empty
list if no value has been added to that collection.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.get_name_scope">
<code class="sig-name descname">get_name_scope</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.get_name_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.get_name_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current name scope.</p>
<p>For example:</p>
<p><a href="#id186"><span class="problematic" id="id187">``</span></a><a href="#id188"><span class="problematic" id="id189">`</span></a>python
with tf.name_scope('scope1'):</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.name_scope('scope2'):</dt><dd><p>print(tf.compat.v1.get_default_graph().get_name_scope())</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id190"><span class="problematic" id="id191">``</span></a>`
would print the string <cite>scope1/scope2</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A string representing the current name scope.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.get_operation_by_name">
<code class="sig-name descname">get_operation_by_name</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.get_operation_by_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.get_operation_by_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <cite>Operation</cite> with the given <cite>name</cite>.</p>
<p>This method may be called concurrently from multiple threads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- The name of the <cite>Operation</cite> to return.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The <cite>Operation</cite> with the given <cite>name</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If <cite>name</cite> is not a string.</p></li>
<li><p><strong>KeyError</strong> -- If <cite>name</cite> does not correspond to an operation in this graph.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.get_operations">
<code class="sig-name descname">get_operations</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.get_operations"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.get_operations" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the list of operations in the graph.</p>
<p>You can modify the operations in place, but modifications
to the list such as inserts/delete have no effect on the
list of operations known to the graph.</p>
<p>This method may be called concurrently from multiple threads.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of Operations.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.get_tensor_by_name">
<code class="sig-name descname">get_tensor_by_name</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.get_tensor_by_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.get_tensor_by_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <cite>Tensor</cite> with the given <cite>name</cite>.</p>
<p>This method may be called concurrently from multiple threads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- The name of the <cite>Tensor</cite> to return.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The <cite>Tensor</cite> with the given <cite>name</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If <cite>name</cite> is not a string.</p></li>
<li><p><strong>KeyError</strong> -- If <cite>name</cite> does not correspond to a tensor in this graph.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.gradient_override_map">
<code class="sig-name descname">gradient_override_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op_type_map</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.gradient_override_map"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.gradient_override_map" title="Permalink to this definition">¶</a></dt>
<dd><p>EXPERIMENTAL: A context manager for overriding gradient functions.</p>
<p>This context manager can be used to override the gradient function
that will be used for ops within the scope of the context.</p>
<p>For example:</p>
<p><a href="#id192"><span class="problematic" id="id193">``</span></a><a href="#id194"><span class="problematic" id="id195">`</span></a>python
&#64;tf.RegisterGradient(&quot;CustomSquare&quot;)
def _custom_square_grad(op, grad):</p>
<blockquote>
<div><p># ...</p>
</div></blockquote>
<dl>
<dt>with tf.Graph().as_default() as g:</dt><dd><p>c = tf.constant(5.0)
s_1 = tf.square(c)  # Uses the default gradient for tf.square.
with g.gradient_override_map({&quot;Square&quot;: &quot;CustomSquare&quot;}):</p>
<blockquote>
<div><dl class="simple">
<dt>s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the</dt><dd><p># gradient of s_2.</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
<p><a href="#id196"><span class="problematic" id="id197">``</span></a><a href="#id198"><span class="problematic" id="id199">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>op_type_map</strong> -- A dictionary mapping op type strings to alternative op type
strings.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A context manager that sets the alternative op type to be used for one
or more ops created in that context.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- If <cite>op_type_map</cite> is not a dictionary mapping strings to
    strings.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.graph_def_versions">
<em class="property">property </em><code class="sig-name descname">graph_def_versions</code><a class="headerlink" href="#tensorflow.Graph.graph_def_versions" title="Permalink to this definition">¶</a></dt>
<dd><p>The GraphDef version information of this graph.</p>
<p>For details on the meaning of each version, see
[<cite>GraphDef</cite>](<a class="reference external" href="https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto">https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto</a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>VersionDef</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.is_feedable">
<code class="sig-name descname">is_feedable</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.is_feedable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.is_feedable" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <cite>True</cite> if and only if <cite>tensor</cite> is feedable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.is_fetchable">
<code class="sig-name descname">is_fetchable</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor_or_op</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.is_fetchable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.is_fetchable" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <cite>True</cite> if and only if <cite>tensor_or_op</cite> is fetchable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.name_scope">
<code class="sig-name descname">name_scope</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.name_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.name_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a context manager that creates hierarchical names for operations.</p>
<p>A graph maintains a stack of name scopes. A <cite>with name_scope(...):</cite>
statement pushes a new name onto the stack for the lifetime of the context.</p>
<p>The <cite>name</cite> argument will be interpreted as follows:</p>
<ul class="simple">
<li><p>A string (not ending with '/') will create a new name scope, in which
<cite>name</cite> is appended to the prefix of all operations created in the
context. If <cite>name</cite> has been used before, it will be made unique by
calling <cite>self.unique_name(name)</cite>.</p></li>
<li><p>A scope previously captured from a <cite>with g.name_scope(...) as
scope:</cite> statement will be treated as an &quot;absolute&quot; name scope, which
makes it possible to re-enter existing scopes.</p></li>
<li><p>A value of <cite>None</cite> or the empty string will reset the current name scope
to the top-level (empty) name scope.</p></li>
</ul>
<p>For example:</p>
<p><a href="#id200"><span class="problematic" id="id201">``</span></a><a href="#id202"><span class="problematic" id="id203">`</span></a>python
with tf.Graph().as_default() as g:</p>
<blockquote>
<div><p>c = tf.constant(5.0, name=&quot;c&quot;)
assert c.op.name == &quot;c&quot;
c_1 = tf.constant(6.0, name=&quot;c&quot;)
assert c_1.op.name == &quot;c_1&quot;</p>
<p># Creates a scope called &quot;nested&quot;
with g.name_scope(&quot;nested&quot;) as scope:</p>
<blockquote>
<div><p>nested_c = tf.constant(10.0, name=&quot;c&quot;)
assert nested_c.op.name == &quot;nested/c&quot;</p>
<p># Creates a nested scope called &quot;inner&quot;.
with g.name_scope(&quot;inner&quot;):</p>
<blockquote>
<div><p>nested_inner_c = tf.constant(20.0, name=&quot;c&quot;)
assert nested_inner_c.op.name == &quot;nested/inner/c&quot;</p>
</div></blockquote>
<p># Create a nested scope called &quot;inner_1&quot;.
with g.name_scope(&quot;inner&quot;):</p>
<blockquote>
<div><p>nested_inner_1_c = tf.constant(30.0, name=&quot;c&quot;)
assert nested_inner_1_c.op.name == &quot;nested/inner_1/c&quot;</p>
<p># Treats <cite>scope</cite> as an absolute name scope, and
# switches to the &quot;nested/&quot; scope.
with g.name_scope(scope):</p>
<blockquote>
<div><p>nested_d = tf.constant(40.0, name=&quot;d&quot;)
assert nested_d.op.name == &quot;nested/d&quot;</p>
<dl class="simple">
<dt>with g.name_scope(&quot;&quot;):</dt><dd><p>e = tf.constant(50.0, name=&quot;e&quot;)
assert e.op.name == &quot;e&quot;</p>
</dd>
</dl>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p><a href="#id204"><span class="problematic" id="id205">``</span></a><a href="#id206"><span class="problematic" id="id207">`</span></a></p>
<p>The name of the scope itself can be captured by <cite>with
g.name_scope(...) as scope:</cite>, which stores the name of the scope
in the variable <cite>scope</cite>. This value can be used to name an
operation that represents the overall result of executing the ops
in a scope. For example:</p>
<p><a href="#id208"><span class="problematic" id="id209">``</span></a><a href="#id210"><span class="problematic" id="id211">`</span></a>python
inputs = tf.constant(...)
with g.name_scope('my_layer') as scope:</p>
<blockquote>
<div><p>weights = tf.Variable(..., name=&quot;weights&quot;)
biases = tf.Variable(..., name=&quot;biases&quot;)
affine = tf.matmul(inputs, weights) + biases
output = tf.nn.relu(affine, name=scope)</p>
</div></blockquote>
<p><a href="#id212"><span class="problematic" id="id213">``</span></a><a href="#id214"><span class="problematic" id="id215">`</span></a></p>
<p>NOTE: This constructor validates the given <cite>name</cite>. Valid scope
names match one of the following regular expressions:</p>
<blockquote>
<div><p>[A-Za-z0-9.][<a href="#id1100"><span class="problematic" id="id1101">A-Za-z0-9_</span></a>.-/]* (for scopes at the root)
[<a href="#id1102"><span class="problematic" id="id1103">A-Za-z0-9_</span></a>.-/]* (for other scopes)</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name for the scope.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A context manager that installs <cite>name</cite> as a new name scope.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>name</cite> is not a valid scope name, according to the rules
    above.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.prevent_feeding">
<code class="sig-name descname">prevent_feeding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.prevent_feeding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.prevent_feeding" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks the given <cite>tensor</cite> as unfeedable in this graph.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.prevent_fetching">
<code class="sig-name descname">prevent_fetching</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.prevent_fetching"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.prevent_fetching" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks the given <cite>op</cite> as unfetchable in this graph.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.seed">
<em class="property">property </em><code class="sig-name descname">seed</code><a class="headerlink" href="#tensorflow.Graph.seed" title="Permalink to this definition">¶</a></dt>
<dd><p>The graph-level random seed of this graph.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.switch_to_thread_local">
<code class="sig-name descname">switch_to_thread_local</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.switch_to_thread_local"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.switch_to_thread_local" title="Permalink to this definition">¶</a></dt>
<dd><p>Make device, colocation and dependencies stacks thread-local.</p>
<p>Device, colocation and dependencies stacks are not thread-local be default.
If multiple threads access them, then the state is shared.  This means that
one thread may affect the behavior of another thread.</p>
<p>After this method is called, the stacks become thread-local.  If multiple
threads access them, then the state is not shared.  Each thread uses its own
value; a thread doesn't affect other threads by mutating such a stack.</p>
<p>The initial value for every thread's stack is set to the current value
of the stack when <cite>switch_to_thread_local()</cite> was first called.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.unique_name">
<code class="sig-name descname">unique_name</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">mark_as_used</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Graph.unique_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Graph.unique_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a unique operation name for <cite>name</cite>.</p>
<p>Note: You rarely need to call <cite>unique_name()</cite> directly.  Most of
the time you just need to create <cite>with g.name_scope()</cite> blocks to
generate structured names.</p>
<p><cite>unique_name</cite> is used to generate structured names, separated by
<cite>&quot;/&quot;</cite>, to help identify operations when debugging a graph.
Operation names are displayed in error messages reported by the
TensorFlow runtime, and in various visualization tools such as
TensorBoard.</p>
<p>If <cite>mark_as_used</cite> is set to <cite>True</cite>, which is the default, a new
unique name is created and marked as in use. If it's set to <cite>False</cite>,
the unique name is returned without actually being marked as used.
This is useful when the caller simply wants to know what the name
to be created will be.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> -- The name for an operation.</p></li>
<li><p><strong>mark_as_used</strong> -- Whether to mark this name as being used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A string to be passed to <cite>create_op()</cite> that will be used
to name the operation being created.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Graph.version">
<em class="property">property </em><code class="sig-name descname">version</code><a class="headerlink" href="#tensorflow.Graph.version" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a version number that increases as ops are added to the graph.</p>
<p>Note that this is unrelated to the
<cite>tf.Graph.graph_def_versions</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An integer version that increases as ops are added to the graph.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.IndexedSlices">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">IndexedSlices</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">dense_shape</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/indexed_slices.html#IndexedSlices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.IndexedSlices" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.tensor_like._TensorLike</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.composite_tensor.CompositeTensor</span></code></p>
<p>A sparse representation of a set of tensor slices at given indices.</p>
<p>This class is a simple wrapper for a pair of <cite>Tensor</cite> objects:</p>
<ul class="simple">
<li><p><cite>values</cite>: A <cite>Tensor</cite> of any dtype with shape <cite>[D0, D1, ..., Dn]</cite>.</p></li>
<li><p><cite>indices</cite>: A 1-D integer <cite>Tensor</cite> with shape <cite>[D0]</cite>.</p></li>
</ul>
<p>An <cite>IndexedSlices</cite> is typically used to represent a subset of a larger
tensor <cite>dense</cite> of shape <cite>[LARGE0, D1, .. , DN]</cite> where <cite>LARGE0 &gt;&gt; D0</cite>.
The values in <cite>indices</cite> are the indices in the first dimension of
the slices that have been extracted from the larger tensor.</p>
<p>The dense tensor <cite>dense</cite> represented by an <cite>IndexedSlices</cite> <cite>slices</cite> has</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dense[slices.indices[i],</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">...]</span> <span class="pre">=</span> <span class="pre">slices.values[i,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">...]</span>
<span class="pre">`</span></code></p>
<p>The <cite>IndexedSlices</cite> class is used principally in the definition of
gradients for operations that have sparse gradients
(e.g. <cite>tf.gather</cite>).</p>
<p>Contrast this representation with
<cite>tf.SparseTensor</cite>,
which uses multi-dimensional indices and scalar values.</p>
<p>Creates an <cite>IndexedSlices</cite>.</p>
<dl class="py method">
<dt id="tensorflow.IndexedSlices.consumers">
<code class="sig-name descname">consumers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/indexed_slices.html#IndexedSlices.consumers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.IndexedSlices.consumers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.dense_shape">
<em class="property">property </em><code class="sig-name descname">dense_shape</code><a class="headerlink" href="#tensorflow.IndexedSlices.dense_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>A 1-D <cite>Tensor</cite> containing the shape of the corresponding dense tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.device">
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#tensorflow.IndexedSlices.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of the device on which <cite>values</cite> will be produced, or <cite>None</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#tensorflow.IndexedSlices.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>DType</cite> of elements in this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.graph">
<em class="property">property </em><code class="sig-name descname">graph</code><a class="headerlink" href="#tensorflow.IndexedSlices.graph" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Graph</cite> that contains the values, indices, and shape tensors.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.indices">
<em class="property">property </em><code class="sig-name descname">indices</code><a class="headerlink" href="#tensorflow.IndexedSlices.indices" title="Permalink to this definition">¶</a></dt>
<dd><p>A 1-D <cite>Tensor</cite> containing the indices of the slices.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#tensorflow.IndexedSlices.name" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of this <cite>IndexedSlices</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.op">
<em class="property">property </em><code class="sig-name descname">op</code><a class="headerlink" href="#tensorflow.IndexedSlices.op" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Operation</cite> that produces <cite>values</cite> as an output.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.shape">
<em class="property">property </em><code class="sig-name descname">shape</code><a class="headerlink" href="#tensorflow.IndexedSlices.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the <cite>tf.TensorShape</cite> representing the shape of the dense tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>tf.TensorShape</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.IndexedSlices.values">
<em class="property">property </em><code class="sig-name descname">values</code><a class="headerlink" href="#tensorflow.IndexedSlices.values" title="Permalink to this definition">¶</a></dt>
<dd><p>A <cite>Tensor</cite> containing the values of the slices.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.IndexedSlicesSpec">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">IndexedSlicesSpec</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">indices_dtype</span><span class="o">=</span><span class="default_value">tf.int64</span></em>, <em class="sig-param"><span class="n">dense_shape_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">indices_shape</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/indexed_slices.html#IndexedSlicesSpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.IndexedSlicesSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.type_spec.TypeSpec</span></code></p>
<p>Type specification for a <cite>tf.IndexedSlices</cite>.</p>
<p>Constructs a type specification for a <cite>tf.IndexedSlices</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> -- The dense shape of the <cite>IndexedSlices</cite>, or <cite>None</cite> to allow any
dense shape.</p></li>
<li><p><strong>dtype</strong> -- <cite>tf.DType</cite> of values in the <cite>IndexedSlices</cite>.</p></li>
<li><p><strong>indices_dtype</strong> -- <cite>tf.DType</cite> of the <cite>indices</cite> in the <cite>IndexedSlices</cite>.  One
of <cite>tf.int32</cite> or <cite>tf.int64</cite>.</p></li>
<li><p><strong>dense_shape_dtype</strong> -- <cite>tf.DType</cite> of the <cite>dense_shape</cite> in the <cite>IndexedSlices</cite>.
One of <cite>tf.int32</cite>, <cite>tf.int64</cite>, or <cite>None</cite> (if the <cite>IndexedSlices</cite> has
no <cite>dense_shape</cite> tensor).</p></li>
<li><p><strong>indices_shape</strong> -- The shape of the <cite>indices</cite> component, which indicates
how many slices are in the <cite>IndexedSlices</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.IndexedSlicesSpec.value_type">
<em class="property">property </em><code class="sig-name descname">value_type</code><a class="headerlink" href="#tensorflow.IndexedSlicesSpec.value_type" title="Permalink to this definition">¶</a></dt>
<dd><p>The Python type for values that are compatible with this TypeSpec.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.Module">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">Module</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/module/module.html#Module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.training.tracking.tracking.AutoTrackable</span></code></p>
<p>Base neural network module class.</p>
<p>A module is a named container for <cite>tf.Variable`s, other `tf.Module`s and
functions which apply to user input. For example a dense layer in a neural
network might be implemented as a `tf.Module</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="gp">... </span>    <span class="nb">super</span><span class="p">(</span><span class="n">Dense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
<span class="gp">... </span>      <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">out_features</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>You can use the Dense layer as you would expect:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=..., dtype=float32)&gt;</span>
</pre></div>
</div>
<p>By subclassing <cite>tf.Module</cite> instead of <cite>object</cite> any <cite>tf.Variable</cite> or
<cite>tf.Module</cite> instances assigned to object properties can be collected using
the <cite>variables</cite>, <cite>trainable_variables</cite> or <cite>submodules</cite> property:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">variables</span>
<span class="go">    (&lt;tf.Variable &#39;b:0&#39; shape=(2,) dtype=float32, numpy=...,</span>
<span class="go">    dtype=float32)&gt;,</span>
<span class="go">    &lt;tf.Variable &#39;w:0&#39; shape=(3, 2) dtype=float32, numpy=..., dtype=float32)&gt;)</span>
</pre></div>
</div>
<p>Subclasses of <cite>tf.Module</cite> can also take advantage of the <cite>_flatten</cite> method
which can be used to implement tracking of any other types.</p>
<p>All <cite>tf.Module</cite> classes have an associated <cite>tf.name_scope</cite> which can be used
to group operations in TensorBoard and create hierarchies for variable names
which can help with debugging. We suggest using the name scope when creating
nested submodules/parameters or for forward methods whose graph you might want
to inspect in TensorBoard. You can enter the name scope explicitly using
<cite>with self.name_scope:</cite> or you can annotate methods (apart from <cite>__init__</cite>)
with <cite>&#64;tf.Module.with_name_scope</cite>.</p>
<p><a href="#id216"><span class="problematic" id="id217">``</span></a><a href="#id218"><span class="problematic" id="id219">`</span></a>python
class MLP(tf.Module):</p>
<blockquote>
<div><dl>
<dt>def __init__(self, input_size, sizes, name=None):</dt><dd><p>super(MLP, self).__init__(name=name)
self.layers = []
with self.name_scope:</p>
<blockquote>
<div><dl class="simple">
<dt>for size in sizes:</dt><dd><p>self.layers.append(Dense(input_size=input_size, output_size=size))
input_size = size</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
<p>&#64;tf.Module.with_name_scope
def __call__(self, x):</p>
<blockquote>
<div><dl class="simple">
<dt>for layer in self.layers:</dt><dd><p>x = layer(x)</p>
</dd>
</dl>
<p>return x</p>
</div></blockquote>
</div></blockquote>
<p><a href="#id220"><span class="problematic" id="id221">``</span></a><a href="#id222"><span class="problematic" id="id223">`</span></a></p>
<dl class="py method">
<dt id="tensorflow.Module.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#tensorflow.Module.name" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the name of this module as passed or determined in the ctor.</p>
<p>NOTE: This is not the same as the <cite>self.name_scope.name</cite> which includes
parent module names.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Module.name_scope">
<em class="property">property </em><code class="sig-name descname">name_scope</code><a class="headerlink" href="#tensorflow.Module.name_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <cite>tf.name_scope</cite> instance for this class.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Module.submodules">
<em class="property">property </em><code class="sig-name descname">submodules</code><a class="headerlink" href="#tensorflow.Module.submodules" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequence of all sub-modules.</p>
<p>Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">submodules</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">submodules</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">c</span><span class="p">]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">submodules</span><span class="p">)</span> <span class="o">==</span> <span class="p">[]</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A sequence of all submodules.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Module.trainable_variables">
<em class="property">property </em><code class="sig-name descname">trainable_variables</code><a class="headerlink" href="#tensorflow.Module.trainable_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequence of trainable variables owned by this module and its submodules.</p>
<p>Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A sequence of variables for the current module (sorted by attribute
name) followed by variables from all submodules recursively (breadth
first).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Module.variables">
<em class="property">property </em><code class="sig-name descname">variables</code><a class="headerlink" href="#tensorflow.Module.variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequence of variables owned by this module and its submodules.</p>
<p>Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A sequence of variables for the current module (sorted by attribute
name) followed by variables from all submodules recursively (breadth
first).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Module.with_name_scope">
<em class="property">classmethod </em><code class="sig-name descname">with_name_scope</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">method</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/module/module.html#Module.with_name_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Module.with_name_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>Decorator to automatically enter the module name scope.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="nd">@tf</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="n">with_name_scope</span>
<span class="gp">... </span>  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">):</span>
<span class="gp">... </span>      <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">]))</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the above module would produce <a href="#id224"><span class="problematic" id="id225">`</span></a>tf.Variable`s and <a href="#id226"><span class="problematic" id="id227">`</span></a>tf.Tensor`s whose
names included the module name:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mod</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mod</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="go">&lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mod</span><span class="o">.</span><span class="n">w</span>
<span class="go">&lt;tf.Variable &#39;my_module/Variable:0&#39; shape=(2, 3) dtype=float32,</span>
<span class="go">numpy=..., dtype=float32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>method</strong> -- The method to wrap.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The original method wrapped such that it enters the module's name scope.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.Operation">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">Operation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node_def</span></em>, <em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">inputs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_types</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">control_inputs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_types</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">original_op</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">op_def</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Operation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Operation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Represents a graph node that performs computation on tensors.</p>
<p>An <cite>Operation</cite> is a node in a <cite>tf.Graph</cite> that takes zero or more <cite>Tensor</cite>
objects as input, and produces zero or more <cite>Tensor</cite> objects as output.
Objects of type <cite>Operation</cite> are created by calling a Python op constructor
(such as <cite>tf.matmul</cite>) within a <cite>tf.function</cite> or under a <cite>tf.Graph.as_default</cite>
context manager.</p>
<p>For example, within a <cite>tf.function</cite>, <cite>c = tf.matmul(a, b)</cite> creates an
<cite>Operation</cite> of type &quot;MatMul&quot; that takes tensors <cite>a</cite> and <cite>b</cite> as input, and
produces <cite>c</cite> as output.</p>
<p>If a <cite>tf.compat.v1.Session</cite> is used, an <cite>Operation</cite> of a <cite>tf.Graph</cite> can be
executed by passing it to <cite>tf.Session.run</cite>. <cite>op.run()</cite> is a shortcut for
calling <cite>tf.compat.v1.get_default_session().run(op)</cite>.</p>
<p>Creates an <cite>Operation</cite>.</p>
<p>NOTE: This constructor validates the name of the <cite>Operation</cite> (passed
as <cite>node_def.name</cite>). Valid <cite>Operation</cite> names match the following
regular expression:</p>
<blockquote>
<div><p>[A-Za-z0-9.][<a href="#id1104"><span class="problematic" id="id1105">A-Za-z0-9_</span></a>.\-/]*</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>node_def</strong> -- <cite>node_def_pb2.NodeDef</cite>.  <cite>NodeDef</cite> for the <cite>Operation</cite>. Used for
attributes of <cite>node_def_pb2.NodeDef</cite>, typically <cite>name</cite>, <cite>op</cite>, and
<cite>device</cite>.  The <cite>input</cite> attribute is irrelevant here as it will be
computed when generating the model.</p></li>
<li><p><strong>g</strong> -- <cite>Graph</cite>. The parent graph.</p></li>
<li><p><strong>inputs</strong> -- list of <cite>Tensor</cite> objects. The inputs to this <cite>Operation</cite>.</p></li>
<li><p><strong>output_types</strong> -- list of <cite>DType</cite> objects.  List of the types of the <cite>Tensors</cite>
computed by this operation.  The length of this list indicates the
number of output endpoints of the <cite>Operation</cite>.</p></li>
<li><p><strong>control_inputs</strong> -- list of operations or tensors from which to have a control
dependency.</p></li>
<li><p><strong>input_types</strong> -- List of <cite>DType</cite> objects representing the types of the tensors
accepted by the <cite>Operation</cite>.  By default uses <cite>[x.dtype.base_dtype for x
in inputs]</cite>.  Operations that expect reference-typed inputs must specify
these explicitly.</p></li>
<li><p><strong>original_op</strong> -- Optional. Used to associate the new <cite>Operation</cite> with an
existing <cite>Operation</cite> (for example, a replica with the op that was
replicated).</p></li>
<li><p><strong>op_def</strong> -- Optional. The <cite>op_def_pb2.OpDef</cite> proto that describes the op type
that this <cite>Operation</cite> represents.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> -- if control inputs are not Operations or Tensors,
    or if <cite>node_def</cite> is not a <cite>NodeDef</cite>,
    or if <cite>g</cite> is not a <cite>Graph</cite>,
    or if <cite>inputs</cite> are not tensors,
    or if <cite>inputs</cite> and <cite>input_types</cite> are incompatible.</p></li>
<li><p><strong>ValueError</strong> -- if the <cite>node_def</cite> name is not valid.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.Operation.colocation_groups">
<code class="sig-name descname">colocation_groups</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Operation.colocation_groups"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Operation.colocation_groups" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the list of colocation groups of the op.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.control_inputs">
<em class="property">property </em><code class="sig-name descname">control_inputs</code><a class="headerlink" href="#tensorflow.Operation.control_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Operation</cite> objects on which this op has a control dependency.</p>
<p>Before this op is executed, TensorFlow will ensure that the
operations in <cite>self.control_inputs</cite> have finished executing. This
mechanism can be used to run ops sequentially for performance
reasons, or to ensure that the side effects of an op are observed
in the correct order.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of <cite>Operation</cite> objects.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.device">
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#tensorflow.Operation.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of the device to which this op has been assigned, if any.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The string name of the device to which this op has been
assigned, or an empty string if it has not been assigned to a
device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.get_attr">
<code class="sig-name descname">get_attr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Operation.get_attr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Operation.get_attr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the attr of this op with the given <cite>name</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- The name of the attr to fetch.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The value of the attr, as a Python object.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If this op does not have an attr with the given <cite>name</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.graph">
<em class="property">property </em><code class="sig-name descname">graph</code><a class="headerlink" href="#tensorflow.Operation.graph" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Graph</cite> that contains this operation.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.inputs">
<em class="property">property </em><code class="sig-name descname">inputs</code><a class="headerlink" href="#tensorflow.Operation.inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>The sequence of <cite>Tensor</cite> objects representing the data inputs of this op.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#tensorflow.Operation.name" title="Permalink to this definition">¶</a></dt>
<dd><p>The full name of this operation.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.node_def">
<em class="property">property </em><code class="sig-name descname">node_def</code><a class="headerlink" href="#tensorflow.Operation.node_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <cite>NodeDef</cite> representation of this operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A
[<cite>NodeDef</cite>](<a class="reference external" href="https://www.tensorflow.org/code/tensorflow/core/framework/node_def.proto">https://www.tensorflow.org/code/tensorflow/core/framework/node_def.proto</a>)
protocol buffer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.op_def">
<em class="property">property </em><code class="sig-name descname">op_def</code><a class="headerlink" href="#tensorflow.Operation.op_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <cite>OpDef</cite> proto that represents the type of this op.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An
[<cite>OpDef</cite>](<a class="reference external" href="https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto">https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto</a>)
protocol buffer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.outputs">
<em class="property">property </em><code class="sig-name descname">outputs</code><a class="headerlink" href="#tensorflow.Operation.outputs" title="Permalink to this definition">¶</a></dt>
<dd><p>The list of <cite>Tensor</cite> objects representing the outputs of this op.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feed_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">session</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Operation.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Operation.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs this operation in a <cite>Session</cite>.</p>
<p>Calling this method will execute all preceding operations that
produce the inputs needed for this operation.</p>
<p><em>N.B.</em> Before invoking <cite>Operation.run()</cite>, its graph must have been
launched in a session, and either a default session must be
available, or <cite>session</cite> must be specified explicitly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feed_dict</strong> -- A dictionary that maps <cite>Tensor</cite> objects to feed values. See
<cite>tf.Session.run</cite> for a description of the valid feed values.</p></li>
<li><p><strong>session</strong> -- (Optional.) The <cite>Session</cite> to be used to run to this operation. If
none, the default session will be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.traceback">
<em class="property">property </em><code class="sig-name descname">traceback</code><a class="headerlink" href="#tensorflow.Operation.traceback" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the call stack from when this operation was constructed.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.type">
<em class="property">property </em><code class="sig-name descname">type</code><a class="headerlink" href="#tensorflow.Operation.type" title="Permalink to this definition">¶</a></dt>
<dd><p>The type of the op (e.g. <cite>&quot;MatMul&quot;</cite>).</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Operation.values">
<code class="sig-name descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Operation.values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Operation.values" title="Permalink to this definition">¶</a></dt>
<dd><p>DEPRECATED: Use outputs.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.OptionalSpec">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">OptionalSpec</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value_structure</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/data/ops/optional_ops.html#OptionalSpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.OptionalSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.type_spec.TypeSpec</span></code></p>
<p>Represents an optional potentially containing a structured value.</p>
<dl class="py method">
<dt id="tensorflow.OptionalSpec.from_value">
<em class="property">static </em><code class="sig-name descname">from_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/data/ops/optional_ops.html#OptionalSpec.from_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.OptionalSpec.from_value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.OptionalSpec.value_type">
<em class="property">property </em><code class="sig-name descname">value_type</code><a class="headerlink" href="#tensorflow.OptionalSpec.value_type" title="Permalink to this definition">¶</a></dt>
<dd><p>The Python type for values that are compatible with this TypeSpec.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.RaggedTensor">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">RaggedTensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">row_splits</span></em>, <em class="sig-param"><span class="n">cached_row_lengths</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cached_value_rowids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cached_nrows</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">internal</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">uniform_row_length</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.composite_tensor.CompositeTensor</span></code></p>
<p>Represents a ragged tensor.</p>
<p>A <cite>RaggedTensor</cite> is a tensor with one or more <em>ragged dimensions</em>, which are
dimensions whose slices may have different lengths.  For example, the inner
(column) dimension of <cite>rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</cite> is ragged,
since the column slices (<cite>rt[0, :]</cite>, ..., <cite>rt[4, :]</cite>) have different lengths.
Dimensions whose slices all have the same length are called <em>uniform
dimensions</em>.  The outermost dimension of a <cite>RaggedTensor</cite> is always uniform,
since it consists of a single slice (and so there is no possibility for
differing slice lengths).</p>
<p>The total number of dimensions in a <cite>RaggedTensor</cite> is called its <em>rank</em>,
and the number of ragged dimensions in a <cite>RaggedTensor</cite> is called its
<em>ragged-rank</em>.  A <cite>RaggedTensor</cite>'s ragged-rank is fixed at graph creation
time: it can't depend on the runtime values of <a href="#id228"><span class="problematic" id="id229">`</span></a>Tensor`s, and can't vary
dynamically for different session runs.</p>
<p>### Potentially Ragged Tensors</p>
<p>Many ops support both <cite>Tensor`s and `RaggedTensor`s.  The term &quot;potentially
ragged tensor&quot; may be used to refer to a tensor that might be either a
`Tensor</cite> or a <cite>RaggedTensor</cite>.  The ragged-rank of a <cite>Tensor</cite> is zero.</p>
<p>### Documenting RaggedTensor Shapes</p>
<p>When documenting the shape of a RaggedTensor, ragged dimensions can be
indicated by enclosing them in parentheses.  For example, the shape of
a 3-D <cite>RaggedTensor</cite> that stores the fixed-size word embedding for each
word in a sentence, for each sentence in a batch, could be written as
<cite>[num_sentences, (num_words), embedding_size]</cite>.  The parentheses around
<cite>(num_words)</cite> indicate that dimension is ragged, and that the length
of each element list in that dimension may vary for each item.</p>
<p>### Component Tensors</p>
<p>Internally, a <cite>RaggedTensor</cite> consists of a concatenated list of values that
are partitioned into variable-length rows.  In particular, each <cite>RaggedTensor</cite>
consists of:</p>
<blockquote>
<div><ul class="simple">
<li><p>A <cite>values</cite> tensor, which concatenates the variable-length rows into a
flattened list.  For example, the <cite>values</cite> tensor for
<cite>[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</cite> is <cite>[3, 1, 4, 1, 5, 9, 2, 6]</cite>.</p></li>
<li><p>A <cite>row_splits</cite> vector, which indicates how those flattened values are
divided into rows.  In particular, the values for row <cite>rt[i]</cite> are stored
in the slice <cite>rt.values[rt.row_splits[i]:rt.row_splits[i+1]]</cite>.</p></li>
</ul>
</div></blockquote>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span>
<span class="gp">... </span>      <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>      <span class="n">row_splits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]))</span>
<span class="go">&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</span>
</pre></div>
</div>
<p>### Alternative Row-Partitioning Schemes</p>
<p>In addition to <cite>row_splits</cite>, ragged tensors provide support for four other
row-partitioning schemes:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>row_lengths</cite>: a vector with shape <cite>[nrows]</cite>, which specifies the length
of each row.</p></li>
<li><p><cite>value_rowids</cite> and <cite>nrows</cite>: <cite>value_rowids</cite> is a vector with shape
<cite>[nvals]</cite>, corresponding one-to-one with <cite>values</cite>, which specifies
each value's row index.  In particular, the row <cite>rt[row]</cite> consists of the
values <cite>rt.values[j]</cite> where <cite>value_rowids[j]==row</cite>.  <cite>nrows</cite> is an
integer scalar that specifies the number of rows in the
<cite>RaggedTensor</cite>. (<cite>nrows</cite> is used to indicate trailing empty rows.)</p></li>
<li><p><cite>row_starts</cite>: a vector with shape <cite>[nrows]</cite>, which specifies the start
offset of each row.  Equivalent to <cite>row_splits[:-1]</cite>.</p></li>
<li><p><cite>row_limits</cite>: a vector with shape <cite>[nrows]</cite>, which specifies the stop
offset of each row.  Equivalent to <cite>row_splits[1:]</cite>.</p></li>
<li><p><cite>uniform_row_length</cite>: A scalar tensor, specifying the length of every
row.  This row-partitioning scheme may only be used if all rows have
the same length.</p></li>
</ul>
</div></blockquote>
<p>Example: The following ragged tensors are equivalent, and all represent the
nested list <cite>[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt1</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">row_splits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt2</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_lengths</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">row_lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt3</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_value_rowids</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="p">,</span> <span class="n">value_rowids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt4</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_starts</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">row_starts</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt5</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_limits</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">row_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
<p>### Multiple Ragged Dimensions</p>
<p><cite>RaggedTensor`s with multiple ragged dimensions can be defined by using
a nested `RaggedTensor</cite> for the <cite>values</cite> tensor.  Each nested <cite>RaggedTensor</cite>
adds a single ragged dimension.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inner_rt</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span>  <span class="c1"># =rt1 from above</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">row_splits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outer_rt</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="n">inner_rt</span><span class="p">,</span> <span class="n">row_splits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">outer_rt</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
<span class="go">[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">outer_rt</span><span class="o">.</span><span class="n">ragged_rank</span><span class="p">)</span>
<span class="go">2</span>
</pre></div>
</div>
<p>The factory function <cite>RaggedTensor.from_nested_row_splits</cite> may be used to
construct a <cite>RaggedTensor</cite> with multiple ragged dimensions directly, by
providing a list of <cite>row_splits</cite> tensors:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_nested_row_splits</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">flat_values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">nested_row_splits</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]))</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="go">[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]</span>
</pre></div>
</div>
<p>### Uniform Inner Dimensions</p>
<p><cite>RaggedTensor`s with uniform inner dimensions can be defined
by using a multidimensional `Tensor</cite> for <cite>values</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
<span class="gp">... </span>                                  <span class="n">row_splits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
<span class="go">[[[1, 1, 1], [1, 1, 1]],</span>
<span class="go"> [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, None, 3)</span>
</pre></div>
</div>
<p>### Uniform Outer Dimensions</p>
<p><cite>RaggedTensor`s with uniform outer dimensions can be defined by using
one or more `RaggedTensor</cite> with a <cite>uniform_row_length</cite> row-partitioning
tensor.  For example, a <cite>RaggedTensor</cite> with shape <cite>[2, 2, None]</cite> can be
constructed with this method from a <cite>RaggedTensor</cite> values with shape
<cite>[4, None]</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(4, None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt6</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_uniform_row_length</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt6</span><span class="p">)</span>
<span class="go">&lt;tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt6</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, None)</span>
</pre></div>
</div>
<p>Note that <cite>rt6</cite> only contains one ragged dimension (the innermost
dimension). In contrast, if <cite>from_row_splits</cite> is used to construct a similar
<cite>RaggedTensor</cite>, then that <cite>RaggedTensor</cite> will have two ragged dimensions:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt7</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt7</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, None, None)</span>
</pre></div>
</div>
<p>Uniform and ragged outer dimensions may be interleaved, meaning that a
tensor with any combination of ragged and uniform dimensions may be created.
For example, a RaggedTensor <cite>t4</cite> with shape <cite>[3, None, 4, 8, None, 2]</cite> could
be constructed as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">t0</span> <span class="pre">=</span> <span class="pre">tf.zeros([1000,</span> <span class="pre">2])</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">Shape:</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">[1000,</span> <span class="pre">2]</span>
<span class="pre">t1</span> <span class="pre">=</span> <span class="pre">RaggedTensor.from_row_lengths(t0,</span> <span class="pre">[...])</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">[160,</span> <span class="pre">None,</span> <span class="pre">2]</span>
<span class="pre">t2</span> <span class="pre">=</span> <span class="pre">RaggedTensor.from_uniform_row_length(t1,</span> <span class="pre">8)</span>&#160;&#160; <span class="pre">#</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">[20,</span> <span class="pre">8,</span> <span class="pre">None,</span> <span class="pre">2]</span>
<span class="pre">t3</span> <span class="pre">=</span> <span class="pre">RaggedTensor.from_uniform_row_length(t2,</span> <span class="pre">4)</span>&#160;&#160; <span class="pre">#</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">[5,</span> <span class="pre">4,</span> <span class="pre">8,</span> <span class="pre">None,</span> <span class="pre">2]</span>
<span class="pre">t4</span> <span class="pre">=</span> <span class="pre">RaggedTensor.from_row_lengths(t3,</span> <span class="pre">[...])</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">[3,</span> <span class="pre">None,</span> <span class="pre">4,</span> <span class="pre">8,</span> <span class="pre">None,</span> <span class="pre">2]</span>
<span class="pre">`</span></code></p>
<p>Creates a <cite>RaggedTensor</cite> with a specified partitioning for <cite>values</cite>.</p>
<p>This constructor is private -- please use one of the following ops to
build <a href="#id230"><span class="problematic" id="id231">`</span></a>RaggedTensor`s:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>tf.RaggedTensor.from_row_lengths</cite></p></li>
<li><p><cite>tf.RaggedTensor.from_value_rowids</cite></p></li>
<li><p><cite>tf.RaggedTensor.from_row_splits</cite></p></li>
<li><p><cite>tf.RaggedTensor.from_row_starts</cite></p></li>
<li><p><cite>tf.RaggedTensor.from_row_limits</cite></p></li>
<li><p><cite>tf.RaggedTensor.from_nested_row_splits</cite></p></li>
<li><p><cite>tf.RaggedTensor.from_nested_row_lengths</cite></p></li>
<li><p><cite>tf.RaggedTensor.from_nested_value_rowids</cite></p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A potentially ragged tensor of any dtype and shape <cite>[nvals, ...]</cite>.</p></li>
<li><p><strong>row_splits</strong> -- A 1-D integer tensor with shape <cite>[nrows+1]</cite>.</p></li>
<li><p><strong>cached_row_lengths</strong> -- A 1-D integer tensor with shape <cite>[nrows]</cite></p></li>
<li><p><strong>cached_value_rowids</strong> -- A 1-D integer tensor with shape <cite>[nvals]</cite>.</p></li>
<li><p><strong>cached_nrows</strong> -- A 1-D integer scalar tensor.</p></li>
<li><p><strong>internal</strong> -- True if the constructor is being called by one of the factory
methods.  If false, an exception will be raised.</p></li>
<li><p><strong>uniform_row_length</strong> -- A scalar tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> -- If a row partitioning tensor has an inappropriate dtype.</p></li>
<li><p><strong>TypeError</strong> -- If exactly one row partitioning argument was not specified.</p></li>
<li><p><strong>ValueError</strong> -- If a row partitioning tensor has an inappropriate shape.</p></li>
<li><p><strong>ValueError</strong> -- If multiple partitioning arguments are specified.</p></li>
<li><p><strong>ValueError</strong> -- If nrows is specified but value_rowids is not None.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.RaggedTensor.bounding_shape">
<code class="sig-name descname">bounding_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">out_type</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.bounding_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.bounding_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the tight bounding box shape for this <cite>RaggedTensor</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> -- An integer scalar or vector indicating which axes to return the
bounding box for.  If not specified, then the full bounding box is
returned.</p></li>
<li><p><strong>name</strong> -- A name prefix for the returned tensor (optional).</p></li>
<li><p><strong>out_type</strong> -- <cite>dtype</cite> for the returned tensor.  Defaults to
<cite>self.row_splits.dtype</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An integer <cite>Tensor</cite> (<cite>dtype=self.row_splits.dtype</cite>).  If <cite>axis</cite> is not
specified, then <cite>output</cite> is a vector with
<cite>output.shape=[self.shape.ndims]</cite>.  If <cite>axis</cite> is a scalar, then the
<cite>output</cite> is a scalar.  If <cite>axis</cite> is a vector, then <cite>output</cite> is a vector,
where <cite>output[i]</cite> is the bounding size for dimension <cite>axis[i]</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span><span class="o">.</span><span class="n">bounding_shape</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([5, 4])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.consumers">
<code class="sig-name descname">consumers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.consumers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.consumers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#tensorflow.RaggedTensor.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>DType</cite> of values in this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.flat_values">
<em class="property">property </em><code class="sig-name descname">flat_values</code><a class="headerlink" href="#tensorflow.RaggedTensor.flat_values" title="Permalink to this definition">¶</a></dt>
<dd><p>The innermost <cite>values</cite> tensor for this ragged tensor.</p>
<p>Concretely, if <cite>rt.values</cite> is a <cite>Tensor</cite>, then <cite>rt.flat_values</cite> is
<cite>rt.values</cite>; otherwise, <cite>rt.flat_values</cite> is <cite>rt.values.flat_values</cite>.</p>
<p>Conceptually, <cite>flat_values</cite> is the tensor formed by flattening the
outermost dimension and all of the ragged dimensions into a single
dimension.</p>
<p><cite>rt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:]</cite>
(where <cite>nvals</cite> is the number of items in the flattened dimensions).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>Tensor</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">flat_values</span><span class="p">)</span>
<span class="go">tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_nested_row_lengths">
<em class="property">classmethod </em><code class="sig-name descname">from_nested_row_lengths</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">flat_values</span></em>, <em class="sig-param"><span class="n">nested_row_lengths</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_nested_row_lengths"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_nested_row_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> from a nested list of <cite>row_lengths</cite> tensors.</p>
<p>Equivalent to:</p>
<p><a href="#id232"><span class="problematic" id="id233">``</span></a><a href="#id234"><span class="problematic" id="id235">`</span></a>python
result = flat_values
for row_lengths in reversed(nested_row_lengths):</p>
<blockquote>
<div><p>result = from_row_lengths(result, row_lengths)</p>
</div></blockquote>
<p><a href="#id236"><span class="problematic" id="id237">``</span></a><a href="#id238"><span class="problematic" id="id239">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flat_values</strong> -- A potentially ragged tensor.</p></li>
<li><p><strong>nested_row_lengths</strong> -- A list of 1-D integer tensors.  The <cite>i`th tensor is
used as the `row_lengths</cite> for the <a href="#id240"><span class="problematic" id="id241">`</span></a>i`th ragged dimension.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite> (or <cite>flat_values</cite> if <cite>nested_row_lengths</cite> is empty).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_nested_row_splits">
<em class="property">classmethod </em><code class="sig-name descname">from_nested_row_splits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">flat_values</span></em>, <em class="sig-param"><span class="n">nested_row_splits</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_nested_row_splits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_nested_row_splits" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> from a nested list of <cite>row_splits</cite> tensors.</p>
<p>Equivalent to:</p>
<p><a href="#id242"><span class="problematic" id="id243">``</span></a><a href="#id244"><span class="problematic" id="id245">`</span></a>python
result = flat_values
for row_splits in reversed(nested_row_splits):</p>
<blockquote>
<div><p>result = from_row_splits(result, row_splits)</p>
</div></blockquote>
<p><a href="#id246"><span class="problematic" id="id247">``</span></a><a href="#id248"><span class="problematic" id="id249">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flat_values</strong> -- A potentially ragged tensor.</p></li>
<li><p><strong>nested_row_splits</strong> -- A list of 1-D integer tensors.  The <cite>i`th tensor is
used as the `row_splits</cite> for the <a href="#id250"><span class="problematic" id="id251">`</span></a>i`th ragged dimension.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite> (or <cite>flat_values</cite> if <cite>nested_row_splits</cite> is empty).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_nested_value_rowids">
<em class="property">classmethod </em><code class="sig-name descname">from_nested_value_rowids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">flat_values</span></em>, <em class="sig-param"><span class="n">nested_value_rowids</span></em>, <em class="sig-param"><span class="n">nested_nrows</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_nested_value_rowids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_nested_value_rowids" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> from a nested list of <cite>value_rowids</cite> tensors.</p>
<p>Equivalent to:</p>
<p><a href="#id252"><span class="problematic" id="id253">``</span></a><a href="#id254"><span class="problematic" id="id255">`</span></a>python
result = flat_values
for (rowids, nrows) in reversed(zip(nested_value_rowids, nested_nrows)):</p>
<blockquote>
<div><p>result = from_value_rowids(result, rowids, nrows)</p>
</div></blockquote>
<p><a href="#id256"><span class="problematic" id="id257">``</span></a><a href="#id258"><span class="problematic" id="id259">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>flat_values</strong> -- A potentially ragged tensor.</p></li>
<li><p><strong>nested_value_rowids</strong> -- A list of 1-D integer tensors.  The <cite>i`th tensor is
used as the `value_rowids</cite> for the <a href="#id260"><span class="problematic" id="id261">`</span></a>i`th ragged dimension.</p></li>
<li><p><strong>nested_nrows</strong> -- A list of integer scalars.  The <cite>i`th scalar is used as the
`nrows</cite> for the <a href="#id262"><span class="problematic" id="id263">`</span></a>i`th ragged dimension.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite> (or <cite>flat_values</cite> if <cite>nested_value_rowids</cite> is empty).</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>len(nested_values_rowids) != len(nested_nrows)</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_row_lengths">
<em class="property">classmethod </em><code class="sig-name descname">from_row_lengths</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">row_lengths</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_row_lengths"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_row_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> with rows partitioned by <cite>row_lengths</cite>.</p>
<p>The returned <cite>RaggedTensor</cite> corresponds with the python list defined by:</p>
<p><a href="#id264"><span class="problematic" id="id265">``</span></a><a href="#id266"><span class="problematic" id="id267">`</span></a>python
result = [[values.pop(0) for i in range(length)]</p>
<blockquote>
<div><p>for length in row_lengths]</p>
</div></blockquote>
<p><a href="#id268"><span class="problematic" id="id269">``</span></a><a href="#id270"><span class="problematic" id="id271">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A potentially ragged tensor with shape <cite>[nvals, ...]</cite>.</p></li>
<li><p><strong>row_lengths</strong> -- A 1-D integer tensor with shape <cite>[nrows]</cite>.  Must be
nonnegative.  <cite>sum(row_lengths)</cite> must be <cite>nvals</cite>.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite>.  <cite>result.rank = values.rank + 1</cite>.
<cite>result.ragged_rank = values.ragged_rank + 1</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_lengths</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">row_lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="go">&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_row_limits">
<em class="property">classmethod </em><code class="sig-name descname">from_row_limits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">row_limits</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_row_limits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_row_limits" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> with rows partitioned by <cite>row_limits</cite>.</p>
<p>Equivalent to: <cite>from_row_splits(values, concat([0, row_limits]))</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A potentially ragged tensor with shape <cite>[nvals, ...]</cite>.</p></li>
<li><p><strong>row_limits</strong> -- A 1-D integer tensor with shape <cite>[nrows]</cite>.  Must be sorted in
ascending order.  If <cite>nrows&gt;0</cite>, then <cite>row_limits[-1]</cite> must be <cite>nvals</cite>.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite>.  <cite>result.rank = values.rank + 1</cite>.
<cite>result.ragged_rank = values.ragged_rank + 1</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_limits</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">row_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]))</span>
<span class="go">&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_row_splits">
<em class="property">classmethod </em><code class="sig-name descname">from_row_splits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">row_splits</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_row_splits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_row_splits" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> with rows partitioned by <cite>row_splits</cite>.</p>
<p>The returned <cite>RaggedTensor</cite> corresponds with the python list defined by:</p>
<p><a href="#id272"><span class="problematic" id="id273">``</span></a><a href="#id274"><span class="problematic" id="id275">`</span></a>python
result = [values[row_splits[i]:row_splits[i + 1]]</p>
<blockquote>
<div><p>for i in range(len(row_splits) - 1)]</p>
</div></blockquote>
<p><a href="#id276"><span class="problematic" id="id277">``</span></a><a href="#id278"><span class="problematic" id="id279">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A potentially ragged tensor with shape <cite>[nvals, ...]</cite>.</p></li>
<li><p><strong>row_splits</strong> -- A 1-D integer tensor with shape <cite>[nrows+1]</cite>.  Must not be
empty, and must be sorted in ascending order.  <cite>row_splits[0]</cite> must be
zero and <cite>row_splits[-1]</cite> must be <cite>nvals</cite>.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite>.  <cite>result.rank = values.rank + 1</cite>.
<cite>result.ragged_rank = values.ragged_rank + 1</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>row_splits</cite> is an empty list.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">row_splits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]))</span>
<span class="go">&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_row_starts">
<em class="property">classmethod </em><code class="sig-name descname">from_row_starts</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">row_starts</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_row_starts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_row_starts" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> with rows partitioned by <cite>row_starts</cite>.</p>
<p>Equivalent to: <cite>from_row_splits(values, concat([row_starts, nvals]))</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A potentially ragged tensor with shape <cite>[nvals, ...]</cite>.</p></li>
<li><p><strong>row_starts</strong> -- A 1-D integer tensor with shape <cite>[nrows]</cite>.  Must be
nonnegative and sorted in ascending order.  If <cite>nrows&gt;0</cite>, then
<cite>row_starts[0]</cite> must be zero.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite>.  <cite>result.rank = values.rank + 1</cite>.
<cite>result.ragged_rank = values.ragged_rank + 1</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_starts</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">row_starts</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]))</span>
<span class="go">&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_sparse">
<em class="property">classmethod </em><code class="sig-name descname">from_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">st_input</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">row_splits_dtype</span><span class="o">=</span><span class="default_value">tf.int64</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_sparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a 2D <cite>tf.SparseTensor</cite> to a <cite>RaggedTensor</cite>.</p>
<p>Each row of the <cite>output</cite> <cite>RaggedTensor</cite> will contain the explicit values
from the same row in <cite>st_input</cite>.  <cite>st_input</cite> must be ragged-right.  If not
it is not ragged-right, then an error will be generated.</p>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">st</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="gp">... </span>                     <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>                     <span class="n">dense_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_sparse</span><span class="p">(</span><span class="n">st</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="go">[[1, 2, 3], [4], [], [5]]</span>
</pre></div>
</div>
<p>Currently, only two-dimensional <cite>SparseTensors</cite> are supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>st_input</strong> -- The sparse tensor to convert.  Must have rank 2.</p></li>
<li><p><strong>name</strong> -- A name prefix for the returned tensors (optional).</p></li>
<li><p><strong>row_splits_dtype</strong> -- <cite>dtype</cite> for the returned <cite>RaggedTensor</cite>'s <cite>row_splits</cite>
tensor.  One of <cite>tf.int32</cite> or <cite>tf.int64</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite> with the same values as <cite>st_input</cite>.
<cite>output.ragged_rank = rank(st_input) - 1</cite>.
<cite>output.shape = [st_input.dense_shape[0], None]</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If the number of dimensions in <cite>st_input</cite> is not known
    statically, or is not two.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_tensor">
<em class="property">classmethod </em><code class="sig-name descname">from_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">lengths</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">padding</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ragged_rank</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">row_splits_dtype</span><span class="o">=</span><span class="default_value">tf.int64</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a <cite>tf.Tensor</cite> into a <cite>RaggedTensor</cite>.</p>
<p>The set of absent/default values may be specified using a vector of lengths
or a padding value (but not both).  If <cite>lengths</cite> is specified, then the
output tensor will satisfy <cite>output[row] = tensor[row][:lengths[row]]</cite>. If
'lengths' is a list of lists or tuple of lists, those lists will be used
as nested row lengths. If <cite>padding</cite> is specified, then any row <em>suffix</em>
consisting entirely of <cite>padding</cite> will be excluded from the returned
<cite>RaggedTensor</cite>.  If neither <cite>lengths</cite> nor <cite>padding</cite> is specified, then the
returned <cite>RaggedTensor</cite> will have no absent/default values.</p>
<p>Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
<span class="go">&lt;tf.RaggedTensor [[5, 7, 0], [0, 3, 0], [6, 0, 0]]&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">&lt;tf.RaggedTensor [[5], [], [6, 0, 0]]&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.RaggedTensor [[5, 7], [0, 3], [6]]&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="gp">... </span>                  <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="gp">... </span>                  <span class="p">[[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">&lt;tf.RaggedTensor [[[5], [7]], [], [[6, 0], [], [0]]]&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- The <cite>Tensor</cite> to convert.  Must have rank <cite>ragged_rank + 1</cite> or
higher.</p></li>
<li><p><strong>lengths</strong> -- An optional set of row lengths, specified using a 1-D integer
<cite>Tensor</cite> whose length is equal to <cite>tensor.shape[0]</cite> (the number of rows
in <cite>tensor</cite>).  If specified, then <cite>output[row]</cite> will contain
<cite>tensor[row][:lengths[row]]</cite>.  Negative lengths are treated as zero. You
may optionally pass a list or tuple of lengths to this argument, which
will be used as nested row lengths to construct a ragged tensor with
multiple ragged dimensions.</p></li>
<li><p><strong>padding</strong> -- An optional padding value.  If specified, then any row suffix
consisting entirely of <cite>padding</cite> will be excluded from the returned
RaggedTensor.  <cite>padding</cite> is a <cite>Tensor</cite> with the same dtype as <cite>tensor</cite>
and with <cite>shape=tensor.shape[ragged_rank + 1:]</cite>.</p></li>
<li><p><strong>ragged_rank</strong> -- Integer specifying the ragged rank for the returned
<cite>RaggedTensor</cite>.  Must be greater than zero.</p></li>
<li><p><strong>name</strong> -- A name prefix for the returned tensors (optional).</p></li>
<li><p><strong>row_splits_dtype</strong> -- <cite>dtype</cite> for the returned <cite>RaggedTensor</cite>'s <cite>row_splits</cite>
tensor.  One of <cite>tf.int32</cite> or <cite>tf.int64</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite> with the specified <cite>ragged_rank</cite>.  The shape of the
returned ragged tensor is compatible with the shape of <cite>tensor</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If both <cite>lengths</cite> and <cite>padding</cite> are specified.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_uniform_row_length">
<em class="property">classmethod </em><code class="sig-name descname">from_uniform_row_length</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">uniform_row_length</span></em>, <em class="sig-param"><span class="n">nrows</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_uniform_row_length"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_uniform_row_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> with rows partitioned by <cite>uniform_row_length</cite>.</p>
<p>This method can be used to create <cite>RaggedTensor`s with multiple uniform
outer dimensions.  For example, a `RaggedTensor</cite> with shape <cite>[2, 2, None]</cite>
can be constructed with this method from a <cite>RaggedTensor</cite> values with shape
<cite>[4, None]</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(4, None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rt1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_uniform_row_length</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt1</span><span class="p">)</span>
<span class="go">&lt;tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, None)</span>
</pre></div>
</div>
<p>Note that <cite>rt1</cite> only contains one ragged dimension (the innermost
dimension). In contrast, if <cite>from_row_splits</cite> is used to construct a similar
<cite>RaggedTensor</cite>, then that <cite>RaggedTensor</cite> will have two ragged dimensions:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, None, None)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A potentially ragged tensor with shape <cite>[nvals, ...]</cite>.</p></li>
<li><p><strong>uniform_row_length</strong> -- A scalar integer tensor.  Must be nonnegative.
The size of the outer axis of <cite>values</cite> must be evenly divisible by
<cite>uniform_row_length</cite>.</p></li>
<li><p><strong>nrows</strong> -- The number of rows in the constructed RaggedTensor.  If not
specified, then it defaults to <cite>nvals/uniform_row_length</cite> (or <cite>0</cite> if
<cite>uniform_row_length==0</cite>).  <cite>nrows</cite> only needs to be specified if
<cite>uniform_row_length</cite> might be zero.  <cite>uniform_row_length*nrows</cite> must
be <cite>nvals</cite>.</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><a href="#id280"><span class="problematic" id="id281">``</span></a><a href="#id282"><span class="problematic" id="id283">`</span></a>python
result = [[values.pop(0) for i in range(uniform_row_length)]</p>
<blockquote>
<div><p>for _ in range(nrows)]</p>
</div></blockquote>
<p><a href="#id284"><span class="problematic" id="id285">``</span></a><a href="#id286"><span class="problematic" id="id287">`</span></a></p>
<p><cite>result.rank = values.rank + 1</cite>.
<cite>result.ragged_rank = values.ragged_rank + 1</cite>.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A <cite>RaggedTensor</cite> that corresponds with the python list defined by</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.from_value_rowids">
<em class="property">classmethod </em><code class="sig-name descname">from_value_rowids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">value_rowids</span></em>, <em class="sig-param"><span class="n">nrows</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validate</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.from_value_rowids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.from_value_rowids" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <cite>RaggedTensor</cite> with rows partitioned by <cite>value_rowids</cite>.</p>
<p>The returned <cite>RaggedTensor</cite> corresponds with the python list defined by:</p>
<p><a href="#id288"><span class="problematic" id="id289">``</span></a><a href="#id290"><span class="problematic" id="id291">`</span></a>python
result = [[values[i] for i in range(len(values)) if value_rowids[i] == row]</p>
<blockquote>
<div><p>for row in range(nrows)]</p>
</div></blockquote>
<p><a href="#id292"><span class="problematic" id="id293">``</span></a><a href="#id294"><span class="problematic" id="id295">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A potentially ragged tensor with shape <cite>[nvals, ...]</cite>.</p></li>
<li><p><strong>value_rowids</strong> -- A 1-D integer tensor with shape <cite>[nvals]</cite>, which corresponds
one-to-one with <cite>values</cite>, and specifies each value's row index.  Must be
nonnegative, and must be sorted in ascending order.</p></li>
<li><p><strong>nrows</strong> -- An integer scalar specifying the number of rows.  This should be
specified if the <cite>RaggedTensor</cite> may containing empty training rows. Must
be greater than <cite>value_rowids[-1]</cite> (or zero if <cite>value_rowids</cite> is empty).
Defaults to <cite>value_rowids[-1]</cite> (or zero if <cite>value_rowids</cite> is empty).</p></li>
<li><p><strong>name</strong> -- A name prefix for the RaggedTensor (optional).</p></li>
<li><p><strong>validate</strong> -- If true, then use assertions to check that the arguments form
a valid <cite>RaggedTensor</cite>.  Note: these assertions incur a runtime cost,
since they must be checked for each tensor value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite>.  <cite>result.rank = values.rank + 1</cite>.
<cite>result.ragged_rank = values.ragged_rank + 1</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>nrows</cite> is incompatible with <cite>value_rowids</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_value_rowids</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">value_rowids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="go">&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.merge_dims">
<code class="sig-name descname">merge_dims</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outer_axis</span></em>, <em class="sig-param"><span class="n">inner_axis</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.merge_dims"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.merge_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>Merges outer_axis...inner_axis into a single dimension.</p>
<p>Returns a copy of this RaggedTensor with the specified range of dimensions
flattened into a single dimension, with elements in row-major order.</p>
<p>#### Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">merge_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">&lt;tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">merge_dims</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">&lt;tf.RaggedTensor [[1, 2, 3], [4, 5, 6]]&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">merge_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32)</span>
</pre></div>
</div>
<p>To mimic the behavior of <cite>np.flatten</cite> (which flattens all dimensions), use
<cite>rt.merge_dims(0, -1).  To mimic the behavior of `tf.layers.Flatten</cite> (which
flattens all dimensions except the outermost batch dimension), use
<cite>rt.merge_dims(1, -1)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outer_axis</strong> -- <cite>int</cite>: The first dimension in the range of dimensions to
merge. May be negative if <cite>self.shape.rank</cite> is statically known.</p></li>
<li><p><strong>inner_axis</strong> -- <cite>int</cite>: The last dimension in the range of dimensions to
merge. May be negative if <cite>self.shape.rank</cite> is statically known.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A copy of this tensor, with the specified dimensions merged into a
single dimension.  The shape of the returned tensor will be
<cite>self.shape[:outer_axis] + [N] + self.shape[inner_axis + 1:]</cite>, where <cite>N</cite>
is the total number of slices in the merged dimensions.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.nested_row_lengths">
<code class="sig-name descname">nested_row_lengths</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.nested_row_lengths"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.nested_row_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tuple containing the row_lengths for all ragged dimensions.</p>
<p><cite>rt.nested_row_lengths()</cite> is a tuple containing the <cite>row_lengths</cite> tensors
for all ragged dimensions in <cite>rt</cite>, ordered from outermost to innermost.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name prefix for the returned tensors (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tuple</cite> of 1-D integer <cite>Tensors</cite>.  The length of the tuple is equal to
<cite>self.ragged_rank</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.nested_row_splits">
<em class="property">property </em><code class="sig-name descname">nested_row_splits</code><a class="headerlink" href="#tensorflow.RaggedTensor.nested_row_splits" title="Permalink to this definition">¶</a></dt>
<dd><p>A tuple containing the row_splits for all ragged dimensions.</p>
<p><cite>rt.nested_row_splits</cite> is a tuple containing the <cite>row_splits</cite> tensors for
all ragged dimensions in <cite>rt</cite>, ordered from outermost to innermost.  In
particular, <cite>rt.nested_row_splits = (rt.row_splits,) + value_splits</cite> where:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>value_splits = ()</cite> if <cite>rt.values</cite> is a <cite>Tensor</cite>.</p></li>
<li><p><cite>value_splits = rt.values.nested_row_splits</cite> otherwise.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>tuple</cite> of 1-D integer <a href="#id296"><span class="problematic" id="id297">`</span></a>Tensor`s.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[[[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">splits</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">nested_row_splits</span><span class="p">):</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Splits for dimension </span><span class="si">%d</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">splits</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
<span class="go">Splits for dimension 1: [0 3]</span>
<span class="go">Splits for dimension 2: [0 3 3 5]</span>
<span class="go">Splits for dimension 3: [0 4 4 7 8 8]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.nested_value_rowids">
<code class="sig-name descname">nested_value_rowids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.nested_value_rowids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.nested_value_rowids" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tuple containing the value_rowids for all ragged dimensions.</p>
<p><cite>rt.nested_value_rowids</cite> is a tuple containing the <cite>value_rowids</cite> tensors
for
all ragged dimensions in <cite>rt</cite>, ordered from outermost to innermost.  In
particular, <cite>rt.nested_value_rowids = (rt.value_rowids(),) + value_ids</cite>
where:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>value_ids = ()</cite> if <cite>rt.values</cite> is a <cite>Tensor</cite>.</p></li>
<li><p><cite>value_ids = rt.values.nested_value_rowids</cite> otherwise.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name prefix for the returned tensors (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tuple</cite> of 1-D integer <a href="#id298"><span class="problematic" id="id299">`</span></a>Tensor`s.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[[[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ids</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">nested_value_rowids</span><span class="p">()):</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;row ids for dimension </span><span class="si">%d</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
<span class="go">row ids for dimension 1: [0 0 0]</span>
<span class="go">row ids for dimension 2: [0 0 0 2 2]</span>
<span class="go">row ids for dimension 3: [0 0 0 0 2 2 2 3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.nrows">
<code class="sig-name descname">nrows</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out_type</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.nrows"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.nrows" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of rows in this ragged tensor.</p>
<p>I.e., the size of the outermost dimension of the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_type</strong> -- <cite>dtype</cite> for the returned tensor.  Defaults to
<cite>self.row_splits.dtype</cite>.</p></li>
<li><p><strong>name</strong> -- A name prefix for the returned tensor (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A scalar <cite>Tensor</cite> with dtype <cite>out_type</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">nrows</span><span class="p">())</span>  <span class="c1"># rt has 5 rows.</span>
<span class="go">tf.Tensor(5, shape=(), dtype=int64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.numpy">
<code class="sig-name descname">numpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.numpy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a numpy <cite>array</cite> with the values for this <cite>RaggedTensor</cite>.</p>
<p>Requires that this <cite>RaggedTensor</cite> was constructed in eager execution mode.</p>
<p>Ragged dimensions are encoded using numpy <cite>arrays</cite> with <cite>dtype=object</cite> and
<cite>rank=1</cite>, where each element is a single row.</p>
<p>#### Examples</p>
<p>In the following example, the value returned by <cite>RaggedTensor.numpy()</cite>
contains three numpy <cite>array</cite> objects: one for each row (with <cite>rank=1</cite> and
<cite>dtype=int64</cite>), and one to combine them (with <cite>rank=1</cite> and <cite>dtype=object</cite>):</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([array([1, 2, 3]), array([4, 5])], dtype=object)</span>
</pre></div>
</div>
<p>Uniform dimensions are encoded using multidimensional numpy <cite>array`s.  In
the following example, the value returned by `RaggedTensor.numpy()</cite> contains
a single numpy <cite>array</cite> object, with <cite>rank=2</cite> and <cite>dtype=int64</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([[1, 2, 3], [4, 5, 6]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A numpy <cite>array</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.ragged_rank">
<em class="property">property </em><code class="sig-name descname">ragged_rank</code><a class="headerlink" href="#tensorflow.RaggedTensor.ragged_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of ragged dimensions in this ragged tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A Python <cite>int</cite> indicating the number of ragged dimensions in this ragged
tensor.  The outermost dimension is not considered ragged.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.row_lengths">
<code class="sig-name descname">row_lengths</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.row_lengths"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.row_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the lengths of the rows in this ragged tensor.</p>
<p><cite>rt.row_lengths()[i]</cite> indicates the number of values in the
<cite>i`th row of `rt</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> -- An integer constant indicating the axis whose row lengths should be
returned.</p></li>
<li><p><strong>name</strong> -- A name prefix for the returned tensor (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>axis]`.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A potentially ragged integer Tensor with shape <a href="#id300"><span class="problematic" id="id301">`</span></a>self.shape[</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>axis</cite> is out of bounds.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">6</span><span class="p">]],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">row_lengths</span><span class="p">())</span>  <span class="c1"># lengths of rows in rt</span>
<span class="go">tf.Tensor([2 0 2 1 0], shape=(5,), dtype=int64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">row_lengths</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># lengths of axis=2 rows.</span>
<span class="go">&lt;tf.RaggedTensor [[3, 1], [], [2, 1], [1], []]&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.row_limits">
<code class="sig-name descname">row_limits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.row_limits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.row_limits" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the limit indices for rows in this ragged tensor.</p>
<p>These indices specify where the values for each row end in
<cite>self.values</cite>.  <cite>rt.row_limits(self)</cite> is equal to <cite>rt.row_splits[:-1]</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name prefix for the returned tensor (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 1-D integer Tensor with shape <cite>[nrows]</cite>.
The returned tensor is nonnegative, and is sorted in ascending order.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="go">tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">row_limits</span><span class="p">())</span>  <span class="c1"># indices of row limits in rt.values</span>
<span class="go">tf.Tensor([4 4 7 8 8], shape=(5,), dtype=int64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.row_splits">
<em class="property">property </em><code class="sig-name descname">row_splits</code><a class="headerlink" href="#tensorflow.RaggedTensor.row_splits" title="Permalink to this definition">¶</a></dt>
<dd><p>The row-split indices for this ragged tensor's <cite>values</cite>.</p>
<p><cite>rt.row_splits</cite> specifies where the values for each row begin and end in
<cite>rt.values</cite>.  In particular, the values for row <cite>rt[i]</cite> are stored in
the slice <cite>rt.values[rt.row_splits[i]:rt.row_splits[i+1]]</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A 1-D integer <cite>Tensor</cite> with shape <cite>[self.nrows+1]</cite>.
The returned tensor is non-empty, and is sorted in ascending order.
<cite>self.row_splits[0]</cite> is zero, and <cite>self.row_splits[-1]</cite> is equal to
<cite>self.values.shape[0]</cite>.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">row_splits</span><span class="p">)</span>  <span class="c1"># indices of row splits in rt.values</span>
<span class="go">tf.Tensor([0 4 4 7 8 8], shape=(6,), dtype=int64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.row_starts">
<code class="sig-name descname">row_starts</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.row_starts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.row_starts" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the start indices for rows in this ragged tensor.</p>
<p>These indices specify where the values for each row begin in
<cite>self.values</cite>.  <cite>rt.row_starts()</cite> is equal to <cite>rt.row_splits[:-1]</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name prefix for the returned tensor (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 1-D integer Tensor with shape <cite>[nrows]</cite>.
The returned tensor is nonnegative, and is sorted in ascending order.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="go">tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">row_starts</span><span class="p">())</span>  <span class="c1"># indices of row starts in rt.values</span>
<span class="go">tf.Tensor([0 4 4 7 8], shape=(5,), dtype=int64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.shape">
<em class="property">property </em><code class="sig-name descname">shape</code><a class="headerlink" href="#tensorflow.RaggedTensor.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>The statically known shape of this ragged tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>TensorShape</cite> containing the statically known shape of this ragged
tensor.  Ragged dimensions have a size of <cite>None</cite>.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span><span class="o">.</span><span class="n">shape</span>
<span class="go">TensorShape([2, None])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]],</span> <span class="n">ragged_rank</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">TensorShape([2, None, 2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.to_list">
<code class="sig-name descname">to_list</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.to_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.to_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a nested Python <cite>list</cite> with the values for this <cite>RaggedTensor</cite>.</p>
<p>Requires that <cite>rt</cite> was constructed in eager execution mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A nested Python <cite>list</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.to_sparse">
<code class="sig-name descname">to_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.to_sparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.to_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts this <cite>RaggedTensor</cite> into a <cite>tf.SparseTensor</cite>.</p>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">())</span>
<span class="go">SparseTensor(indices=tf.Tensor(</span>
<span class="go">                 [[0 0] [0 1] [0 2] [1 0] [3 0] [3 1]],</span>
<span class="go">                 shape=(6, 2), dtype=int64),</span>
<span class="go">             values=tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32),</span>
<span class="go">             dense_shape=tf.Tensor([4 3], shape=(2,), dtype=int64))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name prefix for the returned tensors (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A SparseTensor with the same values as <cite>self</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.to_tensor">
<code class="sig-name descname">to_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">default_value</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">shape</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.to_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.to_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts this <cite>RaggedTensor</cite> into a <cite>tf.Tensor</cite>.</p>
<p>If <cite>shape</cite> is specified, then the result is padded and/or truncated to
the specified shape.</p>
<p>Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">())</span>
<span class="go">tf.Tensor(</span>
<span class="go">    [[9 8 7] [0 0 0] [6 5 0] [4 0 0]], shape=(4, 3), dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="go">tf.Tensor(</span>
<span class="go">    [[9 8] [0 0] [6 5] [4 0] [0 0]], shape=(5, 2), dtype=int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>default_value</strong> -- Value to set for indices not specified in <cite>self</cite>. Defaults
to zero.  <cite>default_value</cite> must be broadcastable to
<cite>self.shape[self.ragged_rank + 1:]</cite>.</p></li>
<li><p><strong>name</strong> -- A name prefix for the returned tensors (optional).</p></li>
<li><p><strong>shape</strong> -- The shape of the resulting dense tensor.  In particular,
<cite>result.shape[i]</cite> is <cite>shape[i]</cite> (if <cite>shape[i]</cite> is not None), or
<cite>self.bounding_shape(i)</cite> (otherwise).`shape.rank` must be <cite>None</cite> or
equal to <cite>self.rank</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> with shape <cite>ragged.bounding_shape(self)</cite> and the
values specified by the non-empty values in <cite>self</cite>.  Empty values are
assigned <cite>default_value</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.uniform_row_length">
<em class="property">property </em><code class="sig-name descname">uniform_row_length</code><a class="headerlink" href="#tensorflow.RaggedTensor.uniform_row_length" title="Permalink to this definition">¶</a></dt>
<dd><p>The length of each row in this ragged tensor, or None if rows are ragged.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt1</span><span class="o">.</span><span class="n">uniform_row_length</span><span class="p">)</span>  <span class="c1"># rows are ragged.</span>
<span class="go">None</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_uniform_row_length</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">values</span><span class="o">=</span><span class="n">rt1</span><span class="p">,</span> <span class="n">uniform_row_length</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt2</span><span class="p">)</span>
<span class="go">&lt;tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt2</span><span class="o">.</span><span class="n">uniform_row_length</span><span class="p">)</span>  <span class="c1"># rows are not ragged (all have size 2).</span>
<span class="go">tf.Tensor(2, shape=(), dtype=int64)</span>
</pre></div>
</div>
<p>A RaggedTensor's rows are only considered to be uniform (i.e. non-ragged)
if it can be determined statically (at graph construction time) that the
rows all have the same length.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A scalar integer <cite>Tensor</cite>, specifying the length of every row in this
ragged tensor (for ragged tensors whose rows are uniform); or <cite>None</cite>
(for ragged tensors whose rows are ragged).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.value_rowids">
<code class="sig-name descname">value_rowids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.value_rowids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.value_rowids" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the row indices for the <cite>values</cite> in this ragged tensor.</p>
<p><cite>rt.value_rowids()</cite> corresponds one-to-one with the outermost dimension of
<cite>rt.values</cite>, and specifies the row containing each value.  In particular,
the row <cite>rt[row]</cite> consists of the values <cite>rt.values[j]</cite> where
<cite>rt.value_rowids()[j] == row</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name prefix for the returned tensor (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1]`.
The returned tensor is nonnegative, and is sorted in ascending order.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A 1-D integer <cite>Tensor</cite> with shape <a href="#id302"><span class="problematic" id="id303">`</span></a>self.values.shape[</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="go">tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">value_rowids</span><span class="p">())</span>  <span class="c1"># corresponds 1:1 with rt.values</span>
<span class="go">tf.Tensor([0 0 0 0 2 2 2 3], shape=(8,), dtype=int64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.values">
<em class="property">property </em><code class="sig-name descname">values</code><a class="headerlink" href="#tensorflow.RaggedTensor.values" title="Permalink to this definition">¶</a></dt>
<dd><p>The concatenated rows for this ragged tensor.</p>
<p><cite>rt.values</cite> is a potentially ragged tensor formed by flattening the two
outermost dimensions of <cite>rt</cite> into a single dimension.</p>
<p><cite>rt.values.shape = [nvals] + rt.shape[2:]</cite> (where <cite>nvals</cite> is the
number of items in the outer two dimensions of <cite>rt</cite>).</p>
<p><cite>rt.ragged_rank = self.ragged_rank - 1</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A potentially ragged tensor.</p>
</dd>
</dl>
<p>#### Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="go">tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.with_flat_values">
<code class="sig-name descname">with_flat_values</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">new_values</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.with_flat_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.with_flat_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of <cite>self</cite> with <cite>flat_values</cite> replaced by <cite>new_value</cite>.</p>
<p>Preserves cached row-partitioning tensors such as <cite>self.cached_nrows</cite> and
<cite>self.cached_value_rowids</cite> if they have values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>new_values</strong> -- Potentially ragged tensor that should replace</p></li>
<li><p><strong>Must have rank &gt; 0</strong> (<em>self.flat_values.</em>) -- </p></li>
<li><p><strong>must have the same</strong> (<em>and</em>) -- </p></li>
<li><p><strong>of rows as self.flat_values.</strong> (<em>number</em>) -- </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite>.
<cite>result.rank = self.ragged_rank + new_values.rank</cite>.
<cite>result.ragged_rank = self.ragged_rank + new_values.ragged_rank</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.with_row_splits_dtype">
<code class="sig-name descname">with_row_splits_dtype</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dtype</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.with_row_splits_dtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.with_row_splits_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this RaggedTensor with the given <cite>row_splits</cite> dtype.</p>
<p>For RaggedTensors with multiple ragged dimensions, the <cite>row_splits</cite> for all
nested <cite>RaggedTensor</cite> objects are cast to the given dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dtype</strong> -- The dtype for <cite>row_splits</cite>.  One of <cite>tf.int32</cite> or <cite>tf.int64</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A copy of this RaggedTensor, with the <cite>row_splits</cite> cast to the given
type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensor.with_values">
<code class="sig-name descname">with_values</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">new_values</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensor.with_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensor.with_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of <cite>self</cite> with <cite>values</cite> replaced by <cite>new_value</cite>.</p>
<p>Preserves cached row-partitioning tensors such as <cite>self.cached_nrows</cite> and
<cite>self.cached_value_rowids</cite> if they have values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_values</strong> -- Potentially ragged tensor to use as the <cite>values</cite> for the
returned <cite>RaggedTensor</cite>.  Must have <cite>rank &gt; 0</cite>, and must have the same
number of rows as <cite>self.values</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>RaggedTensor</cite>.  <cite>result.rank = 1 + new_values.rank</cite>.
<cite>result.ragged_rank = 1 + new_values.ragged_rank</cite></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.RaggedTensorSpec">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">RaggedTensorSpec</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">ragged_rank</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">row_splits_dtype</span><span class="o">=</span><span class="default_value">tf.int64</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensorSpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensorSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.type_spec.BatchableTypeSpec</span></code></p>
<p>Type specification for a <cite>tf.RaggedTensor</cite>.</p>
<p>Constructs a type specification for a <cite>tf.RaggedTensor</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> -- The shape of the RaggedTensor, or <cite>None</cite> to allow any shape.  If
a shape is specified, then all ragged dimensions must have size <cite>None</cite>.</p></li>
<li><p><strong>dtype</strong> -- <cite>tf.DType</cite> of values in the RaggedTensor.</p></li>
<li><p><strong>ragged_rank</strong> -- Python integer, the ragged rank of the RaggedTensor
to be described.  Defaults to <cite>shape.ndims - 1</cite>.</p></li>
<li><p><strong>row_splits_dtype</strong> -- <cite>dtype</cite> for the RaggedTensor's <cite>row_splits</cite> tensor.
One of <cite>tf.int32</cite> or <cite>tf.int64</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.RaggedTensorSpec.from_value">
<em class="property">classmethod </em><code class="sig-name descname">from_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/ragged/ragged_tensor.html#RaggedTensorSpec.from_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RaggedTensorSpec.from_value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.RaggedTensorSpec.value_type">
<em class="property">property </em><code class="sig-name descname">value_type</code><a class="headerlink" href="#tensorflow.RaggedTensorSpec.value_type" title="Permalink to this definition">¶</a></dt>
<dd><p>The Python type for values that are compatible with this TypeSpec.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.RegisterGradient">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">RegisterGradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op_type</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#RegisterGradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.RegisterGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A decorator for registering the gradient function for an op type.</p>
<p>This decorator is only used when defining a new op type. For an op
with <cite>m</cite> inputs and <cite>n</cite> outputs, the gradient function is a function
that takes the original <cite>Operation</cite> and <cite>n</cite> <cite>Tensor</cite> objects
(representing the gradients with respect to each output of the op),
and returns <cite>m</cite> <cite>Tensor</cite> objects (representing the partial gradients
with respect to each input of the op).</p>
<p>For example, assuming that operations of type <cite>&quot;Sub&quot;</cite> take two
inputs <cite>x</cite> and <cite>y</cite>, and return a single output <cite>x - y</cite>, the
following gradient function would be registered:</p>
<p><a href="#id304"><span class="problematic" id="id305">``</span></a><a href="#id306"><span class="problematic" id="id307">`</span></a>python
&#64;tf.RegisterGradient(&quot;Sub&quot;)
def _sub_grad(unused_op, grad):</p>
<blockquote>
<div><p>return grad, tf.negative(grad)</p>
</div></blockquote>
<p><a href="#id308"><span class="problematic" id="id309">``</span></a><a href="#id310"><span class="problematic" id="id311">`</span></a></p>
<p>The decorator argument <cite>op_type</cite> is the string type of an
operation. This corresponds to the <cite>OpDef.name</cite> field for the proto
that defines the operation.</p>
<p>Creates a new decorator with <cite>op_type</cite> as the Operation type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>op_type</strong> -- The string type of an operation. This corresponds to the
<cite>OpDef.name</cite> field for the proto that defines the operation.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> -- If <cite>op_type</cite> is not string.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="tensorflow.SparseTensor">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">SparseTensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">dense_shape</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/sparse_tensor.html#SparseTensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.SparseTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.tensor_like._TensorLike</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.composite_tensor.CompositeTensor</span></code></p>
<p>Represents a sparse tensor.</p>
<p>TensorFlow represents a sparse tensor as three separate dense tensors:
<cite>indices</cite>, <cite>values</cite>, and <cite>dense_shape</cite>.  In Python, the three tensors are
collected into a <cite>SparseTensor</cite> class for ease of use.  If you have separate
<cite>indices</cite>, <cite>values</cite>, and <cite>dense_shape</cite> tensors, wrap them in a <cite>SparseTensor</cite>
object before passing to the ops below.</p>
<p>Concretely, the sparse tensor <cite>SparseTensor(indices, values, dense_shape)</cite>
comprises the following components, where <cite>N</cite> and <cite>ndims</cite> are the number
of values and number of dimensions in the <cite>SparseTensor</cite>, respectively:</p>
<ul class="simple">
<li><p><cite>indices</cite>: A 2-D int64 tensor of shape <cite>[N, ndims]</cite>, which specifies the
indices of the elements in the sparse tensor that contain nonzero values
(elements are zero-indexed). For example, <cite>indices=[[1,3], [2,4]]</cite> specifies
that the elements with indexes of [1,3] and [2,4] have nonzero values.</p></li>
<li><p><cite>values</cite>: A 1-D tensor of any type and shape <cite>[N]</cite>, which supplies the
values for each element in <cite>indices</cite>. For example, given <cite>indices=[[1,3],
[2,4]]</cite>, the parameter <cite>values=[18, 3.6]</cite> specifies that element [1,3] of
the sparse tensor has a value of 18, and element [2,4] of the tensor has a
value of 3.6.</p></li>
<li><p><cite>dense_shape</cite>: A 1-D int64 tensor of shape <cite>[ndims]</cite>, which specifies the
dense_shape of the sparse tensor. Takes a list indicating the number of
elements in each dimension. For example, <cite>dense_shape=[3,6]</cite> specifies a
two-dimensional 3x6 tensor, <cite>dense_shape=[2,3,4]</cite> specifies a
three-dimensional 2x3x4 tensor, and <cite>dense_shape=[9]</cite> specifies a
one-dimensional tensor with 9 elements.</p></li>
</ul>
<p>The corresponding dense tensor satisfies:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dense.shape</span> <span class="pre">=</span> <span class="pre">dense_shape</span>
<span class="pre">dense[tuple(indices[i])]</span> <span class="pre">=</span> <span class="pre">values[i]</span>
<span class="pre">`</span></code></p>
<p>By convention, <cite>indices</cite> should be sorted in row-major order (or equivalently
lexicographic order on the tuples <cite>indices[i]</cite>). This is not enforced when
<cite>SparseTensor</cite> objects are constructed, but most ops assume correct ordering.
If the ordering of sparse tensor <cite>st</cite> is wrong, a fixed version can be
obtained by calling <cite>tf.sparse.reorder(st)</cite>.</p>
<p>Example: The sparse tensor</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">SparseTensor(indices=[[0,</span> <span class="pre">0],</span> <span class="pre">[1,</span> <span class="pre">2]],</span> <span class="pre">values=[1,</span> <span class="pre">2],</span> <span class="pre">dense_shape=[3,</span> <span class="pre">4])</span>
<span class="pre">`</span></code></p>
<p>represents the dense tensor</p>
<p><a href="#id312"><span class="problematic" id="id313">``</span></a><a href="#id314"><span class="problematic" id="id315">`</span></a>python
[[1, 0, 0, 0]</p>
<blockquote>
<div><p>[0, 0, 2, 0]
[0, 0, 0, 0]]</p>
</div></blockquote>
<p><a href="#id316"><span class="problematic" id="id317">``</span></a><a href="#id318"><span class="problematic" id="id319">`</span></a></p>
<p>Creates a <cite>SparseTensor</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- A 2-D int64 tensor of shape <cite>[N, ndims]</cite>.</p></li>
<li><p><strong>values</strong> -- A 1-D tensor of any type and shape <cite>[N]</cite>.</p></li>
<li><p><strong>dense_shape</strong> -- A 1-D int64 tensor of shape <cite>[ndims]</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- When building an eager SparseTensor if <cite>dense_shape</cite> is
    unknown or contains unknown elements (None or -1).</p>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.SparseTensor.consumers">
<code class="sig-name descname">consumers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/sparse_tensor.html#SparseTensor.consumers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.SparseTensor.consumers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.dense_shape">
<em class="property">property </em><code class="sig-name descname">dense_shape</code><a class="headerlink" href="#tensorflow.SparseTensor.dense_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>A 1-D Tensor of int64 representing the shape of the dense tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#tensorflow.SparseTensor.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>DType</cite> of elements in this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feed_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">session</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/sparse_tensor.html#SparseTensor.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.SparseTensor.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates this sparse tensor in a <cite>Session</cite>.</p>
<p>Calling this method will execute all preceding operations that
produce the inputs needed for the operation that produces this
tensor.</p>
<p><em>N.B.</em> Before invoking <cite>SparseTensor.eval()</cite>, its graph must have been
launched in a session, and either a default session must be
available, or <cite>session</cite> must be specified explicitly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feed_dict</strong> -- A dictionary that maps <cite>Tensor</cite> objects to feed values. See
<cite>tf.Session.run</cite> for a description of the valid feed values.</p></li>
<li><p><strong>session</strong> -- (Optional.) The <cite>Session</cite> to be used to evaluate this sparse
tensor. If none, the default session will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>SparseTensorValue</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.from_value">
<em class="property">classmethod </em><code class="sig-name descname">from_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_tensor_value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/sparse_tensor.html#SparseTensor.from_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.SparseTensor.from_value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.get_shape">
<code class="sig-name descname">get_shape</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/sparse_tensor.html#SparseTensor.get_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.SparseTensor.get_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the <cite>TensorShape</cite> representing the shape of the dense tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>TensorShape</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.graph">
<em class="property">property </em><code class="sig-name descname">graph</code><a class="headerlink" href="#tensorflow.SparseTensor.graph" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Graph</cite> that contains the index, value, and dense_shape tensors.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.indices">
<em class="property">property </em><code class="sig-name descname">indices</code><a class="headerlink" href="#tensorflow.SparseTensor.indices" title="Permalink to this definition">¶</a></dt>
<dd><p>The indices of non-zero values in the represented dense tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><dl class="simple">
<dt>A 2-D Tensor of int64 with dense_shape <cite>[N, ndims]</cite>, where <cite>N</cite> is the</dt><dd><p>number of non-zero values in the tensor, and <cite>ndims</cite> is the rank.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.op">
<em class="property">property </em><code class="sig-name descname">op</code><a class="headerlink" href="#tensorflow.SparseTensor.op" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Operation</cite> that produces <cite>values</cite> as an output.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.shape">
<em class="property">property </em><code class="sig-name descname">shape</code><a class="headerlink" href="#tensorflow.SparseTensor.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the <cite>TensorShape</cite> representing the shape of the dense tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>TensorShape</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensor.values">
<em class="property">property </em><code class="sig-name descname">values</code><a class="headerlink" href="#tensorflow.SparseTensor.values" title="Permalink to this definition">¶</a></dt>
<dd><p>The non-zero values in the represented dense tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A 1-D Tensor of any data type.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.SparseTensorSpec">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">SparseTensorSpec</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/sparse_tensor.html#SparseTensorSpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.SparseTensorSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.type_spec.BatchableTypeSpec</span></code></p>
<p>Type specification for a <cite>tf.SparseTensor</cite>.</p>
<p>Constructs a type specification for a <cite>tf.SparseTensor</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> -- The dense shape of the <cite>SparseTensor</cite>, or <cite>None</cite> to allow
any dense shape.</p></li>
<li><p><strong>dtype</strong> -- <cite>tf.DType</cite> of values in the <cite>SparseTensor</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.SparseTensorSpec.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#tensorflow.SparseTensorSpec.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>tf.dtypes.DType</cite> specified by this type for the SparseTensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensorSpec.from_value">
<em class="property">classmethod </em><code class="sig-name descname">from_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/sparse_tensor.html#SparseTensorSpec.from_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.SparseTensorSpec.from_value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensorSpec.shape">
<em class="property">property </em><code class="sig-name descname">shape</code><a class="headerlink" href="#tensorflow.SparseTensorSpec.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>tf.TensorShape</cite> specified by this type for the SparseTensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.SparseTensorSpec.value_type">
<em class="property">property </em><code class="sig-name descname">value_type</code><a class="headerlink" href="#tensorflow.SparseTensorSpec.value_type" title="Permalink to this definition">¶</a></dt>
<dd><p>The Python type for values that are compatible with this TypeSpec.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.Tensor">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">Tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op</span></em>, <em class="sig-param"><span class="n">value_index</span></em>, <em class="sig-param"><span class="n">dtype</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.tensor_like._TensorLike</span></code></p>
<p>A tensor represents a rectangular array of data.</p>
<p>When writing a TensorFlow program, the main object you manipulate and pass
around is the <cite>tf.Tensor</cite>. A <cite>tf.Tensor</cite> object represents a rectangular array
of arbitrary dimension, filled with data of a specific data type.</p>
<p>A <cite>tf.Tensor</cite> has the following properties:</p>
<ul class="simple">
<li><p>a data type (float32, int32, or string, for example)</p></li>
<li><p>a shape</p></li>
</ul>
<p>Each element in the Tensor has the same data type, and the data type is always
known.</p>
<p>In eager execution, which is the default mode in TensorFlow, results are
calculated immediately.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Compute some values using a Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">[[1. 3.]</span>
<span class="go"> [3. 7.]], shape=(2, 2), dtype=float32)</span>
</pre></div>
</div>
<p>Note that during eager execution, you may discover your <cite>Tensors</cite> are actually
of type <cite>EagerTensor</cite>.  This is an internal detail, but it does give you
access to a useful function, <cite>numpy</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="go">&lt;class &#39;...ops.EagerTensor&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">  [[1. 3.]</span>
<span class="go">   [3. 7.]]</span>
</pre></div>
</div>
<p>TensorFlow can define computations without immediately executing them, most
commonly inside <a href="#id320"><span class="problematic" id="id321">`</span></a>tf.function`s, as well as in (legacy) Graph mode. In those
cases, the shape (that is, the rank of the Tensor and the size of
each dimension) might be only partially known.</p>
<p>Most operations produce tensors of fully-known shapes if the shapes of their
inputs are also fully known, but in some cases it's only possible to find the
shape of a tensor at execution time.</p>
<p>There are specialized tensors; for these, see <cite>tf.Variable</cite>, <cite>tf.constant</cite>,
<cite>tf.placeholder</cite>, <cite>tf.SparseTensor</cite>, and <cite>tf.RaggedTensor</cite>.</p>
<p>For more on Tensors, see the [guide](<a class="reference external" href="https://tensorflow.org/guide">https://tensorflow.org/guide</a>/tensor`).</p>
<p>Creates a new <cite>Tensor</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op</strong> -- An <cite>Operation</cite>. <cite>Operation</cite> that computes this tensor.</p></li>
<li><p><strong>value_index</strong> -- An <cite>int</cite>. Index of the operation's endpoint that produces
this tensor.</p></li>
<li><p><strong>dtype</strong> -- A <cite>DType</cite>. Type of elements stored in this tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> -- If the op is not an <cite>Operation</cite>.</p>
</dd>
</dl>
<dl class="py attribute">
<dt id="tensorflow.Tensor.OVERLOADABLE_OPERATORS">
<code class="sig-name descname">OVERLOADABLE_OPERATORS</code><em class="property"> = {'__abs__', '__add__', '__and__', '__div__', '__eq__', '__floordiv__', '__ge__', '__getitem__', '__gt__', '__invert__', '__le__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__sub__', '__truediv__', '__xor__'}</em><a class="headerlink" href="#tensorflow.Tensor.OVERLOADABLE_OPERATORS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.consumers">
<code class="sig-name descname">consumers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Tensor.consumers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Tensor.consumers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of <a href="#id322"><span class="problematic" id="id323">`</span></a>Operation`s that consume this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of <a href="#id324"><span class="problematic" id="id325">`</span></a>Operation`s.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.device">
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#tensorflow.Tensor.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of the device on which this tensor will be produced, or None.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#tensorflow.Tensor.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>DType</cite> of elements in this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feed_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">session</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Tensor.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Tensor.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates this tensor in a <cite>Session</cite>.</p>
<p>Note: If you are not using <cite>compat.v1</cite> libraries, you should not need this,
(or <cite>feed_dict</cite> or <cite>Session</cite>).  In eager execution (or within <cite>tf.function</cite>)
you do not need to call <cite>eval</cite>.</p>
<p>Calling this method will execute all preceding operations that
produce the inputs needed for the operation that produces this
tensor.</p>
<p><em>N.B.</em> Before invoking <cite>Tensor.eval()</cite>, its graph must have been
launched in a session, and either a default session must be
available, or <cite>session</cite> must be specified explicitly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feed_dict</strong> -- A dictionary that maps <cite>Tensor</cite> objects to feed values. See
<cite>tf.Session.run</cite> for a description of the valid feed values.</p></li>
<li><p><strong>session</strong> -- (Optional.) The <cite>Session</cite> to be used to evaluate this tensor. If
none, the default session will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array corresponding to the value of this tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.experimental_ref">
<code class="sig-name descname">experimental_ref</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Tensor.experimental_ref"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Tensor.experimental_ref" title="Permalink to this definition">¶</a></dt>
<dd><p>DEPRECATED FUNCTION</p>
<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use ref() instead.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.get_shape">
<code class="sig-name descname">get_shape</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Tensor.get_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Tensor.get_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias of <cite>tf.Tensor.shape</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.graph">
<em class="property">property </em><code class="sig-name descname">graph</code><a class="headerlink" href="#tensorflow.Tensor.graph" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Graph</cite> that contains this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#tensorflow.Tensor.name" title="Permalink to this definition">¶</a></dt>
<dd><p>The string name of this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.op">
<em class="property">property </em><code class="sig-name descname">op</code><a class="headerlink" href="#tensorflow.Tensor.op" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Operation</cite> that produces this tensor as an output.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.ref">
<code class="sig-name descname">ref</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Tensor.ref"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Tensor.ref" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a hashable reference object to this Tensor.</p>
<p>The primary use case for this API is to put tensors in a set/dictionary.
We can't put tensors in a set/dictionary as <cite>tensor.__hash__()</cite> is no longer
available starting Tensorflow 2.0.</p>
<p>The following will raise an exception starting 2.0</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_set</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">}</span>
<span class="gt">Traceback (most recent call last):</span>
  <span class="c">...</span>
<span class="gr">TypeError</span>: <span class="n">Tensor is unhashable. Instead, use tensor.ref() as the key.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="s1">&#39;five&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="s1">&#39;ten&#39;</span><span class="p">}</span>
<span class="gt">Traceback (most recent call last):</span>
  <span class="c">...</span>
<span class="gr">TypeError</span>: <span class="n">Tensor is unhashable. Instead, use tensor.ref() as the key.</span>
</pre></div>
</div>
<p>Instead, we can use <cite>tensor.ref()</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_set</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">ref</span><span class="p">(),</span> <span class="n">z</span><span class="o">.</span><span class="n">ref</span><span class="p">()}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span> <span class="ow">in</span> <span class="n">tensor_set</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">():</span> <span class="s1">&#39;five&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ref</span><span class="p">():</span> <span class="s1">&#39;ten&#39;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">ref</span><span class="p">():</span> <span class="s1">&#39;ten&#39;</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_dict</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">ref</span><span class="p">()]</span>
<span class="go">&#39;ten&#39;</span>
</pre></div>
</div>
<p>Also, the reference object provides <cite>.deref()</cite> function that returns the
original Tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span><span class="o">.</span><span class="n">deref</span><span class="p">()</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=int32, numpy=5&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.set_shape">
<code class="sig-name descname">set_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#Tensor.set_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Tensor.set_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the shape of this tensor.</p>
<p>This method can be called multiple times, and will merge the given
<cite>shape</cite> with the current shape of this tensor. It can be used to
provide additional information about the shape of this tensor that
cannot be inferred from the graph alone. For example, this can be used
to provide additional information about the shapes of images:</p>
<p><a href="#id326"><span class="problematic" id="id327">``</span></a><a href="#id328"><span class="problematic" id="id329">`</span></a>python
_, image_data = tf.compat.v1.TFRecordReader(...).read(...)
image = tf.image.decode_png(image_data, channels=3)</p>
<p># The height and width dimensions of <cite>image</cite> are data dependent, and
# cannot be computed without executing the op.
print(image.shape)
==&gt; TensorShape([Dimension(None), Dimension(None), Dimension(3)])</p>
<p># We know that each image in this dataset is 28 x 28 pixels.
image.set_shape([28, 28, 3])
print(image.shape)
==&gt; TensorShape([Dimension(28), Dimension(28), Dimension(3)])
<a href="#id330"><span class="problematic" id="id331">``</span></a><a href="#id332"><span class="problematic" id="id333">`</span></a></p>
<p>NOTE: This shape is not enforced at runtime. Setting incorrect shapes can
result in inconsistencies between the statically-known graph and the runtime
value of tensors. For runtime validation of the shape, use <cite>tf.ensure_shape</cite>
instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> -- A <cite>TensorShape</cite> representing the shape of this tensor, a
<cite>TensorShapeProto</cite>, a list, a tuple, or None.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>shape</cite> is not compatible with the current shape of
    this tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.shape">
<em class="property">property </em><code class="sig-name descname">shape</code><a class="headerlink" href="#tensorflow.Tensor.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <cite>TensorShape</cite> that represents the shape of this tensor.</p>
<p>The shape is computed using shape inference functions that are
registered in the Op for each <cite>Operation</cite>.  See
<cite>tf.TensorShape</cite>
for more details of what a shape represents.</p>
<p>The inferred shape of a tensor is used to provide shape
information without having to execute the underlying kernel. This
can be used for debugging and providing early error messages. For
example:</p>
<p><a href="#id334"><span class="problematic" id="id335">``</span></a><a href="#id336"><span class="problematic" id="id337">`</span></a>python
&gt;&gt;&gt; c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
&gt;&gt;&gt; print(c.shape) # will be TensorShape([2, 3])
(2, 3)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(4, 2)</span>
</pre></div>
</div>
<p># Raises a ValueError, because <cite>c</cite> and <cite>d</cite> do not have compatible
# inner dimensions.
&gt;&gt;&gt; e = tf.matmul(c, d)
Traceback (most recent call last):</p>
<blockquote>
<div><p>...</p>
</div></blockquote>
<p>tensorflow.python.framework.errors_impl.InvalidArgumentError: Matrix
size-incompatible: In[0]: [2,3], In[1]: [4,2] [Op:MatMul] name: MatMul/</p>
<p># This works because we have compatible shapes.
&gt;&gt;&gt; f = tf.matmul(c, d, transpose_a=True, transpose_b=True)
&gt;&gt;&gt; print(f.shape)
(3, 4)</p>
<p><a href="#id338"><span class="problematic" id="id339">``</span></a><a href="#id340"><span class="problematic" id="id341">`</span></a></p>
<p>In some cases, the inferred shape may have unknown dimensions. If
the caller has additional information about the values of these
dimensions, <cite>Tensor.set_shape()</cite> can be used to augment the
inferred shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>tf.TensorShape</cite> representing the shape of this tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Tensor.value_index">
<em class="property">property </em><code class="sig-name descname">value_index</code><a class="headerlink" href="#tensorflow.Tensor.value_index" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of this tensor in the outputs of its <cite>Operation</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.TensorArray">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">TensorArray</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dtype</span></em>, <em class="sig-param"><span class="n">size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dynamic_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">clear_after_read</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tensor_array_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">handle</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">flow</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">infer_shape</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">element_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">colocate_with_first_write_call</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p>
<p>This class is meant to be used with dynamic iteration primitives such as
<cite>while_loop</cite> and <cite>map_fn</cite>.  It supports gradient back-propagation via special
&quot;flow&quot; control flow dependencies.</p>
<p>Example 1: Plain reading and writing.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">clear_after_read</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=10.0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=20.0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=30.0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ta</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="go">&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],</span>
<span class="go">dtype=float32)&gt;</span>
</pre></div>
</div>
<p>Example 2: Fibonacci sequence algorithm that writes in a loop then returns.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">fibonacci</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">ta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>  <span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">unstack</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="gp">...</span>
<span class="gp">... </span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ta</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">ta</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">ta</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fibonacci</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(7,), dtype=float32,</span>
<span class="go">numpy=array([0., 1., 1., 2., 3., 5., 8.], dtype=float32)&gt;</span>
</pre></div>
</div>
<p>Example 3: A simple loop interacting with a <cite>tf.Variable</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">ta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">v</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">ta</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 1,  2,  4,  7, 11],</span>
<span class="go">dtype=int32)&gt;</span>
</pre></div>
</div>
<p>Construct a new TensorArray or wrap an existing TensorArray handle.</p>
<p>A note about the parameter <cite>name</cite>:</p>
<p>The name of the <cite>TensorArray</cite> (even if passed in) is uniquified: each time
a new <cite>TensorArray</cite> is created at runtime it is assigned its own name for
the duration of the run.  This avoids name collisions if a <cite>TensorArray</cite>
is created within a <cite>while_loop</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> -- (required) data type of the TensorArray.</p></li>
<li><p><strong>size</strong> -- (optional) int32 scalar <cite>Tensor</cite>: the size of the TensorArray.
Required if handle is not provided.</p></li>
<li><p><strong>dynamic_size</strong> -- (optional) Python bool: If true, writes to the TensorArray
can grow the TensorArray past its initial size.  Default: False.</p></li>
<li><p><strong>clear_after_read</strong> -- Boolean (optional, default: True).  If True, clear
TensorArray values after reading them.  This disables read-many
semantics, but allows early release of memory.</p></li>
<li><p><strong>tensor_array_name</strong> -- (optional) Python string: the name of the TensorArray.
This is used when creating the TensorArray handle.  If this value is
set, handle should be None.</p></li>
<li><p><strong>handle</strong> -- (optional) A <cite>Tensor</cite> handle to an existing TensorArray.  If this
is set, tensor_array_name should be None. Only supported in graph mode.</p></li>
<li><p><strong>flow</strong> -- (optional) A float <cite>Tensor</cite> scalar coming from an existing
<cite>TensorArray.flow</cite>. Only supported in graph mode.</p></li>
<li><p><strong>infer_shape</strong> -- (optional, default: True) If True, shape inference
is enabled.  In this case, all elements must have the same shape.</p></li>
<li><p><strong>element_shape</strong> -- (optional, default: None) A <cite>TensorShape</cite> object specifying
the shape constraints of each of the elements of the TensorArray.
Need not be fully defined.</p></li>
<li><p><strong>colocate_with_first_write_call</strong> -- If <cite>True</cite>, the TensorArray will be
colocated on the same device as the Tensor used on its first write
(write operations include <cite>write</cite>, <cite>unstack</cite>, and <cite>split</cite>).  If <cite>False</cite>,
the TensorArray will be placed on the device determined by the
device context available during its initialization.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> -- if both handle and tensor_array_name are provided.</p></li>
<li><p><strong>TypeError</strong> -- if handle is provided but is not a Tensor.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.TensorArray.close">
<code class="sig-name descname">close</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.close"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.close" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the current TensorArray.</p>
<p><strong>NOTE</strong> The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.concat">
<code class="sig-name descname">concat</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the values in the TensorArray as a concatenated <cite>Tensor</cite>.</p>
<p>All of the values must have been written, their ranks must match, and
and their shapes must all match for all dimensions except the first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name for the operation (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>All the tensors in the TensorArray concatenated into one tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#tensorflow.TensorArray.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The data type of this TensorArray.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.dynamic_size">
<em class="property">property </em><code class="sig-name descname">dynamic_size</code><a class="headerlink" href="#tensorflow.TensorArray.dynamic_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Python bool; if <cite>True</cite> the TensorArray can grow dynamically.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.element_shape">
<em class="property">property </em><code class="sig-name descname">element_shape</code><a class="headerlink" href="#tensorflow.TensorArray.element_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>tf.TensorShape</cite> of elements in this TensorArray.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.flow">
<em class="property">property </em><code class="sig-name descname">flow</code><a class="headerlink" href="#tensorflow.TensorArray.flow" title="Permalink to this definition">¶</a></dt>
<dd><p>The flow <cite>Tensor</cite> forcing ops leading to this TensorArray state.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.gather">
<code class="sig-name descname">gather</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Return selected values in the TensorArray as a packed <cite>Tensor</cite>.</p>
<p>All of selected values must have been written and their shapes
must all match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- A <cite>1-D</cite> <cite>Tensor</cite> taking values in <cite>[0, max_value)</cite>.  If
the <cite>TensorArray</cite> is not dynamic, <cite>max_value=size()</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tensors in the <cite>TensorArray</cite> selected by <cite>indices</cite>, packed into one
tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.grad">
<code class="sig-name descname">grad</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source</span></em>, <em class="sig-param"><span class="n">flow</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.handle">
<em class="property">property </em><code class="sig-name descname">handle</code><a class="headerlink" href="#tensorflow.TensorArray.handle" title="Permalink to this definition">¶</a></dt>
<dd><p>The reference to the TensorArray.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.identity">
<code class="sig-name descname">identity</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.identity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.identity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a TensorArray with the same content and properties.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A new TensorArray object with flow that ensures the control dependencies
from the contexts will become control dependencies for writes, reads, etc.
Use this object all for subsequent operations.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.read">
<code class="sig-name descname">read</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.read"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Read the value at location <cite>index</cite> in the TensorArray.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> -- 0-D.  int32 tensor with the index to read from.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tensor at index <cite>index</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.scatter">
<code class="sig-name descname">scatter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.scatter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatter the values of a <cite>Tensor</cite> in specific indices of a <cite>TensorArray</cite>.</p>
<blockquote>
<div><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>indices: A <cite>1-D</cite> <cite>Tensor</cite> taking values in <cite>[0, max_value)</cite>.  If</dt><dd><p>the <cite>TensorArray</cite> is not dynamic, <cite>max_value=size()</cite>.</p>
</dd>
</dl>
<p>value: (N+1)-D.  Tensor of type <cite>dtype</cite>.  The Tensor to unpack.
name: A name for the operation (optional).</p>
</dd>
<dt>Returns:</dt><dd><p>A new TensorArray object with flow that ensures the scatter occurs.
Use this object all for subsequent operations.</p>
</dd>
<dt>Raises:</dt><dd><p>ValueError: if the shape inference fails.</p>
</dd>
</dl>
</div></blockquote>
<p><strong>NOTE</strong> The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.size">
<code class="sig-name descname">size</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the size of the TensorArray.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.split">
<code class="sig-name descname">split</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">lengths</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.split"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Split the values of a <cite>Tensor</cite> into the TensorArray.</p>
<blockquote>
<div><dl>
<dt>Args:</dt><dd><p>value: (N+1)-D.  Tensor of type <cite>dtype</cite>.  The Tensor to split.
lengths: 1-D.  int32 vector with the lengths to use when splitting</p>
<blockquote>
<div><p><cite>value</cite> along its first dimension.</p>
</div></blockquote>
<p>name: A name for the operation (optional).</p>
</dd>
<dt>Returns:</dt><dd><p>A new TensorArray object with flow that ensures the split occurs.
Use this object all for subsequent operations.</p>
</dd>
<dt>Raises:</dt><dd><p>ValueError: if the shape inference fails.</p>
</dd>
</dl>
</div></blockquote>
<p><strong>NOTE</strong> The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.stack">
<code class="sig-name descname">stack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the values in the TensorArray as a stacked <cite>Tensor</cite>.</p>
<p>All of the values must have been written and their shapes must all match.
If input shapes have rank-<cite>R</cite>, then output shape will have rank-<cite>(R+1)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name for the operation (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>All the tensors in the TensorArray stacked into one tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.unstack">
<code class="sig-name descname">unstack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.unstack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.unstack" title="Permalink to this definition">¶</a></dt>
<dd><p>Unstack the values of a <cite>Tensor</cite> in the TensorArray.</p>
<blockquote>
<div><p>If input value shapes have rank-<cite>R</cite>, then the output TensorArray will
contain elements whose shapes are rank-<cite>(R-1)</cite>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>value: (N+1)-D.  Tensor of type <cite>dtype</cite>.  The Tensor to unstack.
name: A name for the operation (optional).</p>
</dd>
<dt>Returns:</dt><dd><p>A new TensorArray object with flow that ensures the unstack occurs.
Use this object all for subsequent operations.</p>
</dd>
<dt>Raises:</dt><dd><p>ValueError: if the shape inference fails.</p>
</dd>
</dl>
</div></blockquote>
<p><strong>NOTE</strong> The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArray.write">
<code class="sig-name descname">write</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArray.write"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArray.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Write <cite>value</cite> into index <cite>index</cite> of the TensorArray.</p>
<blockquote>
<div><dl class="simple">
<dt>Args:</dt><dd><p>index: 0-D.  int32 scalar with the index to write to.
value: N-D.  Tensor of type <cite>dtype</cite>.  The Tensor to write to this index.
name: A name for the operation (optional).</p>
</dd>
<dt>Returns:</dt><dd><p>A new TensorArray object with flow that ensures the write occurs.
Use this object all for subsequent operations.</p>
</dd>
<dt>Raises:</dt><dd><p>ValueError: if there are more writers than specified.</p>
</dd>
</dl>
</div></blockquote>
<p><strong>NOTE</strong> The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.TensorArraySpec">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">TensorArraySpec</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">element_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">dynamic_size</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">infer_shape</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArraySpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArraySpec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.type_spec.TypeSpec</span></code></p>
<p>Type specification for a <cite>tf.TensorArray</cite>.</p>
<p>Constructs a type specification for a <cite>tf.TensorArray</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>element_shape</strong> -- The shape of each element in the <cite>TensorArray</cite>.</p></li>
<li><p><strong>dtype</strong> -- Data type of the <cite>TensorArray</cite>.</p></li>
<li><p><strong>dynamic_size</strong> -- Whether the <cite>TensorArray</cite> can grow past its initial size.</p></li>
<li><p><strong>infer_shape</strong> -- Whether shape inference is enabled.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.TensorArraySpec.from_value">
<em class="property">static </em><code class="sig-name descname">from_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArraySpec.from_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArraySpec.from_value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArraySpec.is_compatible_with">
<code class="sig-name descname">is_compatible_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArraySpec.is_compatible_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArraySpec.is_compatible_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if <cite>spec_or_value</cite> is compatible with this TypeSpec.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArraySpec.most_specific_compatible_type">
<code class="sig-name descname">most_specific_compatible_type</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/tensor_array_ops.html#TensorArraySpec.most_specific_compatible_type"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorArraySpec.most_specific_compatible_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the most specific TypeSpec compatible with <cite>self</cite> and <cite>other</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- A <cite>TypeSpec</cite>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If there is no TypeSpec that is compatible with both <cite>self</cite>
    and <cite>other</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorArraySpec.value_type">
<em class="property">property </em><code class="sig-name descname">value_type</code><a class="headerlink" href="#tensorflow.TensorArraySpec.value_type" title="Permalink to this definition">¶</a></dt>
<dd><p>The Python type for values that are compatible with this TypeSpec.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.TensorShape">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">TensorShape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dims</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Represents the shape of a <cite>Tensor</cite>.</p>
<p>A <cite>TensorShape</cite> represents a possibly-partial shape specification for a
<cite>Tensor</cite>. It may be one of the following:</p>
<ul class="simple">
<li><p><em>Fully-known shape:</em> has a known number of dimensions and a known size
for each dimension. e.g. <cite>TensorShape([16, 256])</cite></p></li>
<li><p><em>Partially-known shape:</em> has a known number of dimensions, and an unknown
size for one or more dimension. e.g. <cite>TensorShape([None, 256])</cite></p></li>
<li><p><em>Unknown shape:</em> has an unknown number of dimensions, and an unknown
size in all dimensions. e.g. <cite>TensorShape(None)</cite></p></li>
</ul>
<p>If a tensor is produced by an operation of type <cite>&quot;Foo&quot;</cite>, its shape
may be inferred if there is a registered shape function for
<cite>&quot;Foo&quot;</cite>. See [Shape
functions](<a class="reference external" href="https://tensorflow.org/extend/adding_an_op#shape_functions_in_c">https://tensorflow.org/extend/adding_an_op#shape_functions_in_c</a>)
for details of shape functions and how to register them. Alternatively,
the shape may be set explicitly using <cite>tf.Tensor.set_shape</cite>.</p>
<p>Creates a new TensorShape with the given dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dims</strong> -- A list of Dimensions, or None if the shape is unspecified.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> -- If dims cannot be converted to a list of dimensions.</p>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.TensorShape.as_list">
<code class="sig-name descname">as_list</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.as_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.as_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of integers or <cite>None</cite> for each dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of integers or <cite>None</cite> for each dimension.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>self</cite> is an unknown shape with an unknown rank.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.as_proto">
<code class="sig-name descname">as_proto</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.as_proto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.as_proto" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this shape as a <cite>TensorShapeProto</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.assert_has_rank">
<code class="sig-name descname">assert_has_rank</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">rank</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.assert_has_rank"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.assert_has_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Raises an exception if <cite>self</cite> is not compatible with the given <cite>rank</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rank</strong> -- An integer.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>self</cite> does not represent a shape with the given <cite>rank</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.assert_is_compatible_with">
<code class="sig-name descname">assert_is_compatible_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.assert_is_compatible_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.assert_is_compatible_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Raises exception if <cite>self</cite> and <cite>other</cite> do not represent the same shape.</p>
<p>This method can be used to assert that there exists a shape that both
<cite>self</cite> and <cite>other</cite> represent.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- Another TensorShape.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>self</cite> and <cite>other</cite> do not represent the same shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.assert_is_fully_defined">
<code class="sig-name descname">assert_is_fully_defined</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.assert_is_fully_defined"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.assert_is_fully_defined" title="Permalink to this definition">¶</a></dt>
<dd><p>Raises an exception if <cite>self</cite> is not fully defined in every dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>self</cite> does not have a known value for every dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.assert_same_rank">
<code class="sig-name descname">assert_same_rank</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.assert_same_rank"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.assert_same_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Raises an exception if <cite>self</cite> and <cite>other</cite> do not have compatible ranks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- Another <cite>TensorShape</cite>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>self</cite> and <cite>other</cite> do not represent shapes with the
    same rank.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.concatenate">
<code class="sig-name descname">concatenate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.concatenate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.concatenate" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the concatenation of the dimension in <cite>self</cite> and <cite>other</cite>.</p>
<p><em>N.B.</em> If either <cite>self</cite> or <cite>other</cite> is completely unknown,
concatenation will discard information about the other shape. In
future, we might support concatenation that preserves this
information for use with slicing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- Another <cite>TensorShape</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>TensorShape</cite> whose dimensions are the concatenation of the
dimensions in <cite>self</cite> and <cite>other</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.dims">
<em class="property">property </em><code class="sig-name descname">dims</code><a class="headerlink" href="#tensorflow.TensorShape.dims" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated.  Returns list of dimensions for this shape.</p>
<p>Suggest <cite>TensorShape.as_list</cite> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list containing <a href="#id342"><span class="problematic" id="id343">`</span></a>tf.compat.v1.Dimension`s, or None if the shape is
unspecified.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.is_compatible_with">
<code class="sig-name descname">is_compatible_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.is_compatible_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.is_compatible_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True iff <cite>self</cite> is compatible with <cite>other</cite>.</p>
<p>Two possibly-partially-defined shapes are compatible if there
exists a fully-defined shape that both shapes can represent. Thus,
compatibility allows the shape inference code to reason about
partially-defined shapes. For example:</p>
<ul class="simple">
<li><p>TensorShape(None) is compatible with all shapes.</p></li>
<li><p>TensorShape([None, None]) is compatible with all two-dimensional
shapes, such as TensorShape([32, 784]), and also TensorShape(None). It is
not compatible with, for example, TensorShape([None]) or
TensorShape([None, None, None]).</p></li>
<li><p>TensorShape([32, None]) is compatible with all two-dimensional shapes
with size 32 in the 0th dimension, and also TensorShape([None, None])
and TensorShape(None). It is not compatible with, for example,
TensorShape([32]), TensorShape([32, None, 1]) or TensorShape([64, None]).</p></li>
<li><p>TensorShape([32, 784]) is compatible with itself, and also
TensorShape([32, None]), TensorShape([None, 784]), TensorShape([None,
None]) and TensorShape(None). It is not compatible with, for example,
TensorShape([32, 1, 784]) or TensorShape([None]).</p></li>
</ul>
<p>The compatibility relation is reflexive and symmetric, but not
transitive. For example, TensorShape([32, 784]) is compatible with
TensorShape(None), and TensorShape(None) is compatible with
TensorShape([4, 4]), but TensorShape([32, 784]) is not compatible with
TensorShape([4, 4]).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- Another TensorShape.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True iff <cite>self</cite> is compatible with <cite>other</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.is_fully_defined">
<code class="sig-name descname">is_fully_defined</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.is_fully_defined"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.is_fully_defined" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True iff <cite>self</cite> is fully defined in every dimension.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.merge_with">
<code class="sig-name descname">merge_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.merge_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.merge_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <cite>TensorShape</cite> combining the information in <cite>self</cite> and <cite>other</cite>.</p>
<p>The dimensions in <cite>self</cite> and <cite>other</cite> are merged elementwise,
according to the rules defined for <cite>Dimension.merge_with()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- Another <cite>TensorShape</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>TensorShape</cite> containing the combined information of <cite>self</cite> and
<cite>other</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>self</cite> and <cite>other</cite> are not compatible.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.most_specific_compatible_shape">
<code class="sig-name descname">most_specific_compatible_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.most_specific_compatible_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.most_specific_compatible_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the most specific TensorShape compatible with <cite>self</cite> and <cite>other</cite>.</p>
<ul class="simple">
<li><p>TensorShape([None, 1]) is the most specific TensorShape compatible with
both TensorShape([2, 1]) and TensorShape([5, 1]). Note that
TensorShape(None) is also compatible with above mentioned TensorShapes.</p></li>
<li><p>TensorShape([1, 2, 3]) is the most specific TensorShape compatible with
both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are more
less specific TensorShapes compatible with above mentioned TensorShapes,
e.g. TensorShape([1, 2, None]), TensorShape(None).</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- Another <cite>TensorShape</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>TensorShape</cite> which is the most specific compatible shape of <cite>self</cite>
and <cite>other</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.ndims">
<em class="property">property </em><code class="sig-name descname">ndims</code><a class="headerlink" href="#tensorflow.TensorShape.ndims" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated accessor for <cite>rank</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.num_elements">
<code class="sig-name descname">num_elements</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.num_elements"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.num_elements" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total number of elements, or none for incomplete shapes.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.rank">
<em class="property">property </em><code class="sig-name descname">rank</code><a class="headerlink" href="#tensorflow.TensorShape.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the rank of this shape, or None if it is unspecified.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.with_rank">
<code class="sig-name descname">with_rank</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">rank</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.with_rank"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.with_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a shape based on <cite>self</cite> with the given rank.</p>
<p>This method promotes a completely unknown shape to one with a
known rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rank</strong> -- An integer.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A shape that is at least as specific as <cite>self</cite> with the given rank.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>self</cite> does not represent a shape with the given <cite>rank</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.with_rank_at_least">
<code class="sig-name descname">with_rank_at_least</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">rank</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.with_rank_at_least"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.with_rank_at_least" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a shape based on <cite>self</cite> with at least the given rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rank</strong> -- An integer.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A shape that is at least as specific as <cite>self</cite> with at least the given
rank.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>self</cite> does not represent a shape with at least the given
    <cite>rank</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorShape.with_rank_at_most">
<code class="sig-name descname">with_rank_at_most</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">rank</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_shape.html#TensorShape.with_rank_at_most"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorShape.with_rank_at_most" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a shape based on <cite>self</cite> with at most the given rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rank</strong> -- An integer.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A shape that is at least as specific as <cite>self</cite> with at most the given
rank.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>self</cite> does not represent a shape with at most the given
    <cite>rank</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.TensorSpec">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">TensorSpec</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_spec.html#TensorSpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.tensor_spec.DenseSpec</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.type_spec.BatchableTypeSpec</span></code></p>
<p>Describes a tf.Tensor.</p>
<p>Metadata for describing the <cite>tf.Tensor</cite> objects accepted or returned
by some TensorFlow APIs.</p>
<p>Creates a TensorSpec.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> -- Value convertible to <cite>tf.TensorShape</cite>. The shape of the tensor.</p></li>
<li><p><strong>dtype</strong> -- Value convertible to <cite>tf.DType</cite>. The type of the tensor values.</p></li>
<li><p><strong>name</strong> -- Optional name for the Tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> -- If shape is not convertible to a <cite>tf.TensorShape</cite>, or dtype is
    not convertible to a <cite>tf.DType</cite>.</p>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.TensorSpec.from_tensor">
<em class="property">classmethod </em><code class="sig-name descname">from_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_spec.html#TensorSpec.from_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorSpec.from_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorSpec.is_compatible_with">
<code class="sig-name descname">is_compatible_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">spec_or_tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_spec.html#TensorSpec.is_compatible_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TensorSpec.is_compatible_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if spec_or_tensor is compatible with this TensorSpec.</p>
<p>Two tensors are considered compatible if they have the same dtype
and their shapes are compatible (see <cite>tf.TensorShape.is_compatible_with</cite>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>spec_or_tensor</strong> -- A tf.TensorSpec or a tf.Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if spec_or_tensor is compatible with self.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TensorSpec.value_type">
<em class="property">property </em><code class="sig-name descname">value_type</code><a class="headerlink" href="#tensorflow.TensorSpec.value_type" title="Permalink to this definition">¶</a></dt>
<dd><p>The Python type for values that are compatible with this TypeSpec.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.TypeSpec">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">TypeSpec</code><a class="reference internal" href="_modules/tensorflow/python/framework/type_spec.html#TypeSpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TypeSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Specifies a TensorFlow value type.</p>
<p>A <cite>tf.TypeSpec</cite> provides metadata describing an object accepted or returned
by TensorFlow APIs.  Concrete subclasses, such as <cite>tf.TensorSpec</cite> and
<cite>tf.RaggedTensorSpec</cite>, are used to describe different value types.</p>
<p>For example, <cite>tf.function</cite>'s <cite>input_signature</cite> argument accepts a list
(or nested structure) of <a href="#id344"><span class="problematic" id="id345">`</span></a>TypeSpec`s.</p>
<p>Creating new subclasses of TypeSpec (outside of TensorFlow core) is not
currently supported.  In particular, we may make breaking changes to the
private methods and properties defined by this base class.</p>
<dl class="py method">
<dt id="tensorflow.TypeSpec.is_compatible_with">
<code class="sig-name descname">is_compatible_with</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">spec_or_value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/type_spec.html#TypeSpec.is_compatible_with"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TypeSpec.is_compatible_with" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if <cite>spec_or_value</cite> is compatible with this TypeSpec.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TypeSpec.most_specific_compatible_type">
<code class="sig-name descname">most_specific_compatible_type</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/type_spec.html#TypeSpec.most_specific_compatible_type"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.TypeSpec.most_specific_compatible_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the most specific TypeSpec compatible with <cite>self</cite> and <cite>other</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> -- A <cite>TypeSpec</cite>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If there is no TypeSpec that is compatible with both <cite>self</cite>
    and <cite>other</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.TypeSpec.value_type">
<em class="property">abstract property </em><code class="sig-name descname">value_type</code><a class="headerlink" href="#tensorflow.TypeSpec.value_type" title="Permalink to this definition">¶</a></dt>
<dd><p>The Python type for values that are compatible with this TypeSpec.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.UnconnectedGradients">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">UnconnectedGradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/unconnected_gradients.html#UnconnectedGradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.UnconnectedGradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Controls how gradient computation behaves when y does not depend on x.</p>
<p>The gradient of y with respect to x can be zero in two different ways: there
could be no differentiable path in the graph connecting x to y (and so we can
statically prove that the gradient is zero) or it could be that runtime values
of tensors in a particular execution lead to a gradient of zero (say, if a
relu unit happens to not be activated). To allow you to distinguish between
these two cases you can choose what value gets returned for the gradient when
there is no path in the graph from x to y:</p>
<ul class="simple">
<li><p><cite>NONE</cite>: Indicates that [None] will be returned if there is no path from x
to y</p></li>
<li><p><cite>ZERO</cite>: Indicates that a zero tensor will be returned in the shape of x.</p></li>
</ul>
<dl class="py attribute">
<dt id="tensorflow.UnconnectedGradients.NONE">
<code class="sig-name descname">NONE</code><em class="property"> = 'none'</em><a class="headerlink" href="#tensorflow.UnconnectedGradients.NONE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="tensorflow.UnconnectedGradients.ZERO">
<code class="sig-name descname">ZERO</code><em class="property"> = 'zero'</em><a class="headerlink" href="#tensorflow.UnconnectedGradients.ZERO" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="tensorflow.Variable">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">Variable</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.training.tracking.base.Trackable</span></code></p>
<p>See the [variable guide](<a class="reference external" href="https://tensorflow.org/guide/variable">https://tensorflow.org/guide/variable</a>).</p>
<p>A variable maintains shared, persistent state manipulated by a program.</p>
<p>The <cite>Variable()</cite> constructor requires an initial value for the variable, which
can be a <cite>Tensor</cite> of any type and shape. This initial value defines the type
and shape of the variable. After construction, the type and shape of the
variable are fixed. The value can be changed using one of the assign methods.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
<span class="go">&lt;tf.Variable ... shape=() dtype=float32, numpy=2.0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">&lt;tf.Variable ... shape=() dtype=float32, numpy=2.5&gt;</span>
</pre></div>
</div>
<p>The <cite>shape</cite> argument to <cite>Variable</cite>'s constructor allows you to construct a
variable with a less defined shape than its <cite>initial_value</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">assign</span><span class="p">([[</span><span class="mf">1.</span><span class="p">]])</span>
<span class="go">&lt;tf.Variable ... shape=&lt;unknown&gt; dtype=float32, numpy=array([[1.]], ...)&gt;</span>
</pre></div>
</div>
<p>Just like any <cite>Tensor</cite>, variables created with <cite>Variable()</cite> can be used as
inputs to operations. Additionally, all the operators overloaded for the
<cite>Tensor</cite> class are carried over to variables.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor:... shape=(2, 2), ... numpy=</span>
<span class="go">  array([[3., 4.],</span>
<span class="go">         [6., 8.]], dtype=float32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor:... shape=(2, 2), ...&gt;</span>
</pre></div>
</div>
<p>When building a machine learning model it is often convenient to distinguish
between variables holding trainable model parameters and other variables such
as a <cite>step</cite> variable used to count training steps. To make this easier, the
variable constructor supports a <cite>trainable=&lt;bool&gt;</cite>
parameter. <cite>tf.GradientTape</cite> watches trainable variables by default:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
<span class="gp">... </span>  <span class="n">trainable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>
<span class="gp">... </span>  <span class="n">non_trainable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>  <span class="n">x1</span> <span class="o">=</span> <span class="n">trainable</span> <span class="o">*</span> <span class="mf">2.</span>
<span class="gp">... </span>  <span class="n">x2</span> <span class="o">=</span> <span class="n">non_trainable</span> <span class="o">*</span> <span class="mf">3.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">trainable</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor:... shape=(), dtype=float32, numpy=2.0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">non_trainable</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>  <span class="c1"># Unwatched</span>
</pre></div>
</div>
<p>Variables are automatically tracked when assigned to attributes of types
inheriting from <cite>tf.Module</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">trainable_variables</span>
<span class="go">(&lt;tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)&gt;,)</span>
</pre></div>
</div>
<p>This tracking then allows saving variable values to
[training checkpoints](<a class="reference external" href="https://www.tensorflow.org/guide/checkpoint">https://www.tensorflow.org/guide/checkpoint</a>), or to
[SavedModels](<a class="reference external" href="https://www.tensorflow.org/guide/saved_model">https://www.tensorflow.org/guide/saved_model</a>) which include
serialized TensorFlow graphs.</p>
<p>Variables are often captured and manipulated by <a href="#id346"><span class="problematic" id="id347">`</span></a>tf.function`s. This works the
same way the un-decorated function would have:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">read_and_decrement</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">read_and_decrement</span><span class="p">()</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.1&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">read_and_decrement</span><span class="p">()</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.2&gt;</span>
</pre></div>
</div>
<p>Variables created inside a <cite>tf.function</cite> must be owned outside the function
and be created only once:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span>  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">):</span>  <span class="c1"># Or set self.v to None in __init__</span>
<span class="gp">... </span>      <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">*</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">v</span>
<span class="go">&lt;tf.Variable ... shape=() dtype=float32, numpy=2.0&gt;</span>
</pre></div>
</div>
<p>See the <cite>tf.function</cite> documentation for details.</p>
<p>Creates a new variable with value <cite>initial_value</cite>. (deprecated arguments)</p>
<p>Warning: SOME ARGUMENTS ARE DEPRECATED: <cite>(caching_device)</cite>. They will be removed in a future version.
Instructions for updating:
A variable's value can be manually cached by calling tf.Variable.read_value() under a tf.device scope. The caching_device argument does not work properly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initial_value</strong> -- A <cite>Tensor</cite>, or Python object convertible to a <cite>Tensor</cite>,
which is the initial value for the Variable. The initial value must have
a shape specified unless <cite>validate_shape</cite> is set to False. Can also be a
callable with no argument that returns the initial value when called. In
that case, <cite>dtype</cite> must be specified. (Note that initializer functions
from init_ops.py must first be bound to a shape before being used here.)</p></li>
<li><p><strong>trainable</strong> -- If <cite>True</cite>, GradientTapes automatically watch uses of this
variable. Defaults to <cite>True</cite>, unless <cite>synchronization</cite> is set to
<cite>ON_READ</cite>, in which case it defaults to <cite>False</cite>.</p></li>
<li><p><strong>validate_shape</strong> -- If <cite>False</cite>, allows the variable to be initialized with a
value of unknown shape. If <cite>True</cite>, the default, the shape of
<cite>initial_value</cite> must be known.</p></li>
<li><p><strong>caching_device</strong> -- Optional device string describing where the Variable
should be cached for reading.  Defaults to the Variable's device. If not
<cite>None</cite>, caches on another device.  Typical use is to cache on the device
where the Ops using the Variable reside, to deduplicate copying through
<cite>Switch</cite> and other conditional statements.</p></li>
<li><p><strong>name</strong> -- Optional name for the variable. Defaults to <cite>'Variable'</cite> and gets
uniquified automatically.</p></li>
<li><p><strong>variable_def</strong> -- <cite>VariableDef</cite> protocol buffer. If not <cite>None</cite>, recreates the
Variable object with its contents, referencing the variable's nodes in
the graph, which must already exist. The graph is not changed.
<cite>variable_def</cite> and the other arguments are mutually exclusive.</p></li>
<li><p><strong>dtype</strong> -- If set, initial_value will be converted to the given type. If
<cite>None</cite>, either the datatype will be kept (if <cite>initial_value</cite> is a
Tensor), or <cite>convert_to_tensor</cite> will decide.</p></li>
<li><p><strong>import_scope</strong> -- Optional <cite>string</cite>. Name scope to add to the <cite>Variable.</cite> Only
used when initializing from protocol buffer.</p></li>
<li><p><strong>constraint</strong> -- An optional projection function to be applied to the variable
after being updated by an <cite>Optimizer</cite> (e.g. used to implement norm
constraints or value constraints for layer weights). The function must
take as input the unprojected Tensor representing the value of the
variable and return the Tensor for the projected value (which must have
the same shape). Constraints are not safe to use when doing asynchronous
distributed training.</p></li>
<li><p><strong>synchronization</strong> -- Indicates when a distributed a variable will be
aggregated. Accepted values are constants defined in the class
<cite>tf.VariableSynchronization</cite>. By default the synchronization is set to
<cite>AUTO</cite> and the current <cite>DistributionStrategy</cite> chooses when to
synchronize.</p></li>
<li><p><strong>aggregation</strong> -- Indicates how a distributed variable will be aggregated.
Accepted values are constants defined in the class
<cite>tf.VariableAggregation</cite>.</p></li>
<li><p><strong>shape</strong> -- (optional) The shape of this variable. If None, the shape of
<cite>initial_value</cite> will be used. When setting this argument to
<cite>tf.TensorShape(None)</cite> (representing an unspecified shape), the variable
can be assigned with values of different shapes.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> -- If both <cite>variable_def</cite> and initial_value are specified.</p></li>
<li><p><strong>ValueError</strong> -- If the initial value is not specified, or does not have a
    shape and <cite>validate_shape</cite> is <cite>True</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py class">
<dt id="tensorflow.Variable.SaveSliceInfo">
<em class="property">class </em><code class="sig-name descname">SaveSliceInfo</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">full_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">full_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">var_offset</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">var_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">save_slice_info_def</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">import_scope</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.SaveSliceInfo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.SaveSliceInfo" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Information on how to save this Variable as a slice.</p>
<p>Provides internal support for saving variables as slices of a larger
variable.  This API is not public and is subject to change.</p>
<p>Available properties:</p>
<ul class="simple">
<li><p>full_name</p></li>
<li><p>full_shape</p></li>
<li><p>var_offset</p></li>
<li><p>var_shape</p></li>
</ul>
<p>Create a <cite>SaveSliceInfo</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>full_name</strong> -- Name of the full variable of which this <cite>Variable</cite> is a
slice.</p></li>
<li><p><strong>full_shape</strong> -- Shape of the full variable, as a list of int.</p></li>
<li><p><strong>var_offset</strong> -- Offset of this <cite>Variable</cite> into the full variable, as a list
of int.</p></li>
<li><p><strong>var_shape</strong> -- Shape of this <cite>Variable</cite>, as a list of int.</p></li>
<li><p><strong>save_slice_info_def</strong> -- <cite>SaveSliceInfoDef</cite> protocol buffer. If not <cite>None</cite>,
recreates the SaveSliceInfo object its contents. <cite>save_slice_info_def</cite>
and other arguments are mutually exclusive.</p></li>
<li><p><strong>import_scope</strong> -- Optional <cite>string</cite>. Name scope to add. Only used when
initializing from protocol buffer.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="tensorflow.Variable.SaveSliceInfo.spec">
<em class="property">property </em><code class="sig-name descname">spec</code><a class="headerlink" href="#tensorflow.Variable.SaveSliceInfo.spec" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the spec string used for saving.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.SaveSliceInfo.to_proto">
<code class="sig-name descname">to_proto</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">export_scope</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.SaveSliceInfo.to_proto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.SaveSliceInfo.to_proto" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a SaveSliceInfoDef() proto.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>export_scope</strong> -- Optional <cite>string</cite>. Name scope to remove.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>SaveSliceInfoDef</cite> protocol buffer, or None if the <cite>Variable</cite> is not
in the specified name scope.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.aggregation">
<em class="property">property </em><code class="sig-name descname">aggregation</code><a class="headerlink" href="#tensorflow.Variable.aggregation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.assign">
<code class="sig-name descname">assign</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">read_value</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.assign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.assign" title="Permalink to this definition">¶</a></dt>
<dd><p>Assigns a new value to the variable.</p>
<p>This is essentially a shortcut for <cite>assign(self, value)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> -- A <cite>Tensor</cite>. The new value for this variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the assignment.</p></li>
<li><p><strong>name</strong> -- The name of the operation to be created</p></li>
<li><p><strong>read_value</strong> -- if True, will return something which evaluates to the new
value of the variable; if False will return the assign op.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable. If <cite>read_value</cite> is false, instead returns None in
Eager mode and the assign op in graph mode.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.assign_add">
<code class="sig-name descname">assign_add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">read_value</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.assign_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.assign_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a value to this variable.</p>
<blockquote>
<div><p>This is essentially a shortcut for <cite>assign_add(self, delta)</cite>.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>delta</strong> -- A <cite>Tensor</cite>. The value to add to this variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- The name of the operation to be created</p></li>
<li><p><strong>read_value</strong> -- if True, will return something which evaluates to the new
value of the variable; if False will return the assign op.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable. If <cite>read_value</cite> is false, instead returns None in
Eager mode and the assign op in graph mode.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.assign_sub">
<code class="sig-name descname">assign_sub</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">read_value</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.assign_sub"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.assign_sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtracts a value from this variable.</p>
<p>This is essentially a shortcut for <cite>assign_sub(self, delta)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>delta</strong> -- A <cite>Tensor</cite>. The value to subtract from this variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- The name of the operation to be created</p></li>
<li><p><strong>read_value</strong> -- if True, will return something which evaluates to the new
value of the variable; if False will return the assign op.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable. If <cite>read_value</cite> is false, instead returns None in
Eager mode and the assign op in graph mode.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.batch_scatter_update">
<code class="sig-name descname">batch_scatter_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.batch_scatter_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.batch_scatter_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Assigns <cite>tf.IndexedSlices</cite> to this variable batch-wise.</p>
<p>Analogous to <cite>batch_gather</cite>. This assumes that this variable and the
sparse_delta IndexedSlices have a series of leading dimensions that are the
same for all of them, and the updates are performed on the last dimension of
indices. In other words, the dimensions should be the following:</p>
<p><cite>num_prefix_dims = sparse_delta.indices.ndims - 1</cite>
<cite>batch_dim = num_prefix_dims + 1</cite>
<a href="#id348"><span class="problematic" id="id349">`</span></a>sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[</p>
<blockquote>
<div><p>batch_dim:]`</p>
</div></blockquote>
<p>where</p>
<p><cite>sparse_delta.updates.shape[:num_prefix_dims]</cite>
<cite>== sparse_delta.indices.shape[:num_prefix_dims]</cite>
<cite>== var.shape[:num_prefix_dims]</cite></p>
<p>And the operation performed can be expressed as:</p>
<dl class="simple">
<dt><a href="#id350"><span class="problematic" id="id351">`</span></a>var[i_1, ..., i_n,</dt><dd><dl class="simple">
<dt>sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[</dt><dd><p>i_1, ..., i_n, j]`</p>
</dd>
</dl>
</dd>
</dl>
<p>When sparse_delta.indices is a 1D tensor, this operation is equivalent to
<cite>scatter_update</cite>.</p>
<p>To avoid this operation one can looping over the first <cite>ndims</cite> of the
variable and using <cite>scatter_update</cite> on the subtensors that result of slicing
the first dimension. This is a valid option for <cite>ndims = 1</cite>, but less
efficient than this implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to be assigned to this variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.constraint">
<em class="property">property </em><code class="sig-name descname">constraint</code><a class="headerlink" href="#tensorflow.Variable.constraint" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the constraint function associated with this variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The constraint function that was passed to the variable constructor.
Can be <cite>None</cite> if no constraint was passed.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.count_up_to">
<code class="sig-name descname">count_up_to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">limit</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.count_up_to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.count_up_to" title="Permalink to this definition">¶</a></dt>
<dd><p>Increments this variable until it reaches <cite>limit</cite>. (deprecated)</p>
<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Prefer Dataset.range instead.</p>
<p>When that Op is run it tries to increment the variable by <cite>1</cite>. If
incrementing the variable would bring it above <cite>limit</cite> then the Op raises
the exception <cite>OutOfRangeError</cite>.</p>
<p>If no error is raised, the Op outputs the value of the variable before
the increment.</p>
<p>This is essentially a shortcut for <cite>count_up_to(self, limit)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>limit</strong> -- value at which incrementing the variable raises an error.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> that will hold the variable value before the increment. If no
other Op modifies this variable, the values produced will all be
distinct.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.device">
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#tensorflow.Variable.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The device of this variable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.dtype">
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#tensorflow.Variable.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>DType</cite> of this variable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">session</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>In a session, computes and returns the value of this variable.</p>
<p>This is not a graph construction method, it does not add ops to the graph.</p>
<p>This convenience method requires a session where the graph
containing this variable has been launched. If no session is
passed, the default session is used.  See <cite>tf.compat.v1.Session</cite> for more
information on launching a graph and on sessions.</p>
<p><a href="#id352"><span class="problematic" id="id353">``</span></a><a href="#id354"><span class="problematic" id="id355">`</span></a>python
v = tf.Variable([1, 2])
init = tf.compat.v1.global_variables_initializer()</p>
<dl class="simple">
<dt>with tf.compat.v1.Session() as sess:</dt><dd><p>sess.run(init)
# Usage passing the session explicitly.
print(v.eval(sess))
# Usage with the default session.  The 'with' block
# above makes 'sess' the default session.
print(v.eval())</p>
</dd>
</dl>
<p><a href="#id356"><span class="problematic" id="id357">``</span></a><a href="#id358"><span class="problematic" id="id359">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>session</strong> -- The session to use to evaluate this variable. If none, the
default session is used.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy <cite>ndarray</cite> with a copy of the value of this variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.experimental_ref">
<code class="sig-name descname">experimental_ref</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.experimental_ref"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.experimental_ref" title="Permalink to this definition">¶</a></dt>
<dd><p>DEPRECATED FUNCTION</p>
<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use ref() instead.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.from_proto">
<em class="property">static </em><code class="sig-name descname">from_proto</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">variable_def</span></em>, <em class="sig-param"><span class="n">import_scope</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.from_proto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.from_proto" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <cite>Variable</cite> object created from <cite>variable_def</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.gather_nd">
<code class="sig-name descname">gather_nd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.gather_nd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.gather_nd" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather slices from <cite>params</cite> into a Tensor with shape specified by <cite>indices</cite>.</p>
<p>See tf.gather_nd for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Index tensor.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>params</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.get_shape">
<code class="sig-name descname">get_shape</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.get_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.get_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias of <cite>Variable.shape</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.graph">
<em class="property">property </em><code class="sig-name descname">graph</code><a class="headerlink" href="#tensorflow.Variable.graph" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Graph</cite> of this variable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.initial_value">
<em class="property">property </em><code class="sig-name descname">initial_value</code><a class="headerlink" href="#tensorflow.Variable.initial_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the Tensor used as the initial value for the variable.</p>
<p>Note that this is different from <cite>initialized_value()</cite> which runs
the op that initializes the variable before returning its value.
This method returns the tensor that is used by the op that initializes
the variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>Tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.initialized_value">
<code class="sig-name descname">initialized_value</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.initialized_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.initialized_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the initialized variable. (deprecated)</p>
<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.</p>
<p>You should use this instead of the variable itself to initialize another
variable with a value that depends on the value of this variable.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">Initialize</span> <span class="pre">'v'</span> <span class="pre">with</span> <span class="pre">a</span> <span class="pre">random</span> <span class="pre">tensor.</span>
<span class="pre">v</span> <span class="pre">=</span> <span class="pre">tf.Variable(tf.random.truncated_normal([10,</span> <span class="pre">40]))</span>
<span class="pre">#</span> <span class="pre">Use</span> <span class="pre">`initialized_value`</span> <span class="pre">to</span> <span class="pre">guarantee</span> <span class="pre">that</span> <span class="pre">`v`</span> <span class="pre">has</span> <span class="pre">been</span>
<span class="pre">#</span> <span class="pre">initialized</span> <span class="pre">before</span> <span class="pre">its</span> <span class="pre">value</span> <span class="pre">is</span> <span class="pre">used</span> <span class="pre">to</span> <span class="pre">initialize</span> <span class="pre">`w`.</span>
<span class="pre">#</span> <span class="pre">The</span> <span class="pre">random</span> <span class="pre">values</span> <span class="pre">are</span> <span class="pre">picked</span> <span class="pre">only</span> <span class="pre">once.</span>
<span class="pre">w</span> <span class="pre">=</span> <span class="pre">tf.Variable(v.initialized_value()</span> <span class="pre">*</span> <span class="pre">2.0)</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>Tensor</cite> holding the value of this variable after its initializer
has run.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.initializer">
<em class="property">property </em><code class="sig-name descname">initializer</code><a class="headerlink" href="#tensorflow.Variable.initializer" title="Permalink to this definition">¶</a></dt>
<dd><p>The initializer operation for this variable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.load">
<code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">session</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load new value into this variable. (deprecated)</p>
<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Prefer Variable.assign which has equivalent behavior in 2.X.</p>
<p>Writes new value to variable's memory. Doesn't add ops to the graph.</p>
<p>This convenience method requires a session where the graph
containing this variable has been launched. If no session is
passed, the default session is used.  See <cite>tf.compat.v1.Session</cite> for more
information on launching a graph and on sessions.</p>
<p><a href="#id360"><span class="problematic" id="id361">``</span></a><a href="#id362"><span class="problematic" id="id363">`</span></a>python
v = tf.Variable([1, 2])
init = tf.compat.v1.global_variables_initializer()</p>
<dl class="simple">
<dt>with tf.compat.v1.Session() as sess:</dt><dd><p>sess.run(init)
# Usage passing the session explicitly.
v.load([2, 3], sess)
print(v.eval(sess)) # prints [2 3]
# Usage with the default session.  The 'with' block
# above makes 'sess' the default session.
v.load([3, 4], sess)
print(v.eval()) # prints [3 4]</p>
</dd>
</dl>
<p><a href="#id364"><span class="problematic" id="id365">``</span></a><a href="#id366"><span class="problematic" id="id367">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> -- New variable value</p></li>
<li><p><strong>session</strong> -- The session to use to evaluate this variable. If none, the
default session is used.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- Session is not passed and no default session</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#tensorflow.Variable.name" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of this variable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.op">
<em class="property">property </em><code class="sig-name descname">op</code><a class="headerlink" href="#tensorflow.Variable.op" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>Operation</cite> of this variable.</p>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.read_value">
<code class="sig-name descname">read_value</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.read_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.read_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of this variable, read in the current context.</p>
<p>Can be different from value() if it's on another device, with control
dependencies, etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>Tensor</cite> containing the value of the variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.ref">
<code class="sig-name descname">ref</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.ref"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.ref" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a hashable reference object to this Variable.</p>
<p>The primary use case for this API is to put variables in a set/dictionary.
We can't put variables in a set/dictionary as <cite>variable.__hash__()</cite> is no
longer available starting Tensorflow 2.0.</p>
<p>The following will raise an exception starting 2.0</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variable_set</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">}</span>
<span class="gt">Traceback (most recent call last):</span>
  <span class="c">...</span>
<span class="gr">TypeError</span>: <span class="n">Variable is unhashable. Instead, use tensor.ref() as the key.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variable_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="s1">&#39;five&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="s1">&#39;ten&#39;</span><span class="p">}</span>
<span class="gt">Traceback (most recent call last):</span>
  <span class="c">...</span>
<span class="gr">TypeError</span>: <span class="n">Variable is unhashable. Instead, use tensor.ref() as the key.</span>
</pre></div>
</div>
<p>Instead, we can use <cite>variable.ref()</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">variable_set</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">ref</span><span class="p">(),</span> <span class="n">z</span><span class="o">.</span><span class="n">ref</span><span class="p">()}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span> <span class="ow">in</span> <span class="n">variable_set</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variable_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">():</span> <span class="s1">&#39;five&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ref</span><span class="p">():</span> <span class="s1">&#39;ten&#39;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">ref</span><span class="p">():</span> <span class="s1">&#39;ten&#39;</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variable_dict</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">ref</span><span class="p">()]</span>
<span class="go">&#39;ten&#39;</span>
</pre></div>
</div>
<p>Also, the reference object provides <cite>.deref()</cite> function that returns the
original Variable.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">ref</span><span class="p">()</span><span class="o">.</span><span class="n">deref</span><span class="p">()</span>
<span class="go">&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=int32, numpy=5&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_add">
<code class="sig-name descname">scatter_add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds <cite>tf.IndexedSlices</cite> to this variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to be added to this variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_div">
<code class="sig-name descname">scatter_div</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_div"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_div" title="Permalink to this definition">¶</a></dt>
<dd><p>Divide this variable by <cite>tf.IndexedSlices</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to divide this variable by.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_max">
<code class="sig-name descname">scatter_max</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_max" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates this variable with the max of <cite>tf.IndexedSlices</cite> and itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to use as an argument of max with this
variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_min">
<code class="sig-name descname">scatter_min</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_min"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_min" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates this variable with the min of <cite>tf.IndexedSlices</cite> and itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to use as an argument of min with this
variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_mul">
<code class="sig-name descname">scatter_mul</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_mul"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_mul" title="Permalink to this definition">¶</a></dt>
<dd><p>Multiply this variable by <cite>tf.IndexedSlices</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to multiply this variable by.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_nd_add">
<code class="sig-name descname">scatter_nd_add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">updates</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_nd_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_nd_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies sparse addition to individual values or slices in a Variable.</p>
<p>The Variable has rank <cite>P</cite> and <cite>indices</cite> is a <cite>Tensor</cite> of rank <cite>Q</cite>.</p>
<p><cite>indices</cite> must be integer tensor, containing indices into self.
It must be shape <cite>[d_0, ..., d_{Q-2}, K]</cite> where <cite>0 &lt; K &lt;= P</cite>.</p>
<p>The innermost dimension of <cite>indices</cite> (with length <cite>K</cite>) corresponds to
indices into elements (if <cite>K = P</cite>) or slices (if <cite>K &lt; P</cite>) along the <a href="#id368"><span class="problematic" id="id369">`</span></a>K`th
dimension of self.</p>
<p><cite>updates</cite> is <cite>Tensor</cite> of rank <cite>Q-1+P-K</cite> with shape:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">[d_0,</span> <span class="pre">...,</span> <span class="pre">d_{Q-2},</span> <span class="pre">self.shape[K],</span> <span class="pre">...,</span> <span class="pre">self.shape[P-1]].</span>
<span class="pre">`</span></code></p>
<p>For example, say we want to add 4 scattered elements to a rank-1 tensor to
8 elements. In Python, that update would look like this:</p>
<dl>
<dt><a href="#id370"><span class="problematic" id="id371">``</span></a><a href="#id372"><span class="problematic" id="id373">`</span></a>python</dt><dd><p>v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
indices = tf.constant([[4], [3], [1] ,[7]])
updates = tf.constant([9, 10, 11, 12])
add = v.scatter_nd_add(indices, updates)
with tf.compat.v1.Session() as sess:</p>
<blockquote>
<div><p>print sess.run(add)</p>
</div></blockquote>
</dd>
</dl>
<p><a href="#id374"><span class="problematic" id="id375">``</span></a><a href="#id376"><span class="problematic" id="id377">`</span></a></p>
<p>The resulting update to v would look like this:</p>
<blockquote>
<div><p>[1, 13, 3, 14, 14, 6, 7, 20]</p>
</div></blockquote>
<p>See <cite>tf.scatter_nd</cite> for more details about how to make updates to
slices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- The indices to be used in the operation.</p></li>
<li><p><strong>updates</strong> -- The values to be used in the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_nd_sub">
<code class="sig-name descname">scatter_nd_sub</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">updates</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_nd_sub"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_nd_sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies sparse subtraction to individual values or slices in a Variable.</p>
<p>Assuming the variable has rank <cite>P</cite> and <cite>indices</cite> is a <cite>Tensor</cite> of rank <cite>Q</cite>.</p>
<p><cite>indices</cite> must be integer tensor, containing indices into self.
It must be shape <cite>[d_0, ..., d_{Q-2}, K]</cite> where <cite>0 &lt; K &lt;= P</cite>.</p>
<p>The innermost dimension of <cite>indices</cite> (with length <cite>K</cite>) corresponds to
indices into elements (if <cite>K = P</cite>) or slices (if <cite>K &lt; P</cite>) along the <a href="#id378"><span class="problematic" id="id379">`</span></a>K`th
dimension of self.</p>
<p><cite>updates</cite> is <cite>Tensor</cite> of rank <cite>Q-1+P-K</cite> with shape:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">[d_0,</span> <span class="pre">...,</span> <span class="pre">d_{Q-2},</span> <span class="pre">self.shape[K],</span> <span class="pre">...,</span> <span class="pre">self.shape[P-1]].</span>
<span class="pre">`</span></code></p>
<p>For example, say we want to add 4 scattered elements to a rank-1 tensor to
8 elements. In Python, that update would look like this:</p>
<dl>
<dt><a href="#id380"><span class="problematic" id="id381">``</span></a><a href="#id382"><span class="problematic" id="id383">`</span></a>python</dt><dd><p>v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
indices = tf.constant([[4], [3], [1] ,[7]])
updates = tf.constant([9, 10, 11, 12])
op = v.scatter_nd_sub(indices, updates)
with tf.compat.v1.Session() as sess:</p>
<blockquote>
<div><p>print sess.run(op)</p>
</div></blockquote>
</dd>
</dl>
<p><a href="#id384"><span class="problematic" id="id385">``</span></a><a href="#id386"><span class="problematic" id="id387">`</span></a></p>
<p>The resulting update to v would look like this:</p>
<blockquote>
<div><p>[1, -9, 3, -6, -6, 6, 7, -4]</p>
</div></blockquote>
<p>See <cite>tf.scatter_nd</cite> for more details about how to make updates to
slices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- The indices to be used in the operation.</p></li>
<li><p><strong>updates</strong> -- The values to be used in the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_nd_update">
<code class="sig-name descname">scatter_nd_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">updates</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_nd_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_nd_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies sparse assignment to individual values or slices in a Variable.</p>
<p>The Variable has rank <cite>P</cite> and <cite>indices</cite> is a <cite>Tensor</cite> of rank <cite>Q</cite>.</p>
<p><cite>indices</cite> must be integer tensor, containing indices into self.
It must be shape <cite>[d_0, ..., d_{Q-2}, K]</cite> where <cite>0 &lt; K &lt;= P</cite>.</p>
<p>The innermost dimension of <cite>indices</cite> (with length <cite>K</cite>) corresponds to
indices into elements (if <cite>K = P</cite>) or slices (if <cite>K &lt; P</cite>) along the <a href="#id388"><span class="problematic" id="id389">`</span></a>K`th
dimension of self.</p>
<p><cite>updates</cite> is <cite>Tensor</cite> of rank <cite>Q-1+P-K</cite> with shape:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">[d_0,</span> <span class="pre">...,</span> <span class="pre">d_{Q-2},</span> <span class="pre">self.shape[K],</span> <span class="pre">...,</span> <span class="pre">self.shape[P-1]].</span>
<span class="pre">`</span></code></p>
<p>For example, say we want to add 4 scattered elements to a rank-1 tensor to
8 elements. In Python, that update would look like this:</p>
<dl>
<dt><a href="#id390"><span class="problematic" id="id391">``</span></a><a href="#id392"><span class="problematic" id="id393">`</span></a>python</dt><dd><p>v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
indices = tf.constant([[4], [3], [1] ,[7]])
updates = tf.constant([9, 10, 11, 12])
op = v.scatter_nd_assign(indices, updates)
with tf.compat.v1.Session() as sess:</p>
<blockquote>
<div><p>print sess.run(op)</p>
</div></blockquote>
</dd>
</dl>
<p><a href="#id394"><span class="problematic" id="id395">``</span></a><a href="#id396"><span class="problematic" id="id397">`</span></a></p>
<p>The resulting update to v would look like this:</p>
<blockquote>
<div><p>[1, 11, 3, 10, 9, 6, 7, 12]</p>
</div></blockquote>
<p>See <cite>tf.scatter_nd</cite> for more details about how to make updates to
slices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- The indices to be used in the operation.</p></li>
<li><p><strong>updates</strong> -- The values to be used in the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_sub">
<code class="sig-name descname">scatter_sub</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_sub"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtracts <cite>tf.IndexedSlices</cite> from this variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to be subtracted from this variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.scatter_update">
<code class="sig-name descname">scatter_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse_delta</span></em>, <em class="sig-param"><span class="n">use_locking</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.scatter_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.scatter_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Assigns <cite>tf.IndexedSlices</cite> to this variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse_delta</strong> -- <cite>tf.IndexedSlices</cite> to be assigned to this variable.</p></li>
<li><p><strong>use_locking</strong> -- If <cite>True</cite>, use locking during the operation.</p></li>
<li><p><strong>name</strong> -- the name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated variable.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>sparse_delta</cite> is not an <cite>IndexedSlices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.set_shape">
<code class="sig-name descname">set_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.set_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.set_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Overrides the shape for this variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> -- the <cite>TensorShape</cite> representing the overridden shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.shape">
<em class="property">property </em><code class="sig-name descname">shape</code><a class="headerlink" href="#tensorflow.Variable.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>TensorShape</cite> of this variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>TensorShape</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.sparse_read">
<code class="sig-name descname">sparse_read</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.sparse_read"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.sparse_read" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather slices from params axis axis according to indices.</p>
<p>This function supports a subset of tf.gather, see tf.gather for details on
usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- The index <cite>Tensor</cite>.  Must be one of the following types: <cite>int32</cite>,
<cite>int64</cite>. Must be in range <cite>[0, params.shape[axis])</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>params</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.synchronization">
<em class="property">property </em><code class="sig-name descname">synchronization</code><a class="headerlink" href="#tensorflow.Variable.synchronization" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.to_proto">
<code class="sig-name descname">to_proto</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">export_scope</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.to_proto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.to_proto" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a <cite>Variable</cite> to a <cite>VariableDef</cite> protocol buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>export_scope</strong> -- Optional <cite>string</cite>. Name scope to remove.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>VariableDef</cite> protocol buffer, or <cite>None</cite> if the <cite>Variable</cite> is not
in the specified name scope.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.trainable">
<em class="property">property </em><code class="sig-name descname">trainable</code><a class="headerlink" href="#tensorflow.Variable.trainable" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="tensorflow.Variable.value">
<code class="sig-name descname">value</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#Variable.value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.Variable.value" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the last snapshot of this variable.</p>
<p>You usually do not need to call this method as all ops that need the value
of the variable call it automatically through a <cite>convert_to_tensor()</cite> call.</p>
<p>Returns a <cite>Tensor</cite> which holds the value of the variable.  You can not
assign a new value to this tensor as it is not a reference to the variable.</p>
<p>To avoid copies, if the consumer of the returned value is on the same device
as the variable, this actually returns the live value of the variable, not
a copy.  Updates to the variable are seen by the consumer.  If the consumer
is on a different device it will get a copy of the variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>Tensor</cite> containing the value of the variable.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.VariableAggregation">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">VariableAggregation</code><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#VariableAggregation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.VariableAggregation" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.ops.variables.VariableAggregationV2</span></code></p>
</dd></dl>

<dl class="py class">
<dt id="tensorflow.VariableSynchronization">
<em class="property">class </em><code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">VariableSynchronization</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variables.html#VariableSynchronization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.VariableSynchronization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Indicates when a distributed variable will be synced.</p>
<ul class="simple">
<li><p><cite>AUTO</cite>: Indicates that the synchronization will be determined by the current
<cite>DistributionStrategy</cite> (eg. With <cite>MirroredStrategy</cite> this would be
<cite>ON_WRITE</cite>).</p></li>
<li><p><cite>NONE</cite>: Indicates that there will only be one copy of the variable, so
there is no need to sync.</p></li>
<li><p><cite>ON_WRITE</cite>: Indicates that the variable will be updated across devices
every time it is written.</p></li>
<li><p><cite>ON_READ</cite>: Indicates that the variable will be aggregated across devices
when it is read (eg. when checkpointing or when evaluating an op that uses
the variable).</p></li>
</ul>
<dl class="py attribute">
<dt id="tensorflow.VariableSynchronization.AUTO">
<code class="sig-name descname">AUTO</code><em class="property"> = 0</em><a class="headerlink" href="#tensorflow.VariableSynchronization.AUTO" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="tensorflow.VariableSynchronization.NONE">
<code class="sig-name descname">NONE</code><em class="property"> = 1</em><a class="headerlink" href="#tensorflow.VariableSynchronization.NONE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="tensorflow.VariableSynchronization.ON_READ">
<code class="sig-name descname">ON_READ</code><em class="property"> = 3</em><a class="headerlink" href="#tensorflow.VariableSynchronization.ON_READ" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="tensorflow.VariableSynchronization.ON_WRITE">
<code class="sig-name descname">ON_WRITE</code><em class="property"> = 2</em><a class="headerlink" href="#tensorflow.VariableSynchronization.ON_WRITE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="tensorflow.abs">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">abs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#abs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the absolute value of a tensor.</p>
<p>Given a tensor of integer or floating-point values, this operation returns a
tensor of the same type, where each element contains the absolute value of the
corresponding element in the input.</p>
<p>Given a tensor <cite>x</cite> of complex numbers, this operation returns a tensor of type
<cite>float32</cite> or <cite>float64</cite> that is the absolute value of each element in <cite>x</cite>. For
a complex number \(a + bj\), its absolute value is computed as \(sqrt{a^2
+ b^2}\).  For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.25</span> <span class="o">+</span> <span class="mf">4.75</span><span class="n">j</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">3.25</span> <span class="o">+</span> <span class="mf">5.75</span><span class="n">j</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=</span>
<span class="go">array([[5.25594901],</span>
<span class="go">       [6.60492241]])&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite> or <cite>SparseTensor</cite> of type <cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>,
<cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite> or <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <cite>Tensor</cite> or <cite>SparseTensor</cite> of the same size, type and sparsity as <cite>x</cite>,</dt><dd><p>with absolute values. Note, for <cite>complex64</cite> or <cite>complex128</cite> input, the
returned <cite>Tensor</cite> will be of type <cite>float32</cite> or <cite>float64</cite>, respectively.</p>
</dd>
</dl>
<p>If <cite>x</cite> is a <cite>SparseTensor</cite>, returns
<cite>SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)</cite></p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.acos">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">acos</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#acos"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes acos of x element-wise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.acosh">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">acosh</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#acosh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.acosh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes inverse hyperbolic cosine of x element-wise.</p>
<p>Given an input tensor, the function computes inverse hyperbolic cosine of every element.
Input range is <cite>[1, inf]</cite>. It returns <cite>nan</cite> if the input lies outside the range.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-2,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">200,</span> <span class="pre">10000,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.acosh(x)</span> <span class="pre">==&gt;</span> <span class="pre">[nan</span> <span class="pre">nan</span> <span class="pre">0.</span> <span class="pre">0.62236255</span> <span class="pre">5.9914584</span> <span class="pre">9.903487</span> <span class="pre">inf]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.add">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns x + y element-wise.</p>
<p><em>NOTE</em>: <cite>math.add</cite> supports broadcasting. <cite>AddN</cite> does not. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>uint8</cite>, <cite>int8</cite>, <cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>, <cite>string</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.add_n">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">add_n</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#add_n"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.add_n" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds all input tensors element-wise.</p>
<p><cite>tf.math.add_n</cite> performs the same operation as <cite>tf.math.accumulate_n</cite>, but it
waits for all of its inputs to be ready before beginning to sum.
This buffering can result in higher memory consumption when inputs are ready
at different times, since the minimum temporary storage required is
proportional to the input size rather than the output size.</p>
<p>This op does not [broadcast](
<a class="reference external" href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html">https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html</a>)
its inputs. If you need broadcasting, use <cite>tf.math.add</cite> (or the <cite>+</cite> operator)
instead.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">add_n</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=</span>
<span class="go">array([[ 7, 16],</span>
<span class="go">       [10, 25]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> -- A list of <cite>tf.Tensor</cite> or <cite>tf.IndexedSlices</cite> objects, each with the
same shape and type. <cite>tf.IndexedSlices</cite> objects will be converted into
dense tensors prior to adding.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tf.Tensor</cite> of the same shape and type as the elements of <cite>inputs</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> -- If <cite>inputs</cite> don't all have same shape and dtype or the shape</p></li>
<li><p><strong>cannot be inferred.</strong> -- </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.argmax">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">argmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">tf.int64</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the index with the largest value across axes of a tensor.</p>
<p>Note that in case of ties the identity of the return value is not guaranteed.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>  <span class="c1"># A[2] is maximum in tensor A</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
<span class="gp">... </span>                 <span class="p">[</span><span class="mi">14</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">27</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>axis</strong> -- An integer, the axis to reduce across. Default to 0.</p></li>
<li><p><strong>output_type</strong> -- An optional output dtype (<cite>tf.int32</cite> or <cite>tf.int64</cite>). Defaults
to <cite>tf.int64</cite>.</p></li>
<li><p><strong>name</strong> -- An optional name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>output_type</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.argmin">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">argmin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_type</span><span class="o">=</span><span class="default_value">tf.int64</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#argmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.argmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the index with the smallest value across axes of a tensor.</p>
<p>Note that in case of ties the identity of the return value is not guaranteed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>,
<cite>int32</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>int8</cite>, <cite>complex64</cite>, <cite>int64</cite>, <cite>qint8</cite>,
<cite>quint8</cite>, <cite>qint32</cite>, <cite>bfloat16</cite>, <cite>uint16</cite>, <cite>complex128</cite>, <cite>half</cite>, <cite>uint32</cite>,
<cite>uint64</cite>.</p></li>
<li><p><strong>axis</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
int32 or int64, must be in the range <cite>-rank(input), rank(input))</cite>.
Describes which axis of the input Tensor to reduce across. For vectors,
use axis = 0.</p></li>
<li><p><strong>output_type</strong> -- An optional <cite>tf.DType</cite> from: <cite>tf.int32, tf.int64</cite>. Defaults to
<cite>tf.int64</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>output_type</cite>.</p>
</dd>
</dl>
<p>Usage:
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">import</span> <span class="pre">tensorflow</span> <span class="pre">as</span> <span class="pre">tf</span>
<span class="pre">a</span> <span class="pre">=</span> <span class="pre">[1,</span> <span class="pre">10,</span> <span class="pre">26.9,</span> <span class="pre">2.8,</span> <span class="pre">166.32,</span> <span class="pre">62.3]</span>
<span class="pre">b</span> <span class="pre">=</span> <span class="pre">tf.math.argmin(input</span> <span class="pre">=</span> <span class="pre">a)</span>
<span class="pre">c</span> <span class="pre">=</span> <span class="pre">tf.keras.backend.eval(b)</span>
<span class="pre">#</span> <span class="pre">c</span> <span class="pre">=</span> <span class="pre">0</span>
<span class="pre">#</span> <span class="pre">here</span> <span class="pre">a[0]</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">which</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">smallest</span> <span class="pre">element</span> <span class="pre">of</span> <span class="pre">a</span> <span class="pre">across</span> <span class="pre">axis</span> <span class="pre">0</span>
<span class="pre">`</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.argsort">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">argsort</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">direction</span><span class="o">=</span><span class="default_value">'ASCENDING'</span></em>, <em class="sig-param"><span class="n">stable</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/sort_ops.html#argsort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.argsort" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the indices of a tensor that give its sorted order along an axis.</p>
<p>For a 1D tensor, <cite>tf.gather(values, tf.argsort(values))</cite> is equivalent to
<cite>tf.sort(values)</cite>. For higher dimensions, the output has the same shape as
<cite>values</cite>, but along the given axis, values represent the index of the sorted
element in that slice of the tensor at the given position.</p>
<p>Usage:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">import</span> <span class="pre">tensorflow</span> <span class="pre">as</span> <span class="pre">tf</span>
<span class="pre">a</span> <span class="pre">=</span> <span class="pre">[1,</span> <span class="pre">10,</span> <span class="pre">26.9,</span> <span class="pre">2.8,</span> <span class="pre">166.32,</span> <span class="pre">62.3]</span>
<span class="pre">b</span> <span class="pre">=</span> <span class="pre">tf.argsort(a,axis=-1,direction='ASCENDING',stable=False,name=None)</span>
<span class="pre">c</span> <span class="pre">=</span> <span class="pre">tf.keras.backend.eval(b)</span>
<span class="pre">#</span> <span class="pre">Here,</span> <span class="pre">c</span> <span class="pre">=</span> <span class="pre">[0</span> <span class="pre">3</span> <span class="pre">1</span> <span class="pre">2</span> <span class="pre">5</span> <span class="pre">4]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- 1-D or higher numeric <cite>Tensor</cite>.</p></li>
<li><p><strong>axis</strong> -- The axis along which to sort. The default is -1, which sorts the last
axis.</p></li>
<li><p><strong>direction</strong> -- The direction in which to sort the values (<cite>'ASCENDING'</cite> or
<cite>'DESCENDING'</cite>).</p></li>
<li><p><strong>stable</strong> -- If True, equal elements in the original tensor will not be
re-ordered in the returned order. Unstable sort is not yet implemented,
but will eventually be the default for performance reasons. If you require
a stable order, pass <cite>stable=True</cite> for forwards compatibility.</p></li>
<li><p><strong>name</strong> -- Optional name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>An int32 <cite>Tensor</cite> with the same shape as <cite>values</cite>. The indices that would</dt><dd><p>sort each slice of the given <cite>values</cite> along the given <cite>axis</cite>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If axis is not a constant scalar, or the direction is invalid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.as_dtype">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">as_dtype</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">type_value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/dtypes.html#as_dtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.as_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the given <cite>type_value</cite> to a <cite>DType</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>type_value</strong> -- <p>A value that can be converted to a <cite>tf.DType</cite> object. This may
currently be a <cite>tf.DType</cite> object, a [<cite>DataType</cite>
enum](<a class="reference external" href="https://www.tensorflow.org/code/tensorflow/core/framework/types.proto">https://www.tensorflow.org/code/tensorflow/core/framework/types.proto</a>),</p>
<blockquote>
<div><p>a string type name, or a <cite>numpy.dtype</cite>.</p>
</div></blockquote>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>DType</cite> corresponding to <cite>type_value</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- If <cite>type_value</cite> cannot be converted to a <cite>DType</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.as_string">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">as_string</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">precision</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">scientific</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">shortest</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">width</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">fill</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_string_ops.html#as_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.as_string" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts each entry in the given tensor to strings.</p>
<p>Supports many numeric types and boolean.</p>
<p>For Unicode, see the
[<a class="reference external" href="https://www.tensorflow.org/tutorials/representation/unicode](Working">https://www.tensorflow.org/tutorials/representation/unicode](Working</a> with Unicode text)
tutorial.</p>
<p>Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">as_string</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=string, numpy=array([b&#39;3&#39;, b&#39;2&#39;], dtype=object)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">as_string</span><span class="p">([</span><span class="mf">3.1415926</span><span class="p">,</span> <span class="mf">2.71828</span><span class="p">],</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([b&#39;3.14&#39;, b&#39;2.72&#39;], dtype=object)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int8</cite>, <cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>bool</cite>.</p></li>
<li><p><strong>precision</strong> -- An optional <cite>int</cite>. Defaults to <cite>-1</cite>.
The post-decimal precision to use for floating point numbers.
Only used if precision &gt; -1.</p></li>
<li><p><strong>scientific</strong> -- An optional <cite>bool</cite>. Defaults to <cite>False</cite>.
Use scientific notation for floating point numbers.</p></li>
<li><p><strong>shortest</strong> -- An optional <cite>bool</cite>. Defaults to <cite>False</cite>.
Use shortest representation (either scientific or standard) for
floating point numbers.</p></li>
<li><p><strong>width</strong> -- An optional <cite>int</cite>. Defaults to <cite>-1</cite>.
Pad pre-decimal numbers to this width.
Applies to both floating point and integer numbers.
Only used if width &gt; -1.</p></li>
<li><p><strong>fill</strong> -- An optional <cite>string</cite>. Defaults to <cite>&quot;&quot;</cite>.
The value to pad if width &gt; -1.  If empty, pads with spaces.
Another typical value is '0'.  String cannot be longer than 1 character.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>string</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.asin">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">asin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#asin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the trignometric inverse sine of x element-wise.</p>
<p>The <cite>tf.math.asin</cite> operation returns the inverse of <cite>tf.math.sin</cite>, such that
if <cite>y = tf.math.sin(x)</cite> then, <cite>x = tf.math.asin(y)</cite>.</p>
<p><strong>Note</strong>: The output of <cite>tf.math.asin</cite> will lie within the invertible range
of sine, i.e [-pi/2, pi/2].</p>
<p>For example:</p>
<p><a href="#id398"><span class="problematic" id="id399">``</span></a><a href="#id400"><span class="problematic" id="id401">`</span></a>python
# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.sin(x) # [0.8659266, 0.7068252]</p>
<p>tf.math.asin(y) # [1.047, 0.785] = x
<a href="#id402"><span class="problematic" id="id403">``</span></a><a href="#id404"><span class="problematic" id="id405">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.asinh">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">asinh</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#asinh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.asinh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes inverse hyperbolic sine of x element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes inverse hyperbolic sine
for every element in the tensor. Both input and output has a range of
<cite>[-inf, inf]</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-2,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">200,</span> <span class="pre">10000,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.asinh(x)</span> <span class="pre">==&gt;</span> <span class="pre">[-inf</span> <span class="pre">-1.4436355</span> <span class="pre">-0.4812118</span> <span class="pre">0.8813736</span> <span class="pre">1.0159732</span> <span class="pre">5.991471</span> <span class="pre">9.903487</span> <span class="pre">inf]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.assert_equal">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">assert_equal</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">message</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">summarize</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/check_ops.html#assert_equal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.assert_equal" title="Permalink to this definition">¶</a></dt>
<dd><p>Assert the condition <cite>x == y</cite> holds element-wise.</p>
<p>This Op checks that <cite>x[i] == y[i]</cite> holds for every pair of (possibly
broadcast) elements of <cite>x</cite> and <cite>y</cite>. If both <cite>x</cite> and <cite>y</cite> are empty, this is
trivially satisfied.</p>
<p>If <cite>x</cite> and <cite>y</cite> are not equal, <cite>message</cite>, as well as the first <cite>summarize</cite>
entries of <cite>x</cite> and <cite>y</cite> are printed, and <cite>InvalidArgumentError</cite> is raised.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- Numeric <cite>Tensor</cite>.</p></li>
<li><p><strong>y</strong> -- Numeric <cite>Tensor</cite>, same dtype as and broadcastable to <cite>x</cite>.</p></li>
<li><p><strong>message</strong> -- A string to prefix to the default message.</p></li>
<li><p><strong>summarize</strong> -- Print this many entries of each tensor.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).  Defaults to &quot;assert_equal&quot;.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Op that raises <cite>InvalidArgumentError</cite> if <cite>x == y</cite> is False. This can be</dt><dd><p>used with <cite>tf.control_dependencies</cite> inside of <a href="#id406"><span class="problematic" id="id407">`</span></a>tf.function`s to block
followup computation until the check has executed.</p>
</dd>
</dl>
<p>&#64;compatibility(eager)
returns None
&#64;end_compatibility</p>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>InvalidArgumentError</strong> -- if the check can be performed immediately and
    <cite>x == y</cite> is False. The check can be performed immediately during eager
    execution or if <cite>x</cite> and <cite>y</cite> are statically known.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.assert_greater">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">assert_greater</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">message</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">summarize</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/check_ops.html#assert_greater"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.assert_greater" title="Permalink to this definition">¶</a></dt>
<dd><p>Assert the condition <cite>x &gt; y</cite> holds element-wise.</p>
<p>This Op checks that <cite>x[i] &gt; y[i]</cite> holds for every pair of (possibly
broadcast) elements of <cite>x</cite> and <cite>y</cite>. If both <cite>x</cite> and <cite>y</cite> are empty, this is
trivially satisfied.</p>
<p>If <cite>x</cite> is not greater than <cite>y</cite> element-wise, <cite>message</cite>, as well as the first
<cite>summarize</cite> entries of <cite>x</cite> and <cite>y</cite> are printed, and <cite>InvalidArgumentError</cite> is
raised.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- Numeric <cite>Tensor</cite>.</p></li>
<li><p><strong>y</strong> -- Numeric <cite>Tensor</cite>, same dtype as and broadcastable to <cite>x</cite>.</p></li>
<li><p><strong>message</strong> -- A string to prefix to the default message.</p></li>
<li><p><strong>summarize</strong> -- Print this many entries of each tensor.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).  Defaults to &quot;assert_greater&quot;.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Op that raises <cite>InvalidArgumentError</cite> if <cite>x &gt; y</cite> is False. This can be</dt><dd><p>used with <cite>tf.control_dependencies</cite> inside of <a href="#id408"><span class="problematic" id="id409">`</span></a>tf.function`s to block
followup computation until the check has executed.</p>
</dd>
</dl>
<p>&#64;compatibility(eager)
returns None
&#64;end_compatibility</p>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>InvalidArgumentError</strong> -- if the check can be performed immediately and
    <cite>x &gt; y</cite> is False. The check can be performed immediately during eager
    execution or if <cite>x</cite> and <cite>y</cite> are statically known.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.assert_less">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">assert_less</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">message</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">summarize</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/check_ops.html#assert_less"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.assert_less" title="Permalink to this definition">¶</a></dt>
<dd><p>Assert the condition <cite>x &lt; y</cite> holds element-wise.</p>
<p>This Op checks that <cite>x[i] &lt; y[i]</cite> holds for every pair of (possibly
broadcast) elements of <cite>x</cite> and <cite>y</cite>. If both <cite>x</cite> and <cite>y</cite> are empty, this is
trivially satisfied.</p>
<p>If <cite>x</cite> is not less than <cite>y</cite> element-wise, <cite>message</cite>, as well as the first
<cite>summarize</cite> entries of <cite>x</cite> and <cite>y</cite> are printed, and <cite>InvalidArgumentError</cite> is
raised.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- Numeric <cite>Tensor</cite>.</p></li>
<li><p><strong>y</strong> -- Numeric <cite>Tensor</cite>, same dtype as and broadcastable to <cite>x</cite>.</p></li>
<li><p><strong>message</strong> -- A string to prefix to the default message.</p></li>
<li><p><strong>summarize</strong> -- Print this many entries of each tensor.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).  Defaults to &quot;assert_less&quot;.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Op that raises <cite>InvalidArgumentError</cite> if <cite>x &lt; y</cite> is False.
This can be used with <cite>tf.control_dependencies</cite> inside of <a href="#id410"><span class="problematic" id="id411">`</span></a>tf.function`s
to block followup computation until the check has executed.
&#64;compatibility(eager)
returns None
&#64;end_compatibility</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>InvalidArgumentError</strong> -- if the check can be performed immediately and
    <cite>x &lt; y</cite> is False. The check can be performed immediately during eager
    execution or if <cite>x</cite> and <cite>y</cite> are statically known.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.assert_rank">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">assert_rank</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">rank</span></em>, <em class="sig-param"><span class="n">message</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/check_ops.html#assert_rank"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.assert_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Assert that <cite>x</cite> has rank equal to <cite>rank</cite>.</p>
<p>This Op checks that the rank of <cite>x</cite> is equal to <cite>rank</cite>.</p>
<p>If <cite>x</cite> has a different rank, <cite>message</cite>, as well as the shape of <cite>x</cite> are
printed, and <cite>InvalidArgumentError</cite> is raised.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- <cite>Tensor</cite>.</p></li>
<li><p><strong>rank</strong> -- Scalar integer <cite>Tensor</cite>.</p></li>
<li><p><strong>message</strong> -- A string to prefix to the default message.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional). Defaults to
&quot;assert_rank&quot;.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Op raising <cite>InvalidArgumentError</cite> unless <cite>x</cite> has specified rank.
If static checks determine <cite>x</cite> has correct rank, a <cite>no_op</cite> is returned.
This can be used with <cite>tf.control_dependencies</cite> inside of <a href="#id412"><span class="problematic" id="id413">`</span></a>tf.function`s
to block followup computation until the check has executed.
&#64;compatibility(eager)
returns None
&#64;end_compatibility</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>InvalidArgumentError</strong> -- if the check can be performed immediately and
    <cite>x</cite> does not have rank <cite>rank</cite>. The check can be performed immediately
    during eager execution or if the shape of <cite>x</cite> is statically known.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.atan">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">atan</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#atan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the trignometric inverse tangent of x element-wise.</p>
<p>The <cite>tf.math.atan</cite> operation returns the inverse of <cite>tf.math.tan</cite>, such that
if <cite>y = tf.math.tan(x)</cite> then, <cite>x = tf.math.atan(y)</cite>.</p>
<p><strong>Note</strong>: The output of <cite>tf.math.atan</cite> will lie within the invertible range
of tan, i.e (-pi/2, pi/2).</p>
<p>For example:</p>
<p><a href="#id414"><span class="problematic" id="id415">``</span></a><a href="#id416"><span class="problematic" id="id417">`</span></a>python
# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.tan(x) # [1.731261, 0.99920404]</p>
<p>tf.math.atan(y) # [1.047, 0.785] = x
<a href="#id418"><span class="problematic" id="id419">``</span></a><a href="#id420"><span class="problematic" id="id421">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.atan2">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">atan2</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#atan2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes arctangent of <cite>y/x</cite> element-wise, respecting signs of the arguments.</p>
<p>This is the angle ( theta in [-pi, pi] ) such that
[ x = r cos(theta) ]
and
[ y = r sin(theta) ]
where (r = sqrt(x^2 + y^2) ).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>.</p></li>
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>y</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>y</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.atanh">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">atanh</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#atanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.atanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes inverse hyperbolic tangent of x element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes inverse hyperbolic tangent
for every element in the tensor. Input range is <cite>[-1,1]</cite> and output range is
<cite>[-inf, inf]</cite>. If input is <cite>-1</cite>, output will be <cite>-inf</cite> and if the
input is <cite>1</cite>, output will be <cite>inf</cite>. Values outside the range will have
<cite>nan</cite> as output.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-1,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0.5,</span> <span class="pre">10,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.atanh(x)</span> <span class="pre">==&gt;</span> <span class="pre">[nan</span> <span class="pre">-inf</span> <span class="pre">-0.54930615</span> <span class="pre">inf</span>&#160; <span class="pre">0.</span> <span class="pre">0.54930615</span> <span class="pre">nan</span> <span class="pre">nan]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.batch_to_space">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">batch_to_space</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">block_shape</span></em>, <em class="sig-param"><span class="n">crops</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#batch_to_space"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.batch_to_space" title="Permalink to this definition">¶</a></dt>
<dd><p>BatchToSpace for N-D tensors of type T.</p>
<p>This operation reshapes the &quot;batch&quot; dimension 0 into <cite>M + 1</cite> dimensions of
shape <cite>block_shape + [batch]</cite>, interleaves these blocks back into the grid
defined by the spatial dimensions <cite>[1, ..., M]</cite>, to obtain a result with the
same rank as the input.  The spatial dimensions of this intermediate result
are then optionally cropped according to <cite>crops</cite> to produce the output.  This
is the reverse of SpaceToBatch (see <cite>tf.space_to_batch</cite>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A N-D <cite>Tensor</cite> with shape <cite>input_shape = [batch] + spatial_shape +
remaining_shape</cite>, where <cite>spatial_shape</cite> has M dimensions.</p></li>
<li><p><strong>block_shape</strong> -- A 1-D <cite>Tensor</cite> with shape [M]. Must be one of the following
types: <cite>int32</cite>, <cite>int64</cite>. All values must be &gt;= 1. For backwards
compatibility with TF 1.0, this parameter may be an int, in which case it
is converted to
<cite>numpy.array([block_shape, block_shape],
dtype=numpy.int64)</cite>.</p></li>
<li><p><strong>crops</strong> -- <p>A  2-D <cite>Tensor</cite> with shape <cite>[M, 2]</cite>. Must be one of the
following types: <cite>int32</cite>, <cite>int64</cite>. All values must be &gt;= 0.
<cite>crops[i] = [crop_start, crop_end]</cite> specifies the amount to crop from
input dimension <cite>i + 1</cite>, which corresponds to spatial dimension <cite>i</cite>.
It is required that
<cite>crop_start[i] + crop_end[i] &lt;= block_shape[i] * input_shape[i + 1]</cite>.
This operation is equivalent to the following steps:
1. Reshape <cite>input</cite> to <cite>reshaped</cite> of shape: [block_shape[0], ...,</p>
<blockquote>
<div><p>block_shape[M-1], batch / prod(block_shape), input_shape[1], ...,
input_shape[N-1]]</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Permute dimensions of <cite>reshaped</cite> to produce <cite>permuted</cite> of shape
[batch / prod(block_shape),  input_shape[1], block_shape[0], ...,
input_shape[M], block_shape[M-1], input_shape[M+1],</p></li>
</ol>
<blockquote>
<div><p>..., input_shape[N-1]]</p>
</div></blockquote>
<ol class="arabic" start="3">
<li><p>Reshape <cite>permuted</cite> to produce <cite>reshaped_permuted</cite> of shape
[batch / prod(block_shape), input_shape[1] * block_shape[0], ...,
input_shape[M] * block_shape[M-1], input_shape[M+1], ...,
input_shape[N-1]]</p></li>
<li><p>Crop the start and end of dimensions <cite>[1, ..., M]</cite> of
<cite>reshaped_permuted</cite> according to <cite>crops</cite> to produce the output
of shape:
[batch / prod(block_shape),  input_shape[1] *</p>
<blockquote>
<div><p>block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] *
block_shape[M-1] - crops[M-1,0] - crops[M-1,1],  input_shape[M+1],
..., input_shape[N-1]]</p>
</div></blockquote>
</li>
</ol>
<p>Some Examples:
(1) For the following input of shape <cite>[4, 1, 1, 1]</cite>,</p>
<blockquote>
<div><p><cite>block_shape = [2, 2]</cite>, and <cite>crops = [[0, 0], [0, 0]]</cite>:
<a href="#id422"><span class="problematic" id="id423">``</span></a><a href="#id424"><span class="problematic" id="id425">`</span></a>python
[[[[1]]],</p>
<blockquote>
<div><p>[[[2]]],
[[[3]]],
[[[4]]]]</p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">The</span> <span class="pre">output</span> <span class="pre">tensor</span> <span class="pre">has</span> <span class="pre">shape</span> <span class="pre">`[1,</span> <span class="pre">2,</span> <span class="pre">2,</span> <span class="pre">1]`</span> <span class="pre">and</span> <span class="pre">value:</span>
<span class="pre">`</span></code> x = [[[[1], [2]],</p>
<blockquote>
<div><p>[[3], [4]]]] <a href="#id426"><span class="problematic" id="id427">``</span></a><a href="#id428"><span class="problematic" id="id429">`</span></a></p>
</div></blockquote>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>For the following input of shape <cite>[4, 1, 1, 3]</cite>,</p></li>
</ol>
<blockquote>
<div><p><cite>block_shape = [2, 2]</cite>, and <cite>crops = [[0, 0], [0, 0]]</cite>:
<a href="#id430"><span class="problematic" id="id431">``</span></a><a href="#id432"><span class="problematic" id="id433">`</span></a>python
[[[1,  2,   3]],</p>
<blockquote>
<div><p>[[4,  5,   6]],
[[7,  8,   9]],
[[10, 11, 12]]]</p>
</div></blockquote>
<p><a href="#id434"><span class="problematic" id="id435">``</span></a>`
The output tensor has shape <cite>[1, 2, 2, 3]</cite> and value:
<a href="#id436"><span class="problematic" id="id437">``</span></a><a href="#id438"><span class="problematic" id="id439">`</span></a>python
x = [[[[1, 2, 3], [4,  5,  6 ]],</p>
<blockquote>
<div><p>[[7, 8, 9], [10, 11, 12]]]]</p>
</div></blockquote>
<p><a href="#id440"><span class="problematic" id="id441">``</span></a><a href="#id442"><span class="problematic" id="id443">`</span></a></p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>For the following</p></li>
</ol>
<blockquote>
<div><blockquote>
<div><p>input of shape <cite>[4, 2, 2, 1]</cite>,
<cite>block_shape = [2, 2]</cite>, and <cite>crops = [[0, 0], [0, 0]]</cite>:
<a href="#id444"><span class="problematic" id="id445">``</span></a><a href="#id446"><span class="problematic" id="id447">`</span></a>python
x = [[[[1], [3]], [[ 9], [11]]],</p>
<blockquote>
<div><p>[[[2], [4]], [[10], [12]]],
[[[5], [7]], [[13], [15]]],
[[[6], [8]], [[14], [16]]]]</p>
</div></blockquote>
<p><a href="#id448"><span class="problematic" id="id449">``</span></a>`
The output tensor has shape <cite>[1, 4, 4, 1]</cite> and value:
<a href="#id450"><span class="problematic" id="id451">``</span></a><a href="#id452"><span class="problematic" id="id453">`</span></a>python
x = [[[1],  [2],  [ 3], [ 4]],</p>
<blockquote>
<div><p>[[5],  [6],  [ 7], [ 8]],
[[9],  [10], [11], [12]],
[[13], [14], [15], [16]]]</p>
</div></blockquote>
<p><a href="#id454"><span class="problematic" id="id455">``</span></a><a href="#id456"><span class="problematic" id="id457">`</span></a></p>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>For the following input of shape</p></li>
</ol>
<blockquote>
<div><p><cite>[8, 1, 3, 1]</cite>,
<cite>block_shape = [2, 2]</cite>, and <cite>crops = [[0, 0], [2, 0]]</cite>:
<a href="#id458"><span class="problematic" id="id459">``</span></a><a href="#id460"><span class="problematic" id="id461">`</span></a>python
x = [[[[0], [ 1], [ 3]]],</p>
<blockquote>
<div><p>[[[0], [ 9], [11]]],
[[[0], [ 2], [ 4]]],
[[[0], [10], [12]]],
[[[0], [ 5], [ 7]]],
[[[0], [13], [15]]],
[[[0], [ 6], [ 8]]],
[[[0], [14], [16]]]]</p>
</div></blockquote>
<p><a href="#id462"><span class="problematic" id="id463">``</span></a>`
The output tensor has shape <cite>[2, 2, 4, 1]</cite> and value:
<a href="#id464"><span class="problematic" id="id465">``</span></a><a href="#id466"><span class="problematic" id="id467">`</span></a>python
x = [[[[ 1], [ 2], [ 3], [ 4]],</p>
<blockquote>
<div><blockquote>
<div><p>[[ 5], [ 6], [ 7], [ 8]]],</p>
</div></blockquote>
<dl class="simple">
<dt>[[[ 9], [10], [11], [12]],</dt><dd><p>[[13], [14], [15], [16]]]] <a href="#id468"><span class="problematic" id="id469">``</span></a><a href="#id470"><span class="problematic" id="id471">`</span></a></p>
</dd>
</dl>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.bitcast">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">bitcast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">type</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#bitcast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.bitcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Bitcasts a tensor from one type to another without copying data.</p>
<p>Given a tensor <cite>input</cite>, this operation returns a tensor that has the same buffer
data as <cite>input</cite> with datatype <cite>type</cite>.</p>
<p>If the input datatype <cite>T</cite> is larger than the output datatype <cite>type</cite> then the
shape changes from [...] to [..., sizeof(<cite>T</cite>)/sizeof(<cite>type</cite>)].</p>
<p>If <cite>T</cite> is smaller than <cite>type</cite>, the operator requires that the rightmost
dimension be equal to sizeof(<cite>type</cite>)/sizeof(<cite>T</cite>). The shape then goes from
[..., sizeof(<cite>type</cite>)/sizeof(<cite>T</cite>)] to [...].</p>
<p>tf.bitcast() and tf.cast() work differently when real dtype is casted as a complex dtype
(e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary part 0 while tf.bitcast()
gives module error.
For example,</p>
<p>Example 1:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equality_bitcast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">bitcast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">InvalidArgumentError</span>: <span class="n">Cannot bitcast from 1 to 18 [Op:Bitcast]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equality_cast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">equality_cast</span><span class="p">)</span>
<span class="go">tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)</span>
</pre></div>
</div>
<p>Example 2:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">bitcast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">uint32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=uint8, numpy=array([255, 255, 255, 255], dtype=uint8)&gt;</span>
</pre></div>
</div>
<p>Example 3:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equality</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equality_cast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">equality</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equality_bitcast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">bitcast</span><span class="p">(</span><span class="n">equality_cast</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">equality</span><span class="p">)</span>
<span class="go">tf.Tensor([False True True], shape=(3,), dtype=bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">equality_cast</span><span class="p">)</span>
<span class="go">tf.Tensor([0. 1. 1.], shape=(3,), dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">equality_bitcast</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">    [[  0   0   0   0]</span>
<span class="go">     [  0   0 128  63]</span>
<span class="go">     [  0   0 128  63]], shape=(3, 4), dtype=uint8)</span>
</pre></div>
</div>
<p><em>NOTE</em>: Bitcast is implemented as a low-level cast, so machines with different
endian orderings will give different results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int64</cite>, <cite>int32</cite>, <cite>uint8</cite>, <cite>uint16</cite>, <cite>uint32</cite>, <cite>uint64</cite>, <cite>int8</cite>, <cite>int16</cite>, <cite>complex64</cite>, <cite>complex128</cite>, <cite>qint8</cite>, <cite>quint8</cite>, <cite>qint16</cite>, <cite>quint16</cite>, <cite>qint32</cite>.</p></li>
<li><p><strong>type</strong> -- A <cite>tf.DType</cite> from: <cite>tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int64, tf.int32, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.complex64, tf.complex128, tf.qint8, tf.quint8, tf.qint16, tf.quint16, tf.qint32</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>type</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.boolean_mask">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">boolean_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">mask</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'boolean_mask'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#boolean_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.boolean_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply boolean mask to tensor.</p>
<p>Numpy equivalent is <cite>tensor[mask]</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">1-D</span> <span class="pre">example</span>
<span class="pre">tensor</span> <span class="pre">=</span> <span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3]</span>
<span class="pre">mask</span> <span class="pre">=</span> <span class="pre">np.array([True,</span> <span class="pre">False,</span> <span class="pre">True,</span> <span class="pre">False])</span>
<span class="pre">boolean_mask(tensor,</span> <span class="pre">mask)</span>&#160; <span class="pre">#</span> <span class="pre">[0,</span> <span class="pre">2]</span>
<span class="pre">`</span></code></p>
<p>In general, <cite>0 &lt; dim(mask) = K &lt;= dim(tensor)</cite>, and <cite>mask</cite>'s shape must match
the first K dimensions of <cite>tensor</cite>'s shape.  We then have:</p>
<blockquote>
<div><p><cite>boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]</cite></p>
</div></blockquote>
<p>where <cite>(i1,...,iK)</cite> is the ith <cite>True</cite> entry of <cite>mask</cite> (row-major order).
The <cite>axis</cite> could be used with <cite>mask</cite> to indicate the axis to mask from.
In that case, <cite>axis + dim(mask) &lt;= dim(tensor)</cite> and <cite>mask</cite>'s shape must match
the first <cite>axis + dim(mask)</cite> dimensions of <cite>tensor</cite>'s shape.</p>
<p>See also: <cite>tf.ragged.boolean_mask</cite>, which can be applied to both dense and
ragged tensors, and can be used if you need to preserve the masked dimensions
of <cite>tensor</cite> (rather than flattening them, as <cite>tf.boolean_mask</cite> does).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- N-D tensor.</p></li>
<li><p><strong>mask</strong> -- K-D boolean tensor, K &lt;= N and K must be known statically.</p></li>
<li><p><strong>axis</strong> -- A 0-D int Tensor representing the axis in <cite>tensor</cite> to mask from. By
default, axis is 0 which will mask from the first dimension. Otherwise K +
axis &lt;= N.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(N-K+1)-dimensional tensor populated by entries in <cite>tensor</cite> corresponding
to <cite>True</cite> values in <cite>mask</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If shapes do not conform.</p>
</dd>
</dl>
<p>Examples:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">2-D</span> <span class="pre">example</span>
<span class="pre">tensor</span> <span class="pre">=</span> <span class="pre">[[1,</span> <span class="pre">2],</span> <span class="pre">[3,</span> <span class="pre">4],</span> <span class="pre">[5,</span> <span class="pre">6]]</span>
<span class="pre">mask</span> <span class="pre">=</span> <span class="pre">np.array([True,</span> <span class="pre">False,</span> <span class="pre">True])</span>
<span class="pre">boolean_mask(tensor,</span> <span class="pre">mask)</span>&#160; <span class="pre">#</span> <span class="pre">[[1,</span> <span class="pre">2],</span> <span class="pre">[5,</span> <span class="pre">6]]</span>
<span class="pre">`</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.broadcast_dynamic_shape">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">broadcast_dynamic_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape_x</span></em>, <em class="sig-param"><span class="n">shape_y</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#broadcast_dynamic_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.broadcast_dynamic_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the shape of a broadcast given symbolic shapes.</p>
<p>When shape_x and shape_y are Tensors representing shapes (i.e. the result of
calling tf.shape on another Tensor) this computes a Tensor which is the shape
of the result of a broadcasting op applied in tensors of shapes shape_x and
shape_y.</p>
<p>For example, if shape_x is [1, 2, 3] and shape_y is [5, 1, 3], the result is a
Tensor whose value is [5, 2, 3].</p>
<p>This is useful when validating the result of a broadcasting operation when the
tensors do not have statically known shapes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape_x</strong> -- A rank 1 integer <cite>Tensor</cite>, representing the shape of x.</p></li>
<li><p><strong>shape_y</strong> -- A rank 1 integer <cite>Tensor</cite>, representing the shape of y.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A rank 1 integer <cite>Tensor</cite> representing the broadcasted shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.broadcast_static_shape">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">broadcast_static_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape_x</span></em>, <em class="sig-param"><span class="n">shape_y</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#broadcast_static_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.broadcast_static_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the shape of a broadcast given known shapes.</p>
<p>When shape_x and shape_y are fully known TensorShapes this computes a
TensorShape which is the shape of the result of a broadcasting op applied in
tensors of shapes shape_x and shape_y.</p>
<p>For example, if shape_x is [1, 2, 3] and shape_y is [5, 1, 3], the result is a
TensorShape whose value is [5, 2, 3].</p>
<p>This is useful when validating the result of a broadcasting operation when the
tensors have statically known shapes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape_x</strong> -- A <cite>TensorShape</cite></p></li>
<li><p><strong>shape_y</strong> -- A <cite>TensorShape</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>TensorShape</cite> representing the broadcasted shape.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If the two shapes can not be broadcasted.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.broadcast_to">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">broadcast_to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#broadcast_to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.broadcast_to" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcast an array for a compatible shape.</p>
<p>Broadcasting is the process of making arrays to have compatible shapes
for arithmetic operations. Two shapes are compatible if for each
dimension pair they are either equal or one of them is one. When trying
to broadcast a Tensor to a shape, it starts with the trailing dimensions,
and works its way forward.</p>
<p>For example,</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">    [[1 2 3]</span>
<span class="go">     [1 2 3]</span>
<span class="go">     [1 2 3]], shape=(3, 3), dtype=int32)</span>
</pre></div>
</div>
<p>In the above example, the input Tensor with the shape of <cite>[1, 3]</cite>
is broadcasted to output Tensor with shape of <cite>[3, 3]</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. A Tensor to broadcast.</p></li>
<li><p><strong>shape</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
An 1-D <cite>int</cite> Tensor. The shape of the desired output.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.case">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">case</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred_fn_pairs</span></em>, <em class="sig-param"><span class="n">default</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">exclusive</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">strict</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'case'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/control_flow_ops.html#case"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.case" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a case operation.</p>
<p>See also <cite>tf.switch_case</cite>.</p>
<p>The <cite>pred_fn_pairs</cite> parameter is a list of pairs of size N.
Each pair contains a boolean scalar tensor and a python callable that
creates the tensors to be returned if the boolean evaluates to True.
<cite>default</cite> is a callable generating a list of tensors. All the callables
in <cite>pred_fn_pairs</cite> as well as <cite>default</cite> (if provided) should return the same
number and types of tensors.</p>
<p>If <cite>exclusive==True</cite>, all predicates are evaluated, and an exception is
thrown if more than one of the predicates evaluates to <cite>True</cite>.
If <cite>exclusive==False</cite>, execution stops at the first predicate which
evaluates to True, and the tensors generated by the corresponding function
are returned immediately. If none of the predicates evaluate to True, this
operation returns the tensors generated by <cite>default</cite>.</p>
<p><cite>tf.case</cite> supports nested structures as implemented in
<cite>tf.contrib.framework.nest</cite>. All of the callables must return the same
(possibly nested) value structure of lists, tuples, and/or named tuples.
Singleton lists and tuples form the only exceptions to this: when returned by
a callable, they are implicitly unpacked to single values. This
behavior is disabled by passing <cite>strict=True</cite>.</p>
<p>&#64;compatibility(v2)
<cite>pred_fn_pairs</cite> could be a dictionary in v1. However, tf.Tensor and
tf.Variable are no longer hashable in v2, so cannot be used as a key for a
dictionary.  Please use a list or a tuple instead.
&#64;end_compatibility</p>
<p><strong>Example 1:</strong></p>
<p>Pseudocode:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">if</span> <span class="pre">(x</span> <span class="pre">&lt;</span> <span class="pre">y)</span> <span class="pre">return</span> <span class="pre">17;</span>
<span class="pre">else</span> <span class="pre">return</span> <span class="pre">23;</span>
<span class="pre">`</span></code></p>
<p>Expressions:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">f1</span> <span class="pre">=</span> <span class="pre">lambda:</span> <span class="pre">tf.constant(17)</span>
<span class="pre">f2</span> <span class="pre">=</span> <span class="pre">lambda:</span> <span class="pre">tf.constant(23)</span>
<span class="pre">r</span> <span class="pre">=</span> <span class="pre">tf.case([(tf.less(x,</span> <span class="pre">y),</span> <span class="pre">f1)],</span> <span class="pre">default=f2)</span>
<span class="pre">`</span></code></p>
<p><strong>Example 2:</strong></p>
<p>Pseudocode:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">if</span> <span class="pre">(x</span> <span class="pre">&lt;</span> <span class="pre">y</span> <span class="pre">&amp;&amp;</span> <span class="pre">x</span> <span class="pre">&gt;</span> <span class="pre">z)</span> <span class="pre">raise</span> <span class="pre">OpError(&quot;Only</span> <span class="pre">one</span> <span class="pre">predicate</span> <span class="pre">may</span> <span class="pre">evaluate</span> <span class="pre">to</span> <span class="pre">True&quot;);</span>
<span class="pre">if</span> <span class="pre">(x</span> <span class="pre">&lt;</span> <span class="pre">y)</span> <span class="pre">return</span> <span class="pre">17;</span>
<span class="pre">else</span> <span class="pre">if</span> <span class="pre">(x</span> <span class="pre">&gt;</span> <span class="pre">z)</span> <span class="pre">return</span> <span class="pre">23;</span>
<span class="pre">else</span> <span class="pre">return</span> <span class="pre">-1;</span>
<span class="pre">`</span></code></p>
<p>Expressions:</p>
<p><a href="#id472"><span class="problematic" id="id473">``</span></a><a href="#id474"><span class="problematic" id="id475">`</span></a>python
def f1(): return tf.constant(17)
def f2(): return tf.constant(23)
def f3(): return tf.constant(-1)
r = tf.case([(tf.less(x, y), f1), (tf.greater(x, z), f2)],</p>
<blockquote>
<div><p>default=f3, exclusive=True)</p>
</div></blockquote>
<p><a href="#id476"><span class="problematic" id="id477">``</span></a><a href="#id478"><span class="problematic" id="id479">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred_fn_pairs</strong> -- List of pairs of a boolean scalar tensor and a callable which
returns a list of tensors.</p></li>
<li><p><strong>default</strong> -- Optional callable that returns a list of tensors.</p></li>
<li><p><strong>exclusive</strong> -- True iff at most one predicate is allowed to evaluate to <cite>True</cite>.</p></li>
<li><p><strong>strict</strong> -- A boolean that enables/disables 'strict' mode; see above.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tensors returned by the first pair whose predicate evaluated to True, or
those returned by <cite>default</cite> if none does.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If <cite>pred_fn_pairs</cite> is not a list/tuple.</p></li>
<li><p><strong>TypeError</strong> -- If <cite>pred_fn_pairs</cite> is a list but does not contain 2-tuples.</p></li>
<li><p><strong>TypeError</strong> -- If <cite>fns[i]</cite> is not callable for any i, or <cite>default</cite> is not
    callable.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.cast">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">cast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">dtype</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#cast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.cast" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts a tensor to a new type.</p>
<p>The operation casts <cite>x</cite> (in case of <cite>Tensor</cite>) or <cite>x.values</cite>
(in case of <cite>SparseTensor</cite> or <cite>IndexedSlices</cite>) to <cite>dtype</cite>.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>The operation supports data types (for <cite>x</cite> and <cite>dtype</cite>) of
<cite>uint8</cite>, <cite>uint16</cite>, <cite>uint32</cite>, <cite>uint64</cite>, <cite>int8</cite>, <cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>,
<cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>, <cite>bfloat16</cite>.
In case of casting from complex types (<cite>complex64</cite>, <cite>complex128</cite>) to real
types, only the real part of <cite>x</cite> is returned. In case of casting from real
types to complex types (<cite>complex64</cite>, <cite>complex128</cite>), the imaginary part of the
returned value is set to <cite>0</cite>. The handling of complex types here matches the
behavior of numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite> or <cite>SparseTensor</cite> or <cite>IndexedSlices</cite> of numeric type. It could
be <cite>uint8</cite>, <cite>uint16</cite>, <cite>uint32</cite>, <cite>uint64</cite>, <cite>int8</cite>, <cite>int16</cite>, <cite>int32</cite>,
<cite>int64</cite>, <cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>,
<cite>bfloat16</cite>.</p></li>
<li><p><strong>dtype</strong> -- The destination type. The list of supported dtypes is the same as
<cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <cite>Tensor</cite> or <cite>SparseTensor</cite> or <cite>IndexedSlices</cite> with same shape as <cite>x</cite> and</dt><dd><p>same type as <cite>dtype</cite>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- If <cite>x</cite> cannot be cast to the <cite>dtype</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.clip_by_global_norm">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">clip_by_global_norm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">t_list</span></em>, <em class="sig-param"><span class="n">clip_norm</span></em>, <em class="sig-param"><span class="n">use_norm</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/clip_ops.html#clip_by_global_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.clip_by_global_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips values of multiple tensors by the ratio of the sum of their norms.</p>
<p>Given a tuple or list of tensors <cite>t_list</cite>, and a clipping ratio <cite>clip_norm</cite>,
this operation returns a list of clipped tensors <cite>list_clipped</cite>
and the global norm (<cite>global_norm</cite>) of all tensors in <cite>t_list</cite>. Optionally,
if you've already computed the global norm for <cite>t_list</cite>, you can specify
the global norm with <cite>use_norm</cite>.</p>
<p>To perform the clipping, the values <cite>t_list[i]</cite> are set to:</p>
<blockquote>
<div><p>t_list[i] * clip_norm / max(global_norm, clip_norm)</p>
</div></blockquote>
<p>where:</p>
<blockquote>
<div><p>global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))</p>
</div></blockquote>
<p>If <cite>clip_norm &gt; global_norm</cite> then the entries in <cite>t_list</cite> remain as they are,
otherwise they're all shrunk by the global ratio.</p>
<p>If <cite>global_norm == infinity</cite> then the entries in <cite>t_list</cite> are all set to <cite>NaN</cite>
to signal that an error occurred.</p>
<p>Any of the entries of <cite>t_list</cite> that are of type <cite>None</cite> are ignored.</p>
<p>This is the correct way to perform gradient clipping (Pascanu et al., 2012).</p>
<p>However, it is slower than <cite>clip_by_norm()</cite> because all the parameters must be
ready before the clipping operation can be performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t_list</strong> -- A tuple or list of mixed <cite>Tensors</cite>, <cite>IndexedSlices</cite>, or None.</p></li>
<li><p><strong>clip_norm</strong> -- A 0-D (scalar) <cite>Tensor</cite> &gt; 0. The clipping ratio.</p></li>
<li><p><strong>use_norm</strong> -- A 0-D (scalar) <cite>Tensor</cite> of type <cite>float</cite> (optional). The global
norm to use. If not provided, <cite>global_norm()</cite> is used to compute the norm.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <cite>Tensors</cite> of the same type as <cite>list_t</cite>.
global_norm: A 0-D (scalar) <cite>Tensor</cite> representing the global norm.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list_clipped</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> -- If <cite>t_list</cite> is not a sequence.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>On the difficulty of training Recurrent Neural Networks:</dt><dd><p>[Pascanu et al., 2012](<a class="reference external" href="http://proceedings.mlr.press/v28/pascanu13.html">http://proceedings.mlr.press/v28/pascanu13.html</a>)
([pdf](<a class="reference external" href="http://proceedings.mlr.press/v28/pascanu13.pdf">http://proceedings.mlr.press/v28/pascanu13.pdf</a>))</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.clip_by_norm">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">clip_by_norm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">t</span></em>, <em class="sig-param"><span class="n">clip_norm</span></em>, <em class="sig-param"><span class="n">axes</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/clip_ops.html#clip_by_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.clip_by_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips tensor values to a maximum L2-norm.</p>
<p>Given a tensor <cite>t</cite>, and a maximum clip value <cite>clip_norm</cite>, this operation
normalizes <cite>t</cite> so that its L2-norm is less than or equal to <cite>clip_norm</cite>,
along the dimensions given in <cite>axes</cite>. Specifically, in the default case
where all dimensions are used for calculation, if the L2-norm of <cite>t</cite> is
already less than or equal to <cite>clip_norm</cite>, then <cite>t</cite> is not modified. If
the L2-norm is greater than <cite>clip_norm</cite>, then this operation returns a
tensor of the same type and shape as <cite>t</cite> with its values set to:</p>
<p><cite>t * clip_norm / l2norm(t)</cite></p>
<p>In this case, the L2-norm of the output tensor is <cite>clip_norm</cite>.</p>
<p>As another example, if <cite>t</cite> is a matrix and <cite>axes == [1]</cite>, then each row
of the output will have L2-norm less than or equal to <cite>clip_norm</cite>. If
<cite>axes == [0]</cite> instead, each column of the output will be clipped.</p>
<p>This operation is typically used to clip gradients before applying them with
an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> -- A <cite>Tensor</cite> or <cite>IndexedSlices</cite>.</p></li>
<li><p><strong>clip_norm</strong> -- A 0-D (scalar) <cite>Tensor</cite> &gt; 0. A maximum clipping value.</p></li>
<li><p><strong>axes</strong> -- A 1-D (vector) <cite>Tensor</cite> of type int32 containing the dimensions
to use for computing the L2-norm. If <cite>None</cite> (the default), uses all
dimensions.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A clipped <cite>Tensor</cite> or <cite>IndexedSlices</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> -- If the clip_norm tensor is not a 0-D scalar tensor.</p></li>
<li><p><strong>TypeError</strong> -- If dtype of the input is not a floating point or
    complex type.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.clip_by_value">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">clip_by_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">t</span></em>, <em class="sig-param"><span class="n">clip_value_min</span></em>, <em class="sig-param"><span class="n">clip_value_max</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/clip_ops.html#clip_by_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.clip_by_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips tensor values to a specified min and max.</p>
<p>Given a tensor <cite>t</cite>, this operation returns a tensor of the same type and
shape as <cite>t</cite> with its values clipped to <cite>clip_value_min</cite> and <cite>clip_value_max</cite>.
Any values less than <cite>clip_value_min</cite> are set to <cite>clip_value_min</cite>. Any values
greater than <cite>clip_value_max</cite> are set to <cite>clip_value_max</cite>.</p>
<p>Note: <cite>clip_value_min</cite> needs to be smaller or equal to <cite>clip_value_max</cite> for
correct results.</p>
<p>For example:</p>
<p>Basic usage passes a scalar as the min and max value.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="o">-</span><span class="mf">10.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([[-1., -1.,  0.],</span>
<span class="go">       [ 0.,  1.,  1.]], dtype=float32)</span>
</pre></div>
</div>
<p>The min and max can be the same size as <cite>t</cite>, or broadcastable to that size.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clip_min</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t3</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([[ 2.,  2., 10.],</span>
<span class="go">       [ 1.,  1., 10.]], dtype=float32)</span>
</pre></div>
</div>
<p>Broadcasting fails, intentionally, if you would expand the dimensions of <cite>t</cite></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clip_min</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]]</span> <span class="c1"># Has a third axis</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">InvalidArgumentError</span>: <span class="n">Incompatible shapes: [2,3] vs. [1,1,2]</span>
</pre></div>
</div>
<p>It throws a <cite>TypeError</cite> if you try to clip an <cite>int</cite> to a <cite>float</cite> value
(<cite>tf.cast</cite> the input to <cite>float</cite> first).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t5</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=-</span><span class="mf">3.1</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="o">=</span><span class="mf">3.1</span><span class="p">)</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">TypeError</span>: <span class="n">Cannot convert ...</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> -- A <cite>Tensor</cite> or <cite>IndexedSlices</cite>.</p></li>
<li><p><strong>clip_value_min</strong> -- The minimum value to clip to. A scalar <cite>Tensor</cite> or one that
is broadcastable to the shape of <cite>t</cite>.</p></li>
<li><p><strong>clip_value_max</strong> -- The minimum value to clip to. A scalar <cite>Tensor</cite> or one that
is broadcastable to the shape of <cite>t</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A clipped <cite>Tensor</cite> or <cite>IndexedSlices</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tf.errors.InvalidArgumentError</strong> -- If the clip tensors would trigger array
    broadcasting that would make the returned tensor larger than the input.</p></li>
<li><p><strong>TypeError</strong> -- If dtype of the input is <cite>int32</cite> and dtype of
    the <cite>clip_value_min</cite> or <cite>clip_value_max</cite> is <cite>float32</cite></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.complex">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">complex</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">real</span></em>, <em class="sig-param"><span class="n">imag</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#complex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.complex" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts two real numbers to a complex number.</p>
<p>Given a tensor <cite>real</cite> representing the real part of a complex number, and a
tensor <cite>imag</cite> representing the imaginary part of a complex number, this
operation returns complex numbers elementwise of the form \(a + bj\), where
<em>a</em> represents the <cite>real</cite> part and <em>b</em> represents the <cite>imag</cite> part.</p>
<p>The input tensors <cite>real</cite> and <cite>imag</cite> must have the same shape.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">real</span> <span class="pre">=</span> <span class="pre">tf.constant([2.25,</span> <span class="pre">3.25])</span>
<span class="pre">imag</span> <span class="pre">=</span> <span class="pre">tf.constant([4.75,</span> <span class="pre">5.75])</span>
<span class="pre">tf.complex(real,</span> <span class="pre">imag)</span>&#160; <span class="pre">#</span> <span class="pre">[[2.25</span> <span class="pre">+</span> <span class="pre">4.75j],</span> <span class="pre">[3.25</span> <span class="pre">+</span> <span class="pre">5.75j]]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>.</p></li>
<li><p><strong>imag</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>real</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>complex64</cite> or <cite>complex128</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- Real and imag must be correct types</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.concat">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">concat</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">axis</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'concat'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates tensors along one dimension.</p>
<p>See also <cite>tf.tile</cite>, <cite>tf.stack</cite>, <cite>tf.repeat</cite>.</p>
<p>Concatenates the list of tensors <cite>values</cite> along dimension <cite>axis</cite>.  If
<cite>values[i].shape = [D0, D1, ... Daxis(i), ...Dn]</cite>, the concatenated
result has shape</p>
<blockquote>
<div><p>[D0, D1, ... Raxis, ...Dn]</p>
</div></blockquote>
<p>where</p>
<blockquote>
<div><p>Raxis = sum(Daxis(i))</p>
</div></blockquote>
<p>That is, the data from the input tensors is joined along the <cite>axis</cite>
dimension.</p>
<p>The number of dimensions of the input tensors must match, and all dimensions
except <cite>axis</cite> must be equal.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">concat</span><span class="p">([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=</span>
<span class="go">array([[ 1,  2,  3],</span>
<span class="go">       [ 4,  5,  6],</span>
<span class="go">       [ 7,  8,  9],</span>
<span class="go">       [10, 11, 12]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">concat</span><span class="p">([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=</span>
<span class="go">array([[ 1,  2,  3,  7,  8,  9],</span>
<span class="go">       [ 4,  5,  6, 10, 11, 12]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>As in Python, the <cite>axis</cite> could also be negative numbers. Negative <cite>axis</cite>
are interpreted as counting from the end of the rank, i.e.,</p>
<blockquote>
<div><p><cite>axis + rank(values)</cite>-th dimension.</p>
</div></blockquote>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">11</span><span class="p">]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy=</span>
<span class="go">  array([[[ 1,  2,  7,  4],</span>
<span class="go">          [ 2,  3,  8,  4]],</span>
<span class="go">         [[ 4,  4,  2, 10],</span>
<span class="go">          [ 5,  3, 15, 11]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>Note: If you are concatenating along a new axis consider using stack.
E.g.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tf.concat([tf.expand_dims(t,</span> <span class="pre">axis)</span> <span class="pre">for</span> <span class="pre">t</span> <span class="pre">in</span> <span class="pre">tensors],</span> <span class="pre">axis)</span>
<span class="pre">`</span></code></p>
<p>can be rewritten as</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tf.stack(tensors,</span> <span class="pre">axis=axis)</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A list of <cite>Tensor</cite> objects or a single <cite>Tensor</cite>.</p></li>
<li><p><strong>axis</strong> -- 0-D <cite>int32</cite> <cite>Tensor</cite>.  Dimension along which to concatenate. Must be
in the range <cite>[-rank(values), rank(values))</cite>. As in Python, indexing for
axis is 0-based. Positive axis in the rage of <cite>[0, rank(values))</cite> refers
to <cite>axis</cite>-th dimension. And negative axis refers to <cite>axis +
rank(values)</cite>-th dimension.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> resulting from concatenation of the input tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.cond">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">cond</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">true_fn</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">false_fn</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/control_flow_ops.html#cond"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.cond" title="Permalink to this definition">¶</a></dt>
<dd><p>Return <cite>true_fn()</cite> if the predicate <cite>pred</cite> is true else <cite>false_fn()</cite>.</p>
<p><cite>true_fn</cite> and <cite>false_fn</cite> both return lists of output tensors. <cite>true_fn</cite> and
<cite>false_fn</cite> must have the same non-zero number and type of outputs.</p>
<p><strong>WARNING</strong>: Any Tensors or Operations created outside of <cite>true_fn</cite> and
<cite>false_fn</cite> will be executed regardless of which branch is selected at runtime.</p>
<p>Although this behavior is consistent with the dataflow model of TensorFlow,
it has frequently surprised users who expected a lazier semantics.
Consider the following simple program:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">z</span> <span class="pre">=</span> <span class="pre">tf.multiply(a,</span> <span class="pre">b)</span>
<span class="pre">result</span> <span class="pre">=</span> <span class="pre">tf.cond(x</span> <span class="pre">&lt;</span> <span class="pre">y,</span> <span class="pre">lambda:</span> <span class="pre">tf.add(x,</span> <span class="pre">z),</span> <span class="pre">lambda:</span> <span class="pre">tf.square(y))</span>
<span class="pre">`</span></code></p>
<p>If <cite>x &lt; y</cite>, the <cite>tf.add</cite> operation will be executed and <cite>tf.square</cite>
operation will not be executed. Since <cite>z</cite> is needed for at least one
branch of the <cite>cond</cite>, the <cite>tf.multiply</cite> operation is always executed,
unconditionally.</p>
<p>Note that <cite>cond</cite> calls <cite>true_fn</cite> and <cite>false_fn</cite> <em>exactly once</em> (inside the
call to <cite>cond</cite>, and not at all during <cite>Session.run()</cite>). <cite>cond</cite>
stitches together the graph fragments created during the <cite>true_fn</cite> and
<cite>false_fn</cite> calls with some additional graph nodes to ensure that the right
branch gets executed depending on the value of <cite>pred</cite>.</p>
<p><cite>tf.cond</cite> supports nested structures as implemented in
<cite>tensorflow.python.util.nest</cite>. Both <cite>true_fn</cite> and <cite>false_fn</cite> must return the
same (possibly nested) value structure of lists, tuples, and/or named tuples.
Singleton lists and tuples form the only exceptions to this: when returned by
<cite>true_fn</cite> and/or <cite>false_fn</cite>, they are implicitly unpacked to single values.</p>
<p>Note: It is illegal to &quot;directly&quot; use tensors created inside a cond branch
outside it, e.g. by storing a reference to a branch tensor in the python
state. If you need to use a tensor created in a branch function you should
return it as an output of the branch function and use the output from
<cite>tf.cond</cite> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> -- A scalar determining whether to return the result of <cite>true_fn</cite> or
<cite>false_fn</cite>.</p></li>
<li><p><strong>true_fn</strong> -- The callable to be performed if pred is true.</p></li>
<li><p><strong>false_fn</strong> -- The callable to be performed if pred is false.</p></li>
<li><p><strong>name</strong> -- Optional name prefix for the returned tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensors returned by the call to either <cite>true_fn</cite> or <cite>false_fn</cite>. If the
callables return a singleton list, the element is extracted from the list.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- if <cite>true_fn</cite> or <cite>false_fn</cite> is not callable.</p></li>
<li><p><strong>ValueError</strong> -- if <cite>true_fn</cite> and <cite>false_fn</cite> do not return the same number of
    tensors, or return tensors of different types.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant(2)</span>
<span class="pre">y</span> <span class="pre">=</span> <span class="pre">tf.constant(5)</span>
<span class="pre">def</span> <span class="pre">f1():</span> <span class="pre">return</span> <span class="pre">tf.multiply(x,</span> <span class="pre">17)</span>
<span class="pre">def</span> <span class="pre">f2():</span> <span class="pre">return</span> <span class="pre">tf.add(y,</span> <span class="pre">23)</span>
<span class="pre">r</span> <span class="pre">=</span> <span class="pre">tf.cond(tf.less(x,</span> <span class="pre">y),</span> <span class="pre">f1,</span> <span class="pre">f2)</span>
<span class="pre">#</span> <span class="pre">r</span> <span class="pre">is</span> <span class="pre">set</span> <span class="pre">to</span> <span class="pre">f1().</span>
<span class="pre">#</span> <span class="pre">Operations</span> <span class="pre">in</span> <span class="pre">f2</span> <span class="pre">(e.g.,</span> <span class="pre">tf.add)</span> <span class="pre">are</span> <span class="pre">not</span> <span class="pre">executed.</span>
<span class="pre">`</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.constant">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">constant</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'Const'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/constant_op.html#constant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.constant" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a constant tensor from a tensor-like object.</p>
<p>Note: All eager <cite>tf.Tensor</cite> values are immutable (in contrast to
<cite>tf.Variable</cite>). There is nothing especially _constant_ about the value
returned from <cite>tf.constant</cite>. This function it is not fundamentally different
from <cite>tf.convert_to_tensor</cite>. The name <cite>tf.constant</cite> comes from the symbolic
APIs (like <cite>tf.data</cite> or keras functional models) where the <cite>value</cite> is embeded
in a <cite>Const</cite> node in the <cite>tf.Graph</cite>. <cite>tf.constant</cite> is useful for asserting
that the value can be embedded that way.</p>
<p>If the argument <cite>dtype</cite> is not specified, then the type is inferred from
the type of <cite>value</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Constant 1-D Tensor from a python list.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(6,), dtype=int32,</span>
<span class="go">    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Or a numpy array</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int64, numpy=</span>
<span class="go">  array([[1, 2, 3],</span>
<span class="go">         [4, 5, 6]])&gt;</span>
</pre></div>
</div>
<p>If <cite>dtype</cite> is specified the resulting tensor values are cast to the requested
<cite>dtype</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(6,), dtype=float64,</span>
<span class="go">    numpy=array([1., 2., 3., 4., 5., 6.])&gt;</span>
</pre></div>
</div>
<p>If <cite>shape</cite> is set, the <cite>value</cite> is reshaped to match. Scalars are expanded to
fill the <cite>shape</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">  &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">  array([[0, 0, 0],</span>
<span class="go">         [0, 0, 0]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">  array([[1, 2, 3],</span>
<span class="go">         [4, 5, 6]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p><cite>tf.constant</cite> has no effect if an eager Tensor is passed as the <cite>value</cite>, it
even transmits gradients:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">g</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([2.], dtype=float32)</span>
</pre></div>
</div>
<p>But, since <cite>tf.constant</cite> embeds the value in the <cite>tf.Graph</cite> this fails for
symbolic tensors:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">NotImplementedError</span>: <span class="n">...</span>
</pre></div>
</div>
<p><cite>tf.constant</cite> will _always_ create CPU (host) tensors. In order to create
tensors on other devices, use <cite>tf.identity</cite>. (If the <cite>value</cite> is an eager
Tensor, however, the tensor will be returned unmodified as mentioned above.)</p>
<p>Related Ops:</p>
<ul>
<li><p><cite>tf.convert_to_tensor</cite> is similar but:
* It has no <cite>shape</cite> argument.
* Symbolic tensors are allowed to pass through.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><cite>tf.fill</cite>: differs in a few ways:
*   <cite>tf.constant</cite> supports arbitrary constants, not just uniform scalar</p>
<blockquote>
<div><p>Tensors like <cite>tf.fill</cite>.</p>
</div></blockquote>
<ul class="simple">
<li><p><cite>tf.fill</cite> creates an Op in the graph that is expanded at runtime, so it
can efficiently represent large tensors.</p></li>
<li><p>Since <cite>tf.fill</cite> does not embed the value, it can produce dynamically
sized outputs.</p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> -- A constant value (or list) of output type <cite>dtype</cite>.</p></li>
<li><p><strong>dtype</strong> -- The type of the elements of the resulting tensor.</p></li>
<li><p><strong>shape</strong> -- Optional dimensions of resulting tensor.</p></li>
<li><p><strong>name</strong> -- Optional name for the tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Constant Tensor.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- if shape is incorrectly specified or unsupported.</p></li>
<li><p><strong>ValueError</strong> -- if called on a symbolic tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.constant_initializer">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">constant_initializer</code><a class="headerlink" href="#tensorflow.constant_initializer" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.ops.init_ops_v2.Constant</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.control_dependencies">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">control_dependencies</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">control_inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#control_dependencies"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.control_dependencies" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper for <cite>Graph.control_dependencies()</cite> using the default graph.</p>
<p>See <cite>tf.Graph.control_dependencies</cite>
for more details.</p>
<p>When eager execution is enabled, any callable object in the <cite>control_inputs</cite>
list will be called.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>control_inputs</strong> -- A list of <cite>Operation</cite> or <cite>Tensor</cite> objects which must be
executed or computed before running the operations defined in the context.
Can also be <cite>None</cite> to clear the control dependencies. If eager execution
is enabled, any callable object in the <cite>control_inputs</cite> list will be
called.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A context manager that specifies control dependencies for all
operations constructed within the context.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.convert_to_tensor">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">convert_to_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype_hint</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#convert_to_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.convert_to_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the given <cite>value</cite> to a <cite>Tensor</cite>.</p>
<p>This function converts Python objects of various types to <cite>Tensor</cite>
objects. It accepts <cite>Tensor</cite> objects, numpy arrays, Python lists,
and Python scalars. For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_func</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">arg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">arg</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># The following calls are equivalent.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_1</span> <span class="o">=</span> <span class="n">my_func</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">value_1</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">  [[1. 2.]</span>
<span class="go">   [3. 4.]], shape=(2, 2), dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_2</span> <span class="o">=</span> <span class="n">my_func</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">value_2</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">  [[1. 2.]</span>
<span class="go">   [3. 4.]], shape=(2, 2), dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_3</span> <span class="o">=</span> <span class="n">my_func</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">value_3</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">  [[1. 2.]</span>
<span class="go">   [3. 4.]], shape=(2, 2), dtype=float32)</span>
</pre></div>
</div>
<p>This function can be useful when composing a new operation in Python
(such as <cite>my_func</cite> in the example above). All standard Python op
constructors apply this function to each of their Tensor-valued
inputs, which allows those ops to accept numpy arrays, Python lists,
and scalars in addition to <cite>Tensor</cite> objects.</p>
<dl class="simple">
<dt>Note: This function diverges from default Numpy behavior for <cite>float</cite> and</dt><dd><p><cite>string</cite> types when <cite>None</cite> is present in a Python list or scalar. Rather
than silently converting <cite>None</cite> values, an error will be thrown.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> -- An object whose type has a registered <cite>Tensor</cite> conversion function.</p></li>
<li><p><strong>dtype</strong> -- Optional element type for the returned tensor. If missing, the type
is inferred from the type of <cite>value</cite>.</p></li>
<li><p><strong>dtype_hint</strong> -- Optional element type for the returned tensor, used when dtype
is None. In some cases, a caller may not have a dtype in mind when
converting to a tensor, so dtype_hint can be used as a soft preference.
If the conversion to <cite>dtype_hint</cite> is not possible, this argument has no
effect.</p></li>
<li><p><strong>name</strong> -- Optional name to use if a new <cite>Tensor</cite> is created.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> based on <cite>value</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If no conversion function is registered for <cite>value</cite> to <cite>dtype</cite>.</p></li>
<li><p><strong>RuntimeError</strong> -- If a registered conversion function returns an invalid value.</p></li>
<li><p><strong>ValueError</strong> -- If the <cite>value</cite> is a tensor not of given <cite>dtype</cite> in graph mode.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.cos">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">cos</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#cos"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes cos of x element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes cosine of every
element in the tensor. Input range is <cite>(-inf, inf)</cite> and
output range is <cite>[-1,1]</cite>. If input lies outside the boundary, <cite>nan</cite>
is returned.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-9,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">200,</span> <span class="pre">10000,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.cos(x)</span> <span class="pre">==&gt;</span> <span class="pre">[nan</span> <span class="pre">-0.91113025</span> <span class="pre">0.87758255</span> <span class="pre">0.5403023</span> <span class="pre">0.36235774</span> <span class="pre">0.48718765</span> <span class="pre">-0.95215535</span> <span class="pre">nan]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.cosh">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">cosh</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#cosh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes hyperbolic cosine of x element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes hyperbolic cosine of every
element in the tensor. Input range is <cite>[-inf, inf]</cite> and output range
is <cite>[1, inf]</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-9,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">2,</span> <span class="pre">10,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.cosh(x)</span> <span class="pre">==&gt;</span> <span class="pre">[inf</span> <span class="pre">4.0515420e+03</span> <span class="pre">1.1276259e+00</span> <span class="pre">1.5430807e+00</span> <span class="pre">1.8106556e+00</span> <span class="pre">3.7621956e+00</span> <span class="pre">1.1013233e+04</span> <span class="pre">inf]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.cumsum">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">cumsum</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">exclusive</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">reverse</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#cumsum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.cumsum" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the cumulative sum of the tensor <cite>x</cite> along <cite>axis</cite>.</p>
<p>By default, this op performs an inclusive cumsum, which means that the first
element of the input is identical to the first element of the output:
For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># tf.cumsum([a, b, c])   # [a, a + b, a + b + c]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="go">numpy=array([ 2,  6, 12, 20], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># using varying `axis` values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=</span>
<span class="go">array([[ 2,  4,  6,  8],</span>
<span class="go">       [ 3,  7, 11, 15]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=</span>
<span class="go">array([[ 2,  6, 12, 20],</span>
<span class="go">       [ 1,  4,  9, 16]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>By setting the <cite>exclusive</cite> kwarg to <cite>True</cite>, an exclusive cumsum is performed
instead:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># tf.cumsum([a, b, c], exclusive=True)  =&gt; [0, a, a + b]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="go">numpy=array([ 0,  2,  6, 12], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>By setting the <cite>reverse</cite> kwarg to <cite>True</cite>, the cumsum is performed in the
opposite direction:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># tf.cumsum([a, b, c], reverse=True)  # [a + b + c, b + c, c]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="go">numpy=array([20, 18, 14,  8], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>This is more efficient than using separate <cite>tf.reverse</cite> ops.
The <cite>reverse</cite> and <cite>exclusive</cite> kwargs can also be combined:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># tf.cumsum([a, b, c], exclusive=True, reverse=True)  # [b + c, c, 0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32,</span>
<span class="go">numpy=array([18, 14,  8,  0], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>,
<cite>int64</cite>, <cite>int32</cite>, <cite>uint8</cite>, <cite>uint16</cite>, <cite>int16</cite>, <cite>int8</cite>, <cite>complex64</cite>,
<cite>complex128</cite>, <cite>qint8</cite>, <cite>quint8</cite>, <cite>qint32</cite>, <cite>half</cite>.</p></li>
<li><p><strong>axis</strong> -- A <cite>Tensor</cite> of type <cite>int32</cite> (default: 0). Must be in the range
<cite>[-rank(x), rank(x))</cite>.</p></li>
<li><p><strong>exclusive</strong> -- If <cite>True</cite>, perform exclusive cumsum.</p></li>
<li><p><strong>reverse</strong> -- A <cite>bool</cite> (default: False).</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.custom_gradient">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">custom_gradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/custom_gradient.html#custom_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.custom_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Decorator to define a function with a custom gradient.</p>
<p>This decorator allows fine grained control over the gradients of a sequence
for operations.  This may be useful for multiple reasons, including providing
a more efficient or numerically stable gradient for a sequence of operations.</p>
<p>For example, consider the following function that commonly occurs in the
computation of cross entropy and log likelihoods:</p>
<p><a href="#id480"><span class="problematic" id="id481">``</span></a><a href="#id482"><span class="problematic" id="id483">`</span></a>python
def log1pexp(x):</p>
<blockquote>
<div><p>return tf.math.log(1 + tf.exp(x))</p>
</div></blockquote>
<p><a href="#id484"><span class="problematic" id="id485">``</span></a><a href="#id486"><span class="problematic" id="id487">`</span></a></p>
<p>Due to numerical instability, the gradient of this function evaluated at x=100
is NaN.  For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant(100.)</span>
<span class="pre">y</span> <span class="pre">=</span> <span class="pre">log1pexp(x)</span>
<span class="pre">dy</span> <span class="pre">=</span> <span class="pre">tf.gradients(y,</span> <span class="pre">x)</span> <span class="pre">#</span> <span class="pre">Will</span> <span class="pre">be</span> <span class="pre">NaN</span> <span class="pre">when</span> <span class="pre">evaluated.</span>
<span class="pre">`</span></code></p>
<p>The gradient expression can be analytically simplified to provide numerical
stability:</p>
<p><a href="#id488"><span class="problematic" id="id489">``</span></a><a href="#id490"><span class="problematic" id="id491">`</span></a>python
&#64;tf.custom_gradient
def log1pexp(x):</p>
<blockquote>
<div><p>e = tf.exp(x)
def grad(dy):</p>
<blockquote>
<div><p>return dy * (1 - 1 / (1 + e))</p>
</div></blockquote>
<p>return tf.math.log(1 + e), grad</p>
</div></blockquote>
<p><a href="#id492"><span class="problematic" id="id493">``</span></a><a href="#id494"><span class="problematic" id="id495">`</span></a></p>
<p>With this definition, the gradient at x=100 will be correctly evaluated as
1.0.</p>
<p>Nesting custom gradients can lead to unintuitive results. The default
behavior does not correspond to n-th order derivatives. For example</p>
<p><a href="#id496"><span class="problematic" id="id497">``</span></a><a href="#id498"><span class="problematic" id="id499">`</span></a>python
&#64;tf.custom_gradient
def op(x):</p>
<blockquote>
<div><p>y = op1(x)
&#64;tf.custom_gradient
def grad_fn(dy):</p>
<blockquote>
<div><p>gdy = op2(x, y, dy)
def grad_grad_fn(ddy):  # Not the 2nd order gradient of op w.r.t. x.</p>
<blockquote>
<div><p>return op3(x, y, dy, ddy)</p>
</div></blockquote>
<p>return gdy, grad_grad_fn</p>
</div></blockquote>
<p>return y, grad_fn</p>
</div></blockquote>
<p><a href="#id500"><span class="problematic" id="id501">``</span></a><a href="#id502"><span class="problematic" id="id503">`</span></a></p>
<p>The function <cite>grad_grad_fn</cite> will be calculating the first order gradient
of <cite>grad_fn</cite> with respect to <cite>dy</cite>, which is used to generate forward-mode
gradient graphs from backward-mode gradient graphs, but is not the same as
the second order gradient of <cite>op</cite> with respect to <cite>x</cite>.</p>
<p>Instead, wrap nested <cite>&#64;tf.custom_gradients</cite> in another function:</p>
<p><a href="#id504"><span class="problematic" id="id505">``</span></a><a href="#id506"><span class="problematic" id="id507">`</span></a>python
&#64;tf.custom_gradient
def op_with_fused_backprop(x):</p>
<blockquote>
<div><p>y, x_grad = fused_op(x)
def first_order_gradient(dy):</p>
<blockquote>
<div><p>&#64;tf.custom_gradient
def first_order_custom(unused_x):</p>
<blockquote>
<div><dl class="simple">
<dt>def second_order_and_transpose(ddy):</dt><dd><p>return second_order_for_x(...), gradient_wrt_dy(...)</p>
</dd>
</dl>
<p>return x_grad, second_order_and_transpose</p>
</div></blockquote>
<p>return dy * first_order_custom(x)</p>
</div></blockquote>
<p>return y, first_order_gradient</p>
</div></blockquote>
<p><a href="#id508"><span class="problematic" id="id509">``</span></a><a href="#id510"><span class="problematic" id="id511">`</span></a></p>
<p>Additional arguments to the inner <cite>&#64;tf.custom_gradient</cite>-decorated function
control the expected return values of the innermost function.</p>
<p>See also <cite>tf.RegisterGradient</cite> which registers a gradient function for a
primitive TensorFlow operation. <cite>tf.custom_gradient</cite> on the other hand allows
for fine grained control over the gradient computation of a sequence of
operations.</p>
<p>Note that if the decorated function uses <a href="#id512"><span class="problematic" id="id513">`</span></a>Variable`s, the enclosing variable
scope must be using <a href="#id514"><span class="problematic" id="id515">`</span></a>ResourceVariable`s.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>f</strong> -- <p>function <cite>f(*x)</cite> that returns a tuple <cite>(y, grad_fn)</cite> where:
- <cite>x</cite> is a sequence of <cite>Tensor</cite> inputs to the function.
- <cite>y</cite> is a <cite>Tensor</cite> or sequence of <cite>Tensor</cite> outputs of applying</p>
<blockquote>
<div><p>TensorFlow operations in <cite>f</cite> to <cite>x</cite>.</p>
</div></blockquote>
<ul>
<li><p><cite>grad_fn</cite> is a function with the signature <cite>g(*grad_ys)</cite> which returns
a list of <cite>Tensor`s - the derivatives of `Tensor`s in `y</cite> with respect
to the <cite>Tensor`s in `x</cite>.  <cite>grad_ys</cite> is a <cite>Tensor</cite> or sequence of
<cite>Tensor`s the same size as `y</cite> holding the initial value gradients for
each <cite>Tensor</cite> in <cite>y</cite>. In a pure mathematical sense, a vector-argument
vector-valued function <cite>f</cite>'s derivatives should be its Jacobian matrix
<cite>J</cite>. Here we are expressing the Jacobian <cite>J</cite> as a function <cite>grad_fn</cite>
which defines how <cite>J</cite> will transform a vector <cite>grad_ys</cite> when
left-multiplied with it (<cite>grad_ys * J</cite>). This functional representation
of a matrix is convenient to use for chain-rule calculation
(in e.g. the back-propagation algorithm).</p>
<p>If <cite>f</cite> uses <cite>Variable`s (that are not part of the
inputs), i.e. through `get_variable</cite>, then <cite>grad_fn</cite> should have
signature <cite>g(*grad_ys, variables=None)</cite>, where <cite>variables</cite> is a list of
the <cite>Variable`s, and return a 2-tuple `(grad_xs, grad_vars)</cite>, where
<cite>grad_xs</cite> is the same as above, and <cite>grad_vars</cite> is a <cite>list&lt;Tensor&gt;</cite>
with the derivatives of <cite>Tensor`s in `y</cite> with respect to the variables
(that is, grad_vars has one Tensor per variable in variables).</p>
</li>
</ul>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function <cite>h(x)</cite> which returns the same value as <cite>f(x)[0]</cite> and whose
gradient (as calculated by <cite>tf.gradients</cite>) is determined by <cite>f(x)[1]</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.device">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">device</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device_name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the device for ops created/executed in this context.</p>
<p>This function specifies the device to be used for ops created/executed in a
particular context. Nested contexts will inherit and also create/execute
their ops on the specified device. If a specific device is not required,
consider not using this function so that a device can be automatically
assigned.  In general the use of this function is optional. <cite>device_name</cite> can
be fully specified, as in &quot;/job:worker/task:1/device:cpu:0&quot;, or partially
specified, containing only a subset of the &quot;/&quot;-separated fields. Any fields
which are specified will override device annotations from outer scopes.</p>
<p>For example:</p>
<p><a href="#id516"><span class="problematic" id="id517">``</span></a><a href="#id518"><span class="problematic" id="id519">`</span></a>python
with tf.device('/job:foo'):</p>
<blockquote>
<div><p># ops created here have devices with /job:foo
with tf.device('/job:bar/task:0/device:gpu:2'):</p>
<blockquote>
<div><p># ops created here have the fully specified device above</p>
</div></blockquote>
<dl class="simple">
<dt>with tf.device('/device:gpu:1'):</dt><dd><p># ops created here have the device '/job:foo/device:gpu:1'</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id520"><span class="problematic" id="id521">``</span></a><a href="#id522"><span class="problematic" id="id523">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device_name</strong> -- The device name to use in the context.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A context manager that specifies the default device to use for newly
created ops.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> -- If a function is passed in.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.divide">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">divide</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#divide"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.divide" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Python style division of <cite>x</cite> by <cite>y</cite>.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3,), dtype=float64,</span>
<span class="go">numpy=array([4. , 2. , 5.5])&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite></p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite></p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> with same shape as input</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.dynamic_partition">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">dynamic_partition</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">partitions</span></em>, <em class="sig-param"><span class="n">num_partitions</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_data_flow_ops.html#dynamic_partition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.dynamic_partition" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions <cite>data</cite> into <cite>num_partitions</cite> tensors using indices from <cite>partitions</cite>.</p>
<p>For each index tuple <cite>js</cite> of size <cite>partitions.ndim</cite>, the slice <cite>data[js, ...]</cite>
becomes part of <cite>outputs[partitions[js]]</cite>.  The slices with <cite>partitions[js] = i</cite>
are placed in <cite>outputs[i]</cite> in lexicographic order of <cite>js</cite>, and the first
dimension of <cite>outputs[i]</cite> is the number of entries in <cite>partitions</cite> equal to <cite>i</cite>.
In detail,</p>
<dl>
<dt><a href="#id524"><span class="problematic" id="id525">``</span></a><a href="#id526"><span class="problematic" id="id527">`</span></a>python</dt><dd><p>outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]</p>
<p>outputs[i] = pack([data[js, ...] for js if partitions[js] == i])</p>
</dd>
</dl>
<p><a href="#id528"><span class="problematic" id="id529">``</span></a><a href="#id530"><span class="problematic" id="id531">`</span></a></p>
<p><cite>data.shape</cite> must start with <cite>partitions.shape</cite>.</p>
<p>For example:</p>
<dl>
<dt><a href="#id532"><span class="problematic" id="id533">``</span></a><a href="#id534"><span class="problematic" id="id535">`</span></a>python</dt><dd><p># Scalar partitions.
partitions = 1
num_partitions = 2
data = [10, 20]
outputs[0] = []  # Empty with shape [0, 2]
outputs[1] = [[10, 20]]</p>
<p># Vector partitions.
partitions = [0, 0, 1, 1, 0]
num_partitions = 2
data = [10, 20, 30, 40, 50]
outputs[0] = [10, 20, 50]
outputs[1] = [30, 40]</p>
</dd>
</dl>
<p><a href="#id536"><span class="problematic" id="id537">``</span></a><a href="#id538"><span class="problematic" id="id539">`</span></a></p>
<p>See <cite>dynamic_stitch</cite> for an example on how to merge partitions back.</p>
<p>&lt;div style=&quot;width:70%; margin:auto; margin-bottom:10px; margin-top:20px;&quot;&gt;
&lt;img style=&quot;width:100%&quot; src=&quot;https://www.tensorflow.org/images/DynamicPartition.png&quot; alt&gt;
&lt;/div&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>partitions</strong> -- A <cite>Tensor</cite> of type <cite>int32</cite>.
Any shape.  Indices in the range <cite>[0, num_partitions)</cite>.</p></li>
<li><p><strong>num_partitions</strong> -- An <cite>int</cite> that is <cite>&gt;= 1</cite>.
The number of partitions to output.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <cite>num_partitions</cite> <cite>Tensor</cite> objects with the same type as <cite>data</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.dynamic_stitch">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">dynamic_stitch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_data_flow_ops.html#dynamic_stitch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.dynamic_stitch" title="Permalink to this definition">¶</a></dt>
<dd><p>Interleave the values from the <cite>data</cite> tensors into a single tensor.</p>
<p>Builds a merged tensor such that</p>
<dl class="simple">
<dt><a href="#id540"><span class="problematic" id="id541">``</span></a><a href="#id542"><span class="problematic" id="id543">`</span></a>python</dt><dd><p>merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]</p>
</dd>
</dl>
<p><a href="#id544"><span class="problematic" id="id545">``</span></a><a href="#id546"><span class="problematic" id="id547">`</span></a></p>
<p>For example, if each <cite>indices[m]</cite> is scalar or vector, we have</p>
<dl>
<dt><a href="#id548"><span class="problematic" id="id549">``</span></a><a href="#id550"><span class="problematic" id="id551">`</span></a>python</dt><dd><p># Scalar indices:
merged[indices[m], ...] = data[m][...]</p>
<p># Vector indices:
merged[indices[m][i], ...] = data[m][i, ...]</p>
</dd>
</dl>
<p><a href="#id552"><span class="problematic" id="id553">``</span></a><a href="#id554"><span class="problematic" id="id555">`</span></a></p>
<p>Each <cite>data[i].shape</cite> must start with the corresponding <cite>indices[i].shape</cite>,
and the rest of <cite>data[i].shape</cite> must be constant w.r.t. <cite>i</cite>.  That is, we
must have <cite>data[i].shape = indices[i].shape + constant</cite>.  In terms of this
<cite>constant</cite>, the output shape is</p>
<blockquote>
<div><p>merged.shape = [max(indices)] + constant</p>
</div></blockquote>
<p>Values are merged in order, so if an index appears in both <cite>indices[m][i]</cite> and
<cite>indices[n][j]</cite> for <cite>(m,i) &lt; (n,j)</cite> the slice <cite>data[n][j]</cite> will appear in the
merged result. If you do not need this guarantee, ParallelDynamicStitch might
perform better on some devices.</p>
<p>For example:</p>
<dl>
<dt><a href="#id556"><span class="problematic" id="id557">``</span></a><a href="#id558"><span class="problematic" id="id559">`</span></a>python</dt><dd><p>indices[0] = 6
indices[1] = [4, 1]
indices[2] = [[5, 2], [0, 3]]
data[0] = [61, 62]
data[1] = [[41, 42], [11, 12]]
data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]
merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],</p>
<blockquote>
<div><p>[51, 52], [61, 62]]</p>
</div></blockquote>
</dd>
</dl>
<p><a href="#id560"><span class="problematic" id="id561">``</span></a><a href="#id562"><span class="problematic" id="id563">`</span></a></p>
<p>This method can be used to merge partitions created by <cite>dynamic_partition</cite>
as illustrated on the following example:</p>
<dl>
<dt><a href="#id564"><span class="problematic" id="id565">``</span></a><a href="#id566"><span class="problematic" id="id567">`</span></a>python</dt><dd><p># Apply function (increments x_i) on elements for which a certain condition
# apply (x_i != -1 in this example).
x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])
condition_mask=tf.not_equal(x,tf.constant(-1.))
partitioned_data = tf.dynamic_partition(</p>
<blockquote>
<div><p>x, tf.cast(condition_mask, tf.int32) , 2)</p>
</div></blockquote>
<p>partitioned_data[1] = partitioned_data[1] + 1.0
condition_indices = tf.dynamic_partition(</p>
<blockquote>
<div><p>tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)</p>
</div></blockquote>
<p>x = tf.dynamic_stitch(condition_indices, partitioned_data)
# Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain
# unchanged.</p>
</dd>
</dl>
<p><a href="#id568"><span class="problematic" id="id569">``</span></a><a href="#id570"><span class="problematic" id="id571">`</span></a></p>
<p>&lt;div style=&quot;width:70%; margin:auto; margin-bottom:10px; margin-top:20px;&quot;&gt;
&lt;img style=&quot;width:100%&quot; src=&quot;https://www.tensorflow.org/images/DynamicStitch.png&quot; alt&gt;
&lt;/div&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- A list of at least 1 <cite>Tensor</cite> objects with type <cite>int32</cite>.</p></li>
<li><p><strong>data</strong> -- A list with the same length as <cite>indices</cite> of <cite>Tensor</cite> objects with the same type.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>data</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.edit_distance">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">edit_distance</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hypothesis</span></em>, <em class="sig-param"><span class="n">truth</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'edit_distance'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#edit_distance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.edit_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Levenshtein distance between sequences.</p>
<p>This operation takes variable-length sequences (<cite>hypothesis</cite> and <cite>truth</cite>),
each provided as a <cite>SparseTensor</cite>, and computes the Levenshtein distance.
You can normalize the edit distance by length of <cite>truth</cite> by setting
<cite>normalize</cite> to true.</p>
<p>For example, given the following input:</p>
<p><a href="#id572"><span class="problematic" id="id573">``</span></a><cite>python
# 'hypothesis' is a tensor of shape `[2, 1]</cite> with variable-length values:
#   (0,0) = [&quot;a&quot;]
#   (1,0) = [&quot;b&quot;]
hypothesis = tf.SparseTensor(</p>
<blockquote>
<div><dl class="simple">
<dt>[[0, 0, 0],</dt><dd><p>[1, 0, 0]],</p>
</dd>
</dl>
<p>[&quot;a&quot;, &quot;b&quot;],
(2, 1, 1))</p>
</div></blockquote>
<p># 'truth' is a tensor of shape <cite>[2, 2]</cite> with variable-length values:
#   (0,0) = []
#   (0,1) = [&quot;a&quot;]
#   (1,0) = [&quot;b&quot;, &quot;c&quot;]
#   (1,1) = [&quot;a&quot;]
truth = tf.SparseTensor(</p>
<blockquote>
<div><dl class="simple">
<dt>[[0, 1, 0],</dt><dd><p>[1, 0, 0],
[1, 0, 1],
[1, 1, 0]],</p>
</dd>
</dl>
<p>[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;],
(2, 2, 2))</p>
</div></blockquote>
<p>normalize = True
<a href="#id574"><span class="problematic" id="id575">``</span></a><a href="#id576"><span class="problematic" id="id577">`</span></a></p>
<p>This operation would return the following:</p>
<p><a href="#id578"><span class="problematic" id="id579">``</span></a><cite>python
# 'output' is a tensor of shape `[2, 2]</cite> with edit distances normalized
# by 'truth' lengths.
output ==&gt; [[inf, 1.0],  # (0,0): no truth, (0,1): no hypothesis</p>
<blockquote>
<div><p>[0.5, 1.0]]  # (1,0): addition, (1,1): no hypothesis</p>
</div></blockquote>
<p><a href="#id580"><span class="problematic" id="id581">``</span></a><a href="#id582"><span class="problematic" id="id583">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hypothesis</strong> -- A <cite>SparseTensor</cite> containing hypothesis sequences.</p></li>
<li><p><strong>truth</strong> -- A <cite>SparseTensor</cite> containing truth sequences.</p></li>
<li><p><strong>normalize</strong> -- A <cite>bool</cite>. If <cite>True</cite>, normalizes the Levenshtein distance by
length of <cite>truth.</cite></p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dense <cite>Tensor</cite> with rank <cite>R - 1</cite>, where R is the rank of the
<cite>SparseTensor</cite> inputs <cite>hypothesis</cite> and <cite>truth</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- If either <cite>hypothesis</cite> or <cite>truth</cite> are not a <cite>SparseTensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.eig">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">eig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/linalg_ops.html#eig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the eigen decomposition of a batch of matrices.</p>
<p>The eigenvalues
and eigenvectors for a non-Hermitian matrix in general are complex. The
eigenvectors are not guaranteed to be linearly independent.</p>
<p>Computes the eigenvalues and right eigenvectors of the innermost
N-by-N matrices in <cite>tensor</cite> such that
<cite>tensor[...,:,:] * v[..., :,i] = e[..., i] * v[...,:,i]</cite>, for i=0...N-1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- <cite>Tensor</cite> of shape <cite>[..., N, N]</cite>. Only the lower triangular part of
each inner inner matrix is referenced.</p></li>
<li><p><strong>name</strong> -- string, optional name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Eigenvalues. Shape is <cite>[..., N]</cite>. Sorted in non-decreasing order.
v: Eigenvectors. Shape is <cite>[..., N, N]</cite>. The columns of the inner most</p>
<blockquote>
<div><p>matrices contain eigenvectors of the corresponding matrices in <cite>tensor</cite></p>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>e</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.eigvals">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">eigvals</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/linalg_ops.html#eigvals"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.eigvals" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the eigenvalues of one or more matrices.</p>
<p>Note: If your program backpropagates through this function, you should replace
it with a call to tf.linalg.eig (possibly ignoring the second output) to
avoid computing the eigen decomposition twice. This is because the
eigenvectors are used to compute the gradient w.r.t. the eigenvalues. See
_SelfAdjointEigV2Grad in linalg_grad.py.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- <cite>Tensor</cite> of shape <cite>[..., N, N]</cite>.</p></li>
<li><p><strong>name</strong> -- string, optional name of the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Eigenvalues. Shape is <cite>[..., N]</cite>. The vector <cite>e[..., :]</cite> contains the <cite>N</cite></dt><dd><p>eigenvalues of <cite>tensor[..., :, :]</cite>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>e</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.einsum">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">einsum</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">equation</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/special_math_ops.html#einsum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.einsum" title="Permalink to this definition">¶</a></dt>
<dd><p>Tensor contraction over specified indices and outer product.</p>
<p>Einsum allows defining Tensors by defining their element-wise computation.
This computation is defined by <cite>equation</cite>, a shorthand form based on Einstein
summation. As an example, consider multiplying two matrices A and B to form a
matrix C.  The elements of C are given by:</p>
<dl class="simple">
<dt><a href="#id584"><span class="problematic" id="id585">``</span></a><a href="#id586"><span class="problematic" id="id587">`</span></a></dt><dd><p>C[i,k] = sum_j A[i,j] * B[j,k]</p>
</dd>
</dl>
<p><a href="#id588"><span class="problematic" id="id589">``</span></a><a href="#id590"><span class="problematic" id="id591">`</span></a></p>
<p>The corresponding <cite>equation</cite> is:</p>
<dl class="simple">
<dt><a href="#id592"><span class="problematic" id="id593">``</span></a><a href="#id594"><span class="problematic" id="id595">`</span></a></dt><dd><p>ij,jk-&gt;ik</p>
</dd>
</dl>
<p><a href="#id596"><span class="problematic" id="id597">``</span></a><a href="#id598"><span class="problematic" id="id599">`</span></a></p>
<p>In general, to convert the element-wise equation into the <cite>equation</cite> string,
use the following procedure (intermediate strings for matrix multiplication
example provided in parentheses):</p>
<ol class="arabic simple">
<li><p>remove variable names, brackets, and commas, (<cite>ik = sum_j ij * jk</cite>)</p></li>
<li><p>replace &quot;*&quot; with &quot;,&quot;, (<cite>ik = sum_j ij , jk</cite>)</p></li>
<li><p>drop summation signs, and (<cite>ik = ij, jk</cite>)</p></li>
<li><p>move the output to the right, while replacing &quot;=&quot; with &quot;-&gt;&quot;. (<cite>ij,jk-&gt;ik</cite>)</p></li>
</ol>
<p>Many common operations can be expressed in this way.  For example:</p>
<p><a href="#id600"><span class="problematic" id="id601">``</span></a><a href="#id602"><span class="problematic" id="id603">`</span></a>python
# Matrix multiplication
einsum('ij,jk-&gt;ik', m0, m1)  # output[i,k] = sum_j m0[i,j] * m1[j, k]</p>
<p># Dot product
einsum('i,i-&gt;', u, v)  # output = sum_i u[i]*v[i]</p>
<p># Outer product
einsum('i,j-&gt;ij', u, v)  # output[i,j] = u[i]*v[j]</p>
<p># Transpose
einsum('ij-&gt;ji', m)  # output[j,i] = m[i,j]</p>
<p># Trace
einsum('ii', m)  # output[j,i] = trace(m) = sum_i m[i, i]</p>
<p># Batch matrix multiplication
einsum('aij,ajk-&gt;aik', s, t)  # out[a,i,k] = sum_j s[a,i,j] * t[a, j, k]
<a href="#id604"><span class="problematic" id="id605">``</span></a><a href="#id606"><span class="problematic" id="id607">`</span></a></p>
<p>To enable and control broadcasting, use an ellipsis.  For example, to perform
batch matrix multiplication with NumPy-style broadcasting across the batch
dimensions, use:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">einsum('...ij,...jk-&gt;...ik',</span> <span class="pre">u,</span> <span class="pre">v)</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>equation</strong> -- a <cite>str</cite> describing the contraction, in the same format as
<cite>numpy.einsum</cite>.</p></li>
<li><p><strong>*inputs</strong> -- the inputs to contract (each one a <cite>Tensor</cite>), whose shapes should
be consistent with <cite>equation</cite>.</p></li>
<li><p><strong>**kwargs</strong> -- <ul>
<li><p>optimize: Optimization strategy to use to find contraction path using
opt_einsum. Must be 'greedy', 'optimal', 'branch-2', 'branch-all' or</p>
<blockquote>
<div><p>'auto'. (optional, default: 'greedy').</p>
</div></blockquote>
</li>
<li><p>name: A name for the operation (optional).</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The contracted <cite>Tensor</cite>, with shape determined by <cite>equation</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If
    - the format of <cite>equation</cite> is incorrect,
    - number of inputs or their shapes are inconsistent with <cite>equation</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.ensure_shape">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">ensure_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/check_ops.html#ensure_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.ensure_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the shape of a tensor and checks at runtime that the shape holds.</p>
<p>For example:
<a href="#id608"><span class="problematic" id="id609">``</span></a><a href="#id610"><span class="problematic" id="id611">`</span></a>python
x = tf.compat.v1.placeholder(tf.int32)
print(x.shape)
==&gt; TensorShape(None)
y = x * 2
print(y.shape)
==&gt; TensorShape(None)</p>
<p>y = tf.ensure_shape(y, (None, 3, 3))
print(y.shape)
==&gt; TensorShape([Dimension(None), Dimension(3), Dimension(3)])</p>
<dl class="simple">
<dt>with tf.compat.v1.Session() as sess:</dt><dd><p># Raises tf.errors.InvalidArgumentError, because the shape (3,) is not
# compatible with the shape (None, 3, 3)
sess.run(y, feed_dict={x: [1, 2, 3]})</p>
</dd>
</dl>
<p><a href="#id612"><span class="problematic" id="id613">``</span></a><a href="#id614"><span class="problematic" id="id615">`</span></a></p>
<p>NOTE: This differs from <cite>Tensor.set_shape</cite> in that it sets the static shape
of the resulting tensor and enforces it at runtime, raising an error if the
tensor's runtime shape is incompatible with the specified shape.
<cite>Tensor.set_shape</cite> sets the static shape of the tensor without enforcing it
at runtime, which may result in inconsistencies between the statically-known
shape of tensors and the runtime value of tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>shape</strong> -- A <cite>TensorShape</cite> representing the shape of this tensor, a
<cite>TensorShapeProto</cite>, a list, a tuple, or None.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional). Defaults to &quot;EnsureShape&quot;.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type and contents as <cite>x</cite>. At runtime, raises a
<cite>tf.errors.InvalidArgumentError</cite> if <cite>shape</cite> is incompatible with the shape
of <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.equal">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">equal</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#equal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.equal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of (x == y) element-wise.</p>
<p>Performs a [broadcast](
<a class="reference external" href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>) with the
arguments and then an element-wise equality comparison, returning a Tensor of
boolean values.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>tf.Tensor</cite> or <cite>tf.SparseTensor</cite> or <cite>tf.IndexedSlices</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>tf.Tensor</cite> or <cite>tf.SparseTensor</cite> or <cite>tf.IndexedSlices</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tf.Tensor</cite> of type bool with the same size as that of x or y.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>tf.errors.InvalidArgumentError</strong> -- If shapes of arguments are incompatible</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.executing_eagerly">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">executing_eagerly</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/context.html#executing_eagerly"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.executing_eagerly" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether the current thread has eager execution enabled.</p>
<p>Eager execution is enabled by default and this API returns <cite>True</cite>
in most of cases. However, this API might return <cite>False</cite> in the following use
cases.</p>
<ul class="simple">
<li><p>Executing inside <cite>tf.function</cite>, unless under <cite>tf.init_scope</cite> or
<cite>tf.config.experimental_run_functions_eagerly(True)</cite> is previously called.</p></li>
<li><p>Executing inside a transformation function for <cite>tf.dataset</cite>.</p></li>
<li><p><cite>tf.compat.v1.disable_eager_execution()</cite> is called.</p></li>
</ul>
<p>General case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Inside <cite>tf.function</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">fn</span><span class="p">():</span>
<span class="gp">... </span>  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">())</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fn</span><span class="p">()</span>
<span class="go">True</span>
<span class="go">False</span>
</pre></div>
</div>
<p>Inside <cite>tf.function</cite> after</p>
<p><cite>tf.config.experimental_run_functions_eagerly(True)</cite> is called:
&gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(True)
&gt;&gt;&gt; &#64;tf.function
... def fn():
...   with tf.init_scope():
...     print(tf.executing_eagerly())
...   print(tf.executing_eagerly())
&gt;&gt;&gt; fn()
True
True
&gt;&gt;&gt; tf.config.experimental_run_functions_eagerly(False)</p>
<p>Inside a transformation function for <cite>tf.dataset</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">data_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">())</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">data_fn</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><cite>True</cite> if the current thread has eager execution enabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.exp">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">exp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#exp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes exponential of x element-wise.  \(y = e^x\).</p>
<p>This function computes the exponential of the input tensor element-wise.
i.e. <cite>math.exp(x)</cite> or \(e^x\), where <cite>x</cite> is the input tensor.
\(e\) denotes Euler's number and is approximately equal to 2.718281.
Output is positive for any real input.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=7.389056&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=float32,</span>
<span class="go">numpy=array([   7.389056, 2980.958   ], dtype=float32)&gt;</span>
</pre></div>
</div>
<p>For complex numbers, the exponential value is calculated as
\(e^{x+iy}={e^x}{e^{iy}}={e^x}{\cos(y)+i\sin(y)}\)</p>
<p>For <cite>1+1j</cite> the value would be computed as:
\(e^1{\cos(1)+i\sin(1)} = 2.7182817 \times (0.5403023+0.84147096j)\)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=complex128,</span>
<span class="go">numpy=(1.4686939399158851+2.2873552871788423j)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>tf.Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>,
<cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tf.Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.exp
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.expand_dims">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">expand_dims</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">axis</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#expand_dims"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.expand_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with an additional dimension inserted at index <cite>axis</cite>.</p>
<p>Given a tensor <cite>input</cite>, this operation inserts a dimension of size 1 at the
dimension index <cite>axis</cite> of <cite>input</cite>'s shape. The dimension index <cite>axis</cite> starts
at zero; if you specify a negative number for <cite>axis</cite> it is counted backward
from the end.</p>
<p>This operation is useful if you want to add a batch dimension to a single
element. For example, if you have a single image of shape <cite>[height, width,
channels]</cite>, you can make it a batch of one image with <cite>expand_dims(image, 0)</cite>,
which will make the shape <cite>[1, height, width, channels]</cite>.</p>
<p>Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span> <span class="c1"># shape [2, 3]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=</span>
<span class="go">array([[[1, 2, 3],</span>
<span class="go">        [4, 5, 6]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 1, 3), dtype=int32, numpy=</span>
<span class="go">array([[[1, 2, 3]],</span>
<span class="go">       [[4, 5, 6]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3, 1), dtype=int32, numpy=</span>
<span class="go">array([[[1],</span>
<span class="go">        [2],</span>
<span class="go">        [3]],</span>
<span class="go">       [[4],</span>
<span class="go">        [5],</span>
<span class="go">        [6]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Last dimension index. In this case, same as 2.</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3, 1), dtype=int32, numpy=</span>
<span class="go">array([[[1],</span>
<span class="go">        [2],</span>
<span class="go">        [3]],</span>
<span class="go">       [[4],</span>
<span class="go">        [5],</span>
<span class="go">        [6]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>This operation is related to:</p>
<ul class="simple">
<li><p><cite>tf.squeeze</cite>, which removes dimensions of size 1.</p></li>
<li><p><cite>tf.reshape</cite>, which provides more flexible reshaping capability</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>axis</strong> -- Integer specifying the dimension index at which to expand the
shape of <cite>input</cite>. Given an input of D dimensions, <cite>axis</cite> must be in range
<cite>[-(D+1), D]</cite> (inclusive).</p></li>
<li><p><strong>name</strong> -- Optional string. The name of the output <cite>Tensor</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor with the same data as <cite>input</cite>, with an additional dimension
inserted at the index specified by <cite>axis</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> -- If <cite>axis</cite> is not specified.</p></li>
<li><p><strong>InvalidArgumentError</strong> -- If <cite>axis</cite> is out of range <cite>[-(D+1), D]</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.extract_volume_patches">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">extract_volume_patches</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">ksizes</span></em>, <em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">padding</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#extract_volume_patches"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.extract_volume_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract <cite>patches</cite> from <cite>input</cite> and put them in the &quot;depth&quot; output dimension. 3D extension of <cite>extract_image_patches</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>int8</cite>, <cite>int64</cite>, <cite>bfloat16</cite>, <cite>uint16</cite>, <cite>half</cite>, <cite>uint32</cite>, <cite>uint64</cite>.
5-D Tensor with shape <cite>[batch, in_planes, in_rows, in_cols, depth]</cite>.</p></li>
<li><p><strong>ksizes</strong> -- A list of <cite>ints</cite> that has length <cite>&gt;= 5</cite>.
The size of the sliding window for each dimension of <cite>input</cite>.</p></li>
<li><p><strong>strides</strong> -- A list of <cite>ints</cite> that has length <cite>&gt;= 5</cite>.
1-D of length 5. How far the centers of two consecutive patches are in
<cite>input</cite>. Must be: <cite>[1, stride_planes, stride_rows, stride_cols, 1]</cite>.</p></li>
<li><p><strong>padding</strong> -- <p>A <cite>string</cite> from: <cite>&quot;SAME&quot;, &quot;VALID&quot;</cite>.
The type of padding algorithm to use.</p>
<p>We specify the size-related attributes as:</p>
<dl class="simple">
<dt><a href="#id616"><span class="problematic" id="id617">``</span></a><a href="#id618"><span class="problematic" id="id619">`</span></a>python</dt><dd><p>ksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]
strides = [1, stride_planes, strides_rows, strides_cols, 1]</p>
</dd>
</dl>
<p><a href="#id620"><span class="problematic" id="id621">``</span></a><a href="#id622"><span class="problematic" id="id623">`</span></a></p>
</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.eye">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">eye</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_rows</span></em>, <em class="sig-param"><span class="n">num_columns</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/linalg_ops.html#eye"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.eye" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct an identity matrix, or a batch of matrices.</p>
<p><a href="#id624"><span class="problematic" id="id625">``</span></a><a href="#id626"><span class="problematic" id="id627">`</span></a>python
# Construct one identity matrix.
tf.eye(2)
==&gt; [[1., 0.],</p>
<blockquote>
<div><p>[0., 1.]]</p>
</div></blockquote>
<p># Construct a batch of 3 identity matrices, each 2 x 2.
# batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2.
batch_identity = tf.eye(2, batch_shape=[3])</p>
<p># Construct one 2 x 3 &quot;identity&quot; matrix
tf.eye(2, num_columns=3)
==&gt; [[ 1.,  0.,  0.],</p>
<blockquote>
<div><p>[ 0.,  1.,  0.]]</p>
</div></blockquote>
<p><a href="#id628"><span class="problematic" id="id629">``</span></a><a href="#id630"><span class="problematic" id="id631">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_rows</strong> -- Non-negative <cite>int32</cite> scalar <cite>Tensor</cite> giving the number of rows
in each batch matrix.</p></li>
<li><p><strong>num_columns</strong> -- Optional non-negative <cite>int32</cite> scalar <cite>Tensor</cite> giving the number
of columns in each batch matrix.  Defaults to <cite>num_rows</cite>.</p></li>
<li><p><strong>batch_shape</strong> -- A list or tuple of Python integers or a 1-D <cite>int32</cite> <cite>Tensor</cite>.
If provided, the returned <cite>Tensor</cite> will have leading batch dimensions of
this shape.</p></li>
<li><p><strong>dtype</strong> -- The type of an element in the resulting <cite>Tensor</cite></p></li>
<li><p><strong>name</strong> -- A name for this <cite>Op</cite>.  Defaults to &quot;eye&quot;.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of shape <cite>batch_shape + [num_rows, num_columns]</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.fill">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">fill</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dims</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#fill"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a tensor filled with a scalar value.</p>
<p>This operation creates a tensor of shape <cite>dims</cite> and fills it with <cite>value</cite>.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="mi">9</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">array([[9, 9, 9],</span>
<span class="go">       [9, 9, 9]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p><cite>tf.fill</cite> evaluates at graph runtime and supports dynamic shapes based on
other runtime <cite>tf.Tensors</cite>, unlike <cite>tf.constant(value, shape=dims)</cite>, which
embeds the value as a <cite>Const</cite> node.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dims</strong> -- A 1-D sequence of non-negative numbers. Represents the shape of the
output <cite>tf.Tensor</cite>. Entries should be of type: <cite>int32</cite>, <cite>int64</cite>.</p></li>
<li><p><strong>value</strong> -- A value to fill the returned <cite>tf.Tensor</cite>.</p></li>
<li><p><strong>name</strong> -- Optional string. The name of the output <cite>tf.Tensor</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tf.Tensor</cite> with shape <cite>dims</cite> and the same dtype as <cite>value</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>InvalidArgumentError</strong> -- <cite>dims</cite> contains negative entries.</p></li>
<li><p><strong>NotFoundError</strong> -- <cite>dims</cite> contains non-integer entries.</p></li>
</ul>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Similar to <cite>np.full</cite>. In <cite>numpy</cite>, more parameters are supported. Passing a
number argument as the shape (<cite>np.full(5, value)</cite>) is valid in <cite>numpy</cite> for
specifying a 1-D shaped result, while TensorFlow does not support this syntax.
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.fingerprint">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">fingerprint</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">method</span><span class="o">=</span><span class="default_value">'farmhash64'</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#fingerprint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.fingerprint" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates fingerprint values.</p>
<p>Generates fingerprint values of <cite>data</cite>.</p>
<p>Fingerprint op considers the first dimension of <cite>data</cite> as the batch dimension,
and <cite>output[i]</cite> contains the fingerprint value generated from contents in
<cite>data[i, ...]</cite> for all <cite>i</cite>.</p>
<p>Fingerprint op writes fingerprint values as byte arrays. For example, the
default method <cite>farmhash64</cite> generates a 64-bit fingerprint value at a time.
This 8-byte value is written out as an <cite>tf.uint8</cite> array of size 8, in
little-endian order.</p>
<p>For example, suppose that <cite>data</cite> has data type <cite>tf.int32</cite> and shape (2, 3, 4),
and that the fingerprint method is <cite>farmhash64</cite>. In this case, the output
shape is (2, 8), where 2 is the batch dimension size of <cite>data</cite>, and 8 is the
size of each fingerprint value in bytes. <cite>output[0, :]</cite> is generated from
12 integers in <cite>data[0, :, :]</cite> and similarly <cite>output[1, :]</cite> is generated from
other 12 integers in <cite>data[1, :, :]</cite>.</p>
<p>Note that this op fingerprints the raw underlying buffer, and it does not
fingerprint Tensor's metadata such as data type and/or shape. For example, the
fingerprint values are invariant under reshapes and bitcasts as long as the
batch dimension remain the same:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tf.fingerprint(data)</span> <span class="pre">==</span> <span class="pre">tf.fingerprint(tf.reshape(data,</span> <span class="pre">...))</span>
<span class="pre">tf.fingerprint(data)</span> <span class="pre">==</span> <span class="pre">tf.fingerprint(tf.bitcast(data,</span> <span class="pre">...))</span>
<span class="pre">`</span></code></p>
<p>For string data, one should expect <cite>tf.fingerprint(data) !=
tf.fingerprint(tf.string.reduce_join(data))</cite> in general.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> -- A <cite>Tensor</cite>. Must have rank 1 or higher.</p></li>
<li><p><strong>method</strong> -- A <cite>Tensor</cite> of type <cite>tf.string</cite>. Fingerprint method used by this op.
Currently available method is <cite>farmhash64</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A two-dimensional <cite>Tensor</cite> of type <cite>tf.uint8</cite>. The first dimension equals to
<cite>data</cite>'s first dimension, and the second dimension size depends on the
fingerprint algorithm.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.floor">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">floor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#floor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns element-wise largest integer not greater than x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.foldl">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">foldl</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em>, <em class="sig-param"><span class="n">elems</span></em>, <em class="sig-param"><span class="n">initializer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">parallel_iterations</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">back_prop</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">swap_memory</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/functional_ops.html#foldl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.foldl" title="Permalink to this definition">¶</a></dt>
<dd><p>foldl on the list of tensors unpacked from <cite>elems</cite> on dimension 0. (deprecated argument values)</p>
<p>Warning: SOME ARGUMENT VALUES ARE DEPRECATED: <cite>(back_prop=False)</cite>. They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.foldl(fn, elems, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.foldl(fn, elems))</p>
<p>This foldl operator repeatedly applies the callable <cite>fn</cite> to a sequence
of elements from first to last. The elements are made of the tensors
unpacked from <cite>elems</cite> on dimension 0. The callable fn takes two tensors as
arguments. The first argument is the accumulated value computed from the
preceding invocation of fn, and the second is the value at the current
position of <cite>elems</cite>. If <cite>initializer</cite> is None, <cite>elems</cite> must contain at least
one element, and its first element is used as the initializer.</p>
<p>Suppose that <cite>elems</cite> is unpacked into <cite>values</cite>, a list of tensors. The shape
of the result tensor is fn(initializer, values[0]).shape`.</p>
<p>This method also allows multi-arity <cite>elems</cite> and output of <cite>fn</cite>.  If <cite>elems</cite>
is a (possibly nested) list or tuple of tensors, then each of these tensors
must have a matching first (unpack) dimension.  The signature of <cite>fn</cite> may
match the structure of <cite>elems</cite>.  That is, if <cite>elems</cite> is
<cite>(t1, [t2, t3, [t4, t5]])</cite>, then an appropriate signature for <cite>fn</cite> is:
<cite>fn = lambda (t1, [t2, t3, [t4, t5]]):</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- The callable to be performed.</p></li>
<li><p><strong>elems</strong> -- A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension.  The nested sequence of the
resulting slices will be the first argument to <cite>fn</cite>.</p></li>
<li><p><strong>initializer</strong> -- (optional) A tensor or (possibly nested) sequence of tensors,
as the initial value for the accumulator.</p></li>
<li><p><strong>parallel_iterations</strong> -- (optional) The number of iterations allowed to run in
parallel.</p></li>
<li><p><strong>back_prop</strong> -- (optional) Deprecated. False disables support for back
propagation. Prefer using <cite>tf.stop_gradient</cite> instead.</p></li>
<li><p><strong>swap_memory</strong> -- (optional) True enables GPU-CPU memory swapping.</p></li>
<li><p><strong>name</strong> -- (optional) Name prefix for the returned tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor or (possibly nested) sequence of tensors, resulting from applying
<cite>fn</cite> consecutively to the list of tensors unpacked from <cite>elems</cite>, from first
to last.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>fn</cite> is not callable.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">elems</span> <span class="pre">=</span> <span class="pre">tf.constant([1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6])</span>
<span class="pre">sum</span> <span class="pre">=</span> <span class="pre">foldl(lambda</span> <span class="pre">a,</span> <span class="pre">x:</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">x,</span> <span class="pre">elems)</span>
<span class="pre">#</span> <span class="pre">sum</span> <span class="pre">==</span> <span class="pre">21</span>
<span class="pre">`</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.foldr">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">foldr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em>, <em class="sig-param"><span class="n">elems</span></em>, <em class="sig-param"><span class="n">initializer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">parallel_iterations</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">back_prop</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">swap_memory</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/functional_ops.html#foldr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.foldr" title="Permalink to this definition">¶</a></dt>
<dd><p>foldr on the list of tensors unpacked from <cite>elems</cite> on dimension 0. (deprecated argument values)</p>
<p>Warning: SOME ARGUMENT VALUES ARE DEPRECATED: <cite>(back_prop=False)</cite>. They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.foldr(fn, elems, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))</p>
<p>This foldr operator repeatedly applies the callable <cite>fn</cite> to a sequence
of elements from last to first. The elements are made of the tensors
unpacked from <cite>elems</cite>. The callable fn takes two tensors as arguments.
The first argument is the accumulated value computed from the preceding
invocation of fn, and the second is the value at the current position of
<cite>elems</cite>. If <cite>initializer</cite> is None, <cite>elems</cite> must contain at least one element,
and its first element is used as the initializer.</p>
<p>Suppose that <cite>elems</cite> is unpacked into <cite>values</cite>, a list of tensors. The shape
of the result tensor is <cite>fn(initializer, values[0]).shape</cite>.</p>
<p>This method also allows multi-arity <cite>elems</cite> and output of <cite>fn</cite>.  If <cite>elems</cite>
is a (possibly nested) list or tuple of tensors, then each of these tensors
must have a matching first (unpack) dimension.  The signature of <cite>fn</cite> may
match the structure of <cite>elems</cite>.  That is, if <cite>elems</cite> is
<cite>(t1, [t2, t3, [t4, t5]])</cite>, then an appropriate signature for <cite>fn</cite> is:
<cite>fn = lambda (t1, [t2, t3, [t4, t5]]):</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- The callable to be performed.</p></li>
<li><p><strong>elems</strong> -- A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension.  The nested sequence of the
resulting slices will be the first argument to <cite>fn</cite>.</p></li>
<li><p><strong>initializer</strong> -- (optional) A tensor or (possibly nested) sequence of tensors,
as the initial value for the accumulator.</p></li>
<li><p><strong>parallel_iterations</strong> -- (optional) The number of iterations allowed to run in
parallel.</p></li>
<li><p><strong>back_prop</strong> -- (optional) Deprecated. False disables support for back
propagation. Prefer using <cite>tf.stop_gradient</cite> instead.</p></li>
<li><p><strong>swap_memory</strong> -- (optional) True enables GPU-CPU memory swapping.</p></li>
<li><p><strong>name</strong> -- (optional) Name prefix for the returned tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor or (possibly nested) sequence of tensors, resulting from applying
<cite>fn</cite> consecutively to the list of tensors unpacked from <cite>elems</cite>, from last
to first.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if <cite>fn</cite> is not callable.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">elems</span> <span class="pre">=</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6]</span>
<span class="pre">sum</span> <span class="pre">=</span> <span class="pre">foldr(lambda</span> <span class="pre">a,</span> <span class="pre">x:</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">x,</span> <span class="pre">elems)</span>
<span class="pre">#</span> <span class="pre">sum</span> <span class="pre">==</span> <span class="pre">21</span>
<span class="pre">`</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.function">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_signature</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">autograph</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">experimental_implements</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">experimental_autograph_options</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">experimental_relax_shapes</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">experimental_compile</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/eager/def_function.html#function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compiles a function into a callable TensorFlow graph.</p>
<p><cite>tf.function</cite> constructs a callable that executes a TensorFlow graph
(<cite>tf.Graph</cite>) created by trace-compiling the TensorFlow operations in <cite>func</cite>,
effectively executing <cite>func</cite> as a TensorFlow graph.</p>
<p>Example usage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: ... numpy=array([7, 7], ...)&gt;</span>
</pre></div>
</div>
<p>_Features_</p>
<p><cite>func</cite> may use data-dependent control flow, including <cite>if</cite>, <cite>for</cite>, <cite>while</cite>
<cite>break</cite>, <cite>continue</cite> and <cite>return</cite> statements:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="gp">... </span>  <span class="k">else</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="o">-</span><span class="n">x</span> <span class="o">//</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="go">&lt;tf.Tensor: ... numpy=1&gt;</span>
</pre></div>
</div>
<p><cite>func</cite>'s closure may include <cite>tf.Tensor</cite> and <cite>tf.Variable</cite> objects:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">()</span>
<span class="go">&lt;tf.Tensor: ... numpy=array([7, 7], ...)&gt;</span>
</pre></div>
</div>
<p><cite>func</cite> may also use ops with side effects, such as <cite>tf.print</cite>, <cite>tf.Variable</cite>
and others:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">v</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span>
<span class="go">&lt;tf.Variable ... numpy=4&gt;</span>
</pre></div>
</div>
<p>Important: Any Python side-effects (appending to a list, printing with
<cite>print</cite>, etc) will only happen once, when <cite>func</cite> is traced. To have
side-effects executed into your <cite>tf.function</cite> they need to be written
as TF ops:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>    <span class="c1"># Caution! Will only happen once when tracing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span>
<span class="go">[&lt;tf.Tensor ...&gt;]</span>
</pre></div>
</div>
<p>Instead, use TensorFlow collections like <cite>tf.TensorArray</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">ta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
<span class="gp">... </span>    <span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">ta</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">&lt;tf.Tensor: ..., numpy=array([2, 3, 4], ...)&gt;</span>
</pre></div>
</div>
<p><span class="target" id="tf-function">tf.function</span> is <a href="#id1106"><span class="problematic" id="id1107">polymorphic_</span></a></p>
<p>Internally, <cite>tf.function</cite> can build more than one graph, to support arguments
with different data types or shapes, since TensorFlow can build more
efficient graphs that are specialized on shapes and dtypes. <cite>tf.function</cite>
also treats any pure Python value as opaque objects, and builds a separate
graph for each set of Python arguments that it encounters.</p>
<p>To obtain an individual graph, use the <cite>get_concrete_function</cite> method of
the callable created by <cite>tf.function</cite>. It can be called with the same
arguments as <cite>func</cite> and returns a special <cite>tf.Graph</cite> object:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Caution: Passing python scalars or lists as arguments to <cite>tf.function</cite> will
always build a new graph. To avoid this, pass numeric arguments as Tensors
whenever possible:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f2</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Slow - builds new graph</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1</span> <span class="ow">is</span> <span class="n">f2</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f2</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># Fast - reuses f1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1</span> <span class="ow">is</span> <span class="n">f2</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Python numerical arguments should only be used when they take few distinct
values, such as hyperparameters like the number of layers in a neural network.</p>
<p>_Input <a href="#id1108"><span class="problematic" id="id1109">signatures_</span></a></p>
<p>For Tensor arguments, <cite>tf.function</cite> instantiates a separate graph for every
unique set of input shapes and datatypes. The example below creates two
separate graphs, each specialized to a different shape:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matrix</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span> <span class="ow">is</span> <span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<p>An &quot;input signature&quot; can be optionally provided to <cite>tf.function</cite> to control
the graphs traced. The input signature specifies the shape and type of each
Tensor argument to the function using a <cite>tf.TensorSpec</cite> object. More general
shapes can be used. This is useful to avoid creating multiple graphs when
Tensors have dynamic shapes. It also restricts the shape and datatype of
Tensors that can be used:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_signature</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)])</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matrix</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span> <span class="ow">is</span> <span class="n">f</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>_Variables may only be created <a href="#id1110"><span class="problematic" id="id1111">once_</span></a></p>
<p><cite>tf.function</cite> only allows creating new <cite>tf.Variable</cite> objects when it is called
for the first time:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">...</span>
<span class="gp">... </span>  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="gp">... </span>  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>      <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
<p>In general, it is recommended to create stateful objects like <cite>tf.Variable</cite>
outside of <cite>tf.function</cite> and passing them as arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> -- the function to be compiled. If <cite>func</cite> is None, <cite>tf.function</cite> returns
a decorator that can be invoked with a single argument - <cite>func</cite>. In other
words, <cite>tf.function(input_signature=...)(func)</cite> is equivalent to
<cite>tf.function(func, input_signature=...)</cite>. The former can be used as
decorator.</p></li>
<li><p><strong>input_signature</strong> -- A possibly nested sequence of <cite>tf.TensorSpec</cite> objects
specifying the shapes and dtypes of the Tensors that will be supplied to
this function. If <cite>None</cite>, a separate function is instantiated for each
inferred input signature.  If input_signature is specified, every input to
<cite>func</cite> must be a <cite>Tensor</cite>, and <cite>func</cite> cannot accept <cite>**kwargs</cite>.</p></li>
<li><p><strong>autograph</strong> -- Whether autograph should be applied on <cite>func</cite> before tracing a
graph. Data-dependent control flow requires <cite>autograph=True</cite>. For more
information, see the [tf.function and AutoGraph guide](
<a class="reference external" href="https://www.tensorflow.org/guide/function">https://www.tensorflow.org/guide/function</a>).</p></li>
<li><p><strong>experimental_implements</strong> -- <p>If provided, contains a name of a &quot;known&quot; function
this implements. For example &quot;mycompany.my_recurrent_cell&quot;.
This is stored as an attribute in inference function,
which can then be detected when processing serialized function.
See [standardizing composite ops](<a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20190610-standardizing-composite_ops.md">https://github.com/tensorflow/community/blob/master/rfcs/20190610-standardizing-composite_ops.md</a>)  # pylint: disable=line-too-long
for details.  For an example of utilizing this attribute see this
[example](<a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc</a>)
The code above automatically detects and substitutes function that
implements &quot;embedded_matmul&quot; and allows TFLite to substitute its own
implementations. For instance, a tensorflow user can use this</p>
<blockquote>
<div><p>attribute to mark that their function also implements</p>
</div></blockquote>
<p><cite>embedded_matmul</cite> (perhaps more efficiently!)
by specifying it using this parameter:
<cite>&#64;tf.function(experimental_implements=&quot;embedded_matmul&quot;)</cite></p>
</p></li>
<li><p><strong>experimental_autograph_options</strong> -- Optional tuple of
<cite>tf.autograph.experimental.Feature</cite> values.</p></li>
<li><p><strong>experimental_relax_shapes</strong> -- When True, <cite>tf.function</cite> may generate fewer,
graphs that are less specialized on input shapes.</p></li>
<li><p><strong>experimental_compile</strong> -- If True, the function is always compiled by
[XLA](<a class="reference external" href="https://www.tensorflow.org/xla">https://www.tensorflow.org/xla</a>). XLA may be more efficient in some
cases (e.g. TPU, XLA_GPU, dense tensor computations).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If <cite>func</cite> is not None, returns a callable that will execute the compiled
function (and return zero or more <cite>tf.Tensor</cite> objects).
If <cite>func</cite> is None, returns a decorator that, when invoked with a single
<cite>func</cite> argument, returns a callable equivalent to the case above.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError when attempting to use experimental_compile</strong><strong>, </strong><strong>but XLA support is</strong> -- </p></li>
<li><p><strong>not enabled.</strong> -- </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.gather">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">gather</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">validate_indices</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_dims</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather slices from params axis <cite>axis</cite> according to indices.</p>
<p>Gather slices from params axis <cite>axis</cite> according to <cite>indices</cite>.  <cite>indices</cite> must
be an integer tensor of any dimension (usually 0-D or 1-D).</p>
<p>For 0-D (scalar) <cite>indices</cite>:</p>
<p>$$begin{align*}
output[p_0, ..., p_{axis-1}, &amp;&amp;          &amp;&amp;&amp; p_{axis + 1}, ..., p_{N-1}] = \
params[p_0, ..., p_{axis-1}, &amp;&amp; indices, &amp;&amp;&amp; p_{axis + 1}, ..., p_{N-1}]
end{align*}$$</p>
<p>Where <em>N</em> = <cite>ndims(params)</cite>.</p>
<p>For 1-D (vector) <cite>indices</cite> with <cite>batch_dims=0</cite>:</p>
<p>$$begin{align*}
output[p_0, ..., p_{axis-1}, &amp;&amp;         &amp;i,  &amp;&amp;p_{axis + 1}, ..., p_{N-1}] =\
params[p_0, ..., p_{axis-1}, &amp;&amp; indices[&amp;i], &amp;&amp;p_{axis + 1}, ..., p_{N-1}]
end{align*}$$</p>
<p>In the general case, produces an output tensor where:</p>
<p>$$begin{align*}
output[p_0,             &amp;..., p_{axis-1},                       &amp;</p>
<blockquote>
<div><p>&amp;i_{B},           ..., i_{M-1},                          &amp;
p_{axis + 1},    &amp;..., p_{N-1}]                          = \</p>
</div></blockquote>
<dl class="simple">
<dt>params[p_0,             &amp;..., p_{axis-1},                       &amp;</dt><dd><p>indices[p_0, ..., p_{B-1}, &amp;i_{B}, ..., i_{M-1}],        &amp;
p_{axis + 1},    &amp;..., p_{N-1}]</p>
</dd>
</dl>
<p>end{align*}$$</p>
<p>Where <em>N</em> = <cite>ndims(params)</cite>, <em>M</em> = <cite>ndims(indices)</cite>, and <em>B</em> = <cite>batch_dims</cite>.
Note that <cite>params.shape[:batch_dims]</cite> must be identical to
<cite>indices.shape[:batch_dims]</cite>.</p>
<p>The shape of the output tensor is:</p>
<p>&gt; <cite>output.shape = params.shape[:axis] + indices.shape[batch_dims:] +
&gt; params.shape[axis + 1:]</cite>.</p>
<p>Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, a 0 is stored in the corresponding
output value.</p>
<p>See also <cite>tf.gather_nd</cite>.</p>
<p>&lt;div style=&quot;width:70%; margin:auto; margin-bottom:10px; margin-top:20px;&quot;&gt;
&lt;img style=&quot;width:100%&quot; src=&quot;https://www.tensorflow.org/images/Gather.png&quot;
alt&gt;
&lt;/div&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> -- The <cite>Tensor</cite> from which to gather values. Must be at least rank
<cite>axis + 1</cite>.</p></li>
<li><p><strong>indices</strong> -- The index <cite>Tensor</cite>.  Must be one of the following types: <cite>int32</cite>,
<cite>int64</cite>. Must be in range <cite>[0, params.shape[axis])</cite>.</p></li>
<li><p><strong>validate_indices</strong> -- Deprecated, does nothing.</p></li>
<li><p><strong>axis</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>. The
<cite>axis</cite> in <cite>params</cite> to gather <cite>indices</cite> from. Must be greater than or equal
to <cite>batch_dims</cite>.  Defaults to the first non-batch dimension. Supports
negative indexes.</p></li>
<li><p><strong>batch_dims</strong> -- An <cite>integer</cite>.  The number of batch dimensions.  Must be less
than or equal to <cite>rank(indices)</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>params</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.gather_nd">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">gather_nd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">batch_dims</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#gather_nd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.gather_nd" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather slices from <cite>params</cite> into a Tensor with shape specified by <cite>indices</cite>.</p>
<p><cite>indices</cite> is an K-dimensional integer tensor, best thought of as a
(K-1)-dimensional tensor of indices into <cite>params</cite>, where each element defines
a slice of <cite>params</cite>:</p>
<blockquote>
<div><p>output[\(i_0, ..., i_{K-2}\)] = params[indices[\(i_0, ..., i_{K-2}\)]]</p>
</div></blockquote>
<p>Whereas in <cite>tf.gather</cite> <cite>indices</cite> defines slices into the first
dimension of <cite>params</cite>, in <cite>tf.gather_nd</cite>, <cite>indices</cite> defines slices into the
first <cite>N</cite> dimensions of <cite>params</cite>, where <cite>N = indices.shape[-1]</cite>.</p>
<p>The last dimension of <cite>indices</cite> can be at most the rank of
<cite>params</cite>:</p>
<blockquote>
<div><p>indices.shape[-1] &lt;= params.rank</p>
</div></blockquote>
<p>The last dimension of <cite>indices</cite> corresponds to elements
(if <cite>indices.shape[-1] == params.rank</cite>) or slices
(if <cite>indices.shape[-1] &lt; params.rank</cite>) along dimension <cite>indices.shape[-1]</cite>
of <cite>params</cite>.  The output tensor has shape</p>
<blockquote>
<div><p>indices.shape[:-1] + params.shape[indices.shape[-1]:]</p>
</div></blockquote>
<p>Additionally both 'params' and 'indices' can have M leading batch
dimensions that exactly match. In this case 'batch_dims' must be M.</p>
<p>Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, a 0 is stored in the
corresponding output value.</p>
<p>Some examples below.</p>
<p>Simple indexing into a matrix:</p>
<dl class="simple">
<dt><a href="#id632"><span class="problematic" id="id633">``</span></a><a href="#id634"><span class="problematic" id="id635">`</span></a>python</dt><dd><p>indices = [[0, 0], [1, 1]]
params = [['a', 'b'], ['c', 'd']]
output = ['a', 'd']</p>
</dd>
</dl>
<p><a href="#id636"><span class="problematic" id="id637">``</span></a><a href="#id638"><span class="problematic" id="id639">`</span></a></p>
<p>Slice indexing into a matrix:</p>
<dl class="simple">
<dt><a href="#id640"><span class="problematic" id="id641">``</span></a><a href="#id642"><span class="problematic" id="id643">`</span></a>python</dt><dd><p>indices = [[1], [0]]
params = [['a', 'b'], ['c', 'd']]
output = [['c', 'd'], ['a', 'b']]</p>
</dd>
</dl>
<p><a href="#id644"><span class="problematic" id="id645">``</span></a><a href="#id646"><span class="problematic" id="id647">`</span></a></p>
<p>Indexing into a 3-tensor:</p>
<dl>
<dt><a href="#id648"><span class="problematic" id="id649">``</span></a><a href="#id650"><span class="problematic" id="id651">`</span></a>python</dt><dd><p>indices = [[1]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<p>output = [[['a1', 'b1'], ['c1', 'd1']]]</p>
<p>indices = [[0, 1], [1, 0]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<p>output = [['c0', 'd0'], ['a1', 'b1']]</p>
<p>indices = [[0, 0, 1], [1, 0, 1]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<p>output = ['b0', 'b1']</p>
</dd>
</dl>
<p><a href="#id652"><span class="problematic" id="id653">``</span></a><a href="#id654"><span class="problematic" id="id655">`</span></a></p>
<p>The examples below are for the case when only indices have leading extra
dimensions. If both 'params' and 'indices' have leading batch dimensions, use
the 'batch_dims' parameter to run gather_nd in batch mode.</p>
<p>Batched indexing into a matrix:</p>
<dl class="simple">
<dt><a href="#id656"><span class="problematic" id="id657">``</span></a><a href="#id658"><span class="problematic" id="id659">`</span></a>python</dt><dd><p>indices = [[[0, 0]], [[0, 1]]]
params = [['a', 'b'], ['c', 'd']]
output = [['a'], ['b']]</p>
</dd>
</dl>
<p><a href="#id660"><span class="problematic" id="id661">``</span></a><a href="#id662"><span class="problematic" id="id663">`</span></a></p>
<p>Batched slice indexing into a matrix:</p>
<dl class="simple">
<dt><a href="#id664"><span class="problematic" id="id665">``</span></a><a href="#id666"><span class="problematic" id="id667">`</span></a>python</dt><dd><p>indices = [[[1]], [[0]]]
params = [['a', 'b'], ['c', 'd']]
output = [[['c', 'd']], [['a', 'b']]]</p>
</dd>
</dl>
<p><a href="#id668"><span class="problematic" id="id669">``</span></a><a href="#id670"><span class="problematic" id="id671">`</span></a></p>
<p>Batched indexing into a 3-tensor:</p>
<dl>
<dt><a href="#id672"><span class="problematic" id="id673">``</span></a><a href="#id674"><span class="problematic" id="id675">`</span></a>python</dt><dd><p>indices = [[[1]], [[0]]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<dl class="simple">
<dt>output = [[[['a1', 'b1'], ['c1', 'd1']]],</dt><dd><p>[[['a0', 'b0'], ['c0', 'd0']]]]</p>
</dd>
</dl>
<p>indices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<dl class="simple">
<dt>output = [[['c0', 'd0'], ['a1', 'b1']],</dt><dd><p>[['a0', 'b0'], ['c1', 'd1']]]</p>
</dd>
</dl>
<p>indices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<p>output = [['b0', 'b1'], ['d0', 'c1']]</p>
</dd>
</dl>
<p><a href="#id676"><span class="problematic" id="id677">``</span></a><a href="#id678"><span class="problematic" id="id679">`</span></a></p>
<p>Examples with batched 'params' and 'indices':</p>
<dl>
<dt><a href="#id680"><span class="problematic" id="id681">``</span></a><a href="#id682"><span class="problematic" id="id683">`</span></a>python</dt><dd><p>batch_dims = 1
indices = [[1], [0]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<p>output = [['c0', 'd0'], ['a1', 'b1']]</p>
<p>batch_dims = 1
indices = [[[1]], [[0]]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<p>output = [[['c0', 'd0']], [['a1', 'b1']]]</p>
<p>batch_dims = 1
indices = [[[1, 0]], [[0, 1]]]
params = [[['a0', 'b0'], ['c0', 'd0']],</p>
<blockquote>
<div><p>[['a1', 'b1'], ['c1', 'd1']]]</p>
</div></blockquote>
<p>output = [['c0'], ['b1']]</p>
</dd>
</dl>
<p><a href="#id684"><span class="problematic" id="id685">``</span></a><a href="#id686"><span class="problematic" id="id687">`</span></a></p>
<p>See also <cite>tf.gather</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> -- A <cite>Tensor</cite>. The tensor from which to gather values.</p></li>
<li><p><strong>indices</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Index tensor.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
<li><p><strong>batch_dims</strong> -- An integer or a scalar 'Tensor'. The number of batch dimensions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>params</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.get_logger">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">get_logger</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/platform/tf_logging.html#get_logger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.get_logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Return TF logger instance.</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.get_static_value">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">get_static_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">partial</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.get_static_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the constant value of the given tensor, if efficiently calculable.</p>
<p>This function attempts to partially evaluate the given tensor, and
returns its value as a numpy ndarray if this succeeds.</p>
<p>Compatibility(V1): If <cite>constant_value(tensor)</cite> returns a non-<cite>None</cite> result, it
will no longer be possible to feed a different value for <cite>tensor</cite>. This allows
the result of this function to influence the graph that is constructed, and
permits static shape optimizations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- The Tensor to be evaluated.</p></li>
<li><p><strong>partial</strong> -- If True, the returned numpy array is allowed to have partially
evaluated values. Values that can't be evaluated will be None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy ndarray containing the constant value of the given <cite>tensor</cite>,
or None if it cannot be calculated.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if tensor is not an ops.Tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.grad_pass_through">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">grad_pass_through</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/custom_gradient.html#grad_pass_through"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.grad_pass_through" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a grad-pass-through op with the forward behavior provided in f.</p>
<p>Use this function to wrap any op, maintaining its behavior in the forward
pass, but replacing the original op in the backward graph with an identity.
For example:</p>
<p><a href="#id688"><span class="problematic" id="id689">``</span></a><a href="#id690"><span class="problematic" id="id691">`</span></a>python
x = tf.Variable(1.0, name=&quot;x&quot;)
z = tf.Variable(3.0, name=&quot;z&quot;)</p>
<dl class="simple">
<dt>with tf.GradientTape() as tape:</dt><dd><p># y will evaluate to 9.0
y = tf.grad_pass_through(x.assign)(z**2)</p>
</dd>
</dl>
<p># grads will evaluate to 6.0
grads = tape.gradient(y, z)
<a href="#id692"><span class="problematic" id="id693">``</span></a><a href="#id694"><span class="problematic" id="id695">`</span></a></p>
<p>Another example is a 'differentiable' moving average approximation, where
gradients are allowed to flow into the last value fed to the moving average,
but the moving average is still used for the forward pass:</p>
<p><a href="#id696"><span class="problematic" id="id697">``</span></a><a href="#id698"><span class="problematic" id="id699">`</span></a>python
x = ... # Some scalar value
# A moving average object, we don't need to know how this is implemented
moving_average = MovingAverage()
with backprop.GradientTape() as tape:</p>
<blockquote>
<div><p># mavg_x will evaluate to the current running average value
mavg_x = tf.grad_pass_through(moving_average)(x)</p>
</div></blockquote>
<p>grads = tape.gradient(mavg_x, x) # grads will evaluate to 1.0
<a href="#id700"><span class="problematic" id="id701">``</span></a><a href="#id702"><span class="problematic" id="id703">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>f</strong> -- function <cite>f(*x)</cite> that returns a <cite>Tensor</cite> or nested structure of <cite>Tensor</cite>
outputs.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function <cite>h(x)</cite> which returns the same values as <cite>f(x)</cite> and whose
gradients are the same as those of an identity function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.gradients">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">gradients</code><span class="sig-paren">(</span><em class="sig-param">ys</em>, <em class="sig-param">xs</em>, <em class="sig-param">grad_ys=None</em>, <em class="sig-param">name='gradients'</em>, <em class="sig-param">gate_gradients=False</em>, <em class="sig-param">aggregation_method=None</em>, <em class="sig-param">stop_gradients=None</em>, <em class="sig-param">unconnected_gradients=&lt;UnconnectedGradients.NONE: 'none'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gradients_impl.html#gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs symbolic derivatives of sum of <cite>ys</cite> w.r.t. x in <cite>xs</cite>.</p>
<p><cite>ys</cite> and <cite>xs</cite> are each a <cite>Tensor</cite> or a list of tensors.  <cite>grad_ys</cite>
is a list of <cite>Tensor</cite>, holding the gradients received by the
<cite>ys</cite>. The list must be the same length as <cite>ys</cite>.</p>
<p><cite>gradients()</cite> adds ops to the graph to output the derivatives of <cite>ys</cite> with
respect to <cite>xs</cite>.  It returns a list of <cite>Tensor</cite> of length <cite>len(xs)</cite> where
each tensor is the <cite>sum(dy/dx)</cite> for y in <cite>ys</cite> and for x in <cite>xs</cite>.</p>
<p><cite>grad_ys</cite> is a list of tensors of the same length as <cite>ys</cite> that holds
the initial gradients for each y in <cite>ys</cite>.  When <cite>grad_ys</cite> is None,
we fill in a tensor of '1's of the shape of y for each y in <cite>ys</cite>.  A
user can provide their own initial <cite>grad_ys</cite> to compute the
derivatives using a different initial gradient for each y (e.g., if
one wanted to weight the gradient differently for each value in
each y).</p>
<p><cite>stop_gradients</cite> is a <cite>Tensor</cite> or a list of tensors to be considered constant
with respect to all <cite>xs</cite>. These tensors will not be backpropagated through,
as though they had been explicitly disconnected using <cite>stop_gradient</cite>.  Among
other things, this allows computation of partial derivatives as opposed to
total derivatives. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">a</span> <span class="pre">=</span> <span class="pre">tf.constant(0.)</span>
<span class="pre">b</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">a</span>
<span class="pre">g</span> <span class="pre">=</span> <span class="pre">tf.gradients(a</span> <span class="pre">+</span> <span class="pre">b,</span> <span class="pre">[a,</span> <span class="pre">b],</span> <span class="pre">stop_gradients=[a,</span> <span class="pre">b])</span>
<span class="pre">`</span></code></p>
<p>Here the partial derivatives <cite>g</cite> evaluate to <cite>[1.0, 1.0]</cite>, compared to the
total derivatives <cite>tf.gradients(a + b, [a, b])</cite>, which take into account the
influence of <cite>a</cite> on <cite>b</cite> and evaluate to <cite>[3.0, 1.0]</cite>.  Note that the above is
equivalent to:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">a</span> <span class="pre">=</span> <span class="pre">tf.stop_gradient(tf.constant(0.))</span>
<span class="pre">b</span> <span class="pre">=</span> <span class="pre">tf.stop_gradient(2</span> <span class="pre">*</span> <span class="pre">a)</span>
<span class="pre">g</span> <span class="pre">=</span> <span class="pre">tf.gradients(a</span> <span class="pre">+</span> <span class="pre">b,</span> <span class="pre">[a,</span> <span class="pre">b])</span>
<span class="pre">`</span></code></p>
<p><cite>stop_gradients</cite> provides a way of stopping gradient after the graph has
already been constructed, as compared to <cite>tf.stop_gradient</cite> which is used
during graph construction.  When the two approaches are combined,
backpropagation stops at both <cite>tf.stop_gradient</cite> nodes and nodes in
<cite>stop_gradients</cite>, whichever is encountered first.</p>
<p>All integer tensors are considered constant with respect to all <cite>xs</cite>, as if
they were included in <cite>stop_gradients</cite>.</p>
<p><cite>unconnected_gradients</cite> determines the value returned for each x in xs if it
is unconnected in the graph to ys. By default this is None to safeguard
against errors. Mathematically these gradients are zero which can be requested
using the <cite>'zero'</cite> option. <cite>tf.UnconnectedGradients</cite> provides the
following options and behaviors:</p>
<p><a href="#id704"><span class="problematic" id="id705">``</span></a><a href="#id706"><span class="problematic" id="id707">`</span></a>python
a = tf.ones([1, 2])
b = tf.ones([3, 1])
g1 = tf.gradients([b], [a], unconnected_gradients='none')
sess.run(g1)  # [None]</p>
<p>g2 = tf.gradients([b], [a], unconnected_gradients='zero')
sess.run(g2)  # [array([[0., 0.]], dtype=float32)]
<a href="#id708"><span class="problematic" id="id709">``</span></a><a href="#id710"><span class="problematic" id="id711">`</span></a></p>
<p>Let us take one practical example which comes during the back propogation
phase. This function is used to evaluate the derivatives of the cost function
with respect to Weights <cite>Ws</cite> and Biases <cite>bs</cite>. Below sample implementation
provides the exaplantion of what it is actually used for :</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">Ws</span> <span class="pre">=</span> <span class="pre">tf.constant(0.)</span>
<span class="pre">bs</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">Ws</span>
<span class="pre">cost</span> <span class="pre">=</span> <span class="pre">Ws</span> <span class="pre">+</span> <span class="pre">bs</span>&#160; <span class="pre">#</span> <span class="pre">This</span> <span class="pre">is</span> <span class="pre">just</span> <span class="pre">an</span> <span class="pre">example.</span> <span class="pre">So,</span> <span class="pre">please</span> <span class="pre">ignore</span> <span class="pre">the</span> <span class="pre">formulas.</span>
<span class="pre">g</span> <span class="pre">=</span> <span class="pre">tf.gradients(cost,</span> <span class="pre">[Ws,</span> <span class="pre">bs])</span>
<span class="pre">dCost_dW,</span> <span class="pre">dCost_db</span> <span class="pre">=</span> <span class="pre">g</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ys</strong> -- A <cite>Tensor</cite> or list of tensors to be differentiated.</p></li>
<li><p><strong>xs</strong> -- A <cite>Tensor</cite> or list of tensors to be used for differentiation.</p></li>
<li><p><strong>grad_ys</strong> -- Optional. A <cite>Tensor</cite> or list of tensors the same size as
<cite>ys</cite> and holding the gradients computed for each y in <cite>ys</cite>.</p></li>
<li><p><strong>name</strong> -- Optional name to use for grouping all the gradient ops together.
defaults to 'gradients'.</p></li>
<li><p><strong>gate_gradients</strong> -- If True, add a tuple around the gradients returned
for an operations.  This avoids some race conditions.</p></li>
<li><p><strong>aggregation_method</strong> -- Specifies the method used to combine gradient terms.
Accepted values are constants defined in the class <cite>AggregationMethod</cite>.</p></li>
<li><p><strong>stop_gradients</strong> -- Optional. A <cite>Tensor</cite> or list of tensors not to differentiate
through.</p></li>
<li><p><strong>unconnected_gradients</strong> -- Optional. Specifies the gradient value returned when
the given input tensors are unconnected. Accepted values are constants
defined in the class <cite>tf.UnconnectedGradients</cite> and the default value is
<cite>none</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <cite>Tensor</cite> of length <cite>len(xs)</cite> where each tensor is the <cite>sum(dy/dx)</cite>
for y in <cite>ys</cite> and for x in <cite>xs</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>LookupError</strong> -- if one of the operations between <cite>x</cite> and <cite>y</cite> does not
    have a registered gradient function.</p></li>
<li><p><strong>ValueError</strong> -- if the arguments are invalid.</p></li>
<li><p><strong>RuntimeError</strong> -- if called in Eager mode.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.greater">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">greater</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#greater"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.greater" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of (x &gt; y) element-wise.</p>
<p><em>NOTE</em>: <cite>math.greater</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<p>Example:</p>
<p><a href="#id712"><span class="problematic" id="id713">``</span></a><a href="#id714"><span class="problematic" id="id715">`</span></a>python
x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==&gt; [False, True, True]</p>
<p>x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==&gt; [False, False, True]
<a href="#id716"><span class="problematic" id="id717">``</span></a><a href="#id718"><span class="problematic" id="id719">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>int8</cite>, <cite>int64</cite>, <cite>bfloat16</cite>, <cite>uint16</cite>, <cite>half</cite>, <cite>uint32</cite>, <cite>uint64</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>bool</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.greater_equal">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">greater_equal</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#greater_equal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.greater_equal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of (x &gt;= y) element-wise.</p>
<p><em>NOTE</em>: <cite>math.greater_equal</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<p>Example:</p>
<p><a href="#id720"><span class="problematic" id="id721">``</span></a><a href="#id722"><span class="problematic" id="id723">`</span></a>python
x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==&gt; [True, True, True, False]</p>
<p>x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==&gt; [True, False, True, True]
<a href="#id724"><span class="problematic" id="id725">``</span></a><a href="#id726"><span class="problematic" id="id727">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>int8</cite>, <cite>int64</cite>, <cite>bfloat16</cite>, <cite>uint16</cite>, <cite>half</cite>, <cite>uint32</cite>, <cite>uint64</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>bool</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.group">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">group</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/control_flow_ops.html#group"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.group" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an op that groups multiple operations.</p>
<p>When this op finishes, all ops in <cite>inputs</cite> have finished. This op has no
output.</p>
<p>See also <cite>tf.tuple</cite> and
<cite>tf.control_dependencies</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*inputs</strong> -- Zero or more tensors to group.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An Operation that executes all its inputs.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If an unknown keyword argument is provided.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.guarantee_const">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">guarantee_const</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#guarantee_const"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.guarantee_const" title="Permalink to this definition">¶</a></dt>
<dd><p>Gives a guarantee to the TF runtime that the input tensor is a constant.</p>
<p>The runtime is then free to make optimizations based on this.</p>
<p>Only accepts value typed tensors as inputs and rejects resource variable handles
as input.</p>
<p>Returns the input tensor without modification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.hessians">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">hessians</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ys</span></em>, <em class="sig-param"><span class="n">xs</span></em>, <em class="sig-param"><span class="n">gate_gradients</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">aggregation_method</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'hessians'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gradients_impl.html#hessians"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.hessians" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs the Hessian of sum of <cite>ys</cite> with respect to <cite>x</cite> in <cite>xs</cite>.</p>
<p><cite>hessians()</cite> adds ops to the graph to output the Hessian matrix of <cite>ys</cite>
with respect to <cite>xs</cite>.  It returns a list of <cite>Tensor</cite> of length <cite>len(xs)</cite>
where each tensor is the Hessian of <cite>sum(ys)</cite>.</p>
<p>The Hessian is a matrix of second-order partial derivatives of a scalar
tensor (see <a class="reference external" href="https://en.wikipedia.org/wiki/Hessian_matrix">https://en.wikipedia.org/wiki/Hessian_matrix</a> for more details).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ys</strong> -- A <cite>Tensor</cite> or list of tensors to be differentiated.</p></li>
<li><p><strong>xs</strong> -- A <cite>Tensor</cite> or list of tensors to be used for differentiation.</p></li>
<li><p><strong>name</strong> -- Optional name to use for grouping all the gradient ops together.
defaults to 'hessians'.</p></li>
<li><p><strong>colocate_gradients_with_ops</strong> -- See <cite>gradients()</cite> documentation for details.</p></li>
<li><p><strong>gate_gradients</strong> -- See <cite>gradients()</cite> documentation for details.</p></li>
<li><p><strong>aggregation_method</strong> -- See <cite>gradients()</cite> documentation for details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of Hessian matrices of <cite>sum(ys)</cite> for each <cite>x</cite> in <cite>xs</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>LookupError</strong> -- if one of the operations between <cite>xs</cite> and <cite>ys</cite> does not
    have a registered gradient function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.histogram_fixed_width">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">histogram_fixed_width</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">value_range</span></em>, <em class="sig-param"><span class="n">nbins</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/histogram_ops.html#histogram_fixed_width"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.histogram_fixed_width" title="Permalink to this definition">¶</a></dt>
<dd><p>Return histogram of values.</p>
<p>Given the tensor <cite>values</cite>, this operation returns a rank 1 histogram counting
the number of entries in <cite>values</cite> that fell into every bin.  The bins are
equal width and determined by the arguments <cite>value_range</cite> and <cite>nbins</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- Numeric <cite>Tensor</cite>.</p></li>
<li><p><strong>value_range</strong> -- Shape [2] <cite>Tensor</cite> of same <cite>dtype</cite> as <cite>values</cite>.
values &lt;= value_range[0] will be mapped to hist[0],
values &gt;= value_range[1] will be mapped to hist[-1].</p></li>
<li><p><strong>nbins</strong> -- Scalar <cite>int32 Tensor</cite>.  Number of histogram bins.</p></li>
<li><p><strong>dtype</strong> -- dtype for returned histogram.</p></li>
<li><p><strong>name</strong> -- A name for this operation (defaults to 'histogram_fixed_width').</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 1-D <cite>Tensor</cite> holding histogram of values.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If any unsupported dtype is provided.</p></li>
<li><p><strong>tf.errors.InvalidArgumentError</strong> -- If value_range does not
    satisfy value_range[0] &lt; value_range[1].</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id728"><span class="problematic" id="id729">``</span></a><a href="#id730"><span class="problematic" id="id731">`</span></a>python
# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]</p>
<dl class="simple">
<dt>with tf.compat.v1.get_default_session() as sess:</dt><dd><p>hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
variables.global_variables_initializer().run()
sess.run(hist) =&gt; [2, 1, 1, 0, 2]</p>
</dd>
</dl>
<p><a href="#id732"><span class="problematic" id="id733">``</span></a><a href="#id734"><span class="problematic" id="id735">`</span></a></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.histogram_fixed_width_bins">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">histogram_fixed_width_bins</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">value_range</span></em>, <em class="sig-param"><span class="n">nbins</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/histogram_ops.html#histogram_fixed_width_bins"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.histogram_fixed_width_bins" title="Permalink to this definition">¶</a></dt>
<dd><p>Bins the given values for use in a histogram.</p>
<p>Given the tensor <cite>values</cite>, this operation returns a rank 1 <cite>Tensor</cite>
representing the indices of a histogram into which each element
of <cite>values</cite> would be binned. The bins are equal width and
determined by the arguments <cite>value_range</cite> and <cite>nbins</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- Numeric <cite>Tensor</cite>.</p></li>
<li><p><strong>value_range</strong> -- Shape [2] <cite>Tensor</cite> of same <cite>dtype</cite> as <cite>values</cite>.
values &lt;= value_range[0] will be mapped to hist[0],
values &gt;= value_range[1] will be mapped to hist[-1].</p></li>
<li><p><strong>nbins</strong> -- Scalar <cite>int32 Tensor</cite>.  Number of histogram bins.</p></li>
<li><p><strong>dtype</strong> -- dtype for returned histogram.</p></li>
<li><p><strong>name</strong> -- A name for this operation (defaults to 'histogram_fixed_width').</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> holding the indices of the binned values whose shape matches
<cite>values</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If any unsupported dtype is provided.</p></li>
<li><p><strong>tf.errors.InvalidArgumentError</strong> -- If value_range does not
    satisfy value_range[0] &lt; value_range[1].</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id736"><span class="problematic" id="id737">``</span></a><a href="#id738"><span class="problematic" id="id739">`</span></a>python
# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]</p>
<dl class="simple">
<dt>with tf.compat.v1.get_default_session() as sess:</dt><dd><p>indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=5)
variables.global_variables_initializer().run()
sess.run(indices) # [0, 0, 1, 2, 4, 4]</p>
</dd>
</dl>
<p><a href="#id740"><span class="problematic" id="id741">``</span></a><a href="#id742"><span class="problematic" id="id743">`</span></a></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.identity">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">identity</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#identity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.identity" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a Tensor with the same shape and contents as input.</p>
<p>The return value is not the same Tensor as the original, but contains the same
values.  This operation is fast when used on the same device.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.78</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_identity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([0.78], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_identity</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([0.78], dtype=float32)</span>
</pre></div>
</div>
<p>Calling <cite>tf.identity</cite> on a variable will make a Tensor that represents the
value of that variable at the time it is called. This is equivalent to calling
<cite>&lt;variable&gt;.read_value()</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_identity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Variable ... shape=() dtype=int32, numpy=6&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_identity</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">5</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.identity_n">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">identity_n</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#identity_n"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.identity_n" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of tensors with the same shapes and contents as the input</p>
<p>tensors.</p>
<p>This op can be used to override the gradient for complicated functions. For
example, suppose y = f(x) and we wish to apply a custom function g for backprop
such that dx = g(dy). In Python,</p>
<p><a href="#id744"><span class="problematic" id="id745">``</span></a><a href="#id746"><span class="problematic" id="id747">`</span></a>python
with tf.get_default_graph().gradient_override_map(</p>
<blockquote>
<div><blockquote>
<div><p>{'IdentityN': 'OverrideGradientWithG'}):</p>
</div></blockquote>
<p>y, _ = identity_n([f(x), x])</p>
</div></blockquote>
<p>&#64;tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):</p>
<blockquote>
<div><p>return [None, g(dy)]  # Do not backprop to f(x).</p>
</div></blockquote>
<p><a href="#id748"><span class="problematic" id="id749">``</span></a><a href="#id750"><span class="problematic" id="id751">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A list of <cite>Tensor</cite> objects.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <cite>Tensor</cite> objects. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.import_graph_def">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">import_graph_def</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph_def</span></em>, <em class="sig-param"><span class="n">input_map</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_elements</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">op_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">producer_op_list</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/importer.html#import_graph_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.import_graph_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Imports the graph from <cite>graph_def</cite> into the current default <cite>Graph</cite>. (deprecated arguments)</p>
<p>Warning: SOME ARGUMENTS ARE DEPRECATED: <cite>(op_dict)</cite>. They will be removed in a future version.
Instructions for updating:
Please file an issue at <a class="reference external" href="https://github.com/tensorflow/tensorflow/issues">https://github.com/tensorflow/tensorflow/issues</a> if you depend on this feature.</p>
<p>This function provides a way to import a serialized TensorFlow
[<cite>GraphDef</cite>](<a class="reference external" href="https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto">https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto</a>)
protocol buffer, and extract individual objects in the <cite>GraphDef</cite> as
<cite>tf.Tensor</cite> and <cite>tf.Operation</cite> objects. Once extracted,
these objects are placed into the current default <cite>Graph</cite>. See
<cite>tf.Graph.as_graph_def</cite> for a way to create a <cite>GraphDef</cite>
proto.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph_def</strong> -- A <cite>GraphDef</cite> proto containing operations to be imported into
the default graph.</p></li>
<li><p><strong>input_map</strong> -- A dictionary mapping input names (as strings) in <cite>graph_def</cite>
to <cite>Tensor</cite> objects. The values of the named input tensors in the
imported graph will be re-mapped to the respective <cite>Tensor</cite> values.</p></li>
<li><p><strong>return_elements</strong> -- A list of strings containing operation names in
<cite>graph_def</cite> that will be returned as <cite>Operation</cite> objects; and/or
tensor names in <cite>graph_def</cite> that will be returned as <cite>Tensor</cite> objects.</p></li>
<li><p><strong>name</strong> -- (Optional.) A prefix that will be prepended to the names in
<cite>graph_def</cite>. Note that this does not apply to imported function names.
Defaults to <cite>&quot;import&quot;</cite>.</p></li>
<li><p><strong>op_dict</strong> -- (Optional.) Deprecated, do not use.</p></li>
<li><p><strong>producer_op_list</strong> -- (Optional.) An <cite>OpList</cite> proto with the (possibly stripped)
list of <cite>OpDef`s used by the producer of the graph. If provided,
unrecognized attrs for ops in `graph_def</cite> that have their default value
according to <cite>producer_op_list</cite> will be removed. This will allow some more
<a href="#id752"><span class="problematic" id="id753">`</span></a>GraphDef`s produced by later binaries to be accepted by earlier binaries.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <cite>Operation</cite> and/or <cite>Tensor</cite> objects from the imported graph,
corresponding to the names in <cite>return_elements</cite>,
and None if <cite>returns_elements</cite> is None.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If <cite>graph_def</cite> is not a <cite>GraphDef</cite> proto,
    <cite>input_map</cite> is not a dictionary mapping strings to <cite>Tensor</cite> objects,
    or <cite>return_elements</cite> is not a list of strings.</p></li>
<li><p><strong>ValueError</strong> -- If <cite>input_map</cite>, or <cite>return_elements</cite> contains names that
    do not appear in <cite>graph_def</cite>, or <cite>graph_def</cite> is not well-formed (e.g.
    it refers to an unknown tensor).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.init_scope">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">init_scope</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#init_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.init_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>A context manager that lifts ops out of control-flow scopes and function-building graphs.</p>
<p>There is often a need to lift variable initialization ops out of control-flow
scopes, function-building graphs, and gradient tapes. Entering an
<cite>init_scope</cite> is a mechanism for satisfying these desiderata. In particular,
entering an <cite>init_scope</cite> has three effects:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>All control dependencies are cleared the moment the scope is entered;
this is equivalent to entering the context manager returned from
<cite>control_dependencies(None)</cite>, which has the side-effect of exiting
control-flow scopes like <cite>tf.cond</cite> and <cite>tf.while_loop</cite>.</p></li>
<li><p>All operations that are created while the scope is active are lifted
into the lowest context on the <cite>context_stack</cite> that is not building a
graph function. Here, a context is defined as either a graph or an eager
context. Every context switch, i.e., every installation of a graph as
the default graph and every switch into eager mode, is logged in a
thread-local stack called <cite>context_switches</cite>; the log entry for a
context switch is popped from the stack when the context is exited.
Entering an <cite>init_scope</cite> is equivalent to crawling up
<cite>context_switches</cite>, finding the first context that is not building a
graph function, and entering it. A caveat is that if graph mode is
enabled but the default graph stack is empty, then entering an
<cite>init_scope</cite> will simply install a fresh graph as the default one.</p></li>
<li><p>The gradient tape is paused while the scope is active.</p></li>
</ol>
</div></blockquote>
<p>When eager execution is enabled, code inside an init_scope block runs with
eager execution enabled even when tracing a <cite>tf.function</cite>. For example:</p>
<p><a href="#id754"><span class="problematic" id="id755">``</span></a><a href="#id756"><span class="problematic" id="id757">`</span></a>python
tf.compat.v1.enable_eager_execution()</p>
<p>&#64;tf.function
def func():</p>
<blockquote>
<div><p># A function constructs TensorFlow graphs,
# it does not execute eagerly.
assert not tf.executing_eagerly()
with tf.init_scope():</p>
<blockquote>
<div><p># Initialization runs with eager execution enabled
assert tf.executing_eagerly()</p>
</div></blockquote>
</div></blockquote>
<p><a href="#id758"><span class="problematic" id="id759">``</span></a><a href="#id760"><span class="problematic" id="id761">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> -- if graph state is incompatible with this initialization.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.is_tensor">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">is_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_util.html#is_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.is_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether <cite>x</cite> is a tensor or &quot;tensor-like&quot;.</p>
<p>If <cite>is_tensor(x)</cite> returns <cite>True</cite>, it is safe to assume that <cite>x</cite> is a tensor or
can be converted to a tensor using <cite>ops.convert_to_tensor(x)</cite>.</p>
<p>Usage example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]]))</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="s2">&quot;Hello World&quot;</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> -- A python object to check.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>True</cite> if <cite>x</cite> is a tensor or &quot;tensor-like&quot;, <cite>False</cite> if not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.less">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">less</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#less"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.less" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of (x &lt; y) element-wise.</p>
<p><em>NOTE</em>: <cite>math.less</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<p>Example:</p>
<p><a href="#id762"><span class="problematic" id="id763">``</span></a><a href="#id764"><span class="problematic" id="id765">`</span></a>python
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==&gt; [False, True, False]</p>
<p>x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==&gt; [False, True, True]
<a href="#id766"><span class="problematic" id="id767">``</span></a><a href="#id768"><span class="problematic" id="id769">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>int8</cite>, <cite>int64</cite>, <cite>bfloat16</cite>, <cite>uint16</cite>, <cite>half</cite>, <cite>uint32</cite>, <cite>uint64</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>bool</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.less_equal">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">less_equal</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#less_equal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.less_equal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of (x &lt;= y) element-wise.</p>
<p><em>NOTE</em>: <cite>math.less_equal</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<p>Example:</p>
<p><a href="#id770"><span class="problematic" id="id771">``</span></a><a href="#id772"><span class="problematic" id="id773">`</span></a>python
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==&gt; [True, True, False]</p>
<p>x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==&gt; [True, True, True]
<a href="#id774"><span class="problematic" id="id775">``</span></a><a href="#id776"><span class="problematic" id="id777">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>int8</cite>, <cite>int64</cite>, <cite>bfloat16</cite>, <cite>uint16</cite>, <cite>half</cite>, <cite>uint32</cite>, <cite>uint64</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>bool</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.linspace">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">linspace</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start</span></em>, <em class="sig-param"><span class="n">stop</span></em>, <em class="sig-param"><span class="n">num</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.linspace" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates values in an interval.</p>
<p>A sequence of <cite>num</cite> evenly-spaced values are generated beginning at <cite>start</cite>.
If <cite>num &gt; 1</cite>, the values in the sequence increase by <cite>stop - start / num - 1</cite>,
so that the last one is exactly <cite>stop</cite>.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">tf.linspace(10.0,</span> <span class="pre">12.0,</span> <span class="pre">3,</span> <span class="pre">name=&quot;linspace&quot;)</span> <span class="pre">=&gt;</span> <span class="pre">[</span> <span class="pre">10.0</span>&#160; <span class="pre">11.0</span>&#160; <span class="pre">12.0]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>.
0-D tensor. First entry in the range.</p></li>
<li><p><strong>stop</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>start</cite>.
0-D tensor. Last entry in the range.</p></li>
<li><p><strong>num</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
0-D tensor. Number of values to generate.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>start</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.load_library">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">load_library</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">library_location</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/load_library.html#load_library"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.load_library" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a TensorFlow plugin.</p>
<p>&quot;library_location&quot; can be a path to a specific shared object, or a folder.
If it is a folder, all shared objects that are named &quot;libtfkernel*&quot; will be
loaded. When the library is loaded, kernels registered in the library via the
<cite>REGISTER_*</cite> macros are made available in the TensorFlow process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>library_location</strong> -- Path to the plugin or the folder of plugins.
Relative or absolute filesystem path to a dynamic library file or folder.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>OSError</strong> -- When the file to be loaded is not found.</p></li>
<li><p><strong>RuntimeError</strong> -- when unable to load the library.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.load_op_library">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">load_op_library</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">library_filename</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/load_library.html#load_op_library"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.load_op_library" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a TensorFlow plugin, containing custom ops and kernels.</p>
<p>Pass &quot;library_filename&quot; to a platform-specific mechanism for dynamically
loading a library. The rules for determining the exact location of the
library are platform-specific and are not documented here. When the
library is loaded, ops and kernels registered in the library via the
<cite>REGISTER_*</cite> macros are made available in the TensorFlow process. Note
that ops with the same name as an existing op are rejected and not
registered with the process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>library_filename</strong> -- Path to the plugin.
Relative or absolute filesystem path to a dynamic library file.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A python module containing the Python wrappers for Ops defined in
the plugin.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> -- when unable to load the library or get the python wrappers.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.logical_and">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">logical_and</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#logical_and"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.logical_and" title="Permalink to this definition">¶</a></dt>
<dd><p>Logical AND function.</p>
<p>The operation works for the following input types:</p>
<ul class="simple">
<li><p>Two single elements of type <cite>bool</cite></p></li>
<li><p>One <cite>tf.Tensor</cite> of type <cite>bool</cite> and one single <cite>bool</cite>, where the result will
be calculated by applying logical AND with the single element to each
element in the larger Tensor.</p></li>
<li><p>Two <cite>tf.Tensor</cite> objects of type <cite>bool</cite> of the same shape. In this case,
the result will be the element-wise logical AND of the two input tensors.</p></li>
</ul>
<p>Usage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>tf.Tensor</cite> type bool.</p></li>
<li><p><strong>y</strong> -- A <cite>tf.Tensor</cite> of type bool.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tf.Tensor</cite> of type bool with the same size as that of x or y.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.logical_not">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">logical_not</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#logical_not"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.logical_not" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of <cite>NOT x</cite> element-wise.</p>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]))</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite> of type <cite>bool</cite>. A <cite>Tensor</cite> of type <cite>bool</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>bool</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.logical_or">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">logical_or</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#logical_or"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.logical_or" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of x OR y element-wise.</p>
<p><em>NOTE</em>: <cite>math.logical_or</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite> of type <cite>bool</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite> of type <cite>bool</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>bool</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.make_ndarray">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">make_ndarray</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.make_ndarray" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a numpy ndarray from a tensor.</p>
<p>Create a numpy ndarray with the same shape and data as the tensor.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">Tensor</span> <span class="pre">a</span> <span class="pre">has</span> <span class="pre">shape</span> <span class="pre">(2,3)</span>
<span class="pre">a</span> <span class="pre">=</span> <span class="pre">tf.constant([[1,2,3],[4,5,6]])</span>
<span class="pre">proto_tensor</span> <span class="pre">=</span> <span class="pre">tf.make_tensor_proto(a)</span>&#160; <span class="pre">#</span> <span class="pre">convert</span> <span class="pre">`tensor</span> <span class="pre">a`</span> <span class="pre">to</span> <span class="pre">a</span> <span class="pre">proto</span> <span class="pre">tensor</span>
<span class="pre">tf.make_ndarray(proto_tensor)</span> <span class="pre">#</span> <span class="pre">output:</span> <span class="pre">array([[1,</span> <span class="pre">2,</span> <span class="pre">3],</span>
<span class="pre">#</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">6]],</span> <span class="pre">dtype=int32)</span>
<span class="pre">#</span> <span class="pre">output</span> <span class="pre">has</span> <span class="pre">shape</span> <span class="pre">(2,3)</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> -- A TensorProto.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with the tensor contents.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- if tensor has unsupported type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.make_tensor_proto">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">make_tensor_proto</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verify_shape</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">allow_broadcast</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_util.html#make_tensor_proto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.make_tensor_proto" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a TensorProto.</p>
<p>In TensorFlow 2.0, representing tensors as protos should no longer be a
common workflow. That said, this utility function is still useful for
generating TF Serving request protos:</p>
<dl class="simple">
<dt><a href="#id778"><span class="problematic" id="id779">``</span></a><a href="#id780"><span class="problematic" id="id781">`</span></a>python</dt><dd><p>request = tensorflow_serving.apis.predict_pb2.PredictRequest()
request.model_spec.name = &quot;my_model&quot;
request.model_spec.signature_name = &quot;serving_default&quot;
request.inputs[&quot;images&quot;].CopyFrom(tf.make_tensor_proto(X_new))</p>
</dd>
</dl>
<p><a href="#id782"><span class="problematic" id="id783">``</span></a><a href="#id784"><span class="problematic" id="id785">`</span></a></p>
<p><cite>make_tensor_proto</cite> accepts &quot;values&quot; of a python scalar, a python list, a
numpy ndarray, or a numpy scalar.</p>
<p>If &quot;values&quot; is a python scalar or a python list, make_tensor_proto
first convert it to numpy ndarray. If dtype is None, the
conversion tries its best to infer the right numpy data
type. Otherwise, the resulting numpy array has a compatible data
type with the given dtype.</p>
<p>In either case above, the numpy ndarray (either the caller provided
or the auto-converted) must have the compatible type with dtype.</p>
<p><cite>make_tensor_proto</cite> then converts the numpy array to a tensor proto.</p>
<p>If &quot;shape&quot; is None, the resulting tensor proto represents the numpy
array precisely.</p>
<p>Otherwise, &quot;shape&quot; specifies the tensor's shape and the numpy array
can not have more elements than what &quot;shape&quot; specifies.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- Values to put in the TensorProto.</p></li>
<li><p><strong>dtype</strong> -- Optional tensor_pb2 DataType value.</p></li>
<li><p><strong>shape</strong> -- List of integers representing the dimensions of tensor.</p></li>
<li><p><strong>verify_shape</strong> -- Boolean that enables verification of a shape of values.</p></li>
<li><p><strong>allow_broadcast</strong> -- Boolean that enables allowing scalars and 1 length vector
broadcasting. Cannot be true when verify_shape is true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <cite>TensorProto</cite>. Depending on the type, it may contain data in the
&quot;tensor_content&quot; attribute, which is not directly useful to Python programs.
To access the values you should convert the proto back to a numpy ndarray
with <cite>tf.make_ndarray(proto)</cite>.</p>
<p>If <cite>values</cite> is a <cite>TensorProto</cite>, it is immediately returned; <cite>dtype</cite> and
<cite>shape</cite> are ignored.</p>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- if unsupported types are provided.</p></li>
<li><p><strong>ValueError</strong> -- if arguments have inappropriate values or if verify_shape is
    True and shape of values is not equals to a shape from the argument.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.map_fn">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">map_fn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em>, <em class="sig-param"><span class="n">elems</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">parallel_iterations</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">back_prop</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">swap_memory</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">infer_shape</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/map_fn.html#map_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.map_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>map on the list of tensors unpacked from <cite>elems</cite> on dimension 0. (deprecated argument values)</p>
<p>Warning: SOME ARGUMENT VALUES ARE DEPRECATED: <cite>(back_prop=False)</cite>. They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.map_fn(fn, elems, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))</p>
<p>The simplest version of <cite>map_fn</cite> repeatedly applies the callable <cite>fn</cite> to a
sequence of elements from first to last. The elements are made of the
tensors unpacked from <cite>elems</cite>. <cite>dtype</cite> is the data type of the return
value of <cite>fn</cite>. Users must provide <cite>dtype</cite> if it is different from
the data type of <cite>elems</cite>.</p>
<p>Suppose that <cite>elems</cite> is unpacked into <cite>values</cite>, a list of tensors. The shape
of the result tensor is <cite>[values.shape[0]] + fn(values[0]).shape</cite>.</p>
<p>This method also allows multi-arity <cite>elems</cite> and output of <cite>fn</cite>.  If <cite>elems</cite>
is a (possibly nested) list or tuple of tensors, then each of these tensors
must have a matching first (unpack) dimension.  The signature of <cite>fn</cite> may
match the structure of <cite>elems</cite>.  That is, if <cite>elems</cite> is
<cite>(t1, [t2, t3, [t4, t5]])</cite>, then an appropriate signature for <cite>fn</cite> is:
<cite>fn = lambda (t1, [t2, t3, [t4, t5]]):</cite>.</p>
<p>Furthermore, <cite>fn</cite> may emit a different structure than its input.  For example,
<cite>fn</cite> may look like: <cite>fn = lambda t1: return (t1 + 1, t1 - 1)</cite>.  In this case,
the <cite>dtype</cite> parameter is not optional: <cite>dtype</cite> must be a type or (possibly
nested) tuple of types matching the output of <cite>fn</cite>.</p>
<p>To apply a functional operation to the nonzero elements of a SparseTensor
one of the following methods is recommended. First, if the function is
expressible as TensorFlow ops, use</p>
<dl class="simple">
<dt><a href="#id786"><span class="problematic" id="id787">``</span></a><a href="#id788"><span class="problematic" id="id789">`</span></a>python</dt><dd><p>result = SparseTensor(input.indices, fn(input.values), input.dense_shape)</p>
</dd>
</dl>
<p><a href="#id790"><span class="problematic" id="id791">``</span></a><a href="#id792"><span class="problematic" id="id793">`</span></a></p>
<p>If, however, the function is not expressible as a TensorFlow op, then use</p>
<p><a href="#id794"><span class="problematic" id="id795">``</span></a><a href="#id796"><span class="problematic" id="id797">`</span></a>python
result = SparseTensor(</p>
<blockquote>
<div><p>input.indices, map_fn(fn, input.values), input.dense_shape)</p>
</div></blockquote>
<p><a href="#id798"><span class="problematic" id="id799">``</span></a><a href="#id800"><span class="problematic" id="id801">`</span></a></p>
<p>instead.</p>
<p>When executing eagerly, map_fn does not execute in parallel even if
<cite>parallel_iterations</cite> is set to a value &gt; 1. You can still get the
performance benefits of running a function in parallel by using the
<cite>tf.function</cite> decorator,</p>
<p><a href="#id802"><span class="problematic" id="id803">``</span></a><a href="#id804"><span class="problematic" id="id805">`</span></a>python
# Assume the function being used in map_fn is fn.
# To ensure map_fn calls fn in parallel, use the tf.function decorator.
&#64;tf.function
def func(tensor):</p>
<blockquote>
<div><p>return tf.map_fn(fn, tensor)</p>
</div></blockquote>
<p><a href="#id806"><span class="problematic" id="id807">``</span></a><a href="#id808"><span class="problematic" id="id809">`</span></a></p>
<p>Note that if you use the <cite>tf.function</cite> decorator, any non-TensorFlow Python
code that you may have written in your function won't get executed. See
[<cite>tf.function</cite>](<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/function">https://www.tensorflow.org/api_docs/python/tf/function</a>) for
more  details. The recommendation would be to debug without <cite>tf.function</cite> but
switch to it to get performance benefits of running <cite>map_fn</cite> in parallel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- The callable to be performed.  It accepts one argument, which will have
the same (possibly nested) structure as <cite>elems</cite>.  Its output must have the
same structure as <cite>dtype</cite> if one is provided, otherwise it must have the
same structure as <cite>elems</cite>.</p></li>
<li><p><strong>elems</strong> -- A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension.  The nested sequence of the
resulting slices will be applied to <cite>fn</cite>.</p></li>
<li><p><strong>dtype</strong> -- (optional) The output type(s) of <cite>fn</cite>.  If <cite>fn</cite> returns a structure
of Tensors differing from the structure of <cite>elems</cite>, then <cite>dtype</cite> is not
optional and must have the same structure as the output of <cite>fn</cite>.</p></li>
<li><p><strong>parallel_iterations</strong> -- (optional) The number of iterations allowed to run in
parallel. When graph building, the default value is 10. While executing
eagerly, the default value is set to 1.</p></li>
<li><p><strong>back_prop</strong> -- (optional) Deprecated. False disables support for back
propagation. Prefer using <cite>tf.stop_gradient</cite> instead.</p></li>
<li><p><strong>swap_memory</strong> -- (optional) True enables GPU-CPU memory swapping.</p></li>
<li><p><strong>infer_shape</strong> -- (optional) False disables tests for consistent output shapes.</p></li>
<li><p><strong>name</strong> -- (optional) Name prefix for the returned tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor or (possibly nested) sequence of tensors.  Each tensor packs the
results of applying <cite>fn</cite> to tensors unpacked from <cite>elems</cite> along the first
dimension, from first to last.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- if <cite>fn</cite> is not callable or the structure of the output of
    <cite>fn</cite> and <cite>dtype</cite> do not match, or if elems is a SparseTensor.</p></li>
<li><p><strong>ValueError</strong> -- if the lengths of the output of <cite>fn</cite> and <cite>dtype</cite> do not match.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">elems</span> <span class="pre">=</span> <span class="pre">np.array([1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6])</span>
<span class="pre">squares</span> <span class="pre">=</span> <span class="pre">map_fn(lambda</span> <span class="pre">x:</span> <span class="pre">x</span> <span class="pre">*</span> <span class="pre">x,</span> <span class="pre">elems)</span>
<span class="pre">#</span> <span class="pre">squares</span> <span class="pre">==</span> <span class="pre">[1,</span> <span class="pre">4,</span> <span class="pre">9,</span> <span class="pre">16,</span> <span class="pre">25,</span> <span class="pre">36]</span>
<span class="pre">`</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">elems</span> <span class="pre">=</span> <span class="pre">(np.array([1,</span> <span class="pre">2,</span> <span class="pre">3]),</span> <span class="pre">np.array([-1,</span> <span class="pre">1,</span> <span class="pre">-1]))</span>
<span class="pre">alternate</span> <span class="pre">=</span> <span class="pre">map_fn(lambda</span> <span class="pre">x:</span> <span class="pre">x[0]</span> <span class="pre">*</span> <span class="pre">x[1],</span> <span class="pre">elems,</span> <span class="pre">dtype=tf.int64)</span>
<span class="pre">#</span> <span class="pre">alternate</span> <span class="pre">==</span> <span class="pre">[-1,</span> <span class="pre">2,</span> <span class="pre">-3]</span>
<span class="pre">`</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">elems</span> <span class="pre">=</span> <span class="pre">np.array([1,</span> <span class="pre">2,</span> <span class="pre">3])</span>
<span class="pre">alternates</span> <span class="pre">=</span> <span class="pre">map_fn(lambda</span> <span class="pre">x:</span> <span class="pre">(x,</span> <span class="pre">-x),</span> <span class="pre">elems,</span> <span class="pre">dtype=(tf.int64,</span> <span class="pre">tf.int64))</span>
<span class="pre">#</span> <span class="pre">alternates[0]</span> <span class="pre">==</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span>
<span class="pre">#</span> <span class="pre">alternates[1]</span> <span class="pre">==</span> <span class="pre">[-1,</span> <span class="pre">-2,</span> <span class="pre">-3]</span>
<span class="pre">`</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.matmul">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">matmul</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em>, <em class="sig-param"><span class="n">transpose_a</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">transpose_b</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">adjoint_a</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">adjoint_b</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">a_is_sparse</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">b_is_sparse</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#matmul"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.matmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Multiplies matrix <cite>a</cite> by matrix <cite>b</cite>, producing <cite>a</cite> * <cite>b</cite>.</p>
<p>The inputs must, following any transpositions, be tensors of rank &gt;= 2
where the inner 2 dimensions specify valid matrix multiplication dimensions,
and any further outer dimensions specify matching batch size.</p>
<p>Both matrices must be of the same type. The supported types are:
<cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p>
<p>Either matrix can be transposed or adjointed (conjugated and transposed) on
the fly by setting one of the corresponding flag to <cite>True</cite>. These are <cite>False</cite>
by default.</p>
<p>If one or both of the matrices contain a lot of zeros, a more efficient
multiplication algorithm can be used by setting the corresponding
<cite>a_is_sparse</cite> or <cite>b_is_sparse</cite> flag to <cite>True</cite>. These are <cite>False</cite> by default.
This optimization is only available for plain matrices (rank-2 tensors) with
datatypes <cite>bfloat16</cite> or <cite>float32</cite>.</p>
<p>A simple 2-D tensor matrix multiplication:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>  <span class="c1"># 2-D tensor</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">array([[1, 2, 3],</span>
<span class="go">       [4, 5, 6]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>  <span class="c1"># 2-D tensor</span>
<span class="go">&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="go">array([[ 7,  8],</span>
<span class="go">       [ 9, 10],</span>
<span class="go">       [11, 12]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>  <span class="c1"># `a` * `b`</span>
<span class="go">&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=</span>
<span class="go">array([[ 58,  64],</span>
<span class="go">       [139, 154]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>A batch matrix multiplication with batch shape [2]:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>  <span class="c1"># 3-D tensor</span>
<span class="go">&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=</span>
<span class="go">array([[[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]],</span>
<span class="go">       [[ 7,  8,  9],</span>
<span class="go">        [10, 11, 12]]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>  <span class="c1"># 3-D tensor</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=</span>
<span class="go">array([[[13, 14],</span>
<span class="go">        [15, 16],</span>
<span class="go">        [17, 18]],</span>
<span class="go">       [[19, 20],</span>
<span class="go">        [21, 22],</span>
<span class="go">        [23, 24]]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>  <span class="c1"># `a` * `b`</span>
<span class="go">&lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=</span>
<span class="go">array([[[ 94, 100],</span>
<span class="go">        [229, 244]],</span>
<span class="go">       [[508, 532],</span>
<span class="go">        [697, 730]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>Since python &gt;= 3.5 the &#64; operator is supported
(see [PEP 465](<a class="reference external" href="https://www.python.org/dev/peps/pep-0465/">https://www.python.org/dev/peps/pep-0465/</a>)). In TensorFlow,
it simply calls the <cite>tf.matmul()</cite> function, so the following lines are
equivalent:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">b</span> <span class="o">@</span> <span class="p">[[</span><span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">11</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">[[</span><span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">11</span><span class="p">]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> -- <cite>tf.Tensor</cite> of type <cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>,
<cite>complex64</cite>, <cite>complex128</cite> and rank &gt; 1.</p></li>
<li><p><strong>b</strong> -- <cite>tf.Tensor</cite> with same type and rank as <cite>a</cite>.</p></li>
<li><p><strong>transpose_a</strong> -- If <cite>True</cite>, <cite>a</cite> is transposed before multiplication.</p></li>
<li><p><strong>transpose_b</strong> -- If <cite>True</cite>, <cite>b</cite> is transposed before multiplication.</p></li>
<li><p><strong>adjoint_a</strong> -- If <cite>True</cite>, <cite>a</cite> is conjugated and transposed before
multiplication.</p></li>
<li><p><strong>adjoint_b</strong> -- If <cite>True</cite>, <cite>b</cite> is conjugated and transposed before
multiplication.</p></li>
<li><p><strong>a_is_sparse</strong> -- If <cite>True</cite>, <cite>a</cite> is treated as a sparse matrix. Notice, this
<strong>does not support `tf.sparse.SparseTensor`</strong>, it just makes optimizations
that assume most values in <cite>a</cite> are zero.
See <cite>tf.sparse.sparse_dense_matmul</cite>
for some support for <cite>tf.SparseTensor</cite> multiplication.</p></li>
<li><p><strong>b_is_sparse</strong> -- If <cite>True</cite>, <cite>b</cite> is treated as a sparse matrix. Notice, this
<strong>does not support `tf.sparse.SparseTensor`</strong>, it just makes optimizations
that assume most values in <cite>a</cite> are zero.
See <cite>tf.sparse.sparse_dense_matmul</cite>
for some support for <cite>tf.SparseTensor</cite> multiplication.</p></li>
<li><p><strong>name</strong> -- Name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <cite>tf.Tensor</cite> of the same type as <cite>a</cite> and <cite>b</cite> where each inner-most matrix
is the product of the corresponding matrices in <cite>a</cite> and <cite>b</cite>, e.g. if all
transpose or adjoint attributes are <cite>False</cite>:</p>
<p><cite>output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])</cite>,
for all indices <cite>i</cite>, <cite>j</cite>.</p>
<p>Note: This is matrix product, not element-wise product.</p>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>transpose_a</cite> and <cite>adjoint_a</cite>, or <cite>transpose_b</cite> and
    <cite>adjoint_b</cite> are both set to <cite>True</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.matrix_square_root">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">matrix_square_root</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_linalg_ops.html#matrix_square_root"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.matrix_square_root" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the matrix square root of one or more square matrices:</p>
<p>matmul(sqrtm(A), sqrtm(A)) = A</p>
<p>The input matrix should be invertible. If the input matrix is real, it should
have no eigenvalues which are real and negative (pairs of complex conjugate
eigenvalues are allowed).</p>
<p>The matrix square root is computed by first reducing the matrix to
quasi-triangular form with the real Schur decomposition. The square root
of the quasi-triangular matrix is then computed directly. Details of
the algorithm can be found in: Nicholas J. Higham, &quot;Computing real
square roots of a real matrix&quot;, Linear Algebra Appl., 1987.</p>
<p>The input is a tensor of shape <cite>[..., M, M]</cite> whose inner-most 2 dimensions
form square matrices. The output is a tensor of the same shape as the input
containing the matrix square root for all input submatrices <cite>[..., :, :]</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>float64</cite>, <cite>float32</cite>, <cite>half</cite>, <cite>complex64</cite>, <cite>complex128</cite>.
Shape is <cite>[..., M, M]</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.maximum">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">maximum</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#maximum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.maximum" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p>
<p>Example:
&gt;&gt;&gt; x = tf.constant([0., 0., 0., 0.])
&gt;&gt;&gt; y = tf.constant([-2., 0., 2., 5.])
&gt;&gt;&gt; tf.math.maximum(x, y)
&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.meshgrid">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">meshgrid</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#meshgrid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.meshgrid" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts parameters for evaluation on an N-D grid.</p>
<p>Given N one-dimensional coordinate arrays <cite>*args</cite>, returns a list <cite>outputs</cite>
of N-D coordinate arrays for evaluating expressions on an N-D grid.</p>
<p>Notes:</p>
<p><cite>meshgrid</cite> supports cartesian ('xy') and matrix ('ij') indexing conventions.
When the <cite>indexing</cite> argument is set to 'xy' (the default), the broadcasting
instructions for the first two dimensions are swapped.</p>
<p>Examples:</p>
<p>Calling <cite>X, Y = meshgrid(x, y)</cite> with the tensors</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span>
<span class="pre">y</span> <span class="pre">=</span> <span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">6]</span>
<span class="pre">X,</span> <span class="pre">Y</span> <span class="pre">=</span> <span class="pre">tf.meshgrid(x,</span> <span class="pre">y)</span>
<span class="pre">#</span> <span class="pre">X</span> <span class="pre">=</span> <span class="pre">[[1,</span> <span class="pre">2,</span> <span class="pre">3],</span>
<span class="pre">#</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3],</span>
<span class="pre">#</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]]</span>
<span class="pre">#</span> <span class="pre">Y</span> <span class="pre">=</span> <span class="pre">[[4,</span> <span class="pre">4,</span> <span class="pre">4],</span>
<span class="pre">#</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">[5,</span> <span class="pre">5,</span> <span class="pre">5],</span>
<span class="pre">#</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">[6,</span> <span class="pre">6,</span> <span class="pre">6]]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> -- <a href="#id810"><span class="problematic" id="id811">`</span></a>Tensor`s with rank 1.</p></li>
<li><p><strong>**kwargs</strong> -- <ul>
<li><p>indexing: Either 'xy' or 'ij' (optional, default: 'xy').</p></li>
<li><p>name: A name for the operation (optional).</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of N <a href="#id812"><span class="problematic" id="id813">`</span></a>Tensor`s with rank N.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>outputs</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> -- When no keyword arguments (kwargs) are passed.</p></li>
<li><p><strong>ValueError</strong> -- When indexing keyword argument is not one of <cite>xy</cite> or <cite>ij</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.minimum">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">minimum</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#minimum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.minimum" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p>
<p>Example:
&gt;&gt;&gt; x = tf.constant([0., 0., 0., 0.])
&gt;&gt;&gt; y = tf.constant([-5., -2., 0., 3.])
&gt;&gt;&gt; tf.math.minimum(x, y)
&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.multiply">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">multiply</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#multiply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an element-wise x * y.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>Since <cite>tf.math.multiply</cite> will convert its arguments to <cite>Tensor`s, you can also
pass in non-`Tensor</cite> arguments:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt;</span>
</pre></div>
</div>
<p>If <cite>x.shape</cite> is not thes same as <cite>y.shape</cite>, they will be broadcast to a
compatible shape. (More about broadcasting
[here](<a class="reference external" href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>).)</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]);</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]);</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span>  <span class="c1"># Taking advantage of operator overriding</span>
<span class="go">&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span>
<span class="go">array([[1., 1.],</span>
<span class="go">     [1., 1.]], dtype=float32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A Tensor. Must be one of the following types: <cite>bfloat16</cite>,
<cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>uint8</cite>, <cite>int8</cite>, <cite>uint16</cite>,
<cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
</dl>
<p>Returns:</p>
<p>A <cite>Tensor</cite>.  Has the same type as <cite>x</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>* InvalidArgumentError</strong> -- When <cite>x</cite> and <cite>y</cite> have incomptatible shapes or types.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.name_scope">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">name_scope</code><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#name_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.name_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.framework.ops.name_scope_v2</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.negative">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">negative</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.negative" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes numerical negative value element-wise.</p>
<p>I.e., \(y = -x\).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
<p>If <cite>x</cite> is a <cite>SparseTensor</cite>, returns
<cite>SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)</cite></p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.no_gradient">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">no_gradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op_type</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/ops.html#no_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.no_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies that ops of type <cite>op_type</cite> is not differentiable.</p>
<p>This function should <em>not</em> be used for operations that have a
well-defined gradient that is not yet implemented.</p>
<p>This function is only used when defining a new op type. It may be
used for ops such as <cite>tf.size()</cite> that are not differentiable.  For
example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tf.no_gradient(&quot;Size&quot;)</span>
<span class="pre">`</span></code></p>
<p>The gradient computed for 'op_type' will then propagate zeros.</p>
<p>For ops that have a well-defined gradient but are not yet implemented,
no declaration should be made, and an error <em>must</em> be thrown if
an attempt to request its gradient is made.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>op_type</strong> -- The string type of an operation. This corresponds to the
<cite>OpDef.name</cite> field for the proto that defines the operation.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> -- If <cite>op_type</cite> is not a string.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.no_op">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">no_op</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_control_flow_ops.html#no_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.no_op" title="Permalink to this definition">¶</a></dt>
<dd><p>Does nothing. Only useful as a placeholder for control edges.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name for the operation (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The created Operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.nondifferentiable_batch_function">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">nondifferentiable_batch_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_batch_threads</span></em>, <em class="sig-param"><span class="n">max_batch_size</span></em>, <em class="sig-param"><span class="n">batch_timeout_micros</span></em>, <em class="sig-param"><span class="n">allowed_batch_sizes</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_enqueued_batches</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">autograph</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.nondifferentiable_batch_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Batches the computation done by the decorated function.</p>
<p>So, for example, in the following code</p>
<p><a href="#id814"><span class="problematic" id="id815">``</span></a><a href="#id816"><span class="problematic" id="id817">`</span></a>python
&#64;batch_function(1, 2, 3)
def layer(a):</p>
<blockquote>
<div><p>return tf.matmul(a, a)</p>
</div></blockquote>
<p>b = layer(w)
<a href="#id818"><span class="problematic" id="id819">``</span></a><a href="#id820"><span class="problematic" id="id821">`</span></a></p>
<p>if more than one session.run call is simultaneously trying to compute <cite>b</cite>
the values of <cite>w</cite> will be gathered, non-deterministically concatenated
along the first axis, and only one thread will run the computation. See the
documentation of the <cite>Batch</cite> op for more details.</p>
<p>Assumes that all arguments of the decorated function are Tensors which will
be batched along their first dimension.</p>
<p>SparseTensor is not supported. The return value of the decorated function
must be a Tensor or a list/tuple of Tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_batch_threads</strong> -- Number of scheduling threads for processing batches
of work. Determines the number of batches processed in parallel.</p></li>
<li><p><strong>max_batch_size</strong> -- Batch sizes will never be bigger than this.</p></li>
<li><p><strong>batch_timeout_micros</strong> -- Maximum number of microseconds to wait before
outputting an incomplete batch.</p></li>
<li><p><strong>allowed_batch_sizes</strong> -- Optional list of allowed batch sizes. If left empty,
does nothing. Otherwise, supplies a list of batch sizes, causing the op
to pad batches up to one of those sizes. The entries must increase
monotonically, and the final entry must equal max_batch_size.</p></li>
<li><p><strong>max_enqueued_batches</strong> -- The maximum depth of the batch queue. Defaults to 10.</p></li>
<li><p><strong>autograph</strong> -- Whether to use autograph to compile python and eager style code
for efficient graph-mode execution.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The decorated function will return the unbatched computation output Tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.norm">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">norm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">ord</span><span class="o">=</span><span class="default_value">'euclidean'</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/linalg_ops.html#norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the norm of vectors, matrices, and tensors.</p>
<p>This function can compute several different vector norms (the 1-norm, the
Euclidean or 2-norm, the inf-norm, and in general the p-norm for p &gt; 0) and
matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- <cite>Tensor</cite> of types <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite></p></li>
<li><p><strong>ord</strong> -- <p>Order of the norm. Supported values are <cite>'fro'</cite>, <cite>'euclidean'</cite>,
<cite>1</cite>, <cite>2</cite>, <cite>np.inf</cite> and any positive real number yielding the corresponding
p-norm. Default is <cite>'euclidean'</cite> which is equivalent to Frobenius norm if
<cite>tensor</cite> is a matrix and equivalent to 2-norm for vectors.
Some restrictions apply:</p>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>The Frobenius norm <cite>'fro'</cite> is not defined for vectors,</p></li>
<li><p>If axis is a 2-tuple (matrix norm), only <cite>'euclidean'</cite>, '<cite>fro'</cite>, <cite>1</cite>,
<cite>2</cite>, <cite>np.inf</cite> are supported.</p></li>
</ol>
</div></blockquote>
<p>See the description of <cite>axis</cite> on how to compute norms for a batch of
vectors or matrices stored in a tensor.</p>
</p></li>
<li><p><strong>axis</strong> -- If <cite>axis</cite> is <cite>None</cite> (the default), the input is considered a vector
and a single vector norm is computed over the entire set of values in the
tensor, i.e. <cite>norm(tensor, ord=ord)</cite> is equivalent to
<cite>norm(reshape(tensor, [-1]), ord=ord)</cite>.
If <cite>axis</cite> is a Python integer, the input is considered a batch of vectors,
and <cite>axis</cite> determines the axis in <cite>tensor</cite> over which to compute vector
norms.
If <cite>axis</cite> is a 2-tuple of Python integers it is considered a batch of
matrices and <cite>axis</cite> determines the axes in <cite>tensor</cite> over which to compute
a matrix norm.
Negative indices are supported. Example: If you are passing a tensor that
can be either a matrix or a batch of matrices at runtime, pass
<cite>axis=[-2,-1]</cite> instead of <cite>axis=None</cite> to make sure that matrix norms are
computed.</p></li>
<li><p><strong>keepdims</strong> -- If True, the axis indicated in <cite>axis</cite> are kept with size 1.
Otherwise, the dimensions in <cite>axis</cite> are removed from the output shape.</p></li>
<li><p><strong>name</strong> -- The name of the op.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <cite>Tensor</cite> of the same type as tensor, containing the vector or</dt><dd><p>matrix norms. If <cite>keepdims</cite> is True then the rank of output is equal to
the rank of <cite>tensor</cite>. Otherwise, if <cite>axis</cite> is none the output is a scalar,
if <cite>axis</cite> is an integer, the rank of <cite>output</cite> is one less than the rank
of <cite>tensor</cite>, if <cite>axis</cite> is a 2-tuple the rank of <cite>output</cite> is two less
than the rank of <cite>tensor</cite>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>ord</cite> or <cite>axis</cite> is invalid.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Mostly equivalent to numpy.linalg.norm.
Not supported: ord &lt;= 0, 2-norm for matrices, nuclear norm.
Other differences:</p>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>If axis is <cite>None</cite>, treats the flattened <cite>tensor</cite> as a vector</p></li>
</ol>
<blockquote>
<div><p>regardless of rank.</p>
</div></blockquote>
<ol class="loweralpha simple" start="2">
<li><p>Explicitly supports 'euclidean' norm as the default, including for</p></li>
</ol>
<blockquote>
<div><p>higher order tensors.</p>
</div></blockquote>
</div></blockquote>
<p>&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.not_equal">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">not_equal</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#not_equal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.not_equal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the truth value of (x != y) element-wise.</p>
<p>Performs a [broadcast](
<a class="reference external" href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>) with the
arguments and then an element-wise inequality comparison, returning a Tensor
of boolean values.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  False])&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>tf.Tensor</cite> or <cite>tf.SparseTensor</cite> or <cite>tf.IndexedSlices</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>tf.Tensor</cite> or <cite>tf.SparseTensor</cite> or <cite>tf.IndexedSlices</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>tf.Tensor</cite> of type bool with the same size as that of x or y.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>tf.errors.InvalidArgumentError</strong> -- If shapes of arguments are incompatible</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.numpy_function">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">numpy_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func</span></em>, <em class="sig-param"><span class="n">inp</span></em>, <em class="sig-param"><span class="n">Tout</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/script_ops.html#numpy_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.numpy_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps a python function and uses it as a TensorFlow op.</p>
<p>Given a python function <cite>func</cite> wrap this function as an operation in a
TensorFlow function. <cite>func</cite> must take numpy arrays as its arguments and
return numpy arrays as its outputs.</p>
<p>The following example creates a TensorFlow graph with <cite>np.sinh()</cite> as an
operation in the graph:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_numpy_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>  <span class="c1"># x will be a numpy array with the contents of the input to the</span>
<span class="gp">... </span>  <span class="c1"># tf.function</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)])</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">tf_function</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">numpy_function</span><span class="p">(</span><span class="n">my_numpy_func</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_function</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.3810978&gt;</span>
</pre></div>
</div>
<p>Comparison to <cite>tf.py_function</cite>:
<cite>tf.py_function</cite> and <cite>tf.numpy_function</cite> are very similar, except that
<cite>tf.numpy_function</cite> takes numpy arrays, and not <cite>tf.Tensor`s. If you want the
function to contain `tf.Tensors</cite>, and have any TensorFlow operations executed
in the function be differentiable, please use <cite>tf.py_function</cite>.</p>
<p>Note: The <cite>tf.numpy_function</cite> operation has the following known
limitations:</p>
<ul class="simple">
<li><p>The body of the function (i.e. <cite>func</cite>) will not be serialized in a
<cite>tf.SavedModel</cite>. Therefore, you should not use this function if you need to
serialize your model and restore it in a different environment.</p></li>
<li><p>The operation must run in the same address space as the Python program
that calls <cite>tf.numpy_function()</cite>. If you are using distributed
TensorFlow, you must run a <cite>tf.distribute.Server</cite> in the same process as the
program that calls <cite>tf.numpy_function</cite>  you must pin the created
operation to a device in that server (e.g. using <cite>with tf.device():</cite>).</p></li>
<li><p>Since the function takes numpy arrays, you cannot take gradients
through a numpy_function. If you require something that is differentiable,
please consider using tf.py_function.</p></li>
<li><p>The resulting function is assumed stateful and will never be optimized.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> -- <p>A Python function, which accepts <cite>numpy.ndarray</cite> objects as arguments
and returns a list of <cite>numpy.ndarray</cite> objects (or a single
<cite>numpy.ndarray</cite>). This function must accept as many arguments as there are
tensors in <cite>inp</cite>, and these argument types will match the corresponding
<cite>tf.Tensor</cite> objects in <cite>inp</cite>. The returns <cite>numpy.ndarray`s must match the
number and types defined `Tout</cite>.
Important Note: Input and output <cite>numpy.ndarray`s of `func</cite> are not</p>
<blockquote>
<div><p>guaranteed to be copies. In some cases their underlying memory will be
shared with the corresponding TensorFlow tensors. In-place modification
or storing <cite>func</cite> input or return values in python datastructures
without explicit (np.)copy can have non-deterministic consequences.</p>
</div></blockquote>
</p></li>
<li><p><strong>inp</strong> -- A list of <cite>tf.Tensor</cite> objects.</p></li>
<li><p><strong>Tout</strong> -- A list or tuple of tensorflow data types or a single tensorflow data
type if there is only one, indicating what <cite>func</cite> returns.</p></li>
<li><p><strong>name</strong> -- (Optional) A name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Single or list of <cite>tf.Tensor</cite> which <cite>func</cite> computes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.one_hot">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">one_hot</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">depth</span></em>, <em class="sig-param"><span class="n">on_value</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">off_value</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#one_hot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.one_hot" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a one-hot tensor.</p>
<p>The locations represented by indices in <cite>indices</cite> take value <cite>on_value</cite>,
while all other locations take value <cite>off_value</cite>.</p>
<p><cite>on_value</cite> and <cite>off_value</cite> must have matching data types. If <cite>dtype</cite> is also
provided, they must be the same data type as specified by <cite>dtype</cite>.</p>
<p>If <cite>on_value</cite> is not provided, it will default to the value <cite>1</cite> with type
<cite>dtype</cite></p>
<p>If <cite>off_value</cite> is not provided, it will default to the value <cite>0</cite> with type
<cite>dtype</cite></p>
<p>If the input <cite>indices</cite> is rank <cite>N</cite>, the output will have rank <cite>N+1</cite>. The
new axis is created at dimension <cite>axis</cite> (default: the new axis is appended
at the end).</p>
<p>If <cite>indices</cite> is a scalar the output shape will be a vector of length <cite>depth</cite></p>
<p>If <cite>indices</cite> is a vector of length <cite>features</cite>, the output shape will be:</p>
<dl class="simple">
<dt><a href="#id822"><span class="problematic" id="id823">``</span></a><a href="#id824"><span class="problematic" id="id825">`</span></a></dt><dd><p>features x depth if axis == -1
depth x features if axis == 0</p>
</dd>
</dl>
<p><a href="#id826"><span class="problematic" id="id827">``</span></a><a href="#id828"><span class="problematic" id="id829">`</span></a></p>
<p>If <cite>indices</cite> is a matrix (batch) with shape <cite>[batch, features]</cite>, the output
shape will be:</p>
<dl class="simple">
<dt><a href="#id830"><span class="problematic" id="id831">``</span></a><a href="#id832"><span class="problematic" id="id833">`</span></a></dt><dd><p>batch x features x depth if axis == -1
batch x depth x features if axis == 1
depth x batch x features if axis == 0</p>
</dd>
</dl>
<p><a href="#id834"><span class="problematic" id="id835">``</span></a><a href="#id836"><span class="problematic" id="id837">`</span></a></p>
<p>If <cite>indices</cite> is a RaggedTensor, the 'axis' argument must be positive and refer
to a non-ragged axis. The output will be equivalent to applying 'one_hot' on
the values of the RaggedTensor, and creating a new RaggedTensor from the
result.</p>
<p>If <cite>dtype</cite> is not provided, it will attempt to assume the data type of
<cite>on_value</cite> or <cite>off_value</cite>, if one or both are passed in. If none of
<cite>on_value</cite>, <cite>off_value</cite>, or <cite>dtype</cite> are provided, <cite>dtype</cite> will default to the
value <cite>tf.float32</cite>.</p>
<p>Note: If a non-numeric data type output is desired (<cite>tf.string</cite>, <cite>tf.bool</cite>,
etc.), both <cite>on_value</cite> and <cite>off_value</cite> _must_ be provided to <cite>one_hot</cite>.</p>
<p>For example:</p>
<p><a href="#id838"><span class="problematic" id="id839">``</span></a><a href="#id840"><span class="problematic" id="id841">`</span></a>python
indices = [0, 1, 2]
depth = 3
tf.one_hot(indices, depth)  # output: [3 x 3]
# [[1., 0., 0.],
#  [0., 1., 0.],
#  [0., 0., 1.]]</p>
<p>indices = [0, 2, -1, 1]
depth = 3
tf.one_hot(indices, depth,</p>
<blockquote>
<div><p>on_value=5.0, off_value=0.0,
axis=-1)  # output: [4 x 3]</p>
</div></blockquote>
<p># [[5.0, 0.0, 0.0],  # one_hot(0)
#  [0.0, 0.0, 5.0],  # one_hot(2)
#  [0.0, 0.0, 0.0],  # one_hot(-1)
#  [0.0, 5.0, 0.0]]  # one_hot(1)</p>
<p>indices = [[0, 2], [1, -1]]
depth = 3
tf.one_hot(indices, depth,</p>
<blockquote>
<div><p>on_value=1.0, off_value=0.0,
axis=-1)  # output: [2 x 2 x 3]</p>
</div></blockquote>
<p># [[[1.0, 0.0, 0.0],   # one_hot(0)
#   [0.0, 0.0, 1.0]],  # one_hot(2)
#  [[0.0, 1.0, 0.0],   # one_hot(1)
#   [0.0, 0.0, 0.0]]]  # one_hot(-1)</p>
<p>indices = tf.ragged.constant([[0, 1], [2]])
depth = 3
tf.one_hot(indices, depth)  # output: [2 x None x 3]
# [[[1., 0., 0.],
#   [0., 1., 0.]],
#  [[0., 0., 1.]]]
<a href="#id842"><span class="problematic" id="id843">``</span></a><a href="#id844"><span class="problematic" id="id845">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- A <cite>Tensor</cite> of indices.</p></li>
<li><p><strong>depth</strong> -- A scalar defining the depth of the one hot dimension.</p></li>
<li><p><strong>on_value</strong> -- A scalar defining the value to fill in output when <cite>indices[j]
= i</cite>. (default: 1)</p></li>
<li><p><strong>off_value</strong> -- A scalar defining the value to fill in output when <cite>indices[j]
!= i</cite>. (default: 0)</p></li>
<li><p><strong>axis</strong> -- The axis to fill (default: -1, a new inner-most axis).</p></li>
<li><p><strong>dtype</strong> -- The data type of the output tensor.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The one-hot tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> -- If dtype of either <cite>on_value</cite> or <cite>off_value</cite> don't match <cite>dtype</cite></p></li>
<li><p><strong>TypeError</strong> -- If dtype of <cite>on_value</cite> and <cite>off_value</cite> don't match one another</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.ones">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">ones</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#ones"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a tensor with all elements set to one (1).</p>
<p>See also <cite>tf.ones_like</cite>.</p>
<p>This operation returns a tensor of type <cite>dtype</cite> with shape <cite>shape</cite> and
all elements set to one.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=</span>
<span class="go">array([[1, 1, 1, 1],</span>
<span class="go">       [1, 1, 1, 1],</span>
<span class="go">       [1, 1, 1, 1]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> -- A <cite>list</cite> of integers, a <cite>tuple</cite> of integers, or
a 1-D <cite>Tensor</cite> of type <cite>int32</cite>.</p></li>
<li><p><strong>dtype</strong> -- Optional DType of an element in the resulting <cite>Tensor</cite>. Default is
<cite>tf.float32</cite>.</p></li>
<li><p><strong>name</strong> -- Optional string. A name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> with all elements set to one (1).</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.ones_initializer">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">ones_initializer</code><a class="headerlink" href="#tensorflow.ones_initializer" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.ops.init_ops_v2.Ones</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.ones_like">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">ones_like</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#ones_like"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.ones_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a tensor of all ones that has the same shape as the input.</p>
<p>See also <cite>tf.ones</cite>.</p>
<p>Given a single tensor (<cite>tensor</cite>), this operation returns a tensor of the
same type and shape as <cite>tensor</cite> with all elements set to 1. Optionally,
you can use <cite>dtype</cite> to specify a new type for the returned tensor.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">  array([[1, 1, 1],</span>
<span class="go">         [1, 1, 1]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>dtype</strong> -- A type for the returned <cite>Tensor</cite>. Must be <cite>float16</cite>, <cite>float32</cite>,
<cite>float64</cite>, <cite>int8</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>uint16</cite>, <cite>int32</cite>, <cite>int64</cite>,
<cite>complex64</cite>, <cite>complex128</cite>, <cite>bool</cite> or <cite>string</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> with all elements set to one.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.pad">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">pad</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">mode</span><span class="o">=</span><span class="default_value">'CONSTANT'</span></em>, <em class="sig-param"><span class="n">constant_values</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads a tensor.</p>
<p>This operation pads a <cite>tensor</cite> according to the <cite>paddings</cite> you specify.
<cite>paddings</cite> is an integer tensor with shape <cite>[n, 2]</cite>, where n is the rank of
<cite>tensor</cite>. For each dimension D of <cite>input</cite>, <cite>paddings[D, 0]</cite> indicates how
many values to add before the contents of <cite>tensor</cite> in that dimension, and
<cite>paddings[D, 1]</cite> indicates how many values to add after the contents of
<cite>tensor</cite> in that dimension. If <cite>mode</cite> is &quot;REFLECT&quot; then both <cite>paddings[D, 0]</cite>
and <cite>paddings[D, 1]</cite> must be no greater than <cite>tensor.dim_size(D) - 1</cite>. If
<cite>mode</cite> is &quot;SYMMETRIC&quot; then both <cite>paddings[D, 0]</cite> and <cite>paddings[D, 1]</cite> must be
no greater than <cite>tensor.dim_size(D)</cite>.</p>
<p>The padded size of each dimension D of the output is:</p>
<p><cite>paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]</cite></p>
<p>For example:</p>
<p><a href="#id846"><span class="problematic" id="id847">``</span></a><a href="#id848"><span class="problematic" id="id849">`</span></a>python
t = tf.constant([[1, 2, 3], [4, 5, 6]])
paddings = tf.constant([[1, 1,], [2, 2]])
# 'constant_values' is 0.
# rank of 't' is 2.
tf.pad(t, paddings, &quot;CONSTANT&quot;)  # [[0, 0, 0, 0, 0, 0, 0],</p>
<blockquote>
<div><p>#  [0, 0, 1, 2, 3, 0, 0],
#  [0, 0, 4, 5, 6, 0, 0],
#  [0, 0, 0, 0, 0, 0, 0]]</p>
</div></blockquote>
<dl class="simple">
<dt>tf.pad(t, paddings, &quot;REFLECT&quot;)  # [[6, 5, 4, 5, 6, 5, 4],</dt><dd><p>#  [3, 2, 1, 2, 3, 2, 1],
#  [6, 5, 4, 5, 6, 5, 4],
#  [3, 2, 1, 2, 3, 2, 1]]</p>
</dd>
<dt>tf.pad(t, paddings, &quot;SYMMETRIC&quot;)  # [[2, 1, 1, 2, 3, 3, 2],</dt><dd><p>#  [2, 1, 1, 2, 3, 3, 2],
#  [5, 4, 4, 5, 6, 6, 5],
#  [5, 4, 4, 5, 6, 6, 5]]</p>
</dd>
</dl>
<p><a href="#id850"><span class="problematic" id="id851">``</span></a><a href="#id852"><span class="problematic" id="id853">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>paddings</strong> -- A <cite>Tensor</cite> of type <cite>int32</cite>.</p></li>
<li><p><strong>mode</strong> -- One of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot; (case-insensitive)</p></li>
<li><p><strong>constant_values</strong> -- In &quot;CONSTANT&quot; mode, the scalar pad value to use. Must be
same type as <cite>tensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>tensor</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- When mode is not one of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot;.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.parallel_stack">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">parallel_stack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'parallel_stack'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#parallel_stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.parallel_stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Stacks a list of rank-<cite>R</cite> tensors into one rank-<cite>(R+1)</cite> tensor in parallel.</p>
<p>Requires that the shape of inputs be known at graph construction time.</p>
<p>Packs the list of tensors in <cite>values</cite> into a tensor with rank one higher than
each tensor in <cite>values</cite>, by packing them along the first dimension.
Given a list of length <cite>N</cite> of tensors of shape <cite>(A, B, C)</cite>; the <cite>output</cite>
tensor will have the shape <cite>(N, A, B, C)</cite>.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([1,</span> <span class="pre">4])</span>
<span class="pre">y</span> <span class="pre">=</span> <span class="pre">tf.constant([2,</span> <span class="pre">5])</span>
<span class="pre">z</span> <span class="pre">=</span> <span class="pre">tf.constant([3,</span> <span class="pre">6])</span>
<span class="pre">tf.parallel_stack([x,</span> <span class="pre">y,</span> <span class="pre">z])</span>&#160; <span class="pre">#</span> <span class="pre">[[1,</span> <span class="pre">4],</span> <span class="pre">[2,</span> <span class="pre">5],</span> <span class="pre">[3,</span> <span class="pre">6]]</span>
<span class="pre">`</span></code></p>
<p>The difference between <cite>stack</cite> and <cite>parallel_stack</cite> is that <cite>stack</cite> requires
all the inputs be computed before the operation will begin but doesn't require
that the input shapes be known during graph construction.</p>
<p><cite>parallel_stack</cite> will copy pieces of the input into the output as they become
available, in some situations this can provide a performance benefit.</p>
<p>Unlike <cite>stack</cite>, <cite>parallel_stack</cite> does NOT support backpropagation.</p>
<p>This is the opposite of unstack.  The numpy equivalent is</p>
<blockquote>
<div><p>tf.parallel_stack([x, y, z]) = np.asarray([x, y, z])</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A list of <cite>Tensor</cite> objects with the same shape and type.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A stacked <cite>Tensor</cite> with the same type as <cite>values</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.pow">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">pow</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#pow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the power of one value to another.</p>
<p>Given a tensor <cite>x</cite> and a tensor <cite>y</cite>, this operation computes \(x^y\) for
corresponding elements in <cite>x</cite> and <cite>y</cite>. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([[2,</span> <span class="pre">2],</span> <span class="pre">[3,</span> <span class="pre">3]])</span>
<span class="pre">y</span> <span class="pre">=</span> <span class="pre">tf.constant([[8,</span> <span class="pre">16],</span> <span class="pre">[2,</span> <span class="pre">3]])</span>
<span class="pre">tf.pow(x,</span> <span class="pre">y)</span>&#160; <span class="pre">#</span> <span class="pre">[[256,</span> <span class="pre">65536],</span> <span class="pre">[9,</span> <span class="pre">27]]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite> of type <cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>,
<cite>complex64</cite>, or <cite>complex128</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite> of type <cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>,
<cite>complex64</cite>, or <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.print">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">print</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Print the specified inputs.</p>
<p>A TensorFlow operator that prints the specified inputs to a desired
output stream or logging level. The inputs may be dense or sparse Tensors,
primitive python objects, data structures that contain tensors, and printable
Python objects. Printed tensors will recursively show the first and last
elements of each dimension to summarize.</p>
<p>&#64;compatibility(python2)
In python 2.7, make sure to import the following:
<cite>from __future__ import print_function</cite>
&#64;end_compatibility</p>
<p class="rubric">Example</p>
<p>Single-input usage:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tensor</span> <span class="pre">=</span> <span class="pre">tf.range(10)</span>
<span class="pre">tf.print(tensor,</span> <span class="pre">output_stream=sys.stderr)</span>
<span class="pre">`</span></code></p>
<p>(This prints &quot;[0 1 2 ... 7 8 9]&quot; to sys.stderr)</p>
<p>Multi-input usage:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tensor</span> <span class="pre">=</span> <span class="pre">tf.range(10)</span>
<span class="pre">tf.print(&quot;tensors:&quot;,</span> <span class="pre">tensor,</span> <span class="pre">{2:</span> <span class="pre">tensor</span> <span class="pre">*</span> <span class="pre">2},</span> <span class="pre">output_stream=sys.stdout)</span>
<span class="pre">`</span></code></p>
<p>(This prints &quot;tensors: [0 1 2 ... 7 8 9] {2: [0 2 4 ... 14 16 18]}&quot; to
sys.stdout)</p>
<p>Changing the input separator:
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tensor_a</span> <span class="pre">=</span> <span class="pre">tf.range(2)</span>
<span class="pre">tensor_b</span> <span class="pre">=</span> <span class="pre">tensor_a</span> <span class="pre">*</span> <span class="pre">2</span>
<span class="pre">tf.print(tensor_a,</span> <span class="pre">tensor_b,</span> <span class="pre">output_stream=sys.stderr,</span> <span class="pre">sep=',')</span>
<span class="pre">`</span></code></p>
<p>(This prints &quot;[0 1],[0 2]&quot; to sys.stderr)</p>
<p>Usage in a <cite>tf.function</cite>:</p>
<p><a href="#id854"><span class="problematic" id="id855">``</span></a><a href="#id856"><span class="problematic" id="id857">`</span></a>python
&#64;tf.function
def f():</p>
<blockquote>
<div><p>tensor = tf.range(10)
tf.print(tensor, output_stream=sys.stderr)
return tensor</p>
</div></blockquote>
<p>range_tensor = f()
<a href="#id858"><span class="problematic" id="id859">``</span></a><a href="#id860"><span class="problematic" id="id861">`</span></a></p>
<p>(This prints &quot;[0 1 2 ... 7 8 9]&quot; to sys.stderr)</p>
<p>&#64;compatibility(TF 1.x Graphs and Sessions)
In graphs manually created outside of <cite>tf.function</cite>, this method returns
the created TF operator that prints the data. To make sure the
operator runs, users need to pass the produced op to
<cite>tf.compat.v1.Session</cite>'s run method, or to use the op as a control
dependency for executed ops by specifying
<cite>with tf.compat.v1.control_dependencies([print_op])</cite>.
&#64;end_compatibility</p>
<blockquote>
<div><p>Compatibility usage in TF 1.x graphs:</p>
<p><a href="#id862"><span class="problematic" id="id863">``</span></a><a href="#id864"><span class="problematic" id="id865">`</span></a>python
sess = tf.compat.v1.Session()
with sess.as_default():</p>
<blockquote>
<div><p>tensor = tf.range(10)
print_op = tf.print(&quot;tensors:&quot;, tensor, {2: tensor * 2},</p>
<blockquote>
<div><p>output_stream=sys.stdout)</p>
</div></blockquote>
<dl class="simple">
<dt>with tf.control_dependencies([print_op]):</dt><dd><p>tripled_tensor = tensor * 3</p>
</dd>
</dl>
<p>sess.run(tripled_tensor)</p>
</div></blockquote>
<p><a href="#id866"><span class="problematic" id="id867">``</span></a><a href="#id868"><span class="problematic" id="id869">`</span></a></p>
<p>(This prints &quot;tensors: [0 1 2 ... 7 8 9] {2: [0 2 4 ... 14 16 18]}&quot; to
sys.stdout)</p>
</div></blockquote>
<dl class="simple">
<dt>Note: In Jupyter notebooks and colabs, <cite>tf.print</cite> prints to the notebook</dt><dd><p>cell outputs. It will not write to the notebook kernel's console logs.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*inputs</strong> -- Positional arguments that are the inputs to print. Inputs in the
printed output will be separated by spaces. Inputs may be python
primitives, tensors, data structures such as dicts and lists that may
contain tensors (with the data structures possibly nested in arbitrary
ways), and printable python objects.</p></li>
<li><p><strong>output_stream</strong> -- The output stream, logging level, or file to print to.
Defaults to sys.stderr, but sys.stdout, tf.compat.v1.logging.info,
tf.compat.v1.logging.warning, tf.compat.v1.logging.error,
absl.logging.info, absl.logging.warning and absl.logging.error are also
supported. To print to a file, pass a string started with &quot;<a class="reference external" href="file://">file://</a>&quot;
followed by the file path, e.g., &quot;<a class="reference external" href="file:///tmp/foo.out">file:///tmp/foo.out</a>&quot;.</p></li>
<li><p><strong>summarize</strong> -- The first and last <cite>summarize</cite> elements within each dimension are
recursively printed per Tensor. If None, then the first 3 and last 3
elements of each dimension are printed for each tensor. If set to -1, it
will print all elements of every tensor.</p></li>
<li><p><strong>sep</strong> -- The string to use to separate the inputs. Defaults to &quot; &quot;.</p></li>
<li><p><strong>end</strong> -- End character that is appended at the end the printed string.
Defaults to the newline character.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None when executing eagerly. During graph tracing this returns
a TF operator that prints the specified inputs in the specified output
stream or logging level. This operator will be automatically executed
except inside of <cite>tf.compat.v1</cite> graphs and sessions.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If an unsupported output stream is specified.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.py_function">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">py_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func</span></em>, <em class="sig-param"><span class="n">inp</span></em>, <em class="sig-param"><span class="n">Tout</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.py_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps a python function into a TensorFlow op that executes it eagerly.</p>
<p>This function allows expressing computations in a TensorFlow graph as
Python functions. In particular, it wraps a Python function <cite>func</cite>
in a once-differentiable TensorFlow operation that executes it with eager
execution enabled. As a consequence, <cite>tf.py_function</cite> makes it
possible to express control flow using Python constructs (<cite>if</cite>, <cite>while</cite>,
<cite>for</cite>, etc.), instead of TensorFlow control flow constructs (<cite>tf.cond</cite>,
<cite>tf.while_loop</cite>). For example, you might use <cite>tf.py_function</cite> to
implement the log huber function:</p>
<p><a href="#id870"><span class="problematic" id="id871">``</span></a><a href="#id872"><span class="problematic" id="id873">`</span></a>python
def log_huber(x, m):</p>
<blockquote>
<div><dl class="simple">
<dt>if tf.abs(x) &lt;= m:</dt><dd><p>return x**2</p>
</dd>
<dt>else:</dt><dd><p>return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))</p>
</dd>
</dl>
</div></blockquote>
<p>x = tf.compat.v1.placeholder(tf.float32)
m = tf.compat.v1.placeholder(tf.float32)</p>
<p>y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)
dy_dx = tf.gradients(y, x)[0]</p>
<dl class="simple">
<dt>with tf.compat.v1.Session() as sess:</dt><dd><p># The session executes <cite>log_huber</cite> eagerly. Given the feed values below,
# it will take the first branch, so <cite>y</cite> evaluates to 1.0 and
# <cite>dy_dx</cite> evaluates to 2.0.
y, dy_dx = sess.run([y, dy_dx], feed_dict={x: 1.0, m: 2.0})</p>
</dd>
</dl>
<p><a href="#id874"><span class="problematic" id="id875">``</span></a><a href="#id876"><span class="problematic" id="id877">`</span></a></p>
<p>You can also use <cite>tf.py_function</cite> to debug your models at runtime
using Python tools, i.e., you can isolate portions of your code that
you want to debug, wrap them in Python functions and insert <cite>pdb</cite> tracepoints
or print statements as desired, and wrap those functions in
<cite>tf.py_function</cite>.</p>
<p>For more information on eager execution, see the
[Eager guide](<a class="reference external" href="https://tensorflow.org/guide/eager">https://tensorflow.org/guide/eager</a>).</p>
<p><cite>tf.py_function</cite> is similar in spirit to <cite>tf.compat.v1.py_func</cite>, but unlike
the latter, the former lets you use TensorFlow operations in the wrapped
Python function. In particular, while <cite>tf.compat.v1.py_func</cite> only runs on CPUs
and
wraps functions that take NumPy arrays as inputs and return NumPy arrays as
outputs, <cite>tf.py_function</cite> can be placed on GPUs and wraps functions
that take Tensors as inputs, execute TensorFlow operations in their bodies,
and return Tensors as outputs.</p>
<p>Like <cite>tf.compat.v1.py_func</cite>, <cite>tf.py_function</cite> has the following limitations
with respect to serialization and distribution:</p>
<ul class="simple">
<li><p>The body of the function (i.e. <cite>func</cite>) will not be serialized in a
<cite>GraphDef</cite>. Therefore, you should not use this function if you need to
serialize your model and restore it in a different environment.</p></li>
<li><p>The operation must run in the same address space as the Python program
that calls <cite>tf.py_function()</cite>. If you are using distributed
TensorFlow, you must run a <cite>tf.distribute.Server</cite> in the same process as the
program that calls <cite>tf.py_function()</cite> and you must pin the created
operation to a device in that server (e.g. using <cite>with tf.device():</cite>).</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> -- A Python function which accepts a list of <cite>Tensor</cite> objects having
element types that match the corresponding <cite>tf.Tensor</cite> objects in <cite>inp</cite>
and returns a list of <cite>Tensor</cite> objects (or a single <cite>Tensor</cite>, or <cite>None</cite>)
having element types that match the corresponding values in <cite>Tout</cite>.</p></li>
<li><p><strong>inp</strong> -- A list of <cite>Tensor</cite> objects.</p></li>
<li><p><strong>Tout</strong> -- A list or tuple of tensorflow data types or a single tensorflow data
type if there is only one, indicating what <cite>func</cite> returns; an empty list
if no value is returned (i.e., if the return value is <cite>None</cite>).</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <cite>Tensor</cite> or a single <cite>Tensor</cite> which <cite>func</cite> computes; an empty list
if <cite>func</cite> returns None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.random_normal_initializer">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">random_normal_initializer</code><a class="headerlink" href="#tensorflow.random_normal_initializer" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.ops.init_ops_v2.RandomNormal</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.random_uniform_initializer">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">random_uniform_initializer</code><a class="headerlink" href="#tensorflow.random_uniform_initializer" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.ops.init_ops_v2.RandomUniform</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.range">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">range</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start</span></em>, <em class="sig-param"><span class="n">limit</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">delta</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'range'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#range"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.range" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a sequence of numbers.</p>
<p>Creates a sequence of numbers that begins at <cite>start</cite> and extends by
increments of <cite>delta</cite> up to but not including <cite>limit</cite>.</p>
<p>The dtype of the resulting tensor is inferred from the inputs unless
it is provided explicitly.</p>
<p>Like the Python builtin <cite>range</cite>, <cite>start</cite> defaults to 0, so that
<cite>range(n) = range(0, n)</cite>.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">start</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">limit</span> <span class="o">=</span> <span class="mi">18</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(5,), dtype=int32,</span>
<span class="go">numpy=array([ 3,  6,  9, 12, 15], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">start</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">limit</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=float32,</span>
<span class="go">numpy=array([3. , 2.5, 2. , 1.5], dtype=float32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">limit</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">limit</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(5,), dtype=int32,</span>
<span class="go">numpy=array([0, 1, 2, 3, 4], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> -- A 0-D <cite>Tensor</cite> (scalar). Acts as first entry in the range if <cite>limit</cite>
is not None; otherwise, acts as range limit and first entry defaults to 0.</p></li>
<li><p><strong>limit</strong> -- A 0-D <cite>Tensor</cite> (scalar). Upper limit of sequence, exclusive. If None,
defaults to the value of <cite>start</cite> while the first entry of the range
defaults to 0.</p></li>
<li><p><strong>delta</strong> -- A 0-D <cite>Tensor</cite> (scalar). Number that increments <cite>start</cite>. Defaults to
1.</p></li>
<li><p><strong>dtype</strong> -- The type of the elements of the resulting tensor.</p></li>
<li><p><strong>name</strong> -- A name for the operation. Defaults to &quot;range&quot;.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An 1-D <cite>Tensor</cite> of type <cite>dtype</cite>.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.arange
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.rank">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">rank</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#rank"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the rank of a tensor.</p>
<p>Returns a 0-D <cite>int32</cite> <cite>Tensor</cite> representing the rank of <cite>input</cite>.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">shape</span> <span class="pre">of</span> <span class="pre">tensor</span> <span class="pre">'t'</span> <span class="pre">is</span> <span class="pre">[2,</span> <span class="pre">2,</span> <span class="pre">3]</span>
<span class="pre">t</span> <span class="pre">=</span> <span class="pre">tf.constant([[[1,</span> <span class="pre">1,</span> <span class="pre">1],</span> <span class="pre">[2,</span> <span class="pre">2,</span> <span class="pre">2]],</span> <span class="pre">[[3,</span> <span class="pre">3,</span> <span class="pre">3],</span> <span class="pre">[4,</span> <span class="pre">4,</span> <span class="pre">4]]])</span>
<span class="pre">tf.rank(t)</span>&#160; <span class="pre">#</span> <span class="pre">3</span>
<span class="pre">`</span></code></p>
<p><strong>Note</strong>: The rank of a tensor is not the same as the rank of a matrix. The
rank of a tensor is the number of indices required to uniquely select each
element of the tensor. Rank is also known as &quot;order&quot;, &quot;degree&quot;, or &quot;ndims.&quot;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite> or <cite>SparseTensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>int32</cite>.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.ndim
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.realdiv">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">realdiv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.realdiv" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns x / y element-wise for real types.</p>
<p>If <cite>x</cite> and <cite>y</cite> are reals, this will return the floating-point division.</p>
<p><em>NOTE</em>: <cite>Div</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>uint8</cite>, <cite>int8</cite>, <cite>uint16</cite>, <cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.recompute_grad">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">recompute_grad</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/custom_gradient.html#recompute_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.recompute_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>An eager-compatible version of recompute_grad.</p>
<p>For f(<a href="#id878"><span class="problematic" id="id879">*</span></a>args, <a href="#id880"><span class="problematic" id="id881">**</span></a>kwargs), this supports gradients with respect to args or
kwargs, but kwargs are currently only supported in eager-mode.
Note that for keras layer and model objects, this is handled automatically.</p>
<p>Warning: If <cite>f</cite> was originally a tf.keras Model or Layer object, <cite>g</cite> will not
be able to access the member variables of that object, because <cite>g</cite> returns
through the wrapper function <cite>inner</cite>.  When recomputing gradients through
objects that inherit from keras, we suggest keeping a reference to the
underlying object around for the purpose of accessing these variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>f</strong> -- function <cite>f(*x)</cite> that returns a <cite>Tensor</cite> or sequence of <cite>Tensor</cite> outputs.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function <cite>g</cite> that wraps <cite>f</cite>, but which recomputes <cite>f</cite> on the backwards
pass of a gradient call.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_all">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_all</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the &quot;logical and&quot; of elements across dimensions of a tensor.</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions
are retained with length 1.</p>
<p>If <cite>axis</cite> is None, all dimensions are reduced, and a
tensor with a single element is returned.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([[True,</span>&#160; <span class="pre">True],</span> <span class="pre">[False,</span> <span class="pre">False]])</span>
<span class="pre">tf.reduce_all(x)</span>&#160; <span class="pre">#</span> <span class="pre">False</span>
<span class="pre">tf.reduce_all(x,</span> <span class="pre">0)</span>&#160; <span class="pre">#</span> <span class="pre">[False,</span> <span class="pre">False]</span>
<span class="pre">tf.reduce_all(x,</span> <span class="pre">1)</span>&#160; <span class="pre">#</span> <span class="pre">[True,</span> <span class="pre">False]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The boolean tensor to reduce.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.all
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_any">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_any</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_any"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_any" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the &quot;logical or&quot; of elements across dimensions of a tensor.</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions
are retained with length 1.</p>
<p>If <cite>axis</cite> is None, all dimensions are reduced, and a
tensor with a single element is returned.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([[True,</span>&#160; <span class="pre">True],</span> <span class="pre">[False,</span> <span class="pre">False]])</span>
<span class="pre">tf.reduce_any(x)</span>&#160; <span class="pre">#</span> <span class="pre">True</span>
<span class="pre">tf.reduce_any(x,</span> <span class="pre">0)</span>&#160; <span class="pre">#</span> <span class="pre">[True,</span> <span class="pre">True]</span>
<span class="pre">tf.reduce_any(x,</span> <span class="pre">1)</span>&#160; <span class="pre">#</span> <span class="pre">[True,</span> <span class="pre">False]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The boolean tensor to reduce.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.any
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_logsumexp">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_logsumexp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_logsumexp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_logsumexp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes log(sum(exp(elements across dimensions of a tensor))).</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions
are retained with length 1.</p>
<p>If <cite>axis</cite> has no entries, all dimensions are reduced, and a
tensor with a single element is returned.</p>
<p>This function is more numerically stable than log(sum(exp(input))). It avoids
overflows caused by taking the exp of large inputs and underflows caused by
taking the log of small inputs.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([[0.,</span> <span class="pre">0.,</span> <span class="pre">0.],</span> <span class="pre">[0.,</span> <span class="pre">0.,</span> <span class="pre">0.]])</span>
<span class="pre">tf.reduce_logsumexp(x)</span>&#160; <span class="pre">#</span> <span class="pre">log(6)</span>
<span class="pre">tf.reduce_logsumexp(x,</span> <span class="pre">0)</span>&#160; <span class="pre">#</span> <span class="pre">[log(2),</span> <span class="pre">log(2),</span> <span class="pre">log(2)]</span>
<span class="pre">tf.reduce_logsumexp(x,</span> <span class="pre">1)</span>&#160; <span class="pre">#</span> <span class="pre">[log(3),</span> <span class="pre">log(3)]</span>
<span class="pre">tf.reduce_logsumexp(x,</span> <span class="pre">1,</span> <span class="pre">keepdims=True)</span>&#160; <span class="pre">#</span> <span class="pre">[[log(3)],</span> <span class="pre">[log(3)]]</span>
<span class="pre">tf.reduce_logsumexp(x,</span> <span class="pre">[0,</span> <span class="pre">1])</span>&#160; <span class="pre">#</span> <span class="pre">log(6)</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The tensor to reduce. Should have numeric type.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_max">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_max</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_max" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the maximum of elements across dimensions of a tensor.</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions
are retained with length 1.</p>
<p>If <cite>axis</cite> is None, all dimensions are reduced, and a
tensor with a single element is returned.</p>
<p>Usage example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="go">tf.Tensor(5, shape=(), dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="go">tf.Tensor(-1, shape=(), dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="go">tf.Tensor(4.0, shape=(), dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="go">tf.Tensor(-inf, shape=(), dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="go">tf.Tensor(inf, shape=(), dtype=float32)</span>
</pre></div>
</div>
<p>See the numpy docs for <cite>np.amax</cite> and <cite>np.nanmax</cite> behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The tensor to reduce. Should have real numeric type.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_mean">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_mean</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the mean of elements across dimensions of a tensor.</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite> by computing the
mean of elements across the dimensions in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions are retained
with length 1.</p>
<p>If <cite>axis</cite> is None, all dimensions are reduced, and a tensor with a single
element is returned.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.5&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The tensor to reduce. Should have numeric type.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.mean</p>
<p>Please note that <cite>np.mean</cite> has a <cite>dtype</cite> parameter that could be used to
specify the output type. By default this is <cite>dtype=float64</cite>. On the other
hand, <cite>tf.reduce_mean</cite> has an aggressive type inference from <cite>input_tensor</cite>,
for example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5&gt;</span>
</pre></div>
</div>
<p>&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_min">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_min</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_min"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_min" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the minimum of elements across dimensions of a tensor.</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions
are retained with length 1.</p>
<p>If <cite>axis</cite> is None, all dimensions are reduced, and a
tensor with a single element is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The tensor to reduce. Should have real numeric type.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor.</p>
</dd>
</dl>
<dl>
<dt>For example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;</span>
</pre></div>
</div>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.min
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_prod">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_prod</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_prod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_prod" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the product of elements across dimensions of a tensor.</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions
are retained with length 1.</p>
<p>If <cite>axis</cite> is None, all dimensions are reduced, and a
tensor with a single element is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The tensor to reduce. Should have numeric type.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.prod
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reduce_sum">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reduce_sum</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdims</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#reduce_sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reduce_sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sum of elements across dimensions of a tensor.</p>
<p>Reduces <cite>input_tensor</cite> along the dimensions given in <cite>axis</cite>.
Unless <cite>keepdims</cite> is true, the rank of the tensor is reduced by 1 for each
entry in <cite>axis</cite>. If <cite>keepdims</cite> is true, the reduced dimensions
are retained with length 1.</p>
<p>If <cite>axis</cite> is None, all dimensions are reduced, and a
tensor with a single element is returned.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([[1,</span> <span class="pre">1,</span> <span class="pre">1],</span> <span class="pre">[1,</span> <span class="pre">1,</span> <span class="pre">1]])</span>
<span class="pre">tf.reduce_sum(x)</span>&#160; <span class="pre">#</span> <span class="pre">6</span>
<span class="pre">tf.reduce_sum(x,</span> <span class="pre">0)</span>&#160; <span class="pre">#</span> <span class="pre">[2,</span> <span class="pre">2,</span> <span class="pre">2]</span>
<span class="pre">tf.reduce_sum(x,</span> <span class="pre">1)</span>&#160; <span class="pre">#</span> <span class="pre">[3,</span> <span class="pre">3]</span>
<span class="pre">tf.reduce_sum(x,</span> <span class="pre">1,</span> <span class="pre">keepdims=True)</span>&#160; <span class="pre">#</span> <span class="pre">[[3],</span> <span class="pre">[3]]</span>
<span class="pre">tf.reduce_sum(x,</span> <span class="pre">[0,</span> <span class="pre">1])</span>&#160; <span class="pre">#</span> <span class="pre">6</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensor</strong> -- The tensor to reduce. Should have numeric type.</p></li>
<li><p><strong>axis</strong> -- The dimensions to reduce. If <cite>None</cite> (the default), reduces all
dimensions. Must be in the range <cite>[-rank(input_tensor),
rank(input_tensor))</cite>.</p></li>
<li><p><strong>keepdims</strong> -- If true, retains reduced dimensions with length 1.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced tensor, of the same dtype as the input_tensor.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to
int64 while tensorflow returns the same dtype as the input.
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.register_tensor_conversion_function">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">register_tensor_conversion_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">base_type</span></em>, <em class="sig-param"><span class="n">conversion_func</span></em>, <em class="sig-param"><span class="n">priority</span><span class="o">=</span><span class="default_value">100</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/framework/tensor_conversion_registry.html#register_tensor_conversion_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.register_tensor_conversion_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a function for converting objects of <cite>base_type</cite> to <cite>Tensor</cite>.</p>
<p>The conversion function must have the following signature:</p>
<dl class="simple">
<dt><a href="#id882"><span class="problematic" id="id883">``</span></a><a href="#id884"><span class="problematic" id="id885">`</span></a>python</dt><dd><dl class="simple">
<dt>def conversion_func(value, dtype=None, name=None, as_ref=False):</dt><dd><p># ...</p>
</dd>
</dl>
</dd>
</dl>
<p><a href="#id886"><span class="problematic" id="id887">``</span></a><a href="#id888"><span class="problematic" id="id889">`</span></a></p>
<p>It must return a <cite>Tensor</cite> with the given <cite>dtype</cite> if specified. If the
conversion function creates a new <cite>Tensor</cite>, it should use the given
<cite>name</cite> if specified. All exceptions will be propagated to the caller.</p>
<p>The conversion function may return <cite>NotImplemented</cite> for some
inputs. In this case, the conversion process will continue to try
subsequent conversion functions.</p>
<p>If <cite>as_ref</cite> is true, the function must return a <cite>Tensor</cite> reference,
such as a <cite>Variable</cite>.</p>
<p>NOTE: The conversion functions will execute in order of priority,
followed by order of registration. To ensure that a conversion function
<cite>F</cite> runs before another conversion function <cite>G</cite>, ensure that <cite>F</cite> is
registered with a smaller priority than <cite>G</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_type</strong> -- The base type or tuple of base types for all objects that
<cite>conversion_func</cite> accepts.</p></li>
<li><p><strong>conversion_func</strong> -- A function that converts instances of <cite>base_type</cite> to
<cite>Tensor</cite>.</p></li>
<li><p><strong>priority</strong> -- Optional integer that indicates the priority for applying this
conversion function. Conversion functions with smaller priority values run
earlier than conversion functions with larger priority values. Defaults to
100.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> -- If the arguments do not have the appropriate type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.repeat">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">repeat</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">repeats</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#repeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeat elements of <cite>input</cite>.</p>
<p>See also <cite>tf.concat</cite>, <cite>tf.stack</cite>, <cite>tf.tile</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- An <cite>N</cite>-dimensional Tensor.</p></li>
<li><p><strong>repeats</strong> -- An 1-D <cite>int</cite> Tensor. The number of repetitions for each element.
repeats is broadcasted to fit the shape of the given axis. <cite>len(repeats)</cite>
must equal <cite>input.shape[axis]</cite> if axis is not None.</p></li>
<li><p><strong>axis</strong> -- An int. The axis along which to repeat values. By default (axis=None),
use the flattened input array, and return a flat output array.</p></li>
<li><p><strong>name</strong> -- A name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A Tensor which has the same shape as <cite>input</cite>, except along the given axis.</dt><dd><p>If axis is None then the output array is flattened to match the flattened
input array.</p>
</dd>
</dl>
</p>
</dd>
</dl>
<p>Example usage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">repeat</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">],</span> <span class="n">repeats</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(5,), dtype=string,</span>
<span class="go">numpy=array([b&#39;a&#39;, b&#39;a&#39;, b&#39;a&#39;, b&#39;c&#39;, b&#39;c&#39;], dtype=object)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">repeat</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">repeats</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(5, 2), dtype=int32, numpy=</span>
<span class="go">array([[1, 2],</span>
<span class="go">       [1, 2],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [3, 4]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">repeat</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">repeats</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 5), dtype=int32, numpy=</span>
<span class="go">array([[1, 1, 2, 2, 2],</span>
<span class="go">       [3, 3, 4, 4, 4]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">repeat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">repeats</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 3, 3, 3], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">repeat</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]],</span> <span class="n">repeats</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(8,), dtype=int32,</span>
<span class="go">numpy=array([1, 1, 2, 2, 3, 3, 4, 4], dtype=int32)&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.required_space_to_batch_paddings">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">required_space_to_batch_paddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em>, <em class="sig-param"><span class="n">block_shape</span></em>, <em class="sig-param"><span class="n">base_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#required_space_to_batch_paddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.required_space_to_batch_paddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate padding required to make block_shape divide input_shape.</p>
<p>This function can be used to calculate a suitable paddings argument for use
with space_to_batch_nd and batch_to_space_nd.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> -- int32 Tensor of shape [N].</p></li>
<li><p><strong>block_shape</strong> -- int32 Tensor of shape [N].</p></li>
<li><p><strong>base_paddings</strong> -- Optional int32 Tensor of shape [N, 2].  Specifies the minimum
amount of padding to use.  All elements must be &gt;= 0.  If not specified,
defaults to 0.</p></li>
<li><p><strong>name</strong> -- string.  Optional name prefix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><cite>paddings</cite> and <cite>crops</cite> are int32 Tensors of rank 2 and shape [N, 2]
satisfying:</p>
<blockquote>
<div><p>paddings[i, 0] = base_paddings[i, 0].
0 &lt;= paddings[i, 1] - base_paddings[i, 1] &lt; block_shape[i]
(input_shape[i] + paddings[i, 0] + paddings[i, 1]) % block_shape[i] == 0</p>
<p>crops[i, 0] = 0
crops[i, 1] = paddings[i, 1] - base_paddings[i, 1]</p>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(paddings, crops), where</p>
</dd>
</dl>
<p>Raises: ValueError if called with incompatible shapes.</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reshape">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reshape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshapes a tensor.</p>
<p>Given <cite>tensor</cite>, this operation returns a new <cite>tf.Tensor</cite> that has the same
values as <cite>tensor</cite> in the same order, except with a new shape given by
<cite>shape</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>      <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[2 3]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span>
<span class="go">&lt;tf.Tensor: shape=(6,), dtype=int32,</span>
<span class="go">  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="go">  array([[1, 2],</span>
<span class="go">         [3, 4],</span>
<span class="go">         [5, 6]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>The <cite>tf.reshape</cite> does not change the order of or the total number of elements
in the tensor, and so it can reuse the underlying data buffer. This makes it
a fast operation independent of how big of a tensor it is operating on.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">InvalidArgumentError</span>: <span class="n">Input to reshape is a tensor with 3 values, but the</span>
<span class="go">requested shape has 4</span>
</pre></div>
</div>
<p>To instead reorder the data to rearrange the dimensions of a tensor, see
<cite>tf.transpose</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([[1, 2],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [5, 6]], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([[1, 4],</span>
<span class="go">       [2, 5],</span>
<span class="go">       [3, 6]], dtype=int32)</span>
</pre></div>
</div>
<p>If one component of <cite>shape</cite> is the special value -1, the size of that
dimension is computed so that the total size remains constant.  In particular,
a <cite>shape</cite> of <cite>[-1]</cite> flattens into 1-D.  At most one component of <cite>shape</cite> can
be -1.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(6,), dtype=int32,</span>
<span class="go">  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="go">  array([[1, 2],</span>
<span class="go">         [3, 4],</span>
<span class="go">         [5, 6]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="go">  array([[1, 2],</span>
<span class="go">         [3, 4],</span>
<span class="go">         [5, 6]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p><cite>tf.reshape(t, [])</cite> reshapes a tensor <cite>t</cite> with one element to a scalar.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">7</span><span class="p">],</span> <span class="p">[])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">7</span>
</pre></div>
</div>
<p>More examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[9]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=</span>
<span class="go">  array([[1, 2, 3],</span>
<span class="go">         [4, 5, 6],</span>
<span class="go">         [7, 8, 9]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
<span class="gp">... </span>     <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=</span>
<span class="go">  array([[1, 1, 2, 2],</span>
<span class="go">         [3, 3, 4, 4]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>      <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
<span class="gp">... </span>     <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>      <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
<span class="gp">... </span>     <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>      <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[3 2 3]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Pass &#39;[-1]&#39; to flatten &#39;t&#39;.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(18,), dtype=int32,</span>
<span class="go">  numpy=array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],</span>
<span class="go">  dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># -- Using -1 to infer the shape --</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Here -1 is inferred to be 9:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 9), dtype=int32, numpy=</span>
<span class="go">  array([[1, 1, 1, 2, 2, 2, 3, 3, 3],</span>
<span class="go">         [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># -1 is inferred to be 2:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 9), dtype=int32, numpy=</span>
<span class="go">  array([[1, 1, 1, 2, 2, 2, 3, 3, 3],</span>
<span class="go">         [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># -1 is inferred to be 3:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=</span>
<span class="go">  array([[[1, 1, 1],</span>
<span class="go">          [2, 2, 2],</span>
<span class="go">          [3, 3, 3]],</span>
<span class="go">         [[4, 4, 4],</span>
<span class="go">          [5, 5, 5],</span>
<span class="go">          [6, 6, 6]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>shape</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Defines the shape of the output tensor.</p></li>
<li><p><strong>name</strong> -- Optional string. A name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reverse">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reverse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">axis</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#reverse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Reverses specific dimensions of a tensor.</p>
<p>NOTE <cite>tf.reverse</cite> has now changed behavior in preparation for 1.0.
<cite>tf.reverse_v2</cite> is currently an alias that will be deprecated before TF 1.0.</p>
<p>Given a <cite>tensor</cite>, and a <cite>int32</cite> tensor <cite>axis</cite> representing the set of
dimensions of <cite>tensor</cite> to reverse. This operation reverses each dimension
<cite>i</cite> for which there exists <cite>j</cite> s.t. <cite>axis[j] == i</cite>.</p>
<p><cite>tensor</cite> can have up to 8 dimensions. The number of dimensions specified
in <cite>axis</cite> may be 0 or more entries. If an index is specified more than
once, a InvalidArgument error is raised.</p>
<p>For example:</p>
<p><a href="#id890"><span class="problematic" id="id891">``</span></a>`
# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]</p>
<p># 'dims' is [3] or 'dims' is [-1]
reverse(t, dims) ==&gt; [[[[ 3,  2,  1,  0],</p>
<blockquote>
<div><blockquote>
<div><p>[ 7,  6,  5,  4],
[ 11, 10, 9, 8]],</p>
</div></blockquote>
<dl class="simple">
<dt>[[15, 14, 13, 12],</dt><dd><p>[19, 18, 17, 16],
[23, 22, 21, 20]]]]</p>
</dd>
</dl>
</div></blockquote>
<p># 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==&gt; [[[[12, 13, 14, 15],</p>
<blockquote>
<div><blockquote>
<div><p>[16, 17, 18, 19],
[20, 21, 22, 23]</p>
</div></blockquote>
<dl class="simple">
<dt>[[ 0,  1,  2,  3],</dt><dd><p>[ 4,  5,  6,  7],
[ 8,  9, 10, 11]]]]</p>
</dd>
</dl>
</div></blockquote>
<p># 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==&gt; [[[[8, 9, 10, 11],</p>
<blockquote>
<div><blockquote>
<div><p>[4, 5, 6, 7],
[0, 1, 2, 3]]</p>
</div></blockquote>
<dl class="simple">
<dt>[[20, 21, 22, 23],</dt><dd><p>[16, 17, 18, 19],
[12, 13, 14, 15]]]]</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id892"><span class="problematic" id="id893">``</span></a><a href="#id894"><span class="problematic" id="id895">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>uint8</cite>, <cite>int8</cite>, <cite>uint16</cite>, <cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>bool</cite>, <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>, <cite>string</cite>.
Up to 8-D.</p></li>
<li><p><strong>axis</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
1-D. The indices of the dimensions to reverse. Must be in the range
<cite>[-rank(tensor), rank(tensor))</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.reverse_sequence">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">reverse_sequence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">seq_lengths</span></em>, <em class="sig-param"><span class="n">seq_axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#reverse_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.reverse_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Reverses variable length slices. (deprecated arguments) (deprecated arguments)</p>
<p>Warning: SOME ARGUMENTS ARE DEPRECATED: <cite>(seq_dim)</cite>. They will be removed in a future version.
Instructions for updating:
seq_dim is deprecated, use seq_axis instead</p>
<p>Warning: SOME ARGUMENTS ARE DEPRECATED: <cite>(batch_dim)</cite>. They will be removed in a future version.
Instructions for updating:
batch_dim is deprecated, use batch_axis instead</p>
<p>This op first slices <cite>input</cite> along the dimension <cite>batch_axis</cite>, and for
each slice <cite>i</cite>, reverses the first <cite>seq_lengths[i]</cite> elements along the
dimension <cite>seq_axis</cite>.</p>
<p>The elements of <cite>seq_lengths</cite> must obey <cite>seq_lengths[i] &lt;=
input.dims[seq_dim]</cite>, and <cite>seq_lengths</cite> must be a vector of length
<cite>input.dims[batch_dim]</cite>.</p>
<p>The output slice <cite>i</cite> along dimension <cite>batch_axis</cite> is then given by
input slice <cite>i</cite>, with the first <cite>seq_lengths[i]</cite> slices along
dimension <cite>seq_axis</cite> reversed.</p>
<p>Example usage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">seq_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">seq_lengths</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">&lt;tf.Tensor: shape=(4, 8), dtype=int32, numpy=</span>
<span class="go">array([[0, 0, 5, 4, 3, 2, 1, 0],</span>
<span class="go">       [2, 1, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [3, 2, 1, 4, 0, 0, 0, 0],</span>
<span class="go">       [5, 4, 3, 2, 1, 6, 7, 8]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. The input to reverse.</p></li>
<li><p><strong>seq_lengths</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>,
<cite>int64</cite>. 1-D with length <cite>input.dims(batch_dim)</cite> and <cite>max(seq_lengths) &lt;=
input.dims(seq_dim)</cite></p></li>
<li><p><strong>seq_axis</strong> -- An <cite>int</cite>. The dimension which is partially reversed.</p></li>
<li><p><strong>batch_axis</strong> -- An optional <cite>int</cite>. Defaults to <cite>0</cite>. The dimension along which
reversal is performed.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor. Has the same type as input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.roll">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">roll</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">shift</span></em>, <em class="sig-param"><span class="n">axis</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/manip_ops.html#roll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.roll" title="Permalink to this definition">¶</a></dt>
<dd><p>Rolls the elements of a tensor along an axis.</p>
<p>The elements are shifted positively (towards larger indices) by the offset of
<cite>shift</cite> along the dimension of <cite>axis</cite>. Negative <cite>shift</cite> values will shift
elements in the opposite direction. Elements that roll passed the last position
will wrap around to the first and vice versa. Multiple shifts along multiple
axes may be specified.</p>
<p>For example:</p>
<p><a href="#id896"><span class="problematic" id="id897">``</span></a>`
# 't' is [0, 1, 2, 3, 4]
roll(t, shift=2, axis=0) ==&gt; [3, 4, 0, 1, 2]</p>
<p># shifting along multiple dimensions
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[1, -2], axis=[0, 1]) ==&gt; [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]</p>
<p># shifting along the same axis multiple times
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[2, -3], axis=[1, 1]) ==&gt; [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
<a href="#id898"><span class="problematic" id="id899">``</span></a><a href="#id900"><span class="problematic" id="id901">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>shift</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Dimension must be 0-D or 1-D. <cite>shift[i]</cite> specifies the number of places by which
elements are shifted positively (towards larger indices) along the dimension
specified by <cite>axis[i]</cite>. Negative shifts will roll the elements in the opposite
direction.</p></li>
<li><p><strong>axis</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Dimension must be 0-D or 1-D. <cite>axis[i]</cite> specifies the dimension that the shift
<cite>shift[i]</cite> should occur. If the same axis is referenced more than once, the
total shift for that axis will be the sum of all the shifts that belong to that
axis.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.round">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">round</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#round"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.round" title="Permalink to this definition">¶</a></dt>
<dd><p>Rounds the values of a tensor to the nearest integer, element-wise.</p>
<p>Rounds half to even.  Also known as bankers rounding. If you want to round
according to the current system rounding mode use tf::cint.
For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([0.9,</span> <span class="pre">2.5,</span> <span class="pre">2.3,</span> <span class="pre">1.5,</span> <span class="pre">-4.5])</span>
<span class="pre">tf.round(x)</span>&#160; <span class="pre">#</span> <span class="pre">[</span> <span class="pre">1.0,</span> <span class="pre">2.0,</span> <span class="pre">2.0,</span> <span class="pre">2.0,</span> <span class="pre">-4.0</span> <span class="pre">]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite> of type <cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, or <cite>int64</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of same shape and type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.saturate_cast">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">saturate_cast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">dtype</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#saturate_cast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.saturate_cast" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a safe saturating cast of <cite>value</cite> to <cite>dtype</cite>.</p>
<p>This function casts the input to <cite>dtype</cite> without applying any scaling.  If
there is a danger that values would over or underflow in the cast, this op
applies the appropriate clamping before the cast.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>dtype</strong> -- The desired output <cite>DType</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>value</cite> safely cast to <cite>dtype</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.scalar_mul">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">scalar_mul</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">scalar</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#scalar_mul"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.scalar_mul" title="Permalink to this definition">¶</a></dt>
<dd><p>Multiplies a scalar times a <cite>Tensor</cite> or <cite>IndexedSlices</cite> object.</p>
<p>Intended for use in gradient code which might deal with <cite>IndexedSlices</cite>
objects, which are easy to multiply by a scalar but more expensive to
multiply with arbitrary tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scalar</strong> -- A 0-D scalar <cite>Tensor</cite>. Must have known shape.</p></li>
<li><p><strong>x</strong> -- A <cite>Tensor</cite> or <cite>IndexedSlices</cite> to be scaled.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>scalar * x</cite> of the same type (<cite>Tensor</cite> or <cite>IndexedSlices</cite>) as <cite>x</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- if scalar is not a 0-D <cite>scalar</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.scan">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">scan</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em>, <em class="sig-param"><span class="n">elems</span></em>, <em class="sig-param"><span class="n">initializer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">parallel_iterations</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">back_prop</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">swap_memory</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">infer_shape</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">reverse</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/functional_ops.html#scan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.scan" title="Permalink to this definition">¶</a></dt>
<dd><p>scan on the list of tensors unpacked from <cite>elems</cite> on dimension 0. (deprecated argument values)</p>
<p>Warning: SOME ARGUMENT VALUES ARE DEPRECATED: <cite>(back_prop=False)</cite>. They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.scan(fn, elems, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.scan(fn, elems))</p>
<p>The simplest version of <cite>scan</cite> repeatedly applies the callable <cite>fn</cite> to a
sequence of elements from first to last. The elements are made of the tensors
unpacked from <cite>elems</cite> on dimension 0. The callable fn takes two tensors as
arguments. The first argument is the accumulated value computed from the
preceding invocation of fn, and the second is the value at the current
position of <cite>elems</cite>. If <cite>initializer</cite> is None, <cite>elems</cite> must contain at least
one element, and its first element is used as the initializer.</p>
<p>Suppose that <cite>elems</cite> is unpacked into <cite>values</cite>, a list of tensors. The shape
of the result tensor is <cite>[len(values)] + fn(initializer, values[0]).shape</cite>.
If reverse=True, it's fn(initializer, values[-1]).shape.</p>
<p>This method also allows multi-arity <cite>elems</cite> and accumulator.  If <cite>elems</cite>
is a (possibly nested) list or tuple of tensors, then each of these tensors
must have a matching first (unpack) dimension.  The second argument of
<cite>fn</cite> must match the structure of <cite>elems</cite>.</p>
<p>If no <cite>initializer</cite> is provided, the output structure and dtypes of <cite>fn</cite>
are assumed to be the same as its input; and in this case, the first
argument of <cite>fn</cite> must match the structure of <cite>elems</cite>.</p>
<p>If an <cite>initializer</cite> is provided, then the output of <cite>fn</cite> must have the same
structure as <cite>initializer</cite>; and the first argument of <cite>fn</cite> must match
this structure.</p>
<p>For example, if <cite>elems</cite> is <cite>(t1, [t2, t3])</cite> and <cite>initializer</cite> is
<cite>[i1, i2]</cite> then an appropriate signature for <cite>fn</cite> in <cite>python2</cite> is:
<cite>fn = lambda (acc_p1, acc_p2), (t1, [t2, t3]):</cite> and <cite>fn</cite> must return a list,
<cite>[acc_n1, acc_n2]</cite>.  An alternative correct signature for <cite>fn</cite>, and the</p>
<blockquote>
<div><p>one that works in <cite>python3</cite>, is:</p>
</div></blockquote>
<p><cite>fn = lambda a, t:</cite>, where <cite>a</cite> and <cite>t</cite> correspond to the input tuples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- The callable to be performed.  It accepts two arguments.  The first will
have the same structure as <cite>initializer</cite> if one is provided, otherwise it
will have the same structure as <cite>elems</cite>.  The second will have the same
(possibly nested) structure as <cite>elems</cite>.  Its output must have the same
structure as <cite>initializer</cite> if one is provided, otherwise it must have the
same structure as <cite>elems</cite>.</p></li>
<li><p><strong>elems</strong> -- A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension.  The nested sequence of the
resulting slices will be the first argument to <cite>fn</cite>.</p></li>
<li><p><strong>initializer</strong> -- (optional) A tensor or (possibly nested) sequence of tensors,
initial value for the accumulator, and the expected output type of <cite>fn</cite>.</p></li>
<li><p><strong>parallel_iterations</strong> -- (optional) The number of iterations allowed to run in
parallel.</p></li>
<li><p><strong>back_prop</strong> -- (optional) Deprecated. False disables support for back
propagation. Prefer using <cite>tf.stop_gradient</cite> instead.</p></li>
<li><p><strong>swap_memory</strong> -- (optional) True enables GPU-CPU memory swapping.</p></li>
<li><p><strong>infer_shape</strong> -- (optional) False disables tests for consistent output shapes.</p></li>
<li><p><strong>reverse</strong> -- (optional) True scans the tensor last to first (instead of first to
last).</p></li>
<li><p><strong>name</strong> -- (optional) Name prefix for the returned tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor or (possibly nested) sequence of tensors.  Each tensor packs the
results of applying <cite>fn</cite> to tensors unpacked from <cite>elems</cite> along the first
dimension, and the previous accumulator value(s), from first to last (or
last to first, if <cite>reverse=True</cite>).</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- if <cite>fn</cite> is not callable or the structure of the output of
    <cite>fn</cite> and <cite>initializer</cite> do not match.</p></li>
<li><p><strong>ValueError</strong> -- if the lengths of the output of <cite>fn</cite> and <cite>initializer</cite>
    do not match.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">elems</span> <span class="pre">=</span> <span class="pre">np.array([1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6])</span>
<span class="pre">sum</span> <span class="pre">=</span> <span class="pre">scan(lambda</span> <span class="pre">a,</span> <span class="pre">x:</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">x,</span> <span class="pre">elems)</span>
<span class="pre">#</span> <span class="pre">sum</span> <span class="pre">==</span> <span class="pre">[1,</span> <span class="pre">3,</span> <span class="pre">6,</span> <span class="pre">10,</span> <span class="pre">15,</span> <span class="pre">21]</span>
<span class="pre">sum</span> <span class="pre">=</span> <span class="pre">scan(lambda</span> <span class="pre">a,</span> <span class="pre">x:</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">x,</span> <span class="pre">elems,</span> <span class="pre">reverse=True)</span>
<span class="pre">#</span> <span class="pre">sum</span> <span class="pre">==</span> <span class="pre">[21,</span> <span class="pre">20,</span> <span class="pre">18,</span> <span class="pre">15,</span> <span class="pre">11,</span> <span class="pre">6]</span>
<span class="pre">`</span></code></p>
<p><a href="#id902"><span class="problematic" id="id903">``</span></a><a href="#id904"><span class="problematic" id="id905">`</span></a>python
elems = np.array([1, 2, 3, 4, 5, 6])
initializer = np.array(0)
sum_one = scan(</p>
<blockquote>
<div><p>lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)</p>
</div></blockquote>
<p># sum_one == [1, 2, 3, 4, 5, 6]
<a href="#id906"><span class="problematic" id="id907">``</span></a><a href="#id908"><span class="problematic" id="id909">`</span></a></p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">elems</span> <span class="pre">=</span> <span class="pre">np.array([1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0])</span>
<span class="pre">initializer</span> <span class="pre">=</span> <span class="pre">(np.array(0),</span> <span class="pre">np.array(1))</span>
<span class="pre">fibonaccis</span> <span class="pre">=</span> <span class="pre">scan(lambda</span> <span class="pre">a,</span> <span class="pre">_:</span> <span class="pre">(a[1],</span> <span class="pre">a[0]</span> <span class="pre">+</span> <span class="pre">a[1]),</span> <span class="pre">elems,</span> <span class="pre">initializer)</span>
<span class="pre">#</span> <span class="pre">fibonaccis</span> <span class="pre">==</span> <span class="pre">([1,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">8],</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">8,</span> <span class="pre">13])</span>
<span class="pre">`</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.scatter_nd">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">scatter_nd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">updates</span></em>, <em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#scatter_nd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.scatter_nd" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatter <cite>updates</cite> into a new tensor according to <cite>indices</cite>.</p>
<p>Creates a new tensor by applying sparse <cite>updates</cite> to individual values or
slices within a tensor (initially zero for numeric, empty for string) of
the given <cite>shape</cite> according to indices.  This operator is the inverse of the
<cite>tf.gather_nd</cite> operator which extracts values or slices from a given tensor.</p>
<p>This operation is similar to tensor_scatter_add, except that the tensor is
zero-initialized. Calling <cite>tf.scatter_nd(indices, values, shape)</cite> is identical
to <cite>tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values)</cite></p>
<p>If <cite>indices</cite> contains duplicates, then their updates are accumulated (summed).</p>
<p><strong>WARNING</strong>: The order in which updates are applied is nondeterministic, so the
output will be nondeterministic if <cite>indices</cite> contains duplicates -- because
of some numerical approximation issues, numbers summed in different order
may yield different results.</p>
<p><cite>indices</cite> is an integer tensor containing indices into a new tensor of shape
<cite>shape</cite>.  The last dimension of <cite>indices</cite> can be at most the rank of <cite>shape</cite>:</p>
<blockquote>
<div><p>indices.shape[-1] &lt;= shape.rank</p>
</div></blockquote>
<p>The last dimension of <cite>indices</cite> corresponds to indices into elements
(if <cite>indices.shape[-1] = shape.rank</cite>) or slices
(if <cite>indices.shape[-1] &lt; shape.rank</cite>) along dimension <cite>indices.shape[-1]</cite> of
<cite>shape</cite>.  <cite>updates</cite> is a tensor with shape</p>
<blockquote>
<div><p>indices.shape[:-1] + shape[indices.shape[-1]:]</p>
</div></blockquote>
<p>The simplest form of scatter is to insert individual elements in a tensor by
index. For example, say we want to insert 4 scattered elements in a rank-1
tensor with 8 elements.</p>
<p>&lt;div style=&quot;width:70%; margin:auto; margin-bottom:10px; margin-top:20px;&quot;&gt;
&lt;img style=&quot;width:100%&quot; src=&quot;https://www.tensorflow.org/images/ScatterNd1.png&quot; alt&gt;
&lt;/div&gt;</p>
<p>In Python, this scatter operation would look like this:</p>
<dl class="simple">
<dt><a href="#id910"><span class="problematic" id="id911">``</span></a><a href="#id912"><span class="problematic" id="id913">`</span></a>python</dt><dd><p>indices = tf.constant([[4], [3], [1], [7]])
updates = tf.constant([9, 10, 11, 12])
shape = tf.constant([8])
scatter = tf.scatter_nd(indices, updates, shape)
print(scatter)</p>
</dd>
</dl>
<p><a href="#id914"><span class="problematic" id="id915">``</span></a><a href="#id916"><span class="problematic" id="id917">`</span></a></p>
<p>The resulting tensor would look like this:</p>
<blockquote>
<div><p>[0, 11, 0, 10, 9, 0, 0, 12]</p>
</div></blockquote>
<p>We can also, insert entire slices of a higher rank tensor all at once. For
example, if we wanted to insert two slices in the first dimension of a
rank-3 tensor with two matrices of new values.</p>
<p>&lt;div style=&quot;width:70%; margin:auto; margin-bottom:10px; margin-top:20px;&quot;&gt;
&lt;img style=&quot;width:100%&quot; src=&quot;https://www.tensorflow.org/images/ScatterNd2.png&quot; alt&gt;
&lt;/div&gt;</p>
<p>In Python, this scatter operation would look like this:</p>
<dl>
<dt><a href="#id918"><span class="problematic" id="id919">``</span></a><a href="#id920"><span class="problematic" id="id921">`</span></a>python</dt><dd><p>indices = tf.constant([[0], [2]])
updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],</p>
<blockquote>
<div><blockquote>
<div><p>[7, 7, 7, 7], [8, 8, 8, 8]],</p>
</div></blockquote>
<dl class="simple">
<dt>[[5, 5, 5, 5], [6, 6, 6, 6],</dt><dd><p>[7, 7, 7, 7], [8, 8, 8, 8]]])</p>
</dd>
</dl>
</div></blockquote>
<p>shape = tf.constant([4, 4, 4])
scatter = tf.scatter_nd(indices, updates, shape)
print(scatter)</p>
</dd>
</dl>
<p><a href="#id922"><span class="problematic" id="id923">``</span></a><a href="#id924"><span class="problematic" id="id925">`</span></a></p>
<p>The resulting tensor would look like this:</p>
<blockquote>
<div><dl class="simple">
<dt>[[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],</dt><dd><p>[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]</p>
</dd>
</dl>
</div></blockquote>
<p>Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, the index is ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Index tensor.</p></li>
<li><p><strong>updates</strong> -- A <cite>Tensor</cite>. Updates to scatter into output.</p></li>
<li><p><strong>shape</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>indices</cite>.
1-D. The shape of the resulting tensor.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>updates</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.searchsorted">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">searchsorted</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sorted_sequence</span></em>, <em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">side</span><span class="o">=</span><span class="default_value">'left'</span></em>, <em class="sig-param"><span class="n">out_type</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#searchsorted"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.searchsorted" title="Permalink to this definition">¶</a></dt>
<dd><p>Searches input tensor for values on the innermost dimension.</p>
<p>A 2-D example:</p>
<dl>
<dt><a href="#id926"><span class="problematic" id="id927">``</span></a><a href="#id928"><span class="problematic" id="id929">`</span></a></dt><dd><dl class="simple">
<dt>sorted_sequence = [[0, 3, 9, 9, 10],</dt><dd><p>[1, 2, 3, 4, 5]]</p>
</dd>
<dt>values = [[2, 4, 9],</dt><dd><p>[0, 2, 6]]</p>
</dd>
</dl>
<p>result = searchsorted(sorted_sequence, values, side=&quot;left&quot;)</p>
<dl class="simple">
<dt>result == [[1, 2, 2],</dt><dd><p>[0, 1, 5]]</p>
</dd>
</dl>
<p>result = searchsorted(sorted_sequence, values, side=&quot;right&quot;)</p>
<dl class="simple">
<dt>result == [[1, 2, 4],</dt><dd><p>[0, 2, 5]]</p>
</dd>
</dl>
</dd>
</dl>
<p><a href="#id930"><span class="problematic" id="id931">``</span></a><a href="#id932"><span class="problematic" id="id933">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sorted_sequence</strong> -- N-D <cite>Tensor</cite> containing a sorted sequence.</p></li>
<li><p><strong>values</strong> -- N-D <cite>Tensor</cite> containing the search values.</p></li>
<li><p><strong>side</strong> -- 'left' or 'right'; 'left' corresponds to lower_bound and 'right' to
upper_bound.</p></li>
<li><p><strong>out_type</strong> -- The output type (<cite>int32</cite> or <cite>int64</cite>).  Default is <cite>tf.int32</cite>.</p></li>
<li><p><strong>name</strong> -- Optional name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An N-D <cite>Tensor</cite> the size of values containing the result of applying either
lower_bound or upper_bound (depending on side) to each value.  The result
is not a global index to the entire <cite>Tensor</cite>, but the index in the last
dimension.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If the last dimension of <cite>sorted_sequence &gt;= 2^31-1</cite> elements.
    If the total size of values exceeds <cite>2^31 - 1</cite> elements.
    If the first <cite>N-1</cite> dimensions of the two tensors don't match.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.sequence_mask">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">sequence_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lengths</span></em>, <em class="sig-param"><span class="n">maxlen</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.bool</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#sequence_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.sequence_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a mask tensor representing the first N positions of each cell.</p>
<p>If <cite>lengths</cite> has shape <cite>[d_1, d_2, ..., d_n]</cite> the resulting tensor <cite>mask</cite> has
dtype <cite>dtype</cite> and shape <cite>[d_1, d_2, ..., d_n, maxlen]</cite>, with</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">mask[i_1,</span> <span class="pre">i_2,</span> <span class="pre">...,</span> <span class="pre">i_n,</span> <span class="pre">j]</span> <span class="pre">=</span> <span class="pre">(j</span> <span class="pre">&lt;</span> <span class="pre">lengths[i_1,</span> <span class="pre">i_2,</span> <span class="pre">...,</span> <span class="pre">i_n])</span>
<span class="pre">`</span></code></p>
<p>Examples:</p>
<p><a href="#id934"><span class="problematic" id="id935">``</span></a><a href="#id936"><span class="problematic" id="id937">`</span></a>python
tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],</p>
<blockquote>
<div><p>#  [True, True, True, False, False],
#  [True, True, False, False, False]]</p>
</div></blockquote>
<dl class="simple">
<dt>tf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],</dt><dd><p>#   [True, True, True]],
#  [[True, True, False],
#   [False, False, False]]]</p>
</dd>
</dl>
<p><a href="#id938"><span class="problematic" id="id939">``</span></a><a href="#id940"><span class="problematic" id="id941">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lengths</strong> -- integer tensor, all its values &lt;= maxlen.</p></li>
<li><p><strong>maxlen</strong> -- scalar integer tensor, size of last dimension of returned tensor.
Default is the maximum value in <cite>lengths</cite>.</p></li>
<li><p><strong>dtype</strong> -- output type of the resulting tensor.</p></li>
<li><p><strong>name</strong> -- name of the op.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A mask tensor of shape <cite>lengths.shape + (maxlen,)</cite>, cast to specified dtype.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- if <cite>maxlen</cite> is not a scalar.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.shape">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">out_type</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the shape of a tensor.</p>
<p>See also <cite>tf.size</cite>.</p>
<p>This operation returns a 1-D integer tensor representing the shape of <cite>input</cite>.
This represents the minimal set of known information at definition time.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 2, 3], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">array([2, 2, 3], dtype=int32)</span>
</pre></div>
</div>
<p>Note: When using symbolic tensors, such as when using the Keras functional
API, tf.shape() will return the shape of the symbolic tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor ... shape=(3,) dtype=int32&gt;</span>
</pre></div>
</div>
<p>In these cases, using <cite>tf.Tensor.shape</cite> will return more informative results.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="go">TensorShape([None, None, 10])</span>
</pre></div>
</div>
<p><cite>tf.shape</cite> and <cite>Tensor.shape</cite> should be identical in eager mode.  Within
<cite>tf.function</cite> or within a <cite>compat.v1</cite> context, not all dimensions may be
known until execution time.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite> or <cite>SparseTensor</cite>.</p></li>
<li><p><strong>out_type</strong> -- (Optional) The specified output type of the operation (<cite>int32</cite> or
<cite>int64</cite>). Defaults to <cite>tf.int32</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>out_type</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.shape_n">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">shape_n</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">out_type</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#shape_n"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.shape_n" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns shape of tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A list of at least 1 <cite>Tensor</cite> object with the same type.</p></li>
<li><p><strong>out_type</strong> -- The specified output type of the operation (<cite>int32</cite> or <cite>int64</cite>).
Defaults to <a href="#id942"><span class="problematic" id="id943">`</span></a>tf.int32`(optional).</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A list with the same length as <cite>input</cite> of <cite>Tensor</cite> objects with</dt><dd><p>type <cite>out_type</cite>.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.sigmoid">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">sigmoid</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sigmoid of <cite>x</cite> element-wise.</p>
<p>Formula for calculating sigmoid(x): <cite>y = 1 / (1 + exp(-x))</cite>.</p>
<p>For x in (-inf, inf) =&gt; sigmoid(x) in (0, 1)</p>
<p>Example Usage:</p>
<p>If a positive number is large, then its sigmoid will approach to 1 since the
formula will be <cite>y = &lt;large_num&gt; / (1 + &lt;large_num&gt;)</cite></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">50.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=float32,</span>
<span class="go">numpy=array([0.5      , 0.7310586, 1.       , 1.       ], dtype=float32)&gt;</span>
</pre></div>
</div>
<p>If a negative number is large, its sigmoid will approach to 0 since the
formula will be <cite>y = 1 / (1 + &lt;large_num&gt;)</cite></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="o">-</span><span class="mf">100.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=</span>
<span class="go">array([0.0000000e+00, 1.9287499e-22, 2.6894143e-01, 0.5],</span>
<span class="go">      dtype=float32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A Tensor with type <cite>float16</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, or
<cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor with the same type as <cite>x</cite>.</p>
</dd>
</dl>
<p>Usage Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="o">-</span><span class="mf">128.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">128.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3,), dtype=float32,</span>
<span class="go">numpy=array([0. , 0.5, 1. ], dtype=float32)&gt;</span>
</pre></div>
</div>
<p>&#64;compatibility(scipy)
Equivalent to scipy.special.expit
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.sign">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">sign</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#sign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an element-wise indication of the sign of a number.</p>
<p>y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0.</p>
<p>For complex numbers, y = sign(x) = x / <a href="#id1098"><span class="problematic" id="id1099">|x|</span></a> if x != 0, otherwise y = 0.</p>
<p>Example usage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sign</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: ... numpy=array([ 0.,  1., -1.], dtype=float32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A Tensor. Must be one of the following types: bfloat16, half, float32,
float64, int32, int64, complex64, complex128.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A Tensor. Has the same type as x.</p>
<dl>
<dt>If x is a SparseTensor, returns SparseTensor(x.indices,</dt><dd><blockquote>
<div><p>tf.math.sign(x.values, ...), x.dense_shape).</p>
</div></blockquote>
<p>If <cite>x</cite> is a <cite>SparseTensor</cite>, returns
<cite>SparseTensor(x.indices, tf.math.sign(x.values, ...), x.dense_shape)</cite></p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.sin">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">sin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#sin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sine of x element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes sine of every
element in the tensor. Input range is <cite>(-inf, inf)</cite> and
output range is <cite>[-1,1]</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-9,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">200,</span> <span class="pre">10,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.sin(x)</span> <span class="pre">==&gt;</span> <span class="pre">[nan</span> <span class="pre">-0.4121185</span> <span class="pre">-0.47942555</span> <span class="pre">0.84147096</span> <span class="pre">0.9320391</span> <span class="pre">-0.87329733</span> <span class="pre">-0.54402107</span> <span class="pre">nan]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.sinh">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">sinh</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#sinh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes hyperbolic sine of x element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes hyperbolic sine of every
element in the tensor. Input range is <cite>[-inf,inf]</cite> and output range
is <cite>[-inf,inf]</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-9,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">2,</span> <span class="pre">10,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.sinh(x)</span> <span class="pre">==&gt;</span> <span class="pre">[-inf</span> <span class="pre">-4.0515420e+03</span> <span class="pre">-5.2109528e-01</span> <span class="pre">1.1752012e+00</span> <span class="pre">1.5094614e+00</span> <span class="pre">3.6268604e+00</span> <span class="pre">1.1013232e+04</span> <span class="pre">inf]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.size">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">size</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">out_type</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size of a tensor.</p>
<p>See also <cite>tf.shape</cite>.</p>
<p>Returns a 0-D <cite>Tensor</cite> representing the number of elements in <cite>input</cite>
of type <cite>out_type</cite>. Defaults to tf.int32.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(), dtype=int32, numpy=12&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite> or <cite>SparseTensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
<li><p><strong>out_type</strong> -- (Optional) The specified non-quantized numeric output type of the
operation. Defaults to <cite>tf.int32</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>out_type</cite>. Defaults to <cite>tf.int32</cite>.</p>
</dd>
</dl>
<p>&#64;compatibility(numpy)
Equivalent to np.size()
&#64;end_compatibility</p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.slice">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">slice</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_</span></em>, <em class="sig-param"><span class="n">begin</span></em>, <em class="sig-param"><span class="n">size</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#slice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts a slice from a tensor.</p>
<p>This operation extracts a slice of size <cite>size</cite> from a tensor <cite>input_</cite> starting
at the location specified by <cite>begin</cite>. The slice <cite>size</cite> is represented as a
tensor shape, where <cite>size[i]</cite> is the number of elements of the 'i'th dimension
of <cite>input_</cite> that you want to slice. The starting location (<cite>begin</cite>) for the
slice is represented as an offset in each dimension of <cite>input_</cite>. In other
words, <cite>begin[i]</cite> is the offset into the i'th dimension of <cite>input_</cite> that you
want to slice from.</p>
<p>Note that <cite>tf.Tensor.__getitem__</cite> is typically a more pythonic way to
perform slices, as it allows you to write <cite>foo[3:7, :-2]</cite> instead of
<cite>tf.slice(foo, [3, 0], [4, foo.get_shape()[1]-2])</cite>.</p>
<p><cite>begin</cite> is zero-based; <cite>size</cite> is one-based. If <cite>size[i]</cite> is -1,
all remaining elements in dimension i are included in the
slice. In other words, this is equivalent to setting:</p>
<p><cite>size[i] = input_.dim_size(i) - begin[i]</cite></p>
<p>This operation requires that:</p>
<p><cite>0 &lt;= begin[i] &lt;= begin[i] + size[i] &lt;= Di  for i in [0, n]</cite></p>
<p>For example:</p>
<p><a href="#id944"><span class="problematic" id="id945">``</span></a><a href="#id946"><span class="problematic" id="id947">`</span></a>python
t = tf.constant([[[1, 1, 1], [2, 2, 2]],</p>
<blockquote>
<div><p>[[3, 3, 3], [4, 4, 4]],
[[5, 5, 5], [6, 6, 6]]])</p>
</div></blockquote>
<p>tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]
tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],</p>
<blockquote>
<div><p>#   [4, 4, 4]]]</p>
</div></blockquote>
<dl class="simple">
<dt>tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],</dt><dd><p>#  [[5, 5, 5]]]</p>
</dd>
</dl>
<p><a href="#id948"><span class="problematic" id="id949">``</span></a><a href="#id950"><span class="problematic" id="id951">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>begin</strong> -- An <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite>.</p></li>
<li><p><strong>size</strong> -- An <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> the same type as <cite>input_</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.sort">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">sort</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">direction</span><span class="o">=</span><span class="default_value">'ASCENDING'</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/sort_ops.html#sort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Sorts a tensor.</p>
<p>Usage:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">import</span> <span class="pre">tensorflow</span> <span class="pre">as</span> <span class="pre">tf</span>
<span class="pre">a</span> <span class="pre">=</span> <span class="pre">[1,</span> <span class="pre">10,</span> <span class="pre">26.9,</span> <span class="pre">2.8,</span> <span class="pre">166.32,</span> <span class="pre">62.3]</span>
<span class="pre">b</span> <span class="pre">=</span> <span class="pre">tf.sort(a,axis=-1,direction='ASCENDING',name=None)</span>
<span class="pre">c</span> <span class="pre">=</span> <span class="pre">tf.keras.backend.eval(b)</span>
<span class="pre">#</span> <span class="pre">Here,</span> <span class="pre">c</span> <span class="pre">=</span> <span class="pre">[</span>&#160; <span class="pre">1.</span>&#160;&#160;&#160;&#160; <span class="pre">2.8</span>&#160;&#160; <span class="pre">10.</span>&#160;&#160;&#160; <span class="pre">26.9</span>&#160;&#160; <span class="pre">62.3</span>&#160; <span class="pre">166.32]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- 1-D or higher numeric <cite>Tensor</cite>.</p></li>
<li><p><strong>axis</strong> -- The axis along which to sort. The default is -1, which sorts the last
axis.</p></li>
<li><p><strong>direction</strong> -- The direction in which to sort the values (<cite>'ASCENDING'</cite> or
<cite>'DESCENDING'</cite>).</p></li>
<li><p><strong>name</strong> -- Optional name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <cite>Tensor</cite> with the same dtype and shape as <cite>values</cite>, with the elements</dt><dd><p>sorted along the given <cite>axis</cite>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If axis is not a constant scalar, or the direction is invalid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.space_to_batch">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">space_to_batch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">block_shape</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#space_to_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.space_to_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>SpaceToBatch for N-D tensors of type T.</p>
<p>This operation divides &quot;spatial&quot; dimensions <cite>[1, ..., M]</cite> of the input into a
grid of blocks of shape <cite>block_shape</cite>, and interleaves these blocks with the
&quot;batch&quot; dimension (0) such that in the output, the spatial dimensions
<cite>[1, ..., M]</cite> correspond to the position within the grid, and the batch
dimension combines both the position within a spatial block and the original
batch position.  Prior to division into blocks, the spatial dimensions of the
input are optionally zero padded according to <cite>paddings</cite>.  See below for a
precise description.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.
N-D with shape <cite>input_shape = [batch] + spatial_shape + remaining_shape</cite>,
where spatial_shape has <cite>M</cite> dimensions.</p></li>
<li><p><strong>block_shape</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
1-D with shape <cite>[M]</cite>, all values must be &gt;= 1.</p></li>
<li><p><strong>paddings</strong> -- <p>A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
2-D with shape <cite>[M, 2]</cite>, all values must be &gt;= 0.</p>
<blockquote>
<div><p><cite>paddings[i] = [pad_start, pad_end]</cite> specifies the padding for input dimension
<cite>i + 1</cite>, which corresponds to spatial dimension <cite>i</cite>.  It is required that
<cite>block_shape[i]</cite> divides <cite>input_shape[i + 1] + pad_start + pad_end</cite>.</p>
</div></blockquote>
<p>This operation is equivalent to the following steps:</p>
<ol class="arabic">
<li><p>Zero-pad the start and end of dimensions <cite>[1, ..., M]</cite> of the
input according to <cite>paddings</cite> to produce <cite>padded</cite> of shape <cite>padded_shape</cite>.</p></li>
<li><p>Reshape <cite>padded</cite> to <cite>reshaped_padded</cite> of shape:</p>
<blockquote>
<div><p>[batch] +
[padded_shape[1] / block_shape[0],</p>
<blockquote>
<div><blockquote>
<div><p>block_shape[0],</p>
</div></blockquote>
<p>...,
padded_shape[M] / block_shape[M-1],
block_shape[M-1]] +</p>
</div></blockquote>
<p>remaining_shape</p>
</div></blockquote>
</li>
<li><p>Permute dimensions of <cite>reshaped_padded</cite> to produce
<cite>permuted_reshaped_padded</cite> of shape:</p>
<blockquote>
<div><p>block_shape +
[batch] +
[padded_shape[1] / block_shape[0],</p>
<blockquote>
<div><p>...,
padded_shape[M] / block_shape[M-1]] +</p>
</div></blockquote>
<p>remaining_shape</p>
</div></blockquote>
</li>
<li><p>Reshape <cite>permuted_reshaped_padded</cite> to flatten <cite>block_shape</cite> into the batch
dimension, producing an output tensor of shape:</p>
<blockquote>
<div><p>[batch * prod(block_shape)] +
[padded_shape[1] / block_shape[0],</p>
<blockquote>
<div><p>...,
padded_shape[M] / block_shape[M-1]] +</p>
</div></blockquote>
<p>remaining_shape</p>
</div></blockquote>
</li>
</ol>
<p>Some examples:</p>
<ol class="arabic simple">
<li><p>For the following input of shape <cite>[1, 2, 2, 1]</cite>, <cite>block_shape = [2, 2]</cite>, and
<cite>paddings = [[0, 0], [0, 0]]</cite>:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">[[[[1],</span> <span class="pre">[2]],</span> <span class="pre">[[3],</span> <span class="pre">[4]]]]</span>
<span class="pre">`</span></code></p>
<p>The output tensor has shape <cite>[4, 1, 1, 1]</cite> and value:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">[[[[1]]],</span> <span class="pre">[[[2]]],</span> <span class="pre">[[[3]]],</span> <span class="pre">[[[4]]]]</span>
<span class="pre">`</span></code></p>
<ol class="arabic simple" start="2">
<li><p>For the following input of shape <cite>[1, 2, 2, 3]</cite>, <cite>block_shape = [2, 2]</cite>, and
<cite>paddings = [[0, 0], [0, 0]]</cite>:</p></li>
</ol>
<p><a href="#id952"><span class="problematic" id="id953">``</span></a>`
x = [[[[1, 2, 3], [4, 5, 6]],</p>
<blockquote>
<div><p>[[7, 8, 9], [10, 11, 12]]]]</p>
</div></blockquote>
<p><a href="#id954"><span class="problematic" id="id955">``</span></a><a href="#id956"><span class="problematic" id="id957">`</span></a></p>
<p>The output tensor has shape <cite>[4, 1, 1, 3]</cite> and value:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">[[[[1,</span> <span class="pre">2,</span> <span class="pre">3]]],</span> <span class="pre">[[[4,</span> <span class="pre">5,</span> <span class="pre">6]]],</span> <span class="pre">[[[7,</span> <span class="pre">8,</span> <span class="pre">9]]],</span> <span class="pre">[[[10,</span> <span class="pre">11,</span> <span class="pre">12]]]]</span>
<span class="pre">`</span></code></p>
<ol class="arabic simple" start="3">
<li><p>For the following input of shape <cite>[1, 4, 4, 1]</cite>, <cite>block_shape = [2, 2]</cite>, and
<cite>paddings = [[0, 0], [0, 0]]</cite>:</p></li>
</ol>
<p><a href="#id958"><span class="problematic" id="id959">``</span></a>`
x = [[[[1],   [2],  [3],  [4]],</p>
<blockquote>
<div><p>[[5],   [6],  [7],  [8]],
[[9],  [10], [11],  [12]],
[[13], [14], [15],  [16]]]]</p>
</div></blockquote>
<p><a href="#id960"><span class="problematic" id="id961">``</span></a><a href="#id962"><span class="problematic" id="id963">`</span></a></p>
<p>The output tensor has shape <cite>[4, 2, 2, 1]</cite> and value:</p>
<p><a href="#id964"><span class="problematic" id="id965">``</span></a>`
x = [[[[1], [3]], [[9], [11]]],</p>
<blockquote>
<div><p>[[[2], [4]], [[10], [12]]],
[[[5], [7]], [[13], [15]]],
[[[6], [8]], [[14], [16]]]]</p>
</div></blockquote>
<p><a href="#id966"><span class="problematic" id="id967">``</span></a><a href="#id968"><span class="problematic" id="id969">`</span></a></p>
<ol class="arabic simple" start="4">
<li><p>For the following input of shape <cite>[2, 2, 4, 1]</cite>, block_shape = <cite>[2, 2]</cite>, and
paddings = <cite>[[0, 0], [2, 0]]</cite>:</p></li>
</ol>
<p><a href="#id970"><span class="problematic" id="id971">``</span></a>`
x = [[[[1],   [2],  [3],  [4]],</p>
<blockquote>
<div><blockquote>
<div><p>[[5],   [6],  [7],  [8]]],</p>
</div></blockquote>
<dl class="simple">
<dt>[[[9],  [10], [11],  [12]],</dt><dd><p>[[13], [14], [15],  [16]]]]</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id972"><span class="problematic" id="id973">``</span></a><a href="#id974"><span class="problematic" id="id975">`</span></a></p>
<p>The output tensor has shape <cite>[8, 1, 3, 1]</cite> and value:</p>
<p><a href="#id976"><span class="problematic" id="id977">``</span></a>`
x = [[[[0], [1], [3]]], [[[0], [9], [11]]],</p>
<blockquote>
<div><p>[[[0], [2], [4]]], [[[0], [10], [12]]],
[[[0], [5], [7]]], [[[0], [13], [15]]],
[[[0], [6], [8]]], [[[0], [14], [16]]]]</p>
</div></blockquote>
<p><a href="#id978"><span class="problematic" id="id979">``</span></a><a href="#id980"><span class="problematic" id="id981">`</span></a></p>
<p>Among others, this operation is useful for reducing atrous convolution into
regular convolution.</p>
</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.space_to_batch_nd">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">space_to_batch_nd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">block_shape</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#space_to_batch_nd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.space_to_batch_nd" title="Permalink to this definition">¶</a></dt>
<dd><p>SpaceToBatch for N-D tensors of type T.</p>
<p>This operation divides &quot;spatial&quot; dimensions <cite>[1, ..., M]</cite> of the input into a
grid of blocks of shape <cite>block_shape</cite>, and interleaves these blocks with the
&quot;batch&quot; dimension (0) such that in the output, the spatial dimensions
<cite>[1, ..., M]</cite> correspond to the position within the grid, and the batch
dimension combines both the position within a spatial block and the original
batch position.  Prior to division into blocks, the spatial dimensions of the
input are optionally zero padded according to <cite>paddings</cite>.  See below for a
precise description.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.
N-D with shape <cite>input_shape = [batch] + spatial_shape + remaining_shape</cite>,
where spatial_shape has <cite>M</cite> dimensions.</p></li>
<li><p><strong>block_shape</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
1-D with shape <cite>[M]</cite>, all values must be &gt;= 1.</p></li>
<li><p><strong>paddings</strong> -- <p>A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
2-D with shape <cite>[M, 2]</cite>, all values must be &gt;= 0.</p>
<blockquote>
<div><p><cite>paddings[i] = [pad_start, pad_end]</cite> specifies the padding for input dimension
<cite>i + 1</cite>, which corresponds to spatial dimension <cite>i</cite>.  It is required that
<cite>block_shape[i]</cite> divides <cite>input_shape[i + 1] + pad_start + pad_end</cite>.</p>
</div></blockquote>
<p>This operation is equivalent to the following steps:</p>
<ol class="arabic">
<li><p>Zero-pad the start and end of dimensions <cite>[1, ..., M]</cite> of the
input according to <cite>paddings</cite> to produce <cite>padded</cite> of shape <cite>padded_shape</cite>.</p></li>
<li><p>Reshape <cite>padded</cite> to <cite>reshaped_padded</cite> of shape:</p>
<blockquote>
<div><p>[batch] +
[padded_shape[1] / block_shape[0],</p>
<blockquote>
<div><blockquote>
<div><p>block_shape[0],</p>
</div></blockquote>
<p>...,
padded_shape[M] / block_shape[M-1],
block_shape[M-1]] +</p>
</div></blockquote>
<p>remaining_shape</p>
</div></blockquote>
</li>
<li><p>Permute dimensions of <cite>reshaped_padded</cite> to produce
<cite>permuted_reshaped_padded</cite> of shape:</p>
<blockquote>
<div><p>block_shape +
[batch] +
[padded_shape[1] / block_shape[0],</p>
<blockquote>
<div><p>...,
padded_shape[M] / block_shape[M-1]] +</p>
</div></blockquote>
<p>remaining_shape</p>
</div></blockquote>
</li>
<li><p>Reshape <cite>permuted_reshaped_padded</cite> to flatten <cite>block_shape</cite> into the batch
dimension, producing an output tensor of shape:</p>
<blockquote>
<div><p>[batch * prod(block_shape)] +
[padded_shape[1] / block_shape[0],</p>
<blockquote>
<div><p>...,
padded_shape[M] / block_shape[M-1]] +</p>
</div></blockquote>
<p>remaining_shape</p>
</div></blockquote>
</li>
</ol>
<p>Some examples:</p>
<ol class="arabic simple">
<li><p>For the following input of shape <cite>[1, 2, 2, 1]</cite>, <cite>block_shape = [2, 2]</cite>, and
<cite>paddings = [[0, 0], [0, 0]]</cite>:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">[[[[1],</span> <span class="pre">[2]],</span> <span class="pre">[[3],</span> <span class="pre">[4]]]]</span>
<span class="pre">`</span></code></p>
<p>The output tensor has shape <cite>[4, 1, 1, 1]</cite> and value:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">[[[[1]]],</span> <span class="pre">[[[2]]],</span> <span class="pre">[[[3]]],</span> <span class="pre">[[[4]]]]</span>
<span class="pre">`</span></code></p>
<ol class="arabic simple" start="2">
<li><p>For the following input of shape <cite>[1, 2, 2, 3]</cite>, <cite>block_shape = [2, 2]</cite>, and
<cite>paddings = [[0, 0], [0, 0]]</cite>:</p></li>
</ol>
<p><a href="#id982"><span class="problematic" id="id983">``</span></a>`
x = [[[[1, 2, 3], [4, 5, 6]],</p>
<blockquote>
<div><p>[[7, 8, 9], [10, 11, 12]]]]</p>
</div></blockquote>
<p><a href="#id984"><span class="problematic" id="id985">``</span></a><a href="#id986"><span class="problematic" id="id987">`</span></a></p>
<p>The output tensor has shape <cite>[4, 1, 1, 3]</cite> and value:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">[[[[1,</span> <span class="pre">2,</span> <span class="pre">3]]],</span> <span class="pre">[[[4,</span> <span class="pre">5,</span> <span class="pre">6]]],</span> <span class="pre">[[[7,</span> <span class="pre">8,</span> <span class="pre">9]]],</span> <span class="pre">[[[10,</span> <span class="pre">11,</span> <span class="pre">12]]]]</span>
<span class="pre">`</span></code></p>
<ol class="arabic simple" start="3">
<li><p>For the following input of shape <cite>[1, 4, 4, 1]</cite>, <cite>block_shape = [2, 2]</cite>, and
<cite>paddings = [[0, 0], [0, 0]]</cite>:</p></li>
</ol>
<p><a href="#id988"><span class="problematic" id="id989">``</span></a>`
x = [[[[1],   [2],  [3],  [4]],</p>
<blockquote>
<div><p>[[5],   [6],  [7],  [8]],
[[9],  [10], [11],  [12]],
[[13], [14], [15],  [16]]]]</p>
</div></blockquote>
<p><a href="#id990"><span class="problematic" id="id991">``</span></a><a href="#id992"><span class="problematic" id="id993">`</span></a></p>
<p>The output tensor has shape <cite>[4, 2, 2, 1]</cite> and value:</p>
<p><a href="#id994"><span class="problematic" id="id995">``</span></a>`
x = [[[[1], [3]], [[9], [11]]],</p>
<blockquote>
<div><p>[[[2], [4]], [[10], [12]]],
[[[5], [7]], [[13], [15]]],
[[[6], [8]], [[14], [16]]]]</p>
</div></blockquote>
<p><a href="#id996"><span class="problematic" id="id997">``</span></a><a href="#id998"><span class="problematic" id="id999">`</span></a></p>
<ol class="arabic simple" start="4">
<li><p>For the following input of shape <cite>[2, 2, 4, 1]</cite>, block_shape = <cite>[2, 2]</cite>, and
paddings = <cite>[[0, 0], [2, 0]]</cite>:</p></li>
</ol>
<p><a href="#id1000"><span class="problematic" id="id1001">``</span></a>`
x = [[[[1],   [2],  [3],  [4]],</p>
<blockquote>
<div><blockquote>
<div><p>[[5],   [6],  [7],  [8]]],</p>
</div></blockquote>
<dl class="simple">
<dt>[[[9],  [10], [11],  [12]],</dt><dd><p>[[13], [14], [15],  [16]]]]</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id1002"><span class="problematic" id="id1003">``</span></a><a href="#id1004"><span class="problematic" id="id1005">`</span></a></p>
<p>The output tensor has shape <cite>[8, 1, 3, 1]</cite> and value:</p>
<p><a href="#id1006"><span class="problematic" id="id1007">``</span></a>`
x = [[[[0], [1], [3]]], [[[0], [9], [11]]],</p>
<blockquote>
<div><p>[[[0], [2], [4]]], [[[0], [10], [12]]],
[[[0], [5], [7]]], [[[0], [13], [15]]],
[[[0], [6], [8]]], [[[0], [14], [16]]]]</p>
</div></blockquote>
<p><a href="#id1008"><span class="problematic" id="id1009">``</span></a><a href="#id1010"><span class="problematic" id="id1011">`</span></a></p>
<p>Among others, this operation is useful for reducing atrous convolution into
regular convolution.</p>
</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.split">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">split</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">num_or_size_splits</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">num</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'split'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#split"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits a tensor <cite>value</cite> into a list of sub tensors.</p>
<p>See also <cite>tf.unstack</cite>.</p>
<p>If <cite>num_or_size_splits</cite> is an integer, then <cite>value</cite> is split along the
dimension <cite>axis</cite> into <cite>num_split</cite> smaller tensors. This requires that
<cite>value.shape[axis]</cite> is divisible by <cite>num_split</cite>.</p>
<p>If <cite>num_or_size_splits</cite> is a 1-D Tensor (or list), we call it <cite>size_splits</cite>
and <cite>value</cite> is split into <cite>len(size_splits)</cite> elements. The shape of the <cite>i</cite>-th
element has the same size as the <cite>value</cite> except along dimension <cite>axis</cite> where
the size is <cite>size_splits[i]</cite>.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>Split <cite>x</cite> into 3 tensors along dimension 1
&gt;&gt;&gt; s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)
&gt;&gt;&gt; tf.shape(s0).numpy()
array([ 5, 10], dtype=int32)</p>
<p>Split <cite>x</cite> into 3 tensors with sizes [4, 15, 11] along dimension 1
&gt;&gt;&gt; split0, split1, split2 = tf.split(x, [4, 15, 11], 1)
&gt;&gt;&gt; tf.shape(split0).numpy()
array([5, 4], dtype=int32)
&gt;&gt;&gt; tf.shape(split1).numpy()
array([ 5, 15], dtype=int32)
&gt;&gt;&gt; tf.shape(split2).numpy()
array([ 5, 11], dtype=int32)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> -- The <cite>Tensor</cite> to split.</p></li>
<li><p><strong>num_or_size_splits</strong> -- Either an integer indicating the number of splits along
<cite>axis</cite> or a 1-D integer <cite>Tensor</cite> or Python list containing the sizes of
each output tensor along <cite>axis</cite>. If a scalar, then it must evenly divide
<cite>value.shape[axis]</cite>; otherwise the sum of sizes along the split axis
must match that of the <cite>value</cite>.</p></li>
<li><p><strong>axis</strong> -- An integer or scalar <cite>int32</cite> <cite>Tensor</cite>. The dimension along which to
split. Must be in the range <cite>[-rank(value), rank(value))</cite>. Defaults to 0.</p></li>
<li><p><strong>num</strong> -- Optional, used to specify the number of outputs when it cannot be
inferred from the shape of <cite>size_splits</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>if <cite>num_or_size_splits</cite> is a scalar returns a list of <cite>num_or_size_splits</cite>
<cite>Tensor</cite> objects; if <cite>num_or_size_splits</cite> is a 1-D Tensor returns
<cite>num_or_size_splits.get_shape[0]</cite> <cite>Tensor</cite> objects resulting from splitting
<cite>value</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- If <cite>num</cite> is unspecified and cannot be inferred.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.sqrt">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">sqrt</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#sqrt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes element-wise square root of the input tensor.</p>
<p>Note: This operation does not support integer types.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">16.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=</span>
<span class="go">  array([[2.],</span>
<span class="go">         [4.]], dtype=float32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="o">-</span><span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">16.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=</span>
<span class="go">  array([[nan],</span>
<span class="go">         [ 4.]], dtype=float32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">16.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 1), dtype=complex128, numpy=</span>
<span class="go">  array([[0.0+1.j],</span>
<span class="go">         [4.0+0.j]])&gt;</span>
</pre></div>
</div>
<p>Note: In order to support complex complex, please provide an input tensor
of <cite>complex64</cite> or <cite>complex128</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>tf.Tensor</cite> of type <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>,
<cite>complex64</cite>, <cite>complex128</cite></p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <cite>tf.Tensor</cite> of same size, type and sparsity as <cite>x</cite>.</p>
<p>If <cite>x</cite> is a <cite>SparseTensor</cite>, returns
<cite>SparseTensor(x.indices, tf.math.sqrt(x.values, ...), x.dense_shape)</cite></p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.square">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">square</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#square"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.square" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes square of x element-wise.</p>
<p>I.e., \(y = x * x = x^2\).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">([</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
<p>If <cite>x</cite> is a <cite>SparseTensor</cite>, returns
<cite>SparseTensor(x.indices, tf.math.square(x.values, ...), x.dense_shape)</cite></p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.squeeze">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">squeeze</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#squeeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes dimensions of size 1 from the shape of a tensor.</p>
<p>Given a tensor <cite>input</cite>, this operation returns a tensor of the same type with
all dimensions of size 1 removed. If you don't want to remove all size 1
dimensions, you can remove specific size 1 dimensions by specifying
<cite>axis</cite>.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">'t'</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">tensor</span> <span class="pre">of</span> <span class="pre">shape</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">1,</span> <span class="pre">3,</span> <span class="pre">1,</span> <span class="pre">1]</span>
<span class="pre">tf.shape(tf.squeeze(t))</span>&#160; <span class="pre">#</span> <span class="pre">[2,</span> <span class="pre">3]</span>
<span class="pre">`</span></code></p>
<p>Or, to remove specific size 1 dimensions:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">'t'</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">tensor</span> <span class="pre">of</span> <span class="pre">shape</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">1,</span> <span class="pre">3,</span> <span class="pre">1,</span> <span class="pre">1]</span>
<span class="pre">tf.shape(tf.squeeze(t,</span> <span class="pre">[2,</span> <span class="pre">4]))</span>&#160; <span class="pre">#</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">1]</span>
<span class="pre">`</span></code></p>
<p>Unlike the older op <cite>tf.compat.v1.squeeze</cite>, this op does not accept a
deprecated <cite>squeeze_dims</cite> argument.</p>
<p>Note: if <cite>input</cite> is a <cite>tf.RaggedTensor</cite>, then this operation takes <cite>O(N)</cite>
time, where <cite>N</cite> is the number of elements in the squeezed dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. The <cite>input</cite> to squeeze.</p></li>
<li><p><strong>axis</strong> -- An optional list of <cite>ints</cite>. Defaults to <cite>[]</cite>. If specified, only
squeezes the dimensions listed. The dimension index starts at 0. It is an
error to squeeze a dimension that is not 1. Must be in the range
<cite>[-rank(input), rank(input))</cite>. Must be specified if <cite>input</cite> is a
<cite>RaggedTensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.
Contains the same data as <cite>input</cite>, but has one or more dimensions of
size 1 removed.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> -- The input cannot be converted to a tensor, or the specified
    axis cannot be squeezed.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.stack">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">stack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'stack'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Stacks a list of rank-<cite>R</cite> tensors into one rank-<cite>(R+1)</cite> tensor.</p>
<p>See also <cite>tf.concat</cite>, <cite>tf.tile</cite>, <cite>tf.repeat</cite>.</p>
<p>Packs the list of tensors in <cite>values</cite> into a tensor with rank one higher than
each tensor in <cite>values</cite>, by packing them along the <cite>axis</cite> dimension.
Given a list of length <cite>N</cite> of tensors of shape <cite>(A, B, C)</cite>;</p>
<p>if <cite>axis == 0</cite> then the <cite>output</cite> tensor will have the shape <cite>(N, A, B, C)</cite>.
if <cite>axis == 1</cite> then the <cite>output</cite> tensor will have the shape <cite>(A, N, B, C)</cite>.
Etc.</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="go">array([[1, 4],</span>
<span class="go">       [2, 5],</span>
<span class="go">       [3, 6]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">array([[1, 2, 3],</span>
<span class="go">       [4, 5, 6]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>This is the opposite of unstack.  The numpy equivalent is <cite>np.stack</cite></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">]))</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> -- A list of <cite>Tensor</cite> objects with the same shape and type.</p></li>
<li><p><strong>axis</strong> -- An <cite>int</cite>. The axis to stack along. Defaults to the first dimension.
Negative values wrap around, so the valid range is <cite>[-(R+1), R+1)</cite>.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A stacked <cite>Tensor</cite> with the same type as <cite>values</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- If <cite>axis</cite> is out of the range [-(R+1), R+1).</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.stop_gradient">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">stop_gradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#stop_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.stop_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops gradient computation.</p>
<p>When executed in a graph, this op outputs its input tensor as-is.</p>
<p>When building ops to compute gradients, this op prevents the contribution of
its inputs to be taken into account.  Normally, the gradient generator adds ops
to a graph to compute the derivatives of a specified 'loss' by recursively
finding out inputs that contributed to its computation.  If you insert this op
in the graph it inputs are masked from the gradient generator.  They are not
taken into account for computing gradients.</p>
<p>This is useful any time you want to compute a value with TensorFlow but need
to pretend that the value was a constant. Some examples include:</p>
<ul class="simple">
<li><p>The <em>EM</em> algorithm where the <em>M-step</em> should not involve backpropagation
through the output of the <em>E-step</em>.</p></li>
<li><p>Contrastive divergence training of Boltzmann machines where, when
differentiating the energy function, the training must not backpropagate
through the graph that generated the samples from the model.</p></li>
<li><p>Adversarial training, where no backprop should happen through the adversarial
example generation process.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.strided_slice">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">strided_slice</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_</span></em>, <em class="sig-param"><span class="n">begin</span></em>, <em class="sig-param"><span class="n">end</span></em>, <em class="sig-param"><span class="n">strides</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">begin_mask</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">end_mask</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">ellipsis_mask</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">new_axis_mask</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">shrink_axis_mask</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">var</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#strided_slice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.strided_slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts a strided slice of a tensor (generalized python array indexing).</p>
<p><strong>Instead of calling this op directly most users will want to use the
NumPy-style slicing syntax (e.g. `tensor[..., 3:4:-1, tf.newaxis, 3]`), which
is supported via `tf.Tensor.__getitem__` and `tf.Variable.__getitem__`.</strong>
The interface of this op is a low-level encoding of the slicing syntax.</p>
<p>Roughly speaking, this op extracts a slice of size <cite>(end-begin)/stride</cite>
from the given <cite>input_</cite> tensor. Starting at the location specified by <cite>begin</cite>
the slice continues by adding <cite>stride</cite> to the index until all dimensions are
not less than <cite>end</cite>.
Note that a stride can be negative, which causes a reverse slice.</p>
<p>Given a Python slice <cite>input[spec0, spec1, ..., specn]</cite>,
this function will be called as follows.</p>
<p><cite>begin</cite>, <cite>end</cite>, and <cite>strides</cite> will be vectors of length n.
n in general is not equal to the rank of the <cite>input_</cite> tensor.</p>
<p>In each mask field (<cite>begin_mask</cite>, <cite>end_mask</cite>, <cite>ellipsis_mask</cite>,
<cite>new_axis_mask</cite>, <cite>shrink_axis_mask</cite>) the ith bit will correspond to
the ith spec.</p>
<p>If the ith bit of <cite>begin_mask</cite> is set, <cite>begin[i]</cite> is ignored and
the fullest possible range in that dimension is used instead.
<cite>end_mask</cite> works analogously, except with the end range.</p>
<p><cite>foo[5:,:,:3]</cite> on a 7x8x9 tensor is equivalent to <cite>foo[5:7,0:8,0:3]</cite>.
<cite>foo[::-1]</cite> reverses a tensor with shape 8.</p>
<p>If the ith bit of <cite>ellipsis_mask</cite> is set, as many unspecified dimensions
as needed will be inserted between other dimensions. Only one
non-zero bit is allowed in <cite>ellipsis_mask</cite>.</p>
<p>For example <cite>foo[3:5,...,4:5]</cite> on a shape 10x3x3x10 tensor is
equivalent to <cite>foo[3:5,:,:,4:5]</cite> and
<cite>foo[3:5,...]</cite> is equivalent to <cite>foo[3:5,:,:,:]</cite>.</p>
<p>If the ith bit of <cite>new_axis_mask</cite> is set, then <cite>begin</cite>,
<cite>end</cite>, and <cite>stride</cite> are ignored and a new length 1 dimension is
added at this point in the output tensor.</p>
<p>For example,
<cite>foo[:4, tf.newaxis, :2]</cite> would produce a shape <cite>(4, 1, 2)</cite> tensor.</p>
<p>If the ith bit of <cite>shrink_axis_mask</cite> is set, it implies that the ith
specification shrinks the dimensionality by 1, taking on the value at index
<cite>begin[i]</cite>. <cite>end[i]</cite> and <cite>strides[i]</cite> are ignored in this case. For example in
Python one might do <cite>foo[:, 3, :]</cite> which would result in <cite>shrink_axis_mask</cite>
equal to 2.</p>
<p>NOTE: <cite>begin</cite> and <cite>end</cite> are zero-indexed.
<cite>strides</cite> entries must be non-zero.</p>
<p><a href="#id1012"><span class="problematic" id="id1013">``</span></a><a href="#id1014"><span class="problematic" id="id1015">`</span></a>python
t = tf.constant([[[1, 1, 1], [2, 2, 2]],</p>
<blockquote>
<div><p>[[3, 3, 3], [4, 4, 4]],
[[5, 5, 5], [6, 6, 6]]])</p>
</div></blockquote>
<p>tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])  # [[[3, 3, 3]]]
tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])  # [[[3, 3, 3],</p>
<blockquote>
<div><p>#   [4, 4, 4]]]</p>
</div></blockquote>
<dl class="simple">
<dt>tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])  # [[[4, 4, 4],</dt><dd><p>#   [3, 3, 3]]]</p>
</dd>
</dl>
<p><a href="#id1016"><span class="problematic" id="id1017">``</span></a><a href="#id1018"><span class="problematic" id="id1019">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>begin</strong> -- An <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite>.</p></li>
<li><p><strong>end</strong> -- An <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite>.</p></li>
<li><p><strong>strides</strong> -- An <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite>.</p></li>
<li><p><strong>begin_mask</strong> -- An <cite>int32</cite> mask.</p></li>
<li><p><strong>end_mask</strong> -- An <cite>int32</cite> mask.</p></li>
<li><p><strong>ellipsis_mask</strong> -- An <cite>int32</cite> mask.</p></li>
<li><p><strong>new_axis_mask</strong> -- An <cite>int32</cite> mask.</p></li>
<li><p><strong>shrink_axis_mask</strong> -- An <cite>int32</cite> mask.</p></li>
<li><p><strong>var</strong> -- The variable corresponding to <cite>input_</cite> or None</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.subtract">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">subtract</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#subtract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.subtract" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns x - y element-wise.</p>
<p><em>NOTE</em>: <cite>Subtract</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>uint8</cite>, <cite>int8</cite>, <cite>uint16</cite>, <cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.switch_case">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">switch_case</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">branch_index</span></em>, <em class="sig-param"><span class="n">branch_fns</span></em>, <em class="sig-param"><span class="n">default</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'switch_case'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/control_flow_ops.html#switch_case"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.switch_case" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a switch/case operation, i.e. an integer-indexed conditional.</p>
<p>See also <cite>tf.case</cite>.</p>
<p>This op can be substantially more efficient than <cite>tf.case</cite> when exactly one
branch will be selected. <cite>tf.switch_case</cite> is more like a C++ switch/case
statement than <cite>tf.case</cite>, which is more like an if/elif/elif/else chain.</p>
<p>The <cite>branch_fns</cite> parameter is either a dict from <cite>int</cite> to callables, or list
of (<cite>int</cite>, callable) pairs, or simply a list of callables (in which case the
index is implicitly the key). The <cite>branch_index</cite> <cite>Tensor</cite> is used to select an
element in <cite>branch_fns</cite> with matching <cite>int</cite> key, falling back to <cite>default</cite>
if none match, or <cite>max(keys)</cite> if no <cite>default</cite> is provided. The keys must form
a contiguous set from <cite>0</cite> to <cite>len(branch_fns) - 1</cite>.</p>
<p><cite>tf.switch_case</cite> supports nested structures as implemented in <cite>tf.nest</cite>. All
callables must return the same (possibly nested) value structure of lists,
tuples, and/or named tuples.</p>
<p><strong>Example:</strong></p>
<p>Pseudocode:</p>
<p><a href="#id1020"><span class="problematic" id="id1021">``</span></a><a href="#id1022"><span class="problematic" id="id1023">`</span></a>c++
switch (branch_index) {  // c-style switch</p>
<blockquote>
<div><p>case 0: return 17;
case 1: return 31;
default: return -1;</p>
</div></blockquote>
<p>or
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">branches</span> <span class="pre">=</span> <span class="pre">{0:</span> <span class="pre">lambda:</span> <span class="pre">17,</span> <span class="pre">1:</span> <span class="pre">lambda:</span> <span class="pre">31}</span>
<span class="pre">branches.get(branch_index,</span> <span class="pre">lambda:</span> <span class="pre">-1)()</span>
<span class="pre">`</span></code></p>
<p>Expressions:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">def</span> <span class="pre">f1():</span> <span class="pre">return</span> <span class="pre">tf.constant(17)</span>
<span class="pre">def</span> <span class="pre">f2():</span> <span class="pre">return</span> <span class="pre">tf.constant(31)</span>
<span class="pre">def</span> <span class="pre">f3():</span> <span class="pre">return</span> <span class="pre">tf.constant(-1)</span>
<span class="pre">r</span> <span class="pre">=</span> <span class="pre">tf.switch_case(branch_index,</span> <span class="pre">branch_fns={0:</span> <span class="pre">f1,</span> <span class="pre">1:</span> <span class="pre">f2},</span> <span class="pre">default=f3)</span>
<span class="pre">#</span> <span class="pre">Equivalent:</span> <span class="pre">tf.switch_case(branch_index,</span> <span class="pre">branch_fns={0:</span> <span class="pre">f1,</span> <span class="pre">1:</span> <span class="pre">f2,</span> <span class="pre">2:</span> <span class="pre">f3})</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>branch_index</strong> -- An int Tensor specifying which of <cite>branch_fns</cite> should be
executed.</p></li>
<li><p><strong>branch_fns</strong> -- A <cite>dict</cite> mapping <cite>int`s to callables, or a `list</cite> of
(<cite>int</cite>, callable) pairs, or simply a list of callables (in which case the
index serves as the key). Each callable must return a matching structure
of tensors.</p></li>
<li><p><strong>default</strong> -- Optional callable that returns a structure of tensors.</p></li>
<li><p><strong>name</strong> -- A name for this operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tensors returned by the callable identified by <cite>branch_index</cite>, or those
returned by <cite>default</cite> if no key matches and <cite>default</cite> was provided, or those
returned by the max-keyed <cite>branch_fn</cite> if no <cite>default</cite> is provided.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- If <cite>branch_fns</cite> is not a list/dictionary.</p></li>
<li><p><strong>TypeError</strong> -- If <cite>branch_fns</cite> is a list but does not contain 2-tuples or
    callables.</p></li>
<li><p><strong>TypeError</strong> -- If <cite>fns[i]</cite> is not callable for any i, or <cite>default</cite> is not
    callable.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tan">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tan</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#tan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes tan of x element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes tangent of every
element in the tensor. Input range is <cite>(-inf, inf)</cite> and
output range is <cite>(-inf, inf)</cite>. If input lies outside the boundary, <cite>nan</cite>
is returned.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-9,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">200,</span> <span class="pre">10000,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.tan(x)</span> <span class="pre">==&gt;</span> <span class="pre">[nan</span> <span class="pre">0.45231566</span> <span class="pre">-0.5463025</span> <span class="pre">1.5574077</span> <span class="pre">2.572152</span> <span class="pre">-1.7925274</span> <span class="pre">0.32097113</span> <span class="pre">nan]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tanh">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tanh</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_math_ops.html#tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes hyperbolic tangent of <cite>x</cite> element-wise.</p>
<blockquote>
<div><p>Given an input tensor, this function computes hyperbolic tangent of every
element in the tensor. Input range is <cite>[-inf, inf]</cite> and
output range is <cite>[-1,1]</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">x</span> <span class="pre">=</span> <span class="pre">tf.constant([-float(&quot;inf&quot;),</span> <span class="pre">-5,</span> <span class="pre">-0.5,</span> <span class="pre">1,</span> <span class="pre">1.2,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">float(&quot;inf&quot;)])</span>
<span class="pre">tf.math.tanh(x)</span> <span class="pre">==&gt;</span> <span class="pre">[-1.</span> <span class="pre">-0.99990916</span> <span class="pre">-0.46211717</span> <span class="pre">0.7615942</span> <span class="pre">0.8336547</span> <span class="pre">0.9640276</span> <span class="pre">0.9950547</span> <span class="pre">1.]</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
<p>If <cite>x</cite> is a <cite>SparseTensor</cite>, returns
<cite>SparseTensor(x.indices, tf.math.tanh(x.values, ...), x.dense_shape)</cite></p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tensor_scatter_nd_add">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tensor_scatter_nd_add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">updates</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.tensor_scatter_nd_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds sparse <cite>updates</cite> to an existing tensor according to <cite>indices</cite>.</p>
<p>This operation creates a new tensor by adding sparse <cite>updates</cite> to the passed
in <cite>tensor</cite>.
This operation is very similar to <cite>tf.scatter_nd_add</cite>, except that the updates
are added onto an existing tensor (as opposed to a variable). If the memory
for the existing tensor cannot be re-used, a copy is made and updated.</p>
<p><cite>indices</cite> is an integer tensor containing indices into a new tensor of shape
<cite>shape</cite>.  The last dimension of <cite>indices</cite> can be at most the rank of <cite>shape</cite>:</p>
<blockquote>
<div><p>indices.shape[-1] &lt;= shape.rank</p>
</div></blockquote>
<p>The last dimension of <cite>indices</cite> corresponds to indices into elements
(if <cite>indices.shape[-1] = shape.rank</cite>) or slices
(if <cite>indices.shape[-1] &lt; shape.rank</cite>) along dimension <cite>indices.shape[-1]</cite> of
<cite>shape</cite>.  <cite>updates</cite> is a tensor with shape</p>
<blockquote>
<div><p>indices.shape[:-1] + shape[indices.shape[-1]:]</p>
</div></blockquote>
<p>The simplest form of tensor_scatter_add is to add individual elements to a
tensor by index. For example, say we want to add 4 elements in a rank-1
tensor with 8 elements.</p>
<p>In Python, this scatter add operation would look like this:</p>
<dl class="simple">
<dt><a href="#id1024"><span class="problematic" id="id1025">``</span></a><a href="#id1026"><span class="problematic" id="id1027">`</span></a>python</dt><dd><p>indices = tf.constant([[4], [3], [1], [7]])
updates = tf.constant([9, 10, 11, 12])
tensor = tf.ones([8], dtype=tf.int32)
updated = tf.tensor_scatter_nd_add(tensor, indices, updates)
print(updated)</p>
</dd>
</dl>
<p><a href="#id1028"><span class="problematic" id="id1029">``</span></a><a href="#id1030"><span class="problematic" id="id1031">`</span></a></p>
<p>The resulting tensor would look like this:</p>
<blockquote>
<div><p>[1, 12, 1, 11, 10, 1, 1, 13]</p>
</div></blockquote>
<p>We can also, insert entire slices of a higher rank tensor all at once. For
example, if we wanted to insert two slices in the first dimension of a
rank-3 tensor with two matrices of new values.</p>
<p>In Python, this scatter add operation would look like this:</p>
<dl>
<dt><a href="#id1032"><span class="problematic" id="id1033">``</span></a><a href="#id1034"><span class="problematic" id="id1035">`</span></a>python</dt><dd><p>indices = tf.constant([[0], [2]])
updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],</p>
<blockquote>
<div><blockquote>
<div><p>[7, 7, 7, 7], [8, 8, 8, 8]],</p>
</div></blockquote>
<dl class="simple">
<dt>[[5, 5, 5, 5], [6, 6, 6, 6],</dt><dd><p>[7, 7, 7, 7], [8, 8, 8, 8]]])</p>
</dd>
</dl>
</div></blockquote>
<p>tensor = tf.ones([4, 4, 4],dtype=tf.int32)
updated = tf.tensor_scatter_nd_add(tensor, indices, updates)
print(updated)</p>
</dd>
</dl>
<p><a href="#id1036"><span class="problematic" id="id1037">``</span></a><a href="#id1038"><span class="problematic" id="id1039">`</span></a></p>
<p>The resulting tensor would look like this:</p>
<blockquote>
<div><dl class="simple">
<dt>[[[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],</dt><dd><p>[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
[[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],
[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]</p>
</dd>
</dl>
</div></blockquote>
<p>Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, the index is ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- A <cite>Tensor</cite>. Tensor to copy/update.</p></li>
<li><p><strong>indices</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Index tensor.</p></li>
<li><p><strong>updates</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>tensor</cite>.
Updates to scatter into output.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tensor_scatter_nd_sub">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tensor_scatter_nd_sub</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">updates</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.tensor_scatter_nd_sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtracts sparse <cite>updates</cite> from an existing tensor according to <cite>indices</cite>.</p>
<p>This operation creates a new tensor by subtracting sparse <cite>updates</cite> from the
passed in <cite>tensor</cite>.
This operation is very similar to <cite>tf.scatter_nd_sub</cite>, except that the updates
are subtracted from an existing tensor (as opposed to a variable). If the memory
for the existing tensor cannot be re-used, a copy is made and updated.</p>
<p><cite>indices</cite> is an integer tensor containing indices into a new tensor of shape
<cite>shape</cite>.  The last dimension of <cite>indices</cite> can be at most the rank of <cite>shape</cite>:</p>
<blockquote>
<div><p>indices.shape[-1] &lt;= shape.rank</p>
</div></blockquote>
<p>The last dimension of <cite>indices</cite> corresponds to indices into elements
(if <cite>indices.shape[-1] = shape.rank</cite>) or slices
(if <cite>indices.shape[-1] &lt; shape.rank</cite>) along dimension <cite>indices.shape[-1]</cite> of
<cite>shape</cite>.  <cite>updates</cite> is a tensor with shape</p>
<blockquote>
<div><p>indices.shape[:-1] + shape[indices.shape[-1]:]</p>
</div></blockquote>
<p>The simplest form of tensor_scatter_sub is to subtract individual elements
from a tensor by index. For example, say we want to insert 4 scattered elements
in a rank-1 tensor with 8 elements.</p>
<p>In Python, this scatter subtract operation would look like this:</p>
<dl class="simple">
<dt><a href="#id1040"><span class="problematic" id="id1041">``</span></a><a href="#id1042"><span class="problematic" id="id1043">`</span></a>python</dt><dd><p>indices = tf.constant([[4], [3], [1], [7]])
updates = tf.constant([9, 10, 11, 12])
tensor = tf.ones([8], dtype=tf.int32)
updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)
print(updated)</p>
</dd>
</dl>
<p><a href="#id1044"><span class="problematic" id="id1045">``</span></a><a href="#id1046"><span class="problematic" id="id1047">`</span></a></p>
<p>The resulting tensor would look like this:</p>
<blockquote>
<div><p>[1, -10, 1, -9, -8, 1, 1, -11]</p>
</div></blockquote>
<p>We can also, insert entire slices of a higher rank tensor all at once. For
example, if we wanted to insert two slices in the first dimension of a
rank-3 tensor with two matrices of new values.</p>
<p>In Python, this scatter add operation would look like this:</p>
<dl>
<dt><a href="#id1048"><span class="problematic" id="id1049">``</span></a><a href="#id1050"><span class="problematic" id="id1051">`</span></a>python</dt><dd><p>indices = tf.constant([[0], [2]])
updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],</p>
<blockquote>
<div><blockquote>
<div><p>[7, 7, 7, 7], [8, 8, 8, 8]],</p>
</div></blockquote>
<dl class="simple">
<dt>[[5, 5, 5, 5], [6, 6, 6, 6],</dt><dd><p>[7, 7, 7, 7], [8, 8, 8, 8]]])</p>
</dd>
</dl>
</div></blockquote>
<p>tensor = tf.ones([4, 4, 4],dtype=tf.int32)
updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)
print(updated)</p>
</dd>
</dl>
<p><a href="#id1052"><span class="problematic" id="id1053">``</span></a><a href="#id1054"><span class="problematic" id="id1055">`</span></a></p>
<p>The resulting tensor would look like this:</p>
<blockquote>
<div><dl class="simple">
<dt>[[[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],</dt><dd><p>[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
[[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],
[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]</p>
</dd>
</dl>
</div></blockquote>
<p>Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, the index is ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- A <cite>Tensor</cite>. Tensor to copy/update.</p></li>
<li><p><strong>indices</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Index tensor.</p></li>
<li><p><strong>updates</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>tensor</cite>.
Updates to scatter into output.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tensor_scatter_nd_update">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tensor_scatter_nd_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">updates</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.tensor_scatter_nd_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatter <cite>updates</cite> into an existing tensor according to <cite>indices</cite>.</p>
<p>This operation creates a new tensor by applying sparse <cite>updates</cite> to the passed
in <cite>tensor</cite>.
This operation is very similar to <cite>tf.scatter_nd</cite>, except that the updates are
scattered onto an existing tensor (as opposed to a zero-tensor). If the memory
for the existing tensor cannot be re-used, a copy is made and updated.</p>
<p>If <cite>indices</cite> contains duplicates, then their updates are accumulated (summed).</p>
<p><strong>WARNING</strong>: The order in which updates are applied is nondeterministic, so the
output will be nondeterministic if <cite>indices</cite> contains duplicates -- because
of some numerical approximation issues, numbers summed in different order
may yield different results.</p>
<p><cite>indices</cite> is an integer tensor containing indices into a new tensor of shape
<cite>shape</cite>.  The last dimension of <cite>indices</cite> can be at most the rank of <cite>shape</cite>:</p>
<blockquote>
<div><p>indices.shape[-1] &lt;= shape.rank</p>
</div></blockquote>
<p>The last dimension of <cite>indices</cite> corresponds to indices into elements
(if <cite>indices.shape[-1] = shape.rank</cite>) or slices
(if <cite>indices.shape[-1] &lt; shape.rank</cite>) along dimension <cite>indices.shape[-1]</cite> of
<cite>shape</cite>.  <cite>updates</cite> is a tensor with shape</p>
<blockquote>
<div><p>indices.shape[:-1] + shape[indices.shape[-1]:]</p>
</div></blockquote>
<p>The simplest form of scatter is to insert individual elements in a tensor by
index. For example, say we want to insert 4 scattered elements in a rank-1
tensor with 8 elements.</p>
<p>&lt;div style=&quot;width:70%; margin:auto; margin-bottom:10px; margin-top:20px;&quot;&gt;
&lt;img style=&quot;width:100%&quot; src=&quot;https://www.tensorflow.org/images/ScatterNd1.png&quot; alt&gt;
&lt;/div&gt;</p>
<p>In Python, this scatter operation would look like this:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_update</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">))</span>
<span class="go">tf.Tensor([ 1 11  1 10  9  1  1 12], shape=(8,), dtype=int32)</span>
</pre></div>
</div>
<p>We can also, insert entire slices of a higher rank tensor all at once. For
example, if we wanted to insert two slices in the first dimension of a
rank-3 tensor with two matrices of new values.</p>
<p>In Python, this scatter operation would look like this:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>                        <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span>
<span class="gp">... </span>                       <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">... </span>                        <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_update</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="go">[[[5 5 5 5]</span>
<span class="go">  [6 6 6 6]</span>
<span class="go">  [7 7 7 7]</span>
<span class="go">  [8 8 8 8]]</span>
<span class="go"> [[1 1 1 1]</span>
<span class="go">  [1 1 1 1]</span>
<span class="go">  [1 1 1 1]</span>
<span class="go">  [1 1 1 1]]</span>
<span class="go"> [[5 5 5 5]</span>
<span class="go">  [6 6 6 6]</span>
<span class="go">  [7 7 7 7]</span>
<span class="go">  [8 8 8 8]]</span>
<span class="go"> [[1 1 1 1]</span>
<span class="go">  [1 1 1 1]</span>
<span class="go">  [1 1 1 1]</span>
<span class="go">  [1 1 1 1]]]</span>
</pre></div>
</div>
<p>Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, the index is ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> -- A <cite>Tensor</cite>. Tensor to copy/update.</p></li>
<li><p><strong>indices</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
Index tensor.</p></li>
<li><p><strong>updates</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>tensor</cite>.
Updates to scatter into output.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tensordot">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tensordot</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em>, <em class="sig-param"><span class="n">axes</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#tensordot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.tensordot" title="Permalink to this definition">¶</a></dt>
<dd><p>Tensor contraction of a and b along specified axes and outer product.</p>
<p>Tensordot (also known as tensor contraction) sums the product of elements
from <cite>a</cite> and <cite>b</cite> over the indices specified by <cite>a_axes</cite> and <cite>b_axes</cite>.
The lists <cite>a_axes</cite> and <cite>b_axes</cite> specify those pairs of axes along which to
contract the tensors. The axis <cite>a_axes[i]</cite> of <cite>a</cite> must have the same dimension
as axis <cite>b_axes[i]</cite> of <cite>b</cite> for all <cite>i</cite> in <cite>range(0, len(a_axes))</cite>. The lists
<cite>a_axes</cite> and <cite>b_axes</cite> must have identical length and consist of unique
integers that specify valid axes for each of the tensors. Additionally
outer product is supported by passing <cite>axes=0</cite>.</p>
<p>This operation corresponds to <cite>numpy.tensordot(a, b, axes)</cite>.</p>
<p>Example 1: When <cite>a</cite> and <cite>b</cite> are matrices (order 2), the case <cite>axes = 1</cite>
is equivalent to matrix multiplication.</p>
<p>Example 2: When <cite>a</cite> and <cite>b</cite> are matrices (order 2), the case
<cite>axes = [[1], [0]]</cite> is equivalent to matrix multiplication.</p>
<p>Example 3: When <cite>a</cite> and <cite>b</cite> are matrices (order 2), the case <cite>axes=0</cite> gives
the outer product, a tensor of order 4.</p>
<p>Example 4: Suppose that \(a_{ijk}\) and \(b_{lmn}\) represent two
tensors of order 3. Then, <cite>contract(a, b, [[0], [2]])</cite> is the order 4 tensor
\(c_{jklm}\) whose entry
corresponding to the indices \((j,k,l,m)\) is given by:</p>
<p>\( c_{jklm} = sum_i a_{ijk} b_{lmi} \).</p>
<p>In general, <cite>order(c) = order(a) + order(b) - 2*len(axes[0])</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> -- <cite>Tensor</cite> of type <cite>float32</cite> or <cite>float64</cite>.</p></li>
<li><p><strong>b</strong> -- <cite>Tensor</cite> with the same type as <cite>a</cite>.</p></li>
<li><p><strong>axes</strong> -- Either a scalar <cite>N</cite>, or a list or an <cite>int32</cite> <cite>Tensor</cite> of shape [2, k].
If axes is a scalar, sum over the last N axes of a and the first N axes of
b in order. If axes is a list or <cite>Tensor</cite> the first and second row contain
the set of unique integers specifying axes along which the contraction is
computed, for <cite>a</cite> and <cite>b</cite>, respectively. The number of axes for <cite>a</cite> and
<cite>b</cite> must be equal. If <cite>axes=0</cite>, computes the outer product between <cite>a</cite> and
<cite>b</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> with the same type as <cite>a</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> -- If the shapes of <cite>a</cite>, <cite>b</cite>, and <cite>axes</cite> are incompatible.</p></li>
<li><p><strong>IndexError</strong> -- If the values in axes exceed the rank of the corresponding
    tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tile">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tile</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">multiples</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#tile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.tile" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a tensor by tiling a given tensor.</p>
<p>This operation creates a new tensor by replicating <cite>input</cite> <cite>multiples</cite> times.
The output tensor's i'th dimension has <cite>input.dims(i) * multiples[i]</cite> elements,
and the values of <cite>input</cite> are replicated <cite>multiples[i]</cite> times along the 'i'th
dimension. For example, tiling <cite>[a b c d]</cite> by <cite>[2]</cite> produces
<cite>[a b c d a b c d]</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=</span>
<span class="go">array([[1, 2, 3, 1, 2, 3],</span>
<span class="go">       [4, 5, 6, 4, 5, 6]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=</span>
<span class="go">array([[1, 2, 3],</span>
<span class="go">       [4, 5, 6],</span>
<span class="go">       [1, 2, 3],</span>
<span class="go">       [4, 5, 6]], dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4, 6), dtype=int32, numpy=</span>
<span class="go">array([[1, 2, 3, 1, 2, 3],</span>
<span class="go">       [4, 5, 6, 4, 5, 6],</span>
<span class="go">       [1, 2, 3, 1, 2, 3],</span>
<span class="go">       [4, 5, 6, 4, 5, 6]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite>. 1-D or higher.</p></li>
<li><p><strong>multiples</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
1-D. Length must be the same as the number of dimensions in <cite>input</cite></p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>input</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.timestamp">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">timestamp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_logging_ops.html#timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides the time since epoch in seconds.</p>
<p>Returns the timestamp as a <cite>float64</cite> for seconds since the Unix epoch.</p>
<p>Note: the timestamp is computed when the op is executed, not when it is added
to the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> -- A name for the operation (optional).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> of type <cite>float64</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.transpose">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">transpose</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">perm</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">conjugate</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'transpose'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#transpose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Transposes <cite>a</cite>, where <cite>a</cite> is a Tensor.</p>
<p>Permutes the dimensions according to the value of <cite>perm</cite>.</p>
<p>The returned tensor's dimension <cite>i</cite> will correspond to the input dimension
<cite>perm[i]</cite>. If <cite>perm</cite> is not given, it is set to (n-1...0), where n is the rank
of the input tensor. Hence by default, this operation performs a regular
matrix transpose on 2-D input Tensors.</p>
<p>If conjugate is <cite>True</cite> and <cite>a.dtype</cite> is either <cite>complex64</cite> or <cite>complex128</cite>
then the values of <cite>a</cite> are conjugated and transposed.</p>
<p>&#64;compatibility(numpy)
In <cite>numpy</cite> transposes are memory-efficient constant time operations as they
simply return a new view of the same data with adjusted <cite>strides</cite>.</p>
<p>TensorFlow does not support strides, so <cite>transpose</cite> returns a new tensor with
the items permuted.
&#64;end_compatibility</p>
<p>For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=</span>
<span class="go">array([[1, 4],</span>
<span class="go">       [2, 5],</span>
<span class="go">       [3, 6]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>Equivalently, you could call <cite>tf.transpose(x, perm=[1, 0])</cite>.</p>
<p>If <cite>x</cite> is complex, setting conjugate=True gives the conjugate transpose:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="n">j</span><span class="p">,</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span><span class="n">j</span><span class="p">],</span>
<span class="gp">... </span>                 <span class="p">[</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">5</span><span class="n">j</span><span class="p">,</span> <span class="mi">6</span> <span class="o">+</span> <span class="mi">6</span><span class="n">j</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">conjugate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=</span>
<span class="go">array([[1.-1.j, 4.-4.j],</span>
<span class="go">       [2.-2.j, 5.-5.j],</span>
<span class="go">       [3.-3.j, 6.-6.j]])&gt;</span>
</pre></div>
</div>
<p>'perm' is more useful for n-dimensional tensors where n &gt; 2:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">]],</span>
<span class="gp">... </span>                 <span class="p">[[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]])</span>
</pre></div>
</div>
<p>As above, simply calling <cite>tf.transpose</cite> will default to <cite>perm=[2,1,0]</cite>.</p>
<p>To take the transpose of the matrices in dimension-0 (such as when you are
transposing matrices where 0 is the batch dimesnion), you would set
<cite>perm=[0,2,1]</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=</span>
<span class="go">array([[[ 1,  4],</span>
<span class="go">        [ 2,  5],</span>
<span class="go">        [ 3,  6]],</span>
<span class="go">        [[ 7, 10],</span>
<span class="go">        [ 8, 11],</span>
<span class="go">        [ 9, 12]]], dtype=int32)&gt;</span>
</pre></div>
</div>
<p>Note: This has a shorthand <cite>linalg.matrix_transpose</cite>):</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> -- A <cite>Tensor</cite>.</p></li>
<li><p><strong>perm</strong> -- A permutation of the dimensions of <cite>a</cite>.  This should be a vector.</p></li>
<li><p><strong>conjugate</strong> -- Optional bool. Setting it to <cite>True</cite> is mathematically equivalent
to tf.math.conj(tf.transpose(input)).</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A transposed <cite>Tensor</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.truediv">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">truediv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/math_ops.html#truediv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.truediv" title="Permalink to this definition">¶</a></dt>
<dd><p>Divides x / y elementwise (using Python 3 division operator semantics).</p>
<p>NOTE: Prefer using the Tensor operator or tf.divide which obey Python
division operator semantics.</p>
<p>This function forces Python 3 division operator semantics where all integer
arguments are cast to floating types first.   This op is generated by normal
<cite>x / y</cite> division in Python 3 and in Python 2.7 with
<cite>from __future__ import division</cite>.  If you want integer division that rounds
down, use <cite>x // y</cite> or <cite>tf.math.floordiv</cite>.</p>
<p><cite>x</cite> and <cite>y</cite> must have the same numeric type.  If the inputs are floating
point, the output will have the same type.  If the inputs are integral, the
inputs are cast to <cite>float32</cite> for <cite>int8</cite> and <cite>int16</cite> and <cite>float64</cite> for <cite>int32</cite>
and <cite>int64</cite> (matching the behavior of Numpy).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- <cite>Tensor</cite> numerator of numeric type.</p></li>
<li><p><strong>y</strong> -- <cite>Tensor</cite> denominator of numeric type.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>x / y</cite> evaluated in floating point.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>TypeError</strong> -- If <cite>x</cite> and <cite>y</cite> have different dtypes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.truncatediv">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">truncatediv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.truncatediv" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns x / y element-wise for integer types.</p>
<p>Truncation designates that negative numbers will round fractional quantities
toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different
than Python semantics. See <cite>FloorDiv</cite> for a division function that matches
Python Semantics.</p>
<p><em>NOTE</em>: <cite>truncatediv</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>, <cite>uint8</cite>, <cite>int8</cite>, <cite>uint16</cite>, <cite>int16</cite>, <cite>int32</cite>, <cite>int64</cite>, <cite>complex64</cite>, <cite>complex128</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.truncatemod">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">truncatemod</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#tensorflow.truncatemod" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns element-wise remainder of division. This emulates C semantics in that</p>
<p>the result here is consistent with a truncating divide. E.g. <cite>truncate(x / y) *
y + truncate_mod(x, y) = x</cite>.</p>
<p><em>NOTE</em>: <cite>truncatemod</cite> supports broadcasting. More about broadcasting
[here](<a class="reference external" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>, <cite>bfloat16</cite>, <cite>half</cite>, <cite>float32</cite>, <cite>float64</cite>.</p></li>
<li><p><strong>y</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.tuple">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">tuple</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensors</span></em>, <em class="sig-param"><span class="n">control_inputs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/control_flow_ops.html#tuple"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.tuple" title="Permalink to this definition">¶</a></dt>
<dd><p>Group tensors together.</p>
<p>This creates a tuple of tensors with the same values as the <cite>tensors</cite>
argument, except that the value of each tensor is only returned after the
values of all tensors have been computed.</p>
<p><cite>control_inputs</cite> contains additional ops that have to finish before this op
finishes, but whose outputs are not returned.</p>
<p>This can be used as a &quot;join&quot; mechanism for parallel computations: all the
argument tensors can be computed in parallel, but the values of any tensor
returned by <cite>tuple</cite> are only available after all the parallel computations
are done.</p>
<p>See also <cite>tf.group</cite> and
<cite>tf.control_dependencies</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> -- A list of <cite>Tensor`s or `IndexedSlices</cite>, some entries can be <cite>None</cite>.</p></li>
<li><p><strong>control_inputs</strong> -- List of additional ops to finish before returning.</p></li>
<li><p><strong>name</strong> -- (optional) A name to use as a <cite>name_scope</cite> for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Same as <cite>tensors</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> -- If <cite>tensors</cite> does not contain any <cite>Tensor</cite> or <cite>IndexedSlices</cite>.</p></li>
<li><p><strong>TypeError</strong> -- If <cite>control_inputs</cite> is not a list of <cite>Operation</cite> or <cite>Tensor</cite>
    objects.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.unique">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">unique</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">out_idx</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#unique"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.unique" title="Permalink to this definition">¶</a></dt>
<dd><p>Finds unique elements in a 1-D tensor.</p>
<p>This operation returns a tensor <cite>y</cite> containing all of the unique elements of <cite>x</cite>
sorted in the same order that they occur in <cite>x</cite>; <cite>x</cite> does not need to be sorted.
This operation also returns a tensor <cite>idx</cite> the same size as <cite>x</cite> that contains
the index of each value of <cite>x</cite> in the unique output <cite>y</cite>. In other words:</p>
<p><cite>y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]</cite></p>
<p>Examples:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">#</span> <span class="pre">tensor</span> <span class="pre">'x'</span> <span class="pre">is</span> <span class="pre">[1,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">4,</span> <span class="pre">4,</span> <span class="pre">7,</span> <span class="pre">8,</span> <span class="pre">8]</span>
<span class="pre">y,</span> <span class="pre">idx</span> <span class="pre">=</span> <span class="pre">unique(x)</span>
<span class="pre">y</span> <span class="pre">==&gt;</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">7,</span> <span class="pre">8]</span>
<span class="pre">idx</span> <span class="pre">==&gt;</span> <span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">2,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">4]</span>
<span class="pre">`</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">#</span> <span class="pre">tensor</span> <span class="pre">'x'</span> <span class="pre">is</span> <span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5]</span>
<span class="pre">y,</span> <span class="pre">idx</span> <span class="pre">=</span> <span class="pre">unique(x)</span>
<span class="pre">y</span> <span class="pre">==&gt;</span> <span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3]</span>
<span class="pre">idx</span> <span class="pre">==&gt;</span> <span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">4,</span> <span class="pre">0,</span> <span class="pre">1]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. 1-D.</p></li>
<li><p><strong>out_idx</strong> -- An optional <cite>tf.DType</cite> from: <cite>tf.int32, tf.int64</cite>. Defaults to <cite>tf.int32</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A tuple of <cite>Tensor</cite> objects (y, idx).</p>
<p>y: A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.
idx: A <cite>Tensor</cite> of type <cite>out_idx</cite>.</p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.unique_with_counts">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">unique_with_counts</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">out_idx</span><span class="o">=</span><span class="default_value">tf.int32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#unique_with_counts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.unique_with_counts" title="Permalink to this definition">¶</a></dt>
<dd><p>Finds unique elements in a 1-D tensor.</p>
<p>This operation returns a tensor <cite>y</cite> containing all of the unique elements of <cite>x</cite>
sorted in the same order that they occur in <cite>x</cite>. This operation also returns a
tensor <cite>idx</cite> the same size as <cite>x</cite> that contains the index of each value of <cite>x</cite>
in the unique output <cite>y</cite>. Finally, it returns a third tensor <cite>count</cite> that
contains the count of each element of <cite>y</cite> in <cite>x</cite>. In other words:</p>
<p><cite>y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]</cite></p>
<p>For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">#</span> <span class="pre">tensor</span> <span class="pre">'x'</span> <span class="pre">is</span> <span class="pre">[1,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">4,</span> <span class="pre">4,</span> <span class="pre">7,</span> <span class="pre">8,</span> <span class="pre">8]</span>
<span class="pre">y,</span> <span class="pre">idx,</span> <span class="pre">count</span> <span class="pre">=</span> <span class="pre">unique_with_counts(x)</span>
<span class="pre">y</span> <span class="pre">==&gt;</span> <span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">7,</span> <span class="pre">8]</span>
<span class="pre">idx</span> <span class="pre">==&gt;</span> <span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">2,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">4]</span>
<span class="pre">count</span> <span class="pre">==&gt;</span> <span class="pre">[2,</span> <span class="pre">1,</span> <span class="pre">3,</span> <span class="pre">1,</span> <span class="pre">2]</span>
<span class="pre">`</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> -- A <cite>Tensor</cite>. 1-D.</p></li>
<li><p><strong>out_idx</strong> -- An optional <cite>tf.DType</cite> from: <cite>tf.int32, tf.int64</cite>. Defaults to <cite>tf.int32</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A tuple of <cite>Tensor</cite> objects (y, idx, count).</p>
<p>y: A <cite>Tensor</cite>. Has the same type as <cite>x</cite>.
idx: A <cite>Tensor</cite> of type <cite>out_idx</cite>.
count: A <cite>Tensor</cite> of type <cite>out_idx</cite>.</p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.unravel_index">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">unravel_index</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">dims</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/gen_array_ops.html#unravel_index"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.unravel_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an array of flat indices into a tuple of coordinate arrays.</p>
<p>Example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">y</span> <span class="pre">=</span> <span class="pre">tf.unravel_index(indices=[2,</span> <span class="pre">5,</span> <span class="pre">7],</span> <span class="pre">dims=[3,</span> <span class="pre">3])</span>
<span class="pre">#</span> <span class="pre">'dims'</span> <span class="pre">represent</span> <span class="pre">a</span> <span class="pre">hypothetical</span> <span class="pre">(3,</span> <span class="pre">3)</span> <span class="pre">tensor</span> <span class="pre">of</span> <span class="pre">indices:</span>
<span class="pre">#</span> <span class="pre">[[0,</span> <span class="pre">1,</span> <span class="pre">*2*],</span>
<span class="pre">#</span>&#160; <span class="pre">[3,</span> <span class="pre">4,</span> <span class="pre">*5*],</span>
<span class="pre">#</span>&#160; <span class="pre">[6,</span> <span class="pre">*7*,</span> <span class="pre">8]]</span>
<span class="pre">#</span> <span class="pre">For</span> <span class="pre">each</span> <span class="pre">entry</span> <span class="pre">from</span> <span class="pre">'indices',</span> <span class="pre">this</span> <span class="pre">operation</span> <span class="pre">returns</span>
<span class="pre">#</span> <span class="pre">its</span> <span class="pre">coordinates</span> <span class="pre">(marked</span> <span class="pre">with</span> <span class="pre">'*'),</span> <span class="pre">such</span> <span class="pre">as</span>
<span class="pre">#</span> <span class="pre">2</span> <span class="pre">==&gt;</span> <span class="pre">(0,</span> <span class="pre">2)</span>
<span class="pre">#</span> <span class="pre">5</span> <span class="pre">==&gt;</span> <span class="pre">(1,</span> <span class="pre">2)</span>
<span class="pre">#</span> <span class="pre">7</span> <span class="pre">==&gt;</span> <span class="pre">(2,</span> <span class="pre">1)</span>
<span class="pre">y</span> <span class="pre">==&gt;</span> <span class="pre">[[0,</span> <span class="pre">1,</span> <span class="pre">2],</span> <span class="pre">[2,</span> <span class="pre">2,</span> <span class="pre">1]]</span>
<span class="pre">`</span></code></p>
<p>&#64;compatibility(numpy)
Equivalent to np.unravel_index
&#64;end_compatibility</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> -- A <cite>Tensor</cite>. Must be one of the following types: <cite>int32</cite>, <cite>int64</cite>.
An 0-D or 1-D <cite>int</cite> Tensor whose elements are indices into the
flattened version of an array of dimensions dims.</p></li>
<li><p><strong>dims</strong> -- A <cite>Tensor</cite>. Must have the same type as <cite>indices</cite>.
An 1-D <cite>int</cite> Tensor. The shape of the array to use for unraveling
indices.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite>. Has the same type as <cite>indices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.unstack">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">unstack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">num</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">'unstack'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#unstack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.unstack" title="Permalink to this definition">¶</a></dt>
<dd><p>Unpacks the given dimension of a rank-<cite>R</cite> tensor into rank-<cite>(R-1)</cite> tensors.</p>
<p>Unpacks <cite>num</cite> tensors from <cite>value</cite> by chipping it along the <cite>axis</cite> dimension.
If <cite>num</cite> is not specified (the default), it is inferred from <cite>value</cite>'s shape.
If <cite>value.shape[axis]</cite> is not known, <cite>ValueError</cite> is raised.</p>
<p>For example, given a tensor of shape <cite>(A, B, C, D)</cite>;</p>
<dl class="simple">
<dt>If <cite>axis == 0</cite> then the i'th tensor in <cite>output</cite> is the slice</dt><dd><p><cite>value[i, :, :, :]</cite> and each tensor in <cite>output</cite> will have shape <cite>(B, C, D)</cite>.
(Note that the dimension unpacked along is gone, unlike <cite>split</cite>).</p>
</dd>
<dt>If <cite>axis == 1</cite> then the i'th tensor in <cite>output</cite> is the slice</dt><dd><p><cite>value[:, i, :, :]</cite> and each tensor in <cite>output</cite> will have shape <cite>(A, C, D)</cite>.</p>
</dd>
</dl>
<p>Etc.</p>
<p>This is the opposite of stack.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> -- A rank <cite>R &gt; 0</cite> <cite>Tensor</cite> to be unstacked.</p></li>
<li><p><strong>num</strong> -- An <cite>int</cite>. The length of the dimension <cite>axis</cite>. Automatically inferred if
<cite>None</cite> (the default).</p></li>
<li><p><strong>axis</strong> -- An <cite>int</cite>. The axis to unstack along. Defaults to the first dimension.
Negative values wrap around, so the valid range is <cite>[-R, R)</cite>.</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of <cite>Tensor</cite> objects unstacked from <cite>value</cite>.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> -- If <cite>num</cite> is unspecified and cannot be inferred.</p></li>
<li><p><strong>ValueError</strong> -- If <cite>axis</cite> is out of the range [-R, R).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.variable_creator_scope">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">variable_creator_scope</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">variable_creator</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/variable_scope.html#variable_creator_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.variable_creator_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>Scope which defines a variable creation function to be used by variable().</p>
<p>variable_creator is expected to be a function with the following signature:</p>
<dl class="simple">
<dt><a href="#id1056"><span class="problematic" id="id1057">``</span></a><a href="#id1058"><span class="problematic" id="id1059">`</span></a></dt><dd><p>def variable_creator(next_creator, <a href="#id1060"><span class="problematic" id="id1061">**</span></a>kwargs)</p>
</dd>
</dl>
<p><a href="#id1062"><span class="problematic" id="id1063">``</span></a><a href="#id1064"><span class="problematic" id="id1065">`</span></a></p>
<p>The creator is supposed to eventually call the next_creator to create a
variable if it does want to create a variable and not call Variable or
ResourceVariable directly. This helps make creators composable. A creator may
choose to create multiple variables, return already existing variables, or
simply register that a variable was created and defer to the next creators in
line. Creators can also modify the keyword arguments seen by the next
creators.</p>
<p>Custom getters in the variable scope will eventually resolve down to these
custom creators when they do create variables.</p>
<p>The valid keyword arguments in kwds are:</p>
<blockquote>
<div><ul>
<li><dl class="simple">
<dt>initial_value: A <cite>Tensor</cite>, or Python object convertible to a <cite>Tensor</cite>,</dt><dd><p>which is the initial value for the Variable. The initial value must have
a shape specified unless <cite>validate_shape</cite> is set to False. Can also be a
callable with no argument that returns the initial value when called. In
that case, <cite>dtype</cite> must be specified. (Note that initializer functions
from init_ops.py must first be bound to a shape before being used here.)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>trainable: If <cite>True</cite>, the default, GradientTapes automatically watch</dt><dd><p>uses of this Variable.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>validate_shape: If <cite>False</cite>, allows the variable to be initialized with a</dt><dd><p>value of unknown shape. If <cite>True</cite>, the default, the shape of
<cite>initial_value</cite> must be known.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>caching_device: Optional device string describing where the Variable</dt><dd><p>should be cached for reading.  Defaults to the Variable's device.
If not <cite>None</cite>, caches on another device.  Typical use is to cache
on the device where the Ops using the Variable reside, to deduplicate
copying through <cite>Switch</cite> and other conditional statements.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>name: Optional name for the variable. Defaults to <cite>'Variable'</cite> and gets</dt><dd><blockquote>
<div><p>uniquified automatically.</p>
</div></blockquote>
<dl class="simple">
<dt>dtype: If set, initial_value will be converted to the given type.</dt><dd><p>If <cite>None</cite>, either the datatype will be kept (if <cite>initial_value</cite> is
a Tensor), or <cite>convert_to_tensor</cite> will decide.</p>
</dd>
</dl>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>constraint: A constraint function to be applied to the variable after</dt><dd><p>updates by some algorithms.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>synchronization: Indicates when a distributed a variable will be</dt><dd><p>aggregated. Accepted values are constants defined in the class
<cite>tf.VariableSynchronization</cite>. By default the synchronization is set to
<cite>AUTO</cite> and the current <cite>DistributionStrategy</cite> chooses
when to synchronize.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>aggregation: Indicates how a distributed variable will be aggregated.</dt><dd><p>Accepted values are constants defined in the class
<cite>tf.VariableAggregation</cite>.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>This set may grow over time, so it's important the signature of creators is as
mentioned above.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>variable_creator</strong> -- the passed creator</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p>A scope in which the creator is active</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.vectorized_map">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">vectorized_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em>, <em class="sig-param"><span class="n">elems</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/parallel_for/control_flow_ops.html#vectorized_map"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.vectorized_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Parallel map on the list of tensors unpacked from <cite>elems</cite> on dimension 0.</p>
<p>This method works similar to tf.map_fn but is optimized to run much faster,
possibly with a much larger memory footprint. The speedups are obtained by
vectorization (see <a class="reference external" href="https://arxiv.org/pdf/1903.04243.pdf">https://arxiv.org/pdf/1903.04243.pdf</a>). The idea behind
vectorization is to semantically launch all the invocations of <cite>fn</cite> in
parallel and fuse corresponding operations across all these invocations. This
fusion is done statically at graph generation time and the generated code is
often similar in performance to a manually fused version.</p>
<p>Because <cite>tf.vectorized_map</cite> fully parallelizes the batch, this method will
generally be significantly faster than using <cite>tf.map_fn</cite>, especially in eager
mode. However this is an experimental feature and currently has a lot of
limitations:</p>
<blockquote>
<div><ul class="simple">
<li><p>There should be no data dependency between the different semantic
invocations of <cite>fn</cite>, i.e. it should be safe to map the elements of the
inputs in any order.</p></li>
<li><p>Stateful kernels may mostly not be supported since these often imply a
data dependency. We do support a limited set of such stateful kernels
though (like RandomFoo, Variable operations like reads, etc).</p></li>
<li><p><cite>fn</cite> has limited support for control flow operations. <cite>tf.cond</cite> in
particular is not supported.</p></li>
<li><p><cite>fn</cite> should return nested structure of Tensors or Operations. However
if an Operation is returned, it should have zero outputs.</p></li>
<li><p>The shape and dtype of any intermediate or output tensors in the
computation of <cite>fn</cite> should not depend on the input to <cite>fn</cite>.</p></li>
</ul>
</div></blockquote>
<p>Examples:
<a href="#id1066"><span class="problematic" id="id1067">``</span></a><a href="#id1068"><span class="problematic" id="id1069">`</span></a>python
def outer_product(a):</p>
<blockquote>
<div><p>return tf.tensordot(a, a, 0)</p>
</div></blockquote>
<p>batch_size = 100
a = tf.ones((batch_size, 32, 32))
c = tf.vectorized_map(outer_product, a)
assert c.shape == (batch_size, 32, 32, 32, 32)
<a href="#id1070"><span class="problematic" id="id1071">``</span></a><a href="#id1072"><span class="problematic" id="id1073">`</span></a></p>
<p><a href="#id1074"><span class="problematic" id="id1075">``</span></a><a href="#id1076"><span class="problematic" id="id1077">`</span></a>python
# Computing per-example gradients</p>
<p>batch_size = 10
num_features = 32
layer = tf.keras.layers.Dense(1)</p>
<dl>
<dt>def model_fn(arg):</dt><dd><dl class="simple">
<dt>with tf.GradientTape() as g:</dt><dd><p>inp, label = arg
inp = tf.expand_dims(inp, 0)
label = tf.expand_dims(label, 0)
prediction = layer(inp)
loss = tf.nn.l2_loss(label - prediction)</p>
</dd>
</dl>
<p>return g.gradient(loss, (layer.kernel, layer.bias))</p>
</dd>
</dl>
<p>inputs = tf.random.uniform([batch_size, num_features])
labels = tf.random.uniform([batch_size, 1])
per_example_gradients = tf.vectorized_map(model_fn, (inputs, labels))
assert per_example_gradients[0].shape == (batch_size, num_features, 1)
assert per_example_gradients[1].shape == (batch_size, 1)
<a href="#id1078"><span class="problematic" id="id1079">``</span></a><a href="#id1080"><span class="problematic" id="id1081">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- The callable to be performed. It accepts one argument, which will have
the same (possibly nested) structure as <cite>elems</cite>, and returns a possibly
nested structure of Tensors and Operations, which may be different than
the structure of <cite>elems</cite>.</p></li>
<li><p><strong>elems</strong> -- A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension. The nested sequence of the
resulting slices will be mapped over by <cite>fn</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor or (possibly nested) sequence of tensors. Each tensor packs the
results of applying fn to tensors unpacked from elems along the first
dimension, from first to last.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.where">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">where</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">condition</span></em>, <em class="sig-param"><span class="n">x</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#where"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.where" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the elements where <cite>condition</cite> is <cite>True</cite> (multiplexing <cite>x</cite> and <cite>y</cite>).</p>
<p>This operator has two modes: in one mode both <cite>x</cite> and <cite>y</cite> are provided, in
another mode neither are provided. <cite>condition</cite> is always expected to be a
<cite>tf.Tensor</cite> of type <cite>bool</cite>.</p>
<p>#### Retrieving indices of <cite>True</cite> elements</p>
<p>If <cite>x</cite> and <cite>y</cite> are not provided (both are None):</p>
<p><cite>tf.where</cite> will return the indices of <cite>condition</cite> that are <cite>True</cite>, in
the form of a 2-D tensor with shape (n, d).
(Where n is the number of matching indices in <cite>condition</cite>,
and d is the number of dimensions in <cite>condition</cite>).</p>
<p>Indices are output in row-major order.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy=</span>
<span class="go">array([[0],</span>
<span class="go">       [3]])&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy=</span>
<span class="go">array([[0, 0],</span>
<span class="go">       [1, 1]])&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">([[[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]]])</span>
<span class="go">&lt;tf.Tensor: shape=(4, 3), dtype=int64, numpy=</span>
<span class="go">array([[0, 0, 0],</span>
<span class="go">       [0, 1, 1],</span>
<span class="go">       [0, 2, 0],</span>
<span class="go">       [0, 2, 1]])&gt;</span>
</pre></div>
</div>
<p>#### Multiplexing between <cite>x</cite> and <cite>y</cite></p>
<p>If <cite>x</cite> and <cite>y</cite> are provided (both have non-None values):</p>
<p><cite>tf.where</cite> will choose an output shape from the shapes of <cite>condition</cite>, <cite>x</cite>,
and <cite>y</cite> that all three shapes are
[broadcastable](<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/ufuncs.html">https://docs.scipy.org/doc/numpy/reference/ufuncs.html</a>) to.</p>
<p>The <cite>condition</cite> tensor acts as a mask that chooses whether the corresponding
element / row in the output should be taken from <cite>x</cite>
(if the elemment in <cite>condition is True) or `y</cite> (if it is false).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">400</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 200, 300,   4],</span>
<span class="go">dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">100</span><span class="p">])</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   4],</span>
<span class="go">dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   4],</span>
<span class="go">dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   1],</span>
<span class="go">dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4],</span>
<span class="go">dtype=int32)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([100, 100, 100, 100],</span>
<span class="go">dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>condition</strong> -- A <cite>tf.Tensor</cite> of type <cite>bool</cite></p></li>
<li><p><strong>x</strong> -- If provided, a Tensor which is of the same type as <cite>y</cite>, and has a shape
broadcastable with <cite>condition</cite> and <cite>y</cite>.</p></li>
<li><p><strong>y</strong> -- If provided, a Tensor which is of the same type as <cite>y</cite>, and has a shape
broadcastable with <cite>condition</cite> and <cite>x</cite>.</p></li>
<li><p><strong>name</strong> -- A name of the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <cite>Tensor</cite> with the same type as <cite>x</cite> and <cite>y</cite>, and shape that</dt><dd><p>is broadcast from <cite>condition</cite>, <cite>x</cite>, and <cite>y</cite>.</p>
</dd>
</dl>
<p>Otherwise, a <cite>Tensor</cite> with shape <cite>(num_true, dim_size(condition))</cite>.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>If <cite>x</cite> and <cite>y</cite> are provided</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> -- When exactly one of <cite>x</cite> or <cite>y</cite> is non-None, or the shapes
    are not all broadcastable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.while_loop">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">while_loop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">cond</span></em>, <em class="sig-param"><span class="n">body</span></em>, <em class="sig-param"><span class="n">loop_vars</span></em>, <em class="sig-param"><span class="n">shape_invariants</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">parallel_iterations</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">back_prop</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">swap_memory</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">maximum_iterations</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/control_flow_ops.html#while_loop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.while_loop" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeat <cite>body</cite> while the condition <cite>cond</cite> is true. (deprecated argument values)</p>
<p>Warning: SOME ARGUMENT VALUES ARE DEPRECATED: <cite>(back_prop=False)</cite>. They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.while_loop(c, b, vars, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))</p>
<p><cite>cond</cite> is a callable returning a boolean scalar tensor. <cite>body</cite> is a callable
returning a (possibly nested) tuple, namedtuple or list of tensors of the same
arity (length and structure) and types as <cite>loop_vars</cite>. <cite>loop_vars</cite> is a
(possibly nested) tuple, namedtuple or list of tensors that is passed to both
<cite>cond</cite> and <cite>body</cite>. <cite>cond</cite> and <cite>body</cite> both take as many arguments as there are
<cite>loop_vars</cite>.</p>
<p>In addition to regular Tensors or IndexedSlices, the body may accept and
return TensorArray objects.  The flows of the TensorArray objects will
be appropriately forwarded between loops and during gradient calculations.</p>
<p>Note that <cite>while_loop</cite> calls <cite>cond</cite> and <cite>body</cite> <em>exactly once</em> (inside the
call to <cite>while_loop</cite>, and not at all during <cite>Session.run()</cite>). <cite>while_loop</cite>
stitches together the graph fragments created during the <cite>cond</cite> and <cite>body</cite>
calls with some additional graph nodes to create the graph flow that
repeats <cite>body</cite> until <cite>cond</cite> returns false.</p>
<p>For correctness, <cite>tf.while_loop()</cite> strictly enforces shape invariants for
the loop variables. A shape invariant is a (possibly partial) shape that
is unchanged across the iterations of the loop. An error will be raised
if the shape of a loop variable after an iteration is determined to be more
general than or incompatible with its shape invariant. For example, a shape
of [11, None] is more general than a shape of [11, 17], and [11, 21] is not
compatible with [11, 17]. By default (if the argument <cite>shape_invariants</cite> is
not specified), it is assumed that the initial shape of each tensor in
<cite>loop_vars</cite> is the same in every iteration. The <cite>shape_invariants</cite> argument
allows the caller to specify a less specific shape invariant for each loop
variable, which is needed if the shape varies between iterations. The
<cite>tf.Tensor.set_shape</cite>
function may also be used in the <cite>body</cite> function to indicate that
the output loop variable has a particular shape. The shape invariant for
SparseTensor and IndexedSlices are treated specially as follows:</p>
<p>a) If a loop variable is a SparseTensor, the shape invariant must be
TensorShape([r]) where r is the rank of the dense tensor represented
by the sparse tensor. It means the shapes of the three tensors of the
SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here
is the shape of the SparseTensor.dense_shape property. It must be the shape of
a vector.</p>
<p>b) If a loop variable is an IndexedSlices, the shape invariant must be
a shape invariant of the values tensor of the IndexedSlices. It means
the shapes of the three tensors of the IndexedSlices are (shape, [shape[0]],
[shape.ndims]).</p>
<p><cite>while_loop</cite> implements non-strict semantics, enabling multiple iterations
to run in parallel. The maximum number of parallel iterations can be
controlled by <cite>parallel_iterations</cite>, which gives users some control over
memory consumption and execution order. For correct programs, <cite>while_loop</cite>
should return the same result for any parallel_iterations &gt; 0.</p>
<p>For training, TensorFlow stores the tensors that are produced in the
forward inference and are needed in back propagation. These tensors are a
main source of memory consumption and often cause OOM errors when training
on GPUs. When the flag swap_memory is true, we swap out these tensors from
GPU to CPU. This for example allows us to train RNN models with very long
sequences and large batches.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cond</strong> -- A callable that represents the termination condition of the loop.</p></li>
<li><p><strong>body</strong> -- A callable that represents the loop body.</p></li>
<li><p><strong>loop_vars</strong> -- A (possibly nested) tuple, namedtuple or list of numpy array,
<cite>Tensor</cite>, and <cite>TensorArray</cite> objects.</p></li>
<li><p><strong>shape_invariants</strong> -- The shape invariants for the loop variables.</p></li>
<li><p><strong>parallel_iterations</strong> -- The number of iterations allowed to run in parallel. It
must be a positive integer.</p></li>
<li><p><strong>back_prop</strong> -- (optional) Deprecated. False disables support for back
propagation. Prefer using <cite>tf.stop_gradient</cite> instead.</p></li>
<li><p><strong>swap_memory</strong> -- Whether GPU-CPU memory swap is enabled for this loop.</p></li>
<li><p><strong>maximum_iterations</strong> -- Optional maximum number of iterations of the while loop
to run.  If provided, the <cite>cond</cite> output is AND-ed with an additional
condition ensuring the number of iterations executed is no greater than
<cite>maximum_iterations</cite>.</p></li>
<li><p><strong>name</strong> -- Optional name prefix for the returned tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>The output tensors for the loop variables after the loop. The return value</dt><dd><p>has the same structure as <cite>loop_vars</cite>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> -- if <cite>cond</cite> or <cite>body</cite> is not callable.</p></li>
<li><p><strong>ValueError</strong> -- if <cite>loop_vars</cite> is empty.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">i</span> <span class="pre">=</span> <span class="pre">tf.constant(0)</span>
<span class="pre">c</span> <span class="pre">=</span> <span class="pre">lambda</span> <span class="pre">i:</span> <span class="pre">tf.less(i,</span> <span class="pre">10)</span>
<span class="pre">b</span> <span class="pre">=</span> <span class="pre">lambda</span> <span class="pre">i:</span> <span class="pre">(tf.add(i,</span> <span class="pre">1),</span> <span class="pre">)</span>
<span class="pre">r</span> <span class="pre">=</span> <span class="pre">tf.while_loop(c,</span> <span class="pre">b,</span> <span class="pre">[i])</span>
<span class="pre">`</span></code></p>
<p>Example with nesting and a namedtuple:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">import</span> <span class="pre">collections</span>
<span class="pre">Pair</span> <span class="pre">=</span> <span class="pre">collections.namedtuple('Pair',</span> <span class="pre">'j,</span> <span class="pre">k')</span>
<span class="pre">ijk_0</span> <span class="pre">=</span> <span class="pre">(tf.constant(0),</span> <span class="pre">Pair(tf.constant(1),</span> <span class="pre">tf.constant(2)))</span>
<span class="pre">c</span> <span class="pre">=</span> <span class="pre">lambda</span> <span class="pre">i,</span> <span class="pre">p:</span> <span class="pre">i</span> <span class="pre">&lt;</span> <span class="pre">10</span>
<span class="pre">b</span> <span class="pre">=</span> <span class="pre">lambda</span> <span class="pre">i,</span> <span class="pre">p:</span> <span class="pre">(i</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">Pair((p.j</span> <span class="pre">+</span> <span class="pre">p.k),</span> <span class="pre">(p.j</span> <span class="pre">-</span> <span class="pre">p.k)))</span>
<span class="pre">ijk_final</span> <span class="pre">=</span> <span class="pre">tf.while_loop(c,</span> <span class="pre">b,</span> <span class="pre">ijk_0)</span>
<span class="pre">`</span></code></p>
<p>Example using shape_invariants:</p>
<p><a href="#id1082"><span class="problematic" id="id1083">``</span></a><a href="#id1084"><span class="problematic" id="id1085">`</span></a>python
i0 = tf.constant(0)
m0 = tf.ones([2, 2])
c = lambda i, m: i &lt; 10
b = lambda i, m: [i+1, tf.concat([m, m], axis=0)]
tf.while_loop(</p>
<blockquote>
<div><p>c, b, loop_vars=[i0, m0],
shape_invariants=[i0.get_shape(), tf.TensorShape([None, 2])])</p>
</div></blockquote>
<p><a href="#id1086"><span class="problematic" id="id1087">``</span></a><a href="#id1088"><span class="problematic" id="id1089">`</span></a></p>
<p>Example which demonstrates non-strict semantics: In the following
example, the final value of the counter <cite>i</cite> does not depend on <cite>x</cite>. So
the <cite>while_loop</cite> can increment the counter parallel to updates of <cite>x</cite>.
However, because the loop counter at one loop iteration depends
on the value at the previous iteration, the loop counter itself cannot
be incremented in parallel. Hence if we just want the final value of the
counter (which we print on the line <cite>print(sess.run(i))</cite>), then
<cite>x</cite> will never be incremented, but the counter will be updated on a
single thread. Conversely, if we want the value of the output (which we
print on the line <cite>print(sess.run(out).shape)</cite>), then the counter may be
incremented on its own thread, while <cite>x</cite> can be incremented in
parallel on a separate thread. In the extreme case, it is conceivable
that the thread incrementing the counter runs until completion before
<cite>x</cite> is incremented even a single time. The only thing that can never
happen is that the thread updating <cite>x</cite> can never get ahead of the
counter thread because the thread incrementing <cite>x</cite> depends on the value
of the counter.</p>
<p><a href="#id1090"><span class="problematic" id="id1091">``</span></a><a href="#id1092"><span class="problematic" id="id1093">`</span></a>python
import tensorflow as tf</p>
<p>n = 10000
x = tf.constant(list(range(n)))
c = lambda i, x: i &lt; n
b = lambda i, x: (tf.compat.v1.Print(i + 1, [i]), tf.compat.v1.Print(x + 1,
[i], &quot;x:&quot;))
i, out = tf.while_loop(c, b, (0, x))
with tf.compat.v1.Session() as sess:</p>
<blockquote>
<div><p>print(sess.run(i))  # prints [0] ... [9999]</p>
<p># The following line may increment the counter and x in parallel.
# The counter thread may get ahead of the other thread, but not the
# other way around. So you may see things like
# [9996] x:[9987]
# meaning that the counter thread is on iteration 9996,
# while the other thread is on iteration 9987
print(sess.run(out).shape)</p>
</div></blockquote>
<p><a href="#id1094"><span class="problematic" id="id1095">``</span></a><a href="#id1096"><span class="problematic" id="id1097">`</span></a></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.zeros">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">zeros</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#zeros"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a tensor with all elements set to zero.</p>
<p>This operation returns a tensor of type <cite>dtype</cite> with shape <cite>shape</cite> and
all elements set to zero.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=</span>
<span class="go">array([[0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> -- A <cite>list</cite> of integers, a <cite>tuple</cite> of integers, or
a 1-D <cite>Tensor</cite> of type <cite>int32</cite>.</p></li>
<li><p><strong>dtype</strong> -- The DType of an element in the resulting <cite>Tensor</cite>.</p></li>
<li><p><strong>name</strong> -- Optional string. A name for the operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> with all elements set to zero.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="tensorflow.zeros_initializer">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">zeros_initializer</code><a class="headerlink" href="#tensorflow.zeros_initializer" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.ops.init_ops_v2.Zeros</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="tensorflow.zeros_like">
<code class="sig-prename descclassname">tensorflow.</code><code class="sig-name descname">zeros_like</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tensorflow/python/ops/array_ops.html#zeros_like"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tensorflow.zeros_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a tensor with all elements set to zero.</p>
<p>See also <cite>tf.zeros</cite>.</p>
<p>Given a single tensor or array-like object (<cite>input</cite>), this operation returns
a tensor of the same type and shape as <cite>input</cite> with all elements set to zero.
Optionally, you can use <cite>dtype</cite> to specify a new type for the returned tensor.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">array([[0, 0, 0],</span>
<span class="go">       [0, 0, 0]], dtype=int32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=</span>
<span class="go">array([[0., 0., 0.],</span>
<span class="go">       [0., 0., 0.]], dtype=float32)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="go">&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=</span>
<span class="go">array([[0, 0, 0],</span>
<span class="go">       [0, 0, 0]], dtype=int32)&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> -- A <cite>Tensor</cite> or array-like object.</p></li>
<li><p><strong>dtype</strong> -- A type for the returned <cite>Tensor</cite>. Must be <cite>float16</cite>, <cite>float32</cite>,
<cite>float64</cite>, <cite>int8</cite>, <cite>uint8</cite>, <cite>int16</cite>, <cite>uint16</cite>, <cite>int32</cite>, <cite>int64</cite>,
<cite>complex64</cite>, <cite>complex128</cite>, <cite>bool</cite> or <cite>string</cite> (optional).</p></li>
<li><p><strong>name</strong> -- A name for the operation (optional).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>Tensor</cite> with all elements set to zero.</p>
</dd>
</dl>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="01_Introduction/index.html" class="btn btn-neutral float-right" title="TensorFlow如何工作" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright - Wei MEI (Nick Cafferry).

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>